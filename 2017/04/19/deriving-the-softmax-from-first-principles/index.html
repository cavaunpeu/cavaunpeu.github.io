<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>will wolf</title>
	<meta name="description" content="machine learning things and thoughts on the world">
	<meta name="author" content="Will Wolf">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Favicon -->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="https://cavaunpeu.github.io/theme/html5.js"></script>
		<![endif]-->

	<!-- Atom Feed -->

	<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@willwolf_">
  <meta name="twitter:creator" content="@willwolf_">
  <meta name="twitter:domain" content="https://cavaunpeu.github.io">
    <meta property="twitter:title" content="Deriving the Softmax from First Principles"/>
    <meta property="twitter:description" content="Deriving the softmax from first conditional probabilistic principles, and how this framework extends naturally to define the softmax regression, conditional random fields, naive Bayes and hidden Markov models."/>
    <meta property="twitter:image" content="https://cavaunpeu.github.io/images/generative_discriminative_models_flowchart.png"/>

	<!-- CSS -->
	<link href="https://cavaunpeu.github.io/theme/css/ipython.css?v={12345}" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.3.7/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/local.css?v={12345}" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/pygments.css?v={12345}" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/main.css?v={12345}" rel="stylesheet">
</head><body>
<div class="container">
<div class="page-header">
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <h1><a id="site-title" href="https://cavaunpeu.github.io/">will wolf</a></h1>
          <h4 id="site-subtitle">machine learning things and thoughts on the world</h4>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav navbar-right">
              <li><a href="https://cavaunpeu.github.io/about/" title="About"><span class="glyphicon glyphicon-user"></span> About</a></li>
              <li><a href="https://cavaunpeu.github.io/cv/" title="CV"><span class="glyphicon glyphicon-folder-open"></span> CV</a></li>
              <li><a href="https://cavaunpeu.github.io/books/" title="Books"><span class="glyphicon glyphicon-book"></span> Books</a></li>
              <li><a href="https://cavaunpeu.github.io/archive/" title="Archive"><span class="glyphicon glyphicon-th-list"></span> Archive</a></li>
          <li class="dropdown">
            <a class="dropdown-toggle" data-toggle="dropdown" href="https://cavaunpeu.github.io" title=English id="activeLanguage"><span class="glyphicon glyphicon-flag"></span>EN<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li>
                  <a id="inactiveLanguage" href="https://cavaunpeu.github.io/geopolitics/" title=English>GEOPOLITICS</a>
                </li>
                <li>
                  <a id="inactiveLanguage" href="https://cavaunpeu.github.io/es/" title=Español>ES</a>
                </li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</div>	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">Deriving the Softmax from First Principles</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Will Wolf</h4>
		</span>
		<time datetime="2017-04-19T17:26:00-04:00" itemprop="datePublished">April 19, 2017</time>
	</div>
	<div itemprop="articleBody" class="article-body"><p>The original goal of this post was to explore the relationship between the softmax and sigmoid functions. In truth, this relationship had always seemed just out of reach: "One has an exponent in the numerator! One has a summation! One has a 1 in the denominator!" And of course, the two have different names.</p>
<p>Once derived, I quickly realized how this relationship backed out into a more general modeling framework motivated by the conditional probability axiom itself. As such, this post first explores how the sigmoid is but a special case of the softmax, and the underpinnings of each in Gibbs distributions, factor products and probabilistic graphical models. Next, we go on to show how this framework extends naturally to define canonical model classes such as the softmax regression, conditional random fields, naive Bayes and hidden Markov models.</p>
<h2>Our goal</h2>
<p>This is a predictive model. It is a diamond that receives an input and produces an output.</p>
<p><img alt="simple input/output model" class="img-responsive" src="https://cavaunpeu.github.io/images/simple_input_output_model.png"/></p>
<p>The input is a vector <span class="math">\(\mathbf{x} = [x_0, x_1, x_2, x_3]\)</span>. There are 3 possible outputs: <span class="math">\(a, b, c\)</span>. The goal of our model is to predict the probability of producing each output conditional on the input, i.e.</p>
<div class="math">$$P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})$$</div>
<p>Of course, a probability is but a real number that lies on the closed interval <span class="math">\([0, 1]\)</span>.</p>
<h2>How does the input affect the output?</h2>
<p>Our input is a list of 4 numbers; each one affects <em>each possible output</em> to a <em>different extent</em>. We'll call this effect a "weight." 4 inputs times 3 outputs equals 12 distinct weights. They might look like this:</p>
<table class="table table-hover table-striped">
<thead>
<tr>
<th></th>
<th><span class="math">\(a\)</span></th>
<th><span class="math">\(b\)</span></th>
<th><span class="math">\(c\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">\(x_0\)</span></td>
<td>.1</td>
<td>.4</td>
<td>.3</td>
</tr>
<tr>
<td><span class="math">\(x_1\)</span></td>
<td>.2</td>
<td>.3</td>
<td>.4</td>
</tr>
<tr>
<td><span class="math">\(x_2\)</span></td>
<td>.3</td>
<td>.2</td>
<td>.1</td>
</tr>
<tr>
<td><span class="math">\(x_3\)</span></td>
<td>.4</td>
<td>.1</td>
<td>.2</td>
</tr>
</tbody>
</table>
<h2>Producing an output</h2>
<p>Given an input <span class="math">\(x = [x_0, x_1, x_2, x_3]\)</span>, our model will use the above weights to produce a number for each output <span class="math">\(a, b, c\)</span>. The effect of each input element will be additive. The reason why will be explained later on.</p>
<div class="math">$$
\tilde{a} = \sum\limits_{i=0}^{3}w_{i, a}x_i\\
\tilde{b} = \sum\limits_{i=0}^{3}w_{i, b}x_i\\
\tilde{c} = \sum\limits_{i=0}^{3}w_{i, c}x_i\\
$$</div>
<p>These sums will dictate what output our model produces. The biggest number wins. For example, given</p>
<div class="math">$$
\{\tilde{a}: 5, \tilde{b}: 7, \tilde{c}: 9\}
$$</div>
<p>our model will have the best chance of producing a <span class="math">\(c\)</span>.</p>
<h2>Converting to probabilities</h2>
<p>We said before that our goal is to obtain the following:</p>
<div class="math">$$P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})$$</div>
<p>The <span class="math">\(\mathbf{x}\)</span> is <strong>bold</strong> so as to represent <em>any</em> input value. Given that we now have a <em>specific</em> input value, namely <span class="math">\(x\)</span>, we can state our goal more precisely:</p>
<div class="math">$$P(a\vert x), P(b\vert x), P(c\vert x)$$</div>
<p>Thus far, we just have <span class="math">\(\{\tilde{a}: 5, \tilde{b}: 7, \tilde{c}: 9\}\)</span>. To convert each value to a probability, i.e. an un-special number in <span class="math">\([0, 1]\)</span>, we just divide by the sum.</p>
<div class="math">$$
P(a\vert x) = \frac{5}{5+7+9} = \frac{5}{21}\\
P(b\vert x) = \frac{7}{5+7+9} = \frac{7}{21}\\
P(c\vert x) = \frac{9}{5+7+9} = \frac{9}{21}\\
$$</div>
<p>Finally, to be a valid probability distribution, all numbers must sum to 1.</p>
<div class="math">$$
\frac{5}{21} + \frac{7}{21} + \frac{9}{21} = 1 \checkmark
$$</div>
<h2>What if our values are negative?</h2>
<p>If one of our initial unnormalized probabilities were negative, i.e. <span class="math">\(\{\tilde{a}: -5, \tilde{b}: 7, \tilde{c}: 9\}\)</span>, this all breaks down.</p>
<div class="math">$$
P(a\vert x) = \frac{-5}{-5+7+9} = \frac{-5}{11}\\
P(b\vert x) = \frac{7}{-5+7+9} = \frac{7}{11}\\
P(c\vert x) = \frac{9}{-5+7+9} = \frac{9}{11}\\
$$</div>
<p><span class="math">\(\frac{-5}{11}\)</span> is not a valid probability as it does not fall in <span class="math">\([0, 1]\)</span>.</p>
<p>To ensure that all unnormalized probabilities are positive, we must first pass them through a function that takes as input a real number and produces as output a strictly positive real number. This is simply an exponent; let's choose <a href="https://en.wikipedia.org/wiki/E_(mathematical_constant)">Euler's number (<span class="math">\(e\)</span>)</a> for now. The rationale for this choice will be explained later on (though do note that any positive exponent would serve our stated purpose).</p>
<div class="math">$$
\begin{align*}
\tilde{a} &amp;= -5 \rightarrow e^{-5}\\
\tilde{b} &amp;= 7 \rightarrow e^{7}\\
\tilde{c} &amp;= 9 \rightarrow e^{9}
\end{align*}
$$</div>
<p>Our <em>normalized</em> probabilities, i.e. valid probabilities, now look as follows:</p>
<div class="math">$$
P(a\vert x) = \frac{e^{-5}}{e^{-5}+e^7+e^9}\\
P(b\vert x) = \frac{e^{7}}{e^{-5}+e^7+e^9}\\
P(c\vert x) = \frac{e^{9}}{e^{-5}+e^7+e^9}
$$</div>
<p>More generally,</p>
<div class="math">$$
P(y\vert x) = \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}\quad \text{for}\ y = a, b, c
$$</div>
<p>This is the softmax function.</p>
<h2>Relationship to the sigmoid</h2>
<p>Whereas the softmax outputs a valid probability distribution over <span class="math">\(n \gt 2\)</span> distinct outputs, the sigmoid does the same for <span class="math">\(n = 2\)</span>. As such, the sigmoid is simply a special case of the softmax. By this definition, and assuming our model only produces two possible outputs <span class="math">\(p\)</span> and <span class="math">\(q\)</span>, we can write the sigmoid for a given input <span class="math">\(x\)</span> as follows:</p>
<div class="math">$$
P(y\vert \mathbf{x}) = \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}\quad \text{for}\ y = p, q
$$</div>
<p>Similar so far. However, notice that we only need to compute probabilities for <span class="math">\(p\)</span>, as <span class="math">\(P(y = q\vert \mathbf{x}) = 1 - P(y = p\vert \mathbf{x})\)</span>. On this note, let's re-expand the expression for <span class="math">\(P(y = p\vert \mathbf{x})\)</span>:</p>
<div class="math">$$
P(y = p\vert \mathbf{x}) = \frac{e^{\tilde{p}}}{e^{\tilde{p}} + e^{\tilde{q}}}
$$</div>
<p>Then, dividing both the numerator and denominator by <span class="math">\(e^{\tilde{p}}\)</span>:</p>
<div class="math">$$
\begin{align*}
P(y = p\vert \mathbf{x})
&amp;= \frac{e^{\tilde{p}}}{e^{\tilde{p}} + e^{\tilde{q}}}\\
&amp;= \frac{ \frac{e^{\tilde{p}}}{e^{\tilde{p}}} }{\frac{e^{\tilde{p}}}{e^{\tilde{p}}} + \frac{e^{\tilde{q}}}{e^{\tilde{p}}}}\\
&amp;= \frac{1}{1 + e^{\tilde{q} - \tilde{p}}}
\end{align*}
$$</div>
<p>Finally, we can plug this back into our original complement:</p>
<div class="math">$$
\frac{1}{1 + e^{\tilde{q} - \tilde{p}}} = 1 - \frac{1}{1 + e^{\tilde{p} - \tilde{q}}}
$$</div>
<p>Our equation is <a href="https://en.wikipedia.org/wiki/Underdetermined_system"><em>underdetermined</em></a> as there are more unknowns (two) than equations (one). As such, our system will have an infinite number of solutions <span class="math">\((\tilde{p},\tilde{q})\)</span>. For this reason, we can simply fix one of these values outright. Let's set <span class="math">\(\tilde{q} = 0\)</span>.</p>
<div class="math">$$
P(y = p\vert \mathbf{x}) = \frac{1}{1 + e^{- \tilde{p}}}
$$</div>
<p>This is the sigmoid function. Lastly,</p>
<div class="math">$$
P(y = q\vert \mathbf{x}) = 1 - P(y = p\vert \mathbf{x})
$$</div>
<h2>Why is the unnormalized probability a summation?</h2>
<p>We all take for granted the semantics of the canonical linear combination <span class="math">\(\sum\limits_{i}w_ix_i\)</span>. But why do we sum in the first place?</p>
<p>To answer this question, we'll first restate our goal: to predict the probability of producing each output <em>conditional</em> on the input, i.e. <span class="math">\(P(Y = y\vert \mathbf{x})\)</span>. Next, we'll revisit the <a href="https://en.wikipedia.org/wiki/Conditional_probability">definition of conditional probability</a> itself:</p>
<div class="math">$$P(B\vert A) = \frac{P(A, B)}{P(A)}$$</div>
<p>Personally, I find this a bit difficult to explain. Let's rearrange to obtain something more intuitive.</p>
<div class="math">$$P(A, B) = P(A)P(B\vert A)$$</div>
<p>This reads:</p>
<blockquote>
<p>The probability of observing (given values of) <span class="math">\(A\)</span> and <span class="math">\(B\)</span> concurrently, ie. the joint probability of <span class="math">\(A\)</span> and <span class="math">\(B\)</span>, is equal to the probability of observing <span class="math">\(A\)</span> times the probability of observing <span class="math">\(B\)</span> given that <span class="math">\(A\)</span> has occurred.</p>
</blockquote>
<p>For example, assume that the probability of birthing a girl is <span class="math">\(.55\)</span>, and the probability of a girl liking math is <span class="math">\(.88\)</span>. Therefore,</p>
<div class="math">$$P(\text{sex} = girl, \text{likes} = math) = .55 * .88 = .484$$</div>
<p>Now, let's rewrite our original model output in terms of the definition above.</p>
<div class="math">$$
\begin{equation}
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}
= \frac{e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}{\sum\limits_{y} e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}
\end{equation}
$$</div>
<p>Remember, we exponentiated each unnormalized probability <span class="math">\(\tilde{y}\)</span> so as to convert it to a strictly positive number. Technically, this number should be called <span class="math">\(\tilde{P}(y, \mathbf{x})\)</span> as it may be <span class="math">\(\gt 1\)</span> and therefore not yet a valid a probability. As such, we need to introduce one more term to our equality chain, given as:</p>
<div class="math">$$
\frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$</div>
<p>This is the arithmetic equivalent of <span class="math">\(\frac{.2}{1} = \frac{3}{15}\)</span>.</p>
<p>In the left term:</p>
<ul>
<li>The numerator is a valid joint probability distribution.</li>
<li>The denominator, "the probability of observing any value of <span class="math">\(\mathbf{x}\)</span>", is 1.</li>
</ul>
<p>In the right term:</p>
<ul>
<li>The numerator is a strictly positive unnormalized probability distribution.</li>
<li>The denominator is some constant that ensures that</li>
</ul>
<div class="math">$$
\frac{\tilde{P}(a, \mathbf{x})}{\text{normalizer}} + \frac{\tilde{P}(b, \mathbf{x})}{\text{normalizer}} + \frac{\tilde{P}(c, \mathbf{x})}{\text{normalizer}}
$$</div>
<p>sums to 1. In fact, this "normalizer" is called a <strong>partition function</strong>; we'll come back to this below.</p>
<p>With this in mind, let's break down the numerator of our softmax equation a bit further.</p>
<div class="math">$$
\begin{align}
\begin{split}
e^{\tilde{y}}
&amp;= e^{\big(\sum\limits_{i}w_ix_i\big)}\\
&amp;= e^{(w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3)}\\
&amp;= e^{(w_0x_0)} e^{(w_1x_1)} e^{(w_2x_2)} e^{(w_3x_3)}\\
&amp;= \tilde{P}(a, \mathbf{x})
\end{split}
\end{align}
$$</div>
<p>Lemma: Given that our output function<sup>1</sup> performs exponentiation <em>so as to obtain a valid conditional probability distribution over possible model outputs</em>, it follows that our input to this function<sup>2</sup> should be a summation of weighted model input elements<sup>3</sup>.</p>
<ol>
<li>The softmax function.</li>
<li>One of <span class="math">\(\tilde{a}, \tilde{b}, \tilde{c}\)</span>.</li>
<li>Model input elements are <span class="math">\([x_0, x_1, x_2, x_3]\)</span>. Weighted model input elements are <span class="math">\(w_0x_0, w_1x_1, w_2x_2, w_3x_3\)</span>.</li>
</ol>
<p>Unfortunately, this only holds if we buy the fact that <span class="math">\(\tilde{P}(a, \mathbf{x}) = \prod\limits_i e^{(w_ix_i)}\)</span> in the first place. Introducing the <a href="https://www.youtube.com/watch?v=kFcjl3A9QuA&amp;t=314s">Gibbs distribution</a>.</p>
<h2>Gibbs distribution</h2>
<p>A Gibbs distribution gives the unnormalized joint probability distribution over a set of outcomes, analogous to the <span class="math">\(e^{\tilde{a}}, e^{\tilde{b}}, e^{\tilde{c}}\)</span> computed above, as:</p>
<div class="math">$$
\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
= \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)\\
\Phi = \{\phi_1(\mathbf{D_1}), ..., \phi_k(\mathbf{D_k})\}
$$</div>
<p>where <span class="math">\(\Phi\)</span> defines a set of <strong>factors.</strong></p>
<h3>Factors</h3>
<p>A factor is a function that:</p>
<ul>
<li>Takes a list of random variables as input. This list is known as the <strong>scope</strong> of the factor.</li>
<li>Returns a value for every unique combination of values that the random variables can take, i.e. for every entry in the cross-product space of its scope.</li>
</ul>
<p>For example, a factor with scope <span class="math">\(\{\mathbf{A, B}\}\)</span> might look like:</p>
<table class="table table-hover table-striped">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th><span class="math">\(\phi\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">\(a^0\)</span></td>
<td><span class="math">\(b^0\)</span></td>
<td><span class="math">\(20\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^0\)</span></td>
<td><span class="math">\(b^1\)</span></td>
<td><span class="math">\(25\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^1\)</span></td>
<td><span class="math">\(b^0\)</span></td>
<td><span class="math">\(15\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^1\)</span></td>
<td><span class="math">\(b^1\)</span></td>
<td><span class="math">\(4\)</span></td>
</tr>
</tbody>
</table>
<h3>Probabilistic graphical models</h3>
<p>Inferring behavior from complex systems amounts (typically) to computing the joint probability distribution over its possible outcomes. For example, imagine we have a business problem in which:</p>
<ul>
<li>The day-of-week (<span class="math">\(\mathbf{A}\)</span>) and the marketing channel (<span class="math">\(\mathbf{B}\)</span>) impact the probability of customer signup (<span class="math">\(\mathbf{C}\)</span>).</li>
<li>Customer signup impacts our annual recurring revenue (<span class="math">\(\mathbf{D}\)</span>) and end-of-year hiring projections (<span class="math">\(\mathbf{E}\)</span>).</li>
<li>Our ARR and hiring projections impact how much cake we will order for the holiday party (<span class="math">\(\mathbf{F}\)</span>).</li>
</ul>
<p>We might draw our system as such:</p>
<p><img alt="simple probabilistic graphical model" class="img-responsive" src="http://i3.buimg.com/afca455be01523af.png"/></p>
<p>Our goal is to compute <span class="math">\(P(A, B, C, D, E, F)\)</span>. In most cases, we'll only have data on small subsets of our system; for example, a controlled experiment we once ran to investigate the relationships between <span class="math">\(A, B, C\)</span>, or a survey asking employees how much cake they like eating at Christmas. It is rare, if wholly unreasonable, to ever have access to the full joint probability distribution for a moderately complex system.</p>
<p>To compute this distribution we break it into pieces. Each piece is a <strong>factor</strong> which details the behavior of some subset of the system. (As one example, a factor might give the number of times you've observed <span class="math">\((\mathbf{A} &gt; \text{3pm}, \mathbf{B} = \text{Facebook}, \mathbf{C} &gt; \text{50 signups})\)</span> in a given day.) To this effect, we say:</p>
<blockquote>
<p>A desired, <em>unnormalized</em> probability distribution <span class="math">\(\tilde{P}\)</span> <em>factorizes over</em> a graph <span class="math">\(G\)</span> if there exists a set of a factors <span class="math">\(\Phi\)</span> such that
<div class="math">$$
\tilde{P} = \tilde{P}_{\Phi} = \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)
$$</div>
where <span class="math">\(\Phi = \{\phi_1(\mathbf{D_1}), ..., \phi_k(\mathbf{D_k})\}\)</span> and <span class="math">\(G\)</span> is the <em>induced graph</em> for <span class="math">\(\Phi\)</span>.</p>
</blockquote>
<p>The first half of this lemma does nothing more than restate the definition of an unnormalized Gibbs distribution. Expanding on the second half, we note:</p>
<blockquote>
<p>The graph induced by a set of factors is a pretty picture in which we draw a circle around each variable in the factor domain superset and draw lines between those that appear concurrently in a given factor domain.</p>
</blockquote>
<p>With two factors <span class="math">\(\phi(\mathbf{A, B}), \phi(\mathbf{B, C})\)</span> the "factor domain superset" is <span class="math">\(\{\mathbf{A, B, C}\}\)</span>. The induced graph would have three circles with lines connecting <span class="math">\(\mathbf{A}\)</span> to <span class="math">\(\mathbf{B}\)</span> and <span class="math">\(\mathbf{B}\)</span> to <span class="math">\(\mathbf{C}\)</span>.</p>
<p>Finally, it follows that:</p>
<ol>
<li>Given a business problem with variables <span class="math">\(\mathbf{A, B, C, D, E, F}\)</span> — we can draw a picture of it.</li>
<li>We can build factors that describe the behavior of subsets of this problem. Realistically, these will only be small subsets.</li>
<li>If the graph induced by our factors looks like the one we drew, we can represent our system as a factor product.</li>
</ol>
<p>Unfortunately, the resulting factor product <span class="math">\(\tilde{P}_{\Phi}\)</span> is still unnormalized just like <span class="math">\(e^{\tilde{a}}, e^{\tilde{b}}, e^{\tilde{c}}\)</span> in our original model.</p>
<h2>Partition function</h2>
<p>The partition function was the denominator, i.e. "normalizer", in our softmax function. It is used to turn an unnormalized probability distribution into a normalized (i.e. valid) probability distribution. A true Gibbs distribution is given as follows:</p>
<div class="math">$$
\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
= \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)\\
P_{\Phi}(\mathbf{X_1, ..., X_n})
= \frac{1}{\mathbf{Z}_{\Phi}}\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
$$</div>
<p>where <span class="math">\(\mathbf{Z}_{\Phi}\)</span> is the partition function.</p>
<p>To compute this function, we simply add up all the values in the unnormalized table. Given <span class="math">\(\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})\)</span> as:</p>
<table class="table table-hover table-striped">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th><span class="math">\(\phi\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">\(a^0\)</span></td>
<td><span class="math">\(b^0\)</span></td>
<td><span class="math">\(20\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^0\)</span></td>
<td><span class="math">\(b^1\)</span></td>
<td><span class="math">\(25\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^1\)</span></td>
<td><span class="math">\(b^0\)</span></td>
<td><span class="math">\(15\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^1\)</span></td>
<td><span class="math">\(b^1\)</span></td>
<td><span class="math">\(4\)</span></td>
</tr>
</tbody>
</table>
<div class="math">$$
\begin{align*}
\mathbf{Z}_{\Phi}
&amp;= 20 + 25 + 15 + 4\\
&amp;= 64
\end{align*}
$$</div>
<p>Our valid probability distribution then becomes:</p>
<table class="table table-hover table-striped">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th><span class="math">\(\phi\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math">\(a^0\)</span></td>
<td><span class="math">\(b^0\)</span></td>
<td><span class="math">\(\frac{20}{64}\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^0\)</span></td>
<td><span class="math">\(b^1\)</span></td>
<td><span class="math">\(\frac{25}{64}\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^1\)</span></td>
<td><span class="math">\(b^0\)</span></td>
<td><span class="math">\(\frac{15}{64}\)</span></td>
</tr>
<tr>
<td><span class="math">\(a^1\)</span></td>
<td><span class="math">\(b^1\)</span></td>
<td><span class="math">\(\frac{4}{64}\)</span></td>
</tr>
</tbody>
</table>
<p>This is our denominator in the softmax function.</p>
<p>*I have not given an example of the actual arithmetic of a factor product (of multiple factors). It's trivial. Google.</p>
<h2>Softmax regression</h2>
<p>Once more, the goal of our model is to predict the probability of producing each output conditional on the given input, i.e.</p>
<div class="math">$$
P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})
$$</div>
<p>In machine learning training data we're given the building block of a <em>joint probability distribution</em>, e.g. a ledger of observed co-occurences of inputs and outputs. We surmise that each input element affects each possible output to a different extent, i.e. we multiply it by a weight. Next, we exponentiate each product <span class="math">\(w_ix_i\)</span>, i.e. factor, then multiply the results (alternatively, we could exponentiate the linear combination of the factors, i.e. features in machine learning parlance): this gives us an unnormalized joint probability distribution over all (input and output) variables.</p>
<p>What we'd like is a valid probability distribution over possible outputs <em>conditional</em> on the input, i.e. <span class="math">\(P(y\vert \mathbf{x})\)</span>. Furthermore, our output is a single, "1-of-k" variable in <span class="math">\(\{a, b, c\}\)</span> (as opposed to a sequence of variables). This is the definition, almost verbatim, of softmax regression.</p>
<p>*Softmax regression is also known as multinomial regression, or multi-class logistic regression. Binary logistic regression is a special case of softmax regression in the same way that the sigmoid is a special case of the softmax.</p>
<p>To compute our conditional probability distribution, we'll revisit Equation (1):</p>
<div class="math">$$
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}
= \frac{e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}{\sum\limits_{y} e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$</div>
<p>In other words, the probability of producing each output conditional on the input is equivalent to:</p>
<ol>
<li>The softmax function.</li>
<li>An exponentiated factor product of input elements normalized by a partition function.</li>
</ol>
<p>It almost speaks for itself.</p>
<h3>Our partition function depends on <span class="math">\(\mathbf{x}\)</span></h3>
<p>In order to compute a distribution over <span class="math">\(y\)</span> <em>conditional</em> on <span class="math">\(\mathbf{x}\)</span>, our partition function becomes <span class="math">\(x\)</span>-dependent. In other words, for a given input <span class="math">\(x = [x_0, x_1, x_2, x_3]\)</span>, our model computes the conditional probabilities <span class="math">\(P(y\vert x)\)</span>. While this may seem like a trivial if pedantic restatement of what the softmax function does, it is important to note that our model is effectively computing a <em>family</em> of conditional distributions — one for each unique input <span class="math">\(x\)</span>.</p>
<h2>Conditional random field</h2>
<p>Framing our model in this way allows us to extend naturally into other classes of problems. Imagine we are trying to assign a label to each individual word in a given conversation, where possible labels include: <code>neutral</code>, <code>offering an olive branch</code>, and <code>them is fighting words</code>. Our problem now differs from our original model in one key way, and another possibly-key way:</p>
<ol>
<li>Our outcome is now a <em>sequence of labels</em>. It is no longer "1-of-k." A possible sequence of labels in the conversation "hey there jerk shit roar" might be: <code>neutral</code>, <code>neutral</code>, <code>them is fighting words</code>, <code>them is fighting words</code>, <code>them is fighting words</code>.</li>
<li>There <em>might be</em> relationships between the words that would influence the final output label sequence. For example, for each individual word, was the person both cocking their fists while enunciating that word, the one previous <em>and</em> the one previous? In other words, we build factors (i.e. features) that speak to the spatial relationships between our input elements. We do this because we think these relationships might influence the final output (when we say our model "assumes dependencies between features," this is what we mean).</li>
</ol>
<p>The conditional random field output function is a softmax just the same. In other words, if we build a softmax regression for our conversation-classification task where:</p>
<ol>
<li>Our output is a sequence of labels</li>
<li>Our features are a bunch of (spatially-inspired) interaction features, a la <code>sklearn.preprocessing.PolynomialFeatures</code></li>
</ol>
<p>we've essentially just built a conditional random field.</p>
<p>Of course, modeling the full distribution of outputs conditional on the input, where our output is again a sequence of labels, incurs combinatorial explosion really quick (for example, a 5-word speech would already have <span class="math">\(3^5 = 243\)</span> possible outputs). For this we use some dynamic-programming-magic to ensure that we compute <span class="math">\(P(y\vert x)\)</span> in a reasonable amount of time. I won't cover this topic here.</p>
<h2>Naive Bayes</h2>
<p>Naive Bayes is identical to softmax regression with one key difference: instead of modeling the conditional distribution <span class="math">\(P(y\vert \mathbf{x})\)</span> we model the joint distribution <span class="math">\(P(y, \mathbf{x})\)</span>, given as:</p>
<div class="math">$$
P(y, \mathbf{x}) = P(y)\prod\limits_{i=1}^{K}P(x_i\vert y)
$$</div>
<p>In effect, this model gives a (normalized) Gibbs distribution outright where the factors are <em>already valid probabilities</em> expressing the relationship between each input element and the output.</p>
<h3>The distribution of our data</h3>
<p>Crucially, neither Naive Bayes nor softmax regression make any assumptions about the distribution of the data, <span class="math">\(P(\mathbf{x})\)</span>. (Were this not the case, we'd have to state information like "I think the probability of observing the <em>input</em> <span class="math">\(x = [x_0 = .12, x_1 = .34, x_2 = .56, x_3 = .78]\)</span> is <span class="math">\(.00047\)</span>," which would imply in the most trivial sense of the word that we are making <em>assumptions</em> about the distribution of our data.)</p>
<p>In softmax regression, our model looks as follows:</p>
<div class="math">$$
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$</div>
<p><em>While the second term is equal to the third, we never actually have to compute its denominator in order to obtain the first.</em></p>
<p>In Naive Bayes, we simply assume that the probability of observing each input element <span class="math">\(x_i\)</span> depends on <span class="math">\(y\)</span> and nothing else, evidenced by its functional form. As such, <span class="math">\(P(\mathbf{x})\)</span> is not required.</p>
<h2>Hidden Markov models and beyond</h2>
<p>Finally, hidden Markov models are to naive Bayes what conditional random fields are to softmax regression: the former in each pair builds upon the latter by modeling a <em>sequence</em> of labels. This graphic<sup id="fnref-1"><a class="footnote-ref" href="#fn-1">1</a></sup> gives a bit more insight into these relationships:</p>
<p><img alt="generative vs. discriminative models" class="img-responsive" src="https://cavaunpeu.github.io/images/generative_discriminative_models_flowchart.png"/></p>
<h2>Where does <span class="math">\(e\)</span> come from?</h2>
<p>Equation (2) states that the numerator of the softmax, i.e. the exponentiated linear combination of input elements, is equivalent to the unnormalized joint probability of our inputs and outputs as given by the Gibbs distribution factor product.</p>
<p>However, this only holds if one of the following two are true:</p>
<ol>
<li>Our factors are of the form <span class="math">\(e^{z}\)</span>.</li>
<li>Our factors take any arbitrary form, and we "anticipate" that this form will be exponentiated within the softmax function.</li>
</ol>
<p>Remember, the point of this exponentiation was to put our weighted input elements "on the arithmetic path to becoming valid probabilities," i.e. to make them strictly positive. This said, there is nothing (to my knowledge) that mandates that a factor produce a strictly positive number. So which came first — the chicken or the egg (the exponent or the softmax)?</p>
<p>In truth, I'm not actually sure, but I do believe we can safely treat the softmax numerator and an unnormalized Gibbs distribution as equivalent and simply settle on: <em>call it what you will, we need an exponent in there somewhere to put this thing in <span class="math">\([0, 1]\)</span>.</em></p>
<h2>Summary</h2>
<p>This exercise has made the relationships between canonical machine learning models, activation functions and the basic axiom of conditional probability a whole lot clearer. For more information, please reference the resources below — especially Daphne Koller's material on <a href="https://www.coursera.org/learn/probabilistic-graphical-models">probabilistic graphical models</a>. Thanks so much for reading.</p>
<hr/>
<p>References:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=PGBlyKtfB74&amp;index=19&amp;list=P6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Conditional random fields - linear chain CRF</a></li>
<li><a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a></li>
<li><a href="https://arxiv.org/pdf/1011.4088v1.pdf">An Introduction to Conditional Random Fields</a></li>
<li><a href="https://www.youtube.com/watch?v=kFcjl3A9QuA&amp;t=559s">General Gibbs Distribution - Professor Daphne Koller</a></li>
<li><a href="https://www.youtube.com/watch?v=2BXoj778YU8&amp;t=636s">Conditional Random Fields - Professor Daphne Koller</a></li>
</ul>
<div class="footnote">
<hr/>
<ol>
<li id="fn-1">
<p><a href="https://arxiv.org/pdf/1011.4088v1.pdf">An Introduction to Conditional Random Fields</a> <a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="willwolf_">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'willwolf';
    var disqus_title = 'Deriving the Softmax from First Principles';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</div> </div>
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1" id="footer-wrapper">
				<div id="social-links">
					<h4>
						Social:
						<a href="http://twitter.com/willwolf_"><i class="fa fa-twitter" aria-hidden="true"></i></a>
						<a href="http://github.com/cavaunpeu"><i class="fa fa-github" aria-hidden="true"></i></a>
						<a href="http://linkedin.com/in/williamabrwolf"><i class="fa fa-linkedin-square" aria-hidden="true"></i></a>
						<a href="mailto:williamabrwolf@gmail.com"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>
					</h4>
				</div>
				<div id="travel-blog">
					<h4>
						Links:
						<a href="http://willtravellife.com">Travel Blog</a>, <a href="https://github.com/cavaunpeu/willwolf.io-source">Source Code</a>
					</h4>
				</div>
			</div>
		</div>
	</div>
</footer><div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Will Wolf 2017</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div><!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-97412095-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>