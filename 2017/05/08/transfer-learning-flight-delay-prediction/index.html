<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>will wolf</title>
	<meta name="description" content="writings on machine learning, crypto, geopolitics, life">
	<meta name="author" content="Will Wolf">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Favicon -->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="https://willwolf.io/theme/html5.js"></script>
		<![endif]-->

	<!-- Atom Feed -->

	<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@willwolf_">
  <meta name="twitter:creator" content="@willwolf_">
  <meta name="twitter:domain" content="https://willwolf.io">
    <meta property="twitter:title" content="Transfer Learning for Flight Delay Prediction via Variational Autoencoders"/>
    <meta property="twitter:description" content="Autoencoding airports via variational autoencoders to improve flight delay prediction. Additionally, a principled look at variational inference itself and its connections to machine learning."/>
    <meta property="twitter:image" content="https://willwolf.io/images/vae_map.png"/>

	<!-- CSS -->
	<link href="https://willwolf.io/theme/css/ipython.css?v={12345}" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
	<link href='https://fonts.googleapis.com/css?family=Berkshire Swash' rel='stylesheet' type='text/css'>
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.3.7/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/local.css?v={12345}" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/pygments.css?v={12345}" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/main.css?v={12345}" rel="stylesheet">
</head><body>
<div class="container">
<div class="page-header">
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <h1><a id="site-title" href="https://willwolf.io/">will wolf</a></h1>
            <h4 id="site-subtitle-with-links">writings on <a id="sitesubtitle-machine-learning" href="/machine-learning">machine learning</a>, <a id="sitesubtitle-crypto" href="/crypto">crypto</a>, <a id="sitesubtitle-geopolitics" href="/geopolitics">geopolitics</a>, <a id="sitesubtitle-life" href="/life">life</a></h4>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav navbar-right">
              <li><a href="https://willwolf.io/about/" title="About">About</a></li>
              <li><a href="https://willwolf.io/images/resume.pdf" title="Résumé"></span> Résumé</a></li>
              <li><a href="https://willwolf.io/books/" title="Books">Books</a></li>
          <li>
            <button id="subscribeButton">
              <a href="https://willwolf.io/subscribe/" title="Get new posts by email">Subscribe</a>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</div>	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">Transfer Learning for Flight Delay Prediction via Variational Autoencoders</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Will Wolf</h4>
		</span>
		<time datetime="2017-05-08T12:45:00-04:00" itemprop="datePublished">May 8, 2017</time>
	</div>
	<div itemprop="articleBody" class="article-body"><p>In this work, we explore improving a vanilla regression model with knowledge learned elsewhere. As a motivating example, consider the task of predicting the number of checkins a given user will make at a given location. Our training data consist of checkins from 4 users across 4 locations in the week of May 1st, 2017 and looks as follows:</p>
<table class="table table-striped">
<thead>
<tr>
<th>user_id</th>
<th>location</th>
<th>checkins</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>a</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>b</td>
<td>6</td>
</tr>
<tr>
<td>2</td>
<td>c</td>
<td>7</td>
</tr>
<tr>
<td>2</td>
<td>d</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>a</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>c</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>b</td>
<td>9</td>
</tr>
<tr>
<td>4</td>
<td>d</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>We'd like to predict how many checkins user 3 will make at location <code>b</code> in the coming week. How well will our model do?</p>
<p>While each <code>user_id</code> might represent some unique behavior - e.g. user <code>3</code> sleeps late yet likes going out for dinner - and each location might represent its basic characteristics - e.g. location <code>b</code> is an open-late sushi bar - this is currently unbeknownst to our model. To this end, gathering this metadata and joining it to our training set is a clear option. If quality, thorough, explicit metadata are available, affordable and practical to acquire, this is likely the path to pursue. If not, we'll need to explore a more creative approach. How far can we get with <em>implicit</em> metadata learned from an external task?</p>
<h2>Transfer learning</h2>
<p>Transfer learning allows us to use knowledge acquired in one task to improve performance in another. Suppose, for example, that we've been tasked with translating Portuguese to English and are given a basic phrasebook from which to learn. After a week, we take a lengthy test. A friend of ours - a fluent Spanish speaker who knows nothing of Portuguese - is tasked the same. Who gets a better score?</p>
<h2>Predicting flight delays</h2>
<p>The goal of this work is to predict flight delays - a basic regression task. The data comprise 6,872,294 flights from 2008 via the <a href="https://www.transportation.gov/">United States Department of Transportation's</a> <a href="https://www.bts.gov/">Bureau of Transportation Statistics</a>. I downloaded them from <a href="http://stat-computing.org/dataexpo/2009/the-data.html">stat-computing.org</a>.</p>
<p>Each row consists of, among other things: <code>DayOfWeek</code>, <code>DayofMonth</code>, <code>Month</code>, <code>ScheduledDepTimestamp</code> (munged from <code>CRSDepTime</code>), <code>Origin</code>, <code>Dest</code> and <code>UniqueCarrier</code> (airline), and well as <code>CarrierDelay</code>, <code>WeatherDelay</code>, <code>NASDelay</code>, <code>SecurityDelay</code>, <code>LateAircraftDelay</code> - all in minutes - which we will sum to create <code>total_delay</code>. We'll consider a random sample of 50,000 flights to make things easier. (For a more in-depth exploration of these data, please see this project's <a href="http://github.com/cavaunpeu/transfer-learning-for-flight-prediction/explore.R">repository</a>.)</p>
<h2>Routes, airports</h2>
<p>While we can expect <code>DayOfWeek</code>, <code>DayofMonth</code> and <code>Month</code> to give some seasonal delay trends - delays are likely higher on Sundays or Christmas, for example - the <code>Origin</code> and <code>Dest</code> columns might suffer from the same pathology as <code>user_id</code> and <code>location</code> above: a rich behavioral indicator represented in a crude, "isolated" way. (A token in a bag-of-words model, as opposed to its respective word2vec representation, gives a clear analogy.) How can we infuse this behavioral knowledge into our original task?</p>
<h2>An auxiliary task</h2>
<p>In 2015, I read a particularly-memorable blog post entitled <a href="http://allentran.github.io/graph2vec">Towards Anything2Vec</a> by Allen Tran. Therein, Allen states:</p>
<blockquote>
<p>Like pretty much everyone, I'm obsessed with word embeddings word2vec or GloVe. Although most of machine learning in general is based on turning things into vectors, it got me thinking that we should probably be learning more fundamental representations for objects, rather than hand tuning features. Here is my attempt at turning random things into vectors, starting with graphs.</p>
</blockquote>
<p>In this post, Allen seeks to embed nodes - U.S. patents, incidentally - in a directed graph into vector space by predicting the inverse of the path-length to nodes nearby. To me, this (thus-far) epitomizes the "data describe the individual better than they describe themself:" while we could ask the nodes to self-classify into patents on "computing," "pharma," "materials," etc., the connections between these nodes - formal citations, incidentally - will capture their "true" subject matters (and similarities therein) better than the authors ever could. Formal language, necessarily, generalizes.</p>
<p><a href="https://openflights.org/data.html">OpenFlights</a> contains data for over "10,000 airports, train stations and ferry terminals spanning the globe" and the routes between. My goal is to train a neural network that, given an origin airport and its latitude and longitude, predicts the destination airport, latitude and longitude. This network will thereby "encode" each airport into a vector of arbitrary size containing rich information about, presumably, the diversity and geography of the destinations it services: its "place" in the global air network. Surely, a global hub like Heathrow - a fact presumably known to our neural network, yet unknown to our initial dataset with one-hot airport indices - has longer delays on Christmas than than a two-plane airstrip in Alaska.</p>
<p>Crucially, we note that while our original (down-sampled) dataset contains delays amongst 298 unique airports, our auxiliary <code>routes</code> dataset comprises flights amongst 3186 unique airports. Notwithstanding, information about <em>all</em> airports in the latter is <em>distilled</em> into vector representations then injected into the former; even though we might not know about delays to/from Casablanca Mohammed V Airport (CMN), latent information about this airport will still be <em>intrinsically considered</em> when predicting delays between other airports to/from which CMN flies.</p>
<h2>Data preparation</h2>
<p>Our flight-delay design matrix <span class="math">\(X\)</span> will include the following columns: <code>DayOfWeek</code>, <code>DayofMonth</code>, <code>Month</code>, <code>ScheduledDepTimestamp</code>, <code>Origin</code>, <code>Dest</code> and <code>UniqueCarrier</code>. All columns will be one-hotted for simplicity. (Alternatively, I explored mapping each column to its respective <code>value_counts()</code>, i.e. <code>X.loc[:, col] = X[col].map(col_val_counts)</code>, which led to less agreeable convergence.)</p>
<p>Let's get started.</p>
<div class="highlight"><pre><span></span><span class="n">TEST_SIZE</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="o">.</span><span class="mi">4</span><span class="p">)</span>


<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">TEST_SIZE</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">TEST_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Dataset sizes:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Train:      </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Validation: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Test:       </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">Dataset sizes:</span>
<span class="err">    Train:      (30000, 657)</span>
<span class="err">    Validation: (10000, 657)</span>
<span class="err">    Test:       (10000, 657)</span>
</pre></div>
<h2>Flight-delay models</h2>
<p>Let's build two baseline models with the data we have. Both models have a single ReLU output and are trained to minimize the mean squared error of the predicted delay via stochastic gradient descent.</p>
<p>ReLU was chosen as an output activation because delays are both bounded below at 0 and bi-modal. I considered three separate strategies for predicting this distribution.</p>
<ol>
<li>Train a network with two outputs: <code>total_delay</code> and <code>total_delay == 0</code> (Boolean). Optimize this network with a composite loss function: mean squared error and binary cross-entropy, respectively.</li>
<li>Train a "poor-man's" hierarchical model: a logistic regression to predict <code>total_delay == 0</code> and a standard regression to predict <code>total_delay</code>. Then, compute the final prediction as a thresholded ternary, e.g. <code>y_pred = np.where(y_pred_lr &gt; threshhold, 0, y_pred_reg)</code>. Train the regression model with both all observations, and just those where <code>total_delay &gt; 0</code>, and see which works best.</li>
<li>Train a single network with a ReLU activation. This gives a reasonably elegant way to clip our outputs below at 0, and mean-squared-error still tries to place our observations into the correct mode (of the bimodal output distribution; this said, mean-squared-error may try to "play it safe" and predict between the modes).</li>
</ol>
<p>I chose Option #3 because it performed best in brief experimentation and was the simplest to both fit and explain.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseEmbeddingModel</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>


<span class="k">class</span> <span class="nc">SimpleRegression</span><span class="p">(</span><span class="n">BaseEmbeddingModel</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">λ</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sd">'''Initializes the model parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dim : The number of columns in our design matrix.</span>
<span class="sd">            λ : The regularization strength to apply to the model's</span>
<span class="sd">                dense layers.</span>
<span class="sd">        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">λ</span> <span class="o">=</span> <span class="n">λ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">144</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'regression_output'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DeeperRegression</span><span class="p">(</span><span class="n">BaseEmbeddingModel</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">λ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sd">'''Initializes the model parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dim : The number of columns in our design matrix.</span>
<span class="sd">            λ : The regularization strength to apply to the model's</span>
<span class="sd">                dense layers.</span>
<span class="sd">            dropout_p : The percentage of units to drop in the model's</span>
<span class="sd">                dropout layer.</span>
<span class="sd">        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">λ</span> <span class="o">=</span> <span class="n">λ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">dropout_p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'input'</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">144</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">144</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout_p</span><span class="p">)(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'regression_output'</span><span class="p">)(</span><span class="n">dense</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
<h2>Simple regression</h2>
<div class="highlight"><pre><span></span><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="o">.</span><span class="mi">0001</span>


<span class="n">simple_reg</span> <span class="o">=</span> <span class="n">SimpleRegression</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">λ</span><span class="o">=.</span><span class="mi">05</span><span class="p">)</span>
<span class="n">simple_reg</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'mean_squared_error'</span><span class="p">)</span>

<span class="n">simple_reg_fit</span> <span class="o">=</span> <span class="n">fit_flight_model</span><span class="p">(</span><span class="n">simple_reg</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plot_model_fit</span><span class="p">(</span><span class="n">simple_reg_fit</span><span class="p">)</span>
</pre></div>
<p><img alt="simple regression initial" class="img-fluid" src="https://willwolf.io/figures/transfer_learning_simple_regression_initial.png"/></p>
<h2>Deeper regression</h2>
<div class="highlight"><pre><span></span><span class="n">deeper_reg</span> <span class="o">=</span> <span class="n">DeeperRegression</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">λ</span><span class="o">=.</span><span class="mi">03</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">deeper_reg</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=.</span><span class="mi">0001</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'mean_squared_error'</span><span class="p">)</span>

<span class="n">deeper_reg_fit</span> <span class="o">=</span> <span class="n">fit_flight_model</span><span class="p">(</span><span class="n">deeper_reg</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plot_model_fit</span><span class="p">(</span><span class="n">deeper_reg_fit</span><span class="p">)</span>
</pre></div>
<p><img alt="deeper regression initial" class="img-fluid" src="https://willwolf.io/figures/transfer_learning_deeper_regression_initial.png"/></p>
<h2>Test set predictions</h2>
<div class="highlight"><pre><span></span><span class="n">y_pred_simple</span> <span class="o">=</span> <span class="n">simple_reg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">y_pred_deeper</span> <span class="o">=</span> <span class="n">deeper_reg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">mse_simple</span> <span class="o">=</span> <span class="n">mean_squared_error_scikit</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_simple</span><span class="p">)</span>
<span class="n">mse_deeper</span> <span class="o">=</span> <span class="n">mean_squared_error_scikit</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_deeper</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean squared error, simple regression: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse_simple</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean squared error, deeper regression: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse_deeper</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">Mean squared error, simple regression: 2.331459019628268</span>
<span class="err">Mean squared error, deeper regression: 2.3186310632259204</span>
</pre></div>
<h2>Learning airport embeddings</h2>
<p>We propose two networks through which to learn airport embeddings: a dot product siamese network, and a <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">variational autoencoder</a>.</p>
<h2>Dot product siamese network</h2>
<p>This network takes as input origin and destination IDs, latitudes and longitudes. It gives as output a binary value indicating whether or not a flight-route between these airports exists. The <code>airports</code> DataFrame gives the geographic metadata. The <code>routes</code> DataFrame gives <em>positive</em> training examples for our network. To build negative samples, we employ, delightfully, "negative sampling."</p>
<h3>Negative sampling</h3>
<p><code>routes</code> gives exlusively <code>(origin, dest, exists = 1)</code> triplets. To create triplets where <code>exists = 0</code>, we simply build them ourself: <code>(origin, fake_dest, exists = 0)</code>. It's that simple.</p>
<p>Inspired by <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">word2vec's approach</a> to an almost identical problem, I pick <code>fake_dest</code>'s based on the frequency with which they occur in the dataset - more frequent samples being more likely to be selected - via:</p>
<div class="math">$$P(a_i) = \frac{  {f(a_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(a_j)}^{3/4} \right) }$$</div>
<p>where <span class="math">\(a_i\)</span> is an airport. To choose a <code>fake_dest</code> for a given <code>origin</code>, we first remove all of the real <code>dest</code>'s, re-normalize <span class="math">\(P(a)\)</span>, then take a multinomial draw.</p>
<p>For a more complete yet equally approachable explanation, please see <a href="https://arxiv.org/pdf/1402.3722.pdf">Goldberg and Levy</a>. For an <em>extremely thorough</em> review of related methods, see Sebastian Ruder's <a href="http://sebastianruder.com/word-embeddings-softmax/">On word embeddings - Part 2: Approximating the Softmax</a>.</p>
<h2>Variational autoencoder</h2>
<h3>Discriminative models</h3>
<p>The previous network is a <em>discriminative</em> model: given two inputs <code>origin</code> and <code>dest</code>, it outputs the conditional probability that <code>exists = 1</code>. While discriminative models are effective in distinguishing <em>between</em> output classes, they don't offer an idea of what data look like within each class itself. To see why, let's restate Bayes' rule for a given input <span class="math">\(x\)</span>:</p>
<div class="math">$$P(Y\vert x) = \frac{P(x\vert Y)P(Y)}{P(x)} = \frac{P(x, Y)}{P(x)}$$</div>
<p><em><strong>Discriminative classifiers jump directly to estimating <span class="math">\(P(Y\vert x)\)</span> without modeling its component parts <span class="math">\(P(x, Y)\)</span> and <span class="math">\(P(x)\)</span>.</strong></em></p>
<p>Instead, as the intermediate step, they simply compute an <em>unnormalized</em> joint distribution <span class="math">\(\tilde{P}(x, Y)\)</span> and a normalizing "partition function." The following then gives the model's predictions for the same reason that <span class="math">\(\frac{.2}{1} = \frac{3}{15}\)</span>:</p>
<div class="math">$$P(Y\vert x) = \frac{P(x, Y)}{P(x)} = \frac{\tilde{P}(x, Y)}{\text{partition function}}$$</div>
<p>This is explained much more thoroughly in a previous blog post: <a href="https://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/">Deriving the Softmax from First Principles</a>.</p>
<h3>Generative models</h3>
<p>Conversely, a variational autoencoder is a <em>generative</em> model: instead of jumping <em>directly</em> to the conditional probability of all possible outputs given a specific input, they first compute the true component parts: the joint probability distribution over data and inputs alike, <span class="math">\(P(X, Y)\)</span>, and the distribution over our data, <span class="math">\(P(X)\)</span>.</p>
<p>The joint probability can be rewritten as <span class="math">\(P(X, Y) = P(Y)P(X\vert Y)\)</span>: as such, generative models tell us the distribution over classes in our dataset, as well as the distribution of inputs within each class. Suppose we are trying to predict t-shirt colors with a 3-feature input; generative models would tell us: "30% of your t-shirts are green - typically produced by inputs near <code>x = [1, 2, 3]</code>; 40% are red - typically produced by inputs near <code>x = [10, 20, 30]</code>; 30% are blue - typically produced by inputs near <code>x = [100, 200, 300]</code>. This is in contrast to a discriminative model which would simply compute: given an input <span class="math">\(x\)</span>, your output probabilities are: <span class="math">\(\{\text{red}: .2, \text{green}: .3, \text{blue}: .5\}\)</span>.</p>
<p><em><strong>To generate new data with a generative model, we draw from <span class="math">\(P(Y)\)</span>, then <span class="math">\(P(X\vert Y)\)</span>. To make predictions, we solicit <span class="math">\(P(Y), P(x\vert Y)\)</span> and <span class="math">\(P(x)\)</span> and employ Bayes' rule outright.</strong></em></p>
<h3>Manifold assumption</h3>
<p>The goal of both autoencoders is to discover underlying "structure" in our data: while each airport can be one-hot encoded into a 3186-dimensional vector, we wish to learn a, or even the, reduced space in which our data both live and vary. This concept is well understood through the "manifold assumption," explained succinctly in this <a href="https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning">CrossValidated thread</a>:</p>
<blockquote>
<p>Imagine that you have a bunch of seeds fastened on a glass plate, which is resting horizontally on a table. Because of the way we typically think about space, it would be safe to say that these seeds live in a two-dimensional space, more or less, because each seed can be identified by the two numbers that give that seed's coordinates on the surface of the glass.</p>
<p>Now imagine that you take the plate and tilt it diagonally upwards, so that the surface of the glass is no longer horizontal with respect to the ground. Now, if you wanted to locate one of the seeds, you have a couple of options. If you decide to ignore the glass, then each seed would appear to be floating in the three-dimensional space above the table, and so you'd need to describe each seed's location using three numbers, one for each spatial direction. But just by tilting the glass, you haven't changed the fact that the seeds still live on a two-dimensional surface. So you could describe how the surface of the glass lies in three-dimensional space, and then you could describe the locations of the seeds on the glass using your original two dimensions.</p>
<p>In this thought experiment, the glass surface is akin to a low-dimensional manifold that exists in a higher-dimensional space : no matter how you rotate the plate in three dimensions, the seeds still live along the surface of a two-dimensional plane.</p>
</blockquote>
<p>In other words, the full spectrum of that which characterizes an airport can be described by just a few numbers. Varying one of these numbers - making it larger or smaller - would result in an airport of slightly different "character;" if one dimension were to represent "global travel hub"-ness, a value of <span class="math">\(-1000\)</span> along this dimension might give us that hangar in Alaska.</p>
<p><em><strong>In the context of autoencoders (and dimensionality reduction algorithms), "learning 'structure' in our data" means nothing more than finding that ceramic plate amidst a galaxy of stars</strong></em>.</p>
<h3>Graphical models</h3>
<p>Variational autoencoders do not have the same notion of an "output" - namely, "does a route between two airports exist?" - as our dot product siamese network. To detail this model, we'll start near first principles with probabilistic graphical models with our notion of the ceramic plate in mind:</p>
<p><img alt="VAE pgm" class="img-fluid" src="https://willwolf.io/images/vae_pgm.png"/></p>
<p>Coordinates on the plate detail airport character; choosing coordinates - say, <code>[global_hub_ness = 500, is_in_asia = 500]</code> - allows us to <em>generate</em> an airport. In this case, it might be Seoul. In variational autoencoders, ceramic-plate coordinates are called the "latent vector," denoted <span class="math">\(z\)</span>. The joint probability of our graphical model is given as:</p>
<div class="math">$$P(z)P(x\vert z) = P(z, x)$$</div>
<p>Our goal is to infer the priors that likely generated these data via Bayes' rule:</p>
<div class="math">$$P(z\vert x) = \frac{P(z)P(x\vert z)}{P(x)}$$</div>
<p>The denominator is called the <strong>evidence</strong>; we obtain it by marginalizing the joint distribution over the latent variables:</p>
<div class="math">$$P(x) = \int P(x\vert z)P(z)dz$$</div>
<p>Unfortunately, this asks us to consider <em>all possible configurations</em> of the latent vector <span class="math">\(z\)</span>. Should <span class="math">\(z\)</span> exist on the vertices of a cube in <span class="math">\(\mathbb{R}^3\)</span>, this would not be very difficult; should <span class="math">\(z\)</span> be a continuous-valued vector in <span class="math">\(\mathbb{R}^{10}\)</span>, this becomes a whole lot harder. Computing <span class="math">\(P(x)\)</span> is problematic.</p>
<h3>Variational inference</h3>
<p>In fact, we could attempt to use MCMC to compute <span class="math">\(P(z\vert x)\)</span>; however, this is slow to converge. Instead, let's compute an <em>approximation</em> to this distribution then try to make it closely resemble the (intractable) original. In this vein, we introduce <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">variational inference</a>, which "allows us to re-write statistical inference problems (i.e. infer the value of a random variable given the value of another random variable) as optimization problems (i.e. find the parameter values that minimize some objective function)."<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p>Let's choose our approximating distribution as simple, parametric and one we know well: the Normal (Gaussian) distribution. Were we able to compute <span class="math">\(P(z\vert x) = \frac{P(x, z)}{P(x)}\)</span>, it is <em>intrinsic</em> that <span class="math">\(z\)</span> is contingent on <span class="math">\(x\)</span>; when building our own distribution to approximate <span class="math">\(P(z\vert x)\)</span>, we need to be <em>explicit</em> about this contingency: different values for <span class="math">\(x\)</span> should be assumed to have been generated by different values of <span class="math">\(z\)</span>. Let's write our approximation as follows, where <span class="math">\(\lambda\)</span> parameterizes the Gaussian for a given <span class="math">\(x\)</span>:</p>
<div class="math">$$q_{\lambda}(z\vert x)$$</div>
<p>Finally, as stated previously, we want to make this approximation closely resemble the original; the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> quantifies their difference:</p>
<div class="math">$$KL(q_{\lambda}(z\vert x)\Vert P(z\vert x)) = \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)}{P(z\vert x)}dz}$$</div>
<p>Our goal is to obtain the argmin with respect to <span class="math">\(\lambda\)</span>:</p>
<div class="math">$$q_{\lambda}^{*}(z\vert x) = \underset{\lambda}{\arg\min}\ \text{KL}(q_{\lambda}(z\vert x)\Vert P(z\vert x))$$</div>
<p>Expanding the divergence, we obtain:</p>
<div class="math">$$
\begin{align*}
KL(q_{\lambda}(z\vert x)\Vert P(z\vert x))
&amp;= \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)}{P(z\vert x)}dz}\\
&amp;= \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)P(x)}{P(z, x)}dz}\\
&amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x) -\log{P(z, x)} + \log{P(x)}}\bigg)dz}\\
&amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz + \log{P(x)}\int{q_{\lambda}(z\vert x)dz}\\
&amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz + \log{P(x)} \cdot 1
\end{align*}
$$</div>
<p>As such, since only the left term depends on <span class="math">\(\lambda\)</span>, minimizing the entire expression with respect to <span class="math">\(\lambda\)</span> amounts to minimizing this term. Incidentally, the opposite (negative) of this term is called the <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">ELBO</a>, or the "evidence lower bound." To see why, let's plug the ELBO into the equation above and solve for <span class="math">\(\log{P(x)}\)</span>:</p>
<div class="math">$$\log{P(x)} = ELBO(\lambda) + KL(q_{\lambda}(z\vert x)\Vert P(z\vert x))$$</div>
<p>In English: "the log of the evidence is at least the lower bound of the evidence plus the divergence between our true posterior <span class="math">\(P(z\vert x)\)</span> and our (variational) approximation to this posterior <span class="math">\(q_{\lambda}(z\vert x)\)</span>."</p>
<p>Since the left term above is the opposite of the ELBO, minimizing this term is equivalent to <em>maximizing</em> the ELBO.</p>
<p>Let's restate the equation and rearrange further:</p>
<div class="math">$$
\begin{align*}
ELBO(\lambda)
&amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz\\
&amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(x\vert z)} - \log{P(z)}}\bigg)dz\\
&amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} - \log{P(z)}}\bigg)dz + \log{P(x\vert z)}\int{q_{\lambda}(z\vert x)dz}\\
&amp;= -\int{q_{\lambda}(z\vert x)\log{\frac{q_{\lambda}(z\vert x)}{P(z)}}dz} + \log{P(x\vert z)} \cdot 1\\
&amp;= \log{P(x\vert z)} -KL(q_{\lambda}(z\vert x)\Vert P(z))
\end{align*}
$$</div>
<p>Our goal is to maximize this expression, or minimize the opposite:</p>
<div class="math">$$-\log{P(x\vert z)} + KL(q_{\lambda}(z\vert x)\Vert P(z))$$</div>
<p>In machine learning parlance: "minimize the negative log likelihood of our data (generated via <span class="math">\(z\)</span>) plus the divergence between the true distribution of <span class="math">\(z\)</span> (the ceramic plate) and our approximation thereof."</p>
<p>See what we did?</p>
<h3>Finally, back to neural nets</h3>
<p>The variational autoencoder consists of an encoder network and a decoder network.</p>
<h4>Encoder</h4>
<p>The encoder network takes as input <span class="math">\(x\)</span> (an airport) and produces as output <span class="math">\(z\)</span> (the latent "code" of that airport, i.e. its location on the ceramic plate). As an intermediate step, it produces multivariate Gaussian parameters <span class="math">\((\mu_{x_i}, \sigma_{x_i})\)</span> for each airport. These parameters are then plugged into a Gaussian <span class="math">\(q\)</span>, from which we <em>sample</em> a value <span class="math">\(z\)</span>. The encoder is parameterized by a weight matrix <span class="math">\(\theta\)</span>.</p>
<h4>Decoder</h4>
<p>The decoder network takes as input <span class="math">\(z\)</span> and produces <span class="math">\(P(x\vert z)\)</span>: a reconstruction of the airport vector (hence, autoencoder). It is parameterized by a weight matrix <span class="math">\(\phi\)</span>.</p>
<h4>Loss function</h4>
<p>The network's loss function is the sum of the mean squared reconstruction error of the original input <span class="math">\(x\)</span> and the KL divergence between the true distribution of <span class="math">\(z\)</span> and its approximation <span class="math">\(q\)</span>. Given the reparameterization trick (next section) and another healthy scoop of algebra, we write this in Python code as follows:</p>
<div class="highlight"><pre><span></span><span class="sd">'''</span>
<span class="sd">`z_mean` gives the mean of the Gaussian that generates `z`</span>
<span class="sd">`z_log_var` gives the log-variance of the Gaussian that generates `z`</span>
<span class="sd">`z` is generated via:</span>

<span class="sd">  z = z_mean + K.exp(z_log_var / 2) * epsilon</span>
<span class="sd">    = z_mean + K.exp( log(z_std)**2 / 2 ) * epsilon</span>
<span class="sd">    = z_mean + K.exp( (2 * log(z_std) / 2 ) * epsilon</span>
<span class="sd">    = z_mean + K.exp( log(z_std) ) * epsilon</span>
<span class="sd">    = z_mean + z_std * epsilon</span>
<span class="sd">'''</span>
<span class="n">kl_loss_numerator</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">z_log_var</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">)</span>
<span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl_loss_numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">)</span> <span class="o">+</span> <span class="n">kl_loss</span>
</pre></div>
<h3>Reparameterization trick</h3>
<p>When back-propagating the network's loss to <span class="math">\(\theta\)</span> , we need to go through <span class="math">\(z\)</span> — <em>a sample</em> taken from <span class="math">\(q_{\theta}(z\vert x)\)</span>. Trivially, this sample is a scalar; intuitively, its derivative should be non-zero. In solution, we'd like the sample to depend not on the <em>stochasticity</em> of the random variable, but on the random variable's <em>parameters</em>. To this end, we employ the <a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important">"reparametrization trick"</a>, such that the sample depends on these parameters <em>deterministically</em>.</p>
<p>As a quick example, this trick allows us to write <span class="math">\(\mathcal{N}(\mu, \sigma)\)</span> as <span class="math">\(z = \mu + \sigma \odot \epsilon\)</span>, where <span class="math">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>. Drawing samples this way allows us to propagate error backwards through our network.</p>
<h2>Auxiliary data</h2>
<div class="highlight"><pre><span></span><span class="c1"># build X_routes, y_routes</span>
<span class="n">geo_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'origin_latitude'</span><span class="p">,</span> <span class="s1">'origin_longitude'</span><span class="p">,</span> <span class="s1">'dest_latitude'</span><span class="p">,</span> <span class="s1">'dest_longitude'</span><span class="p">]</span>
<span class="n">X_r</span> <span class="o">=</span> <span class="n">routes</span><span class="p">[</span> <span class="p">[</span><span class="s1">'origin_id'</span><span class="p">,</span> <span class="s1">'dest_id'</span><span class="p">]</span> <span class="o">+</span> <span class="n">geo_cols</span> <span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y_r</span> <span class="o">=</span> <span class="n">routes</span><span class="p">[</span><span class="s1">'exists'</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">X_r</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">geo_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span> <span class="n">X_r</span><span class="p">[</span><span class="n">geo_cols</span><span class="p">]</span> <span class="p">)</span>

<span class="c1"># split training, test data</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="n">X_r</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
<span class="n">val_size</span> <span class="o">=</span> <span class="n">test_size</span> <span class="o">//</span> <span class="mi">2</span>

<span class="n">X_train_r</span><span class="p">,</span> <span class="n">X_test_r</span><span class="p">,</span> <span class="n">y_train_r</span><span class="p">,</span> <span class="n">y_test_r</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_r</span><span class="p">,</span> <span class="n">y_r</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_val_r</span><span class="p">,</span> <span class="n">X_test_r</span><span class="p">,</span> <span class="n">y_val_r</span><span class="p">,</span> <span class="n">y_test_r</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_test_r</span><span class="p">,</span> <span class="n">y_test_r</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">val_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Dataset sizes:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Train:      </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_r</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Validation: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_val_r</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Test:       </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test_r</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">Dataset sizes:</span>
<span class="err">    Train:      (87630, 6)</span>
<span class="err">    Validation: (21907, 6)</span>
<span class="err">    Test:       (21907, 6)</span>
</pre></div>
<h2>Dot product embedding model</h2>
<p>To start, let's train our model with a single latent dimension then visualize the results on the world map.</p>
<div class="highlight"><pre><span></span><span class="n">N_UNIQUE_AIRPORTS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_airports</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DotProductEmbeddingModel</span><span class="p">(</span><span class="n">BaseEmbeddingModel</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">λ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_unique_airports</span><span class="o">=</span><span class="n">N_UNIQUE_AIRPORTS</span><span class="p">):</span>
        <span class="sd">'''Initializes the model parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            embedding_size : The desired number of latent dimensions in our</span>
<span class="sd">                embedding space.</span>
<span class="sd">            λ : The regularization strength to apply to the model's</span>
<span class="sd">                dense layers.</span>
<span class="sd">        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_unique_airports</span> <span class="o">=</span> <span class="n">n_unique_airports</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">λ</span> <span class="o">=</span> <span class="n">λ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># inputs</span>
        <span class="n">origin</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'origin'</span><span class="p">)</span>
        <span class="n">dest</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'dest'</span><span class="p">)</span>
        <span class="n">origin_geo</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'origin_geo'</span><span class="p">)</span>
        <span class="n">dest_geo</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'dest_geo'</span><span class="p">)</span>

        <span class="c1"># embeddings</span>
        <span class="n">origin_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unique_airports</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span>
                                     <span class="n">embeddings_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'origin_embedding'</span><span class="p">)(</span><span class="n">origin</span><span class="p">)</span>
        <span class="n">dest_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unique_airports</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embeddings_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dest</span><span class="p">)</span>

        <span class="c1"># dot product</span>
        <span class="n">dot_product</span> <span class="o">=</span> <span class="n">dot</span><span class="p">([</span><span class="n">origin_embedding</span><span class="p">,</span> <span class="n">dest_embedding</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">dot_product</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">dot_product</span><span class="p">)</span>
        <span class="n">dot_product</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">dot_product</span><span class="p">,</span> <span class="n">origin_geo</span><span class="p">,</span> <span class="n">dest_geo</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># dense layers</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">)(</span><span class="n">dot_product</span><span class="p">)</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">tanh</span><span class="p">)</span>

        <span class="c1"># output</span>
        <span class="n">exists</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)(</span><span class="n">tanh</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">origin</span><span class="p">,</span> <span class="n">dest</span><span class="p">,</span> <span class="n">origin_geo</span><span class="p">,</span> <span class="n">dest_geo</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">exists</span><span class="p">])</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">dp_model</span> <span class="o">=</span> <span class="n">DotProductEmbeddingModel</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">λ</span><span class="o">=.</span><span class="mi">0001</span><span class="p">)</span>
<span class="n">dp_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=.</span><span class="mi">001</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">)</span>
<span class="n">SVG</span><span class="p">(</span><span class="n">model_to_dot</span><span class="p">(</span><span class="n">dp_model</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">prog</span><span class="o">=</span><span class="s1">'dot'</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'svg'</span><span class="p">))</span>
</pre></div>
<p><img alt="dot product embedding model" class="img-fluid" src="https://willwolf.io/figures/dot_product_embedding_model.svg"/></p>
<h2>Fit</h2>
<p><img alt="dot product embedding model fit" class="img-fluid" src="https://willwolf.io/figures/dot_product_embedding_model_fit.png"/></p>
<h2>Visualize embeddings</h2>
<p>To visualize results, we'll:
1. Compose a list of unique origin airports.
2. Extract the learned (1-dimensional) embedding for each.
3. Scale the results to <span class="math">\([0, 1]\)</span>.
4. Use the scaled embedding as a percentile-index into a color gradient. Here, we've chosen the colors of the rainbow: low values are blue/purple, and high values are orange/red.</p>
<div class="highlight"><pre><span></span><span class="n">plot_embeddings_on_world_map</span><span class="p">(</span><span class="n">unique_origins</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="s1">'</span><span class="si">{static}</span><span class="s1">/figures/dp_model_map.html'</span><span class="p">)</span>
</pre></div>
<iframe allowfullscreen="" frameborder="0" height="800" src="https://willwolf-public.s3.amazonaws.com/transfer-learning-flight-delays/dp_model_map.html" width="946"></iframe>
<h2>Variational autoencoder</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VariationalLayer</span><span class="p">(</span><span class="n">KerasLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">epsilon_std</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="sd">'''A custom "variational" Keras layer that completes the</span>
<span class="sd">        variational autoencoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_dim : The desired number of latent dimensions in our</span>
<span class="sd">                embedding space.</span>
<span class="sd">        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_std</span> <span class="o">=</span> <span class="n">epsilon_std</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'glorot_normal'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'zero'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'glorot_normal'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'zero'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z_mean</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_bias</span>
        <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_bias</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">),</span>
            <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
            <span class="n">stddev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_std</span>
        <span class="p">)</span>

        <span class="n">kl_loss_numerator</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">z_log_var</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl_loss_numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z_mean</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VariationalAutoEncoderEmbeddingModel</span><span class="p">(</span><span class="n">BaseEmbeddingModel</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dense_layer_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">λ</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n_unique_airports</span><span class="o">=</span><span class="n">N_UNIQUE_AIRPORTS</span><span class="p">):</span>
        <span class="sd">'''Initializes the model parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            embedding_size : The desired number of latent dimensions in our</span>
<span class="sd">                embedding space.</span>
<span class="sd">            λ : The regularization strength to apply to the model's</span>
<span class="sd">                dense layers.</span>
<span class="sd">        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_layer_size</span> <span class="o">=</span> <span class="n">dense_layer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">λ</span> <span class="o">=</span> <span class="n">λ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_unique_airports</span> <span class="o">=</span> <span class="n">n_unique_airports</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">variational_layer</span> <span class="o">=</span> <span class="n">VariationalLayer</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># encoder</span>
        <span class="n">origin</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unique_airports</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'origin'</span><span class="p">)</span>
        <span class="n">origin_geo</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'origin_geo'</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">origin</span><span class="p">,</span> <span class="n">origin_geo</span><span class="p">])</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense_layer_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">variational_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variational_layer</span><span class="p">(</span><span class="n">dense</span><span class="p">)</span>

        <span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">origin</span><span class="p">,</span> <span class="n">origin_geo</span><span class="p">],</span> <span class="n">variational_output</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'encoder'</span><span class="p">)</span>

        <span class="c1"># decoder</span>
        <span class="n">latent_vars</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,))</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense_layer_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">latent_vars</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense_layer_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dense</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dest</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unique_airports</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'dest'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">λ</span><span class="p">))(</span><span class="n">dense</span><span class="p">)</span>
        <span class="n">dest_geo</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'dest_geo'</span><span class="p">)(</span><span class="n">dense</span><span class="p">)</span>

        <span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">latent_vars</span><span class="p">,</span> <span class="p">[</span><span class="n">dest</span><span class="p">,</span> <span class="n">dest_geo</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'decoder'</span><span class="p">)</span>

        <span class="c1"># end-to-end</span>
        <span class="n">encoder_decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">origin</span><span class="p">,</span> <span class="n">origin_geo</span><span class="p">],</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">([</span><span class="n">origin</span><span class="p">,</span> <span class="n">origin_geo</span><span class="p">])))</span>
        <span class="k">return</span> <span class="n">encoder_decoder</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">vae_model</span> <span class="o">=</span> <span class="n">VariationalAutoEncoderEmbeddingModel</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dense_layer_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">λ</span><span class="o">=.</span><span class="mi">003</span><span class="p">)</span>
<span class="n">vae_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="n">vae_model</span><span class="o">.</span><span class="n">variational_layer</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="s1">'mean_squared_logarithmic_error'</span><span class="p">],</span>
                  <span class="n">loss_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">])</span>
<span class="n">SVG</span><span class="p">(</span><span class="n">model_to_dot</span><span class="p">(</span><span class="n">vae_model</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">prog</span><span class="o">=</span><span class="s1">'dot'</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'svg'</span><span class="p">))</span>
</pre></div>
<p><img alt="vae embedding model" class="img-fluid" src="https://willwolf.io/figures/vae_embedding_model.svg"/></p>
<div class="highlight"><pre><span></span><span class="c1"># build VAE training, test sets</span>
<span class="n">one_hot_airports</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N_UNIQUE_AIRPORTS</span><span class="p">)</span>

<span class="n">X_train_r_origin</span> <span class="o">=</span> <span class="n">one_hot_airports</span><span class="p">[</span> <span class="n">X_train_r</span><span class="p">[</span><span class="s1">'origin_id'</span><span class="p">]</span> <span class="p">]</span>
<span class="n">X_val_r_origin</span>   <span class="o">=</span> <span class="n">one_hot_airports</span><span class="p">[</span> <span class="n">X_val_r</span><span class="p">[</span><span class="s1">'origin_id'</span><span class="p">]</span>   <span class="p">]</span>
<span class="n">X_test_r_origin</span>  <span class="o">=</span> <span class="n">one_hot_airports</span><span class="p">[</span> <span class="n">X_test_r</span><span class="p">[</span><span class="s1">'origin_id'</span><span class="p">]</span>  <span class="p">]</span>
<span class="n">X_train_r_dest</span>   <span class="o">=</span> <span class="n">one_hot_airports</span><span class="p">[</span> <span class="n">X_train_r</span><span class="p">[</span><span class="s1">'dest_id'</span><span class="p">]</span>   <span class="p">]</span>
<span class="n">X_val_r_dest</span>     <span class="o">=</span> <span class="n">one_hot_airports</span><span class="p">[</span> <span class="n">X_val_r</span><span class="p">[</span><span class="s1">'dest_id'</span><span class="p">]</span>     <span class="p">]</span>
<span class="n">X_test_r_dest</span>    <span class="o">=</span> <span class="n">one_hot_airports</span><span class="p">[</span> <span class="n">X_test_r</span><span class="p">[</span><span class="s1">'dest_id'</span><span class="p">]</span>    <span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Dataset sizes:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Train:      </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_r_origin</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Validation: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_val_r_origin</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'    Test:       </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test_r_origin</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">Dataset sizes:</span>
<span class="err">    Train:      (87630, 3186)</span>
<span class="err">    Validation: (21907, 3186)</span>
<span class="err">    Test:       (21907, 3186)</span>
</pre></div>
<h2>Fit</h2>
<p><img alt="vae product embedding model fit" class="img-fluid" src="https://willwolf.io/figures/vae_product_embedding_model_fit.png"/></p>
<h2>Visualize</h2>
<p><iframe allowfullscreen="" frameborder="0" height="800" src="https://willwolf-public.s3.amazonaws.com/transfer-learning-flight-delays/vae_model_map.html" width="946"></iframe></p>
<h2>Finally, transfer the learning</h2>
<p>Retrain both models with 20 latent dimensions, then join the embedding back to our original dataset.</p>
<h2>Extract embeddings, construct joint dataset</h2>
<div class="highlight"><pre><span></span><span class="err">Dataset sizes:</span>
<span class="err">    Train:      (30000, 151)</span>
<span class="err">    Validation: (10000, 151)</span>
<span class="err">    Test:       (10000, 151)</span>
</pre></div>
<h2>Train original models</h2>
<div class="highlight"><pre><span></span><span class="n">simple_reg</span> <span class="o">=</span> <span class="n">SimpleRegression</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">λ</span><span class="o">=.</span><span class="mi">05</span><span class="p">)</span>
<span class="n">simple_reg</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=.</span><span class="mi">0005</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'mean_squared_error'</span><span class="p">)</span>
<span class="n">simple_reg_fit</span> <span class="o">=</span> <span class="n">fit_flight_model</span><span class="p">(</span><span class="n">simple_reg</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plot_model_fit</span><span class="p">(</span><span class="n">simple_reg_fit</span><span class="p">)</span>
</pre></div>
<p><img alt="simple regression augmented" class="img-fluid" src="https://willwolf.io/figures/transfer_learning_simple_regression_augmented.png"/></p>
<div class="highlight"><pre><span></span><span class="n">deeper_reg</span> <span class="o">=</span> <span class="n">DeeperRegression</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">λ</span><span class="o">=.</span><span class="mi">03</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">deeper_reg</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=.</span><span class="mi">0001</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'mean_squared_error'</span><span class="p">)</span>
<span class="n">deeper_reg_fit</span> <span class="o">=</span> <span class="n">fit_flight_model</span><span class="p">(</span><span class="n">deeper_reg</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plot_model_fit</span><span class="p">(</span><span class="n">deeper_reg_fit</span><span class="p">)</span>
</pre></div>
<p><img alt="deeper regression augmented" class="img-fluid" src="https://willwolf.io/figures/transfer_learning_deeper_regression_augmented.png"/></p>
<div class="highlight"><pre><span></span><span class="n">y_pred_simple</span> <span class="o">=</span> <span class="n">simple_reg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">y_pred_deeper</span> <span class="o">=</span> <span class="n">deeper_reg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">mse_simple</span> <span class="o">=</span> <span class="n">mean_squared_error_scikit</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_simple</span><span class="p">)</span>
<span class="n">mse_deeper</span> <span class="o">=</span> <span class="n">mean_squared_error_scikit</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_deeper</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean squared error, simple regression: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse_simple</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Mean squared error, deeper regression: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mse_deeper</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="err">Mean squared error, simple regression: 2.3176028493805263</span>
<span class="err">Mean squared error, deeper regression: 2.291221474968889</span>
</pre></div>
<h2>Summary</h2>
<p>In fitting these models to both the original and "augmented" datasets, I spent time tuning their parameters — regularization strengths, amount of dropout, number of epochs, learning rates, etc. Additionally, the respective datasets are of different dimensionality. For these reasons, comparison between the two sets of models is clearly not "apples to apples."</p>
<p>Notwithstanding, the airport embeddings do seem to provide a nice lift over our original one-hot encodings. Of course, their use is not limited to predicting flight delays: they can be used in any task concerned with airports. Additionally, these embeddings give insight into the nature of the airports themselves: those nearby in vector space can be considered as "similar" by some latent metric. To figure out what these metrics mean, though - it's back to the map.</p>
<h2>Additional Resources</h2>
<ul>
<li><a href="http://allentran.github.io/graph2vec">Towards Anything2Vec</a></li>
<li><a href="http://ben.bolte.cc/blog/2017/unsupervised-calcium-modeling.html">Deep Learning for Calcium Imaging</a></li>
<li><a href="https://arxiv.org/pdf/1403.6652.pdf">DeepWalk: Online Learning of Social Representations</a></li>
<li><a href="http://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/">Variational Autoencoder: Intuition and Implementation</a></li>
<li><a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Introducing Variational Autoencoders (in Prose and Code)</a></li>
<li><a href="http://dohmatob.github.io/research/2016/10/22/VAE.html">Variational auto-encoder for "Frey faces" using keras</a></li>
<li><a href="http://sebastianruder.com/transfer-learning/">Transfer Learning - Machine Learning's Next Frontier</a></li>
<li><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Tutorial - What is a variational autoencoder?</a></li>
<li><a href="http://blog.evjang.com/2016/08/variational-bayes.html">A Beginner's Guide to Variational Methods: Mean-Field Approximation</a></li>
<li><a href="http://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/">Variational Autoencoder: Intuition and Implementation</a></li>
<li><a href="https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning">CrossValidated - What is the manifold assumption in semi-supervised learning?</a></li>
<li><a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">David Blei - Variational Inference</a></li>
<li><a href="http://edwardlib.org/tutorials/variational-inference">Edward - Variational Inference</a></li>
<li><a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf">On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes</a></li>
</ul>
<h2>Code</h2>
<p>The <a href="https://github.com/cavaunpeu/flight-delays">repository</a> and <a href="http://nbviewer.jupyter.org/github/cavaunpeu/flight-delays/blob/master/notebooks/flight-prediction.ipynb?flush_cache=true">rendered notebook</a> for this project can be found at their respective links.</p>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://blog.evjang.com/2016/08/variational-bayes.html">A Beginner's Guide to Variational Methods: Mean-Field Approximation</a> <a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="willwolf_">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'willwolf';
    var disqus_title = 'Transfer Learning for Flight Delay Prediction via Variational Autoencoders';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</div> </div>
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-11 col-md-offset-1" id="footer-wrapper">
				<div class="col-md-3">
					<div id="social-links">
						<h4>
							Contact
						</h4>
						<li><a href="mailto:williamabrwolf@gmail.com">Email</a></li>
							<li><a href="http://twitter.com/willwolf_">Twitter</a></li>
							<li><a href="http://linkedin.com/in/williamabrwolf">LinkedIn</a></li>
							<li><a href="http://calendly.com/willwolf">Calendly</a></li>
					</div>
				</div>
				<div class="col-md-3">
					<div id="other-links">
						<h4>
							Links
						</h4>
						<ul>
							<li><a href="https://tinyletter.com/willwolf">Newsletter</a></li>
							<li><a href="http://willtravellife.com">Travel Blog</a></li>
							<li><a href="http://github.com/cavaunpeu">Github</a></li>
							<li><a href="https://github.com/cavaunpeu/willwolf.io-source">Source Code</a></li>
						</ul>
					</div>
				</div>
				<div class="col-md-3">
				  <div id="categories">
				    <h4>
				      Categories
				    </h4>
				    <ul>
				      <li><a href="https://willwolf.io/crypto/">crypto</li>
				      <li><a href="https://willwolf.io/geopolitics/">geopolitics</li>
				      <li><a href="https://willwolf.io/life/">life</li>
				      <li><a href="https://willwolf.io/machine-learning/">machine-learning</li>
				    </ul>
				  </div>
				</div>
				<div class="col-md-3">
				  <div id="pages">
				    <h4>
				      Pages
				    </h4>
				    <ul>
				      <li><a href="https://willwolf.io/archive/" title="Archive">Archive</a></li>
				    </ul>
				  </div>
				</div>
			</div>
		</div>
	</div>
</footer><div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Will Wolf 2020</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div><!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-97412095-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>