<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>will wolf</title>
	<meta name="description" content="data science things and thoughts on the world">
	<meta name="author" content="Will Wolf">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Favicon -->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="https://cavaunpeu.github.io/theme/html5.js"></script>
		<![endif]-->

	<!-- Atom Feed -->

	<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@willwolf_">
  <meta name="twitter:creator" content="@willwolf_">
  <meta name="twitter:domain" content="https://cavaunpeu.github.io">
    <meta property="twitter:title" content="Neurally Embedded Emojis"/>
    <meta property="twitter:description" content="Convolutional variational autoencoders for emoji generation and Siamese text-question-emoji-answer models. Keras, bidirectional LSTMs and snarky tweets @united within."/>
    <meta property="twitter:image" content="https://cavaunpeu.github.io/images/emojis.png"/>

	<!-- CSS -->
	<link href="https://cavaunpeu.github.io/theme/css/ipython.css?v={12345}" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.3.7/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/local.css?v={12345}" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/pygments.css?v={12345}" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/main.css?v={12345}" rel="stylesheet">
</head><body>
<div class="container">
<div class="page-header">
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <h1><a id="site-title" href="https://cavaunpeu.github.io/">will wolf</a></h1>
          <h4 id="site-subtitle">data science things and thoughts on the world</h4>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav navbar-right">
              <li><a href="https://cavaunpeu.github.io/about/" title="About"><span class="glyphicon glyphicon-user"></span> About</a></li>
              <li><a href="https://cavaunpeu.github.io/cv/" title="CV"><span class="glyphicon glyphicon-folder-open"></span> CV</a></li>
              <li><a href="https://cavaunpeu.github.io/archive/" title="Archive"><span class="glyphicon glyphicon-th-list"></span> Archive</a></li>
              <li><a href="https://cavaunpeu.github.io/feeds/all.atom.xml" title="willwolf.io Atom feed"><span class="icon-rss"></span> RSS</a></li>
          <li class="dropdown">
            <a class="dropdown-toggle" data-toggle="dropdown" href="https://cavaunpeu.github.io" title=English id="activeLanguage"><span class="glyphicon glyphicon-flag"></span>EN<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li>
                  <a id="inactiveLanguage" href="https://cavaunpeu.github.io/es/" title=Espa√±ol>ES</a>
                </li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</div>	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">Neurally Embedded Emojis</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Will Wolf</h4>
		</span>
		<time datetime="2017-06-19T13:00:00-04:00" itemprop="datePublished">June 19, 2017</time>
	</div>
	<div itemprop="articleBody" class="article-body"><p>As I move through my 20's I'm consistently delighted by the subtle ways in which I've changed.</p>
<blockquote>
<p>Will at 22: Reggaeton is a miserable, criminal assault to my ears.</p>
<p>Will at 28: <a href="https://www.youtube.com/watch?v=72UO0v5ESUo">Despacito (Remix)</a> for breakfast, lunch, dinner.</p>
<p><br/></p>
<p>Will at 22: Western Europe is boring. No ‚Äî I've seen a lot of it! Everything is too clean, too nice, too perfect for my taste.</p>
<p>Will at 28, in Barcelona, after 9 months in <a href="https://cavaunpeu.github.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/">Casablanca</a>: Wait a second: <em>I get it now</em>. What <em>is</em> this summertime paradise of crosswalks, vehicle civility and apple-green parks and where has it been all my life?</p>
<p><br/></p>
<p>Will at 22: Emojis are weird.</p>
<p>Will at 28: üöÄ ü§ò üíÉüèø üö¥üèª üôÉ.</p>
</blockquote>
<p>Emojis are an increasingly-pervasive sub-lingua-franca of the internet. They capture meaning in a rich, concise manner ‚Äî alternative to the 13 seconds of mobile thumb-fumbling required to capture the same meaning with text. Furthermore, they bring two levels of semantic information: their context within raw text and the pixels of the emoji itself.</p>
<h2>Question-answer models</h2>
<p>The original aim of this post was to explore Siamese question-answer models of the type typically applied to the <a href="https://github.com/shuzi/insuranceQA">InsuranceQA Corpus</a> as introduced in "Applying Deep Learning To Answer Selection: A Study And An Open Task" (<a href="https://arxiv.org/pdf/1508.01585v2.pdf">Feng, Xiang, Glass, Wang, &amp; Zhou, 2015</a>). We'll call them SQAM for clarity. The basic architecture looks as follows:</p>
<p><img alt="qa model architecture" class="img-responsive" src="https://cavaunpeu.github.io/figures/qa_model_architecture.png"/></p>
<p>By layer and in general terms:</p>
<ol>
<li>An input ‚Äî typically a sequence of token ids ‚Äî for both question (Q) and answer (A).</li>
<li>An embedding layer.</li>
<li>Convolutional layer(s), or any layers that extract features from the matrix of embeddings. (A matrix, because the respective inputs are sequences of token ids; each id is embedded into its own vector.)</li>
<li>A max-pooling layer.</li>
<li>A <code>tanh</code> non-linearity.</li>
<li>The cosine of the angle between the resulting, respective embeddings.</li>
</ol>
<h3>As canonical recommendation</h3>
<p>Question answering can be viewed as canonical recommendation: embed entities into Euclidean space in a meaningful way, then compute dot products between these entities and sort the list. In this vein, the above network is (thus far) quite similar to classic matrix factorization yet with the following subtle tweaks:</p>
<ol>
<li>Instead of factorizing our matrix via <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a> or <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">OLS</a> we build a neural network that accepts <code>(question, answer)</code>, i.e. <code>(user, item)</code>, pairs and outputs their similarity. The second-to-last layer gives the respective embeddings. We train this network in a supervised fashion, optimizing its parameters via stochastic gradient descent.</li>
<li>Instead of jumping directly from input-index (or sequence thereof) to embedding, we first compute convolutional features.</li>
</ol>
<p>In contrast, the network above boasts one key difference: both question and answer, i.e. user and item, are transformed via a single set of parameters ‚Äî an initial embedding layer, then convolutional layers ‚Äî en route to their final embedding.</p>
<p>Furthermore, and not unique to SQAMs, our network inputs can be <em>any</em> two sequences of (tokenized, max-padded, etc.) text: we are not restricted to only those observed in the training set.</p>
<h2>Question-emoji models</h2>
<p>Given my accelerating proclivity for the internet's new alphabet, I decided to build text-question-<em>emoji</em>-answer models instead. In fact, this setup gives an additional avenue for prediction: if we make a model of the answers (emojis) themselves, we can now predict on, i.e. compute similarity with, each of</p>
<ol>
<li>Emojis we saw in the training set.</li>
<li>New emojis, i.e. either not in the training set or new (like, released months from now) altogether.</li>
<li>Novel emojis <em>generated</em> from the model of our data. In this way, we could conceivably answer a question with: "we suggest this new emoji we've algorithmically created ourselves that no one's ever seen before."</li>
</ol>
<p>Let's get started.</p>
<h2>Convolutional variational autoencoders</h2>
<p>Variational autoencoders are comprised of two models: an encoder and a decoder. The encoder embeds our 872 <a href="https://github.com/twitter/twemoji">emojis</a> of size <span class="math">\((36, 36, 4)\)</span> into a low-dimensional latent code, <span class="math">\(z_e \in \mathbb{R}^{16}\)</span>, where <span class="math">\(z_e\)</span> is a sample from an emoji-specific Gaussian. The decoder takes as input <span class="math">\(z_e\)</span> and produces a reconstruction of the original emoji. As each individual <span class="math">\(z_e\)</span> is normally distributed, <span class="math">\(z\)</span> should be distributed normally as well. We can verify this with a quick simulation.</p>
<div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">z_samples</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mu</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sd</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="n">z_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">samples</span> <span class="p">)</span>


<span class="n">z_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">z_samples</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>
<p><img alt="aggregate gaussian" class="img-responsive" src="https://cavaunpeu.github.io/figures/aggregate_gaussian.png"/></p>
<p>Training a variational autoencoder to learn low-dimensional emoji embeddings serves two principal ends:</p>
<ol>
<li>We can feed these low-dimensional embeddings as input to our SQAM.</li>
<li>We can generate novel emojis with which to answer questions.</li>
</ol>
<p>As the embeddings in #1 are multivariate Gaussian, we can perform #2 by passing Gaussian samples into our decoder. We can do this by sampling evenly-spaced percentiles from the inverse CDF of the aggregate embedding distribution:</p>
<div class="highlight"><pre><span></span><span class="n">percentiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">percentiles</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">generated_emoji</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">z</span><span class="p">])</span>
</pre></div>
<p>NB: <code>norm.ppf</code> does <em>not</em> accept a <code>size</code> parameter; I believe sampling from the inverse CDF of a <em>multivariate</em> Gaussian is non-trivial in Python.</p>
<p>Similarly, we could simply iterate over <code>(mu, sd)</code> pairs outright:</p>
<div class="highlight"><pre><span></span><span class="n">axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">sd</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">generated_emoji</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">z</span><span class="p">])</span>
</pre></div>
<p>The ability to generate new emojis via samples from a well-studied distribution, the Gaussian, is a key reason for choosing a variational autoencoder.</p>
<p>Finally, as we are working with images, I employ convolutional intermediary layers.</p>
<h2>Data preparation</h2>
<div class="highlight"><pre><span></span><span class="n">EMOJIS_DIR</span> <span class="o">=</span> <span class="s1">'data/emojis'</span>
<span class="n">N_CHANNELS</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">EMOJI_SHAPE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="n">N_CHANNELS</span><span class="p">)</span>


<span class="n">emojis_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">slug</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">EMOJIS_DIR</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">EMOJIS_DIR</span><span class="p">,</span> <span class="n">slug</span><span class="p">)</span>
    <span class="n">emoji</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">emoji</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
        <span class="n">emojis_dict</span><span class="p">[</span><span class="n">slug</span><span class="p">]</span> <span class="o">=</span> <span class="n">emoji</span>

<span class="n">emojis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">emojis_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="p">)</span>
</pre></div>
<h3>Split data into train, validation sets</h3>
<p>Additionally, scale pixel values to <span class="math">\([0, 1]\)</span>.</p>
<div class="highlight"><pre><span></span><span class="n">train_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span><span class="n">emojis</span><span class="p">)</span> <span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.8</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">emojis</span><span class="p">[</span><span class="n">train_mask</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">emojis</span><span class="p">[</span><span class="o">~</span><span class="n">train_mask</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.</span>
</pre></div>
<div class="highlight"><pre><span></span>Dataset sizes:
    X_train:  (685, 36, 36, 4)
    X_val:    (182, 36, 36, 4)
    y_train:  (685, 36, 36, 4)
    y_val:    (182, 36, 36, 4)
</pre></div>
<p>Before we begin, let's examine some emojis.</p>
<p><img alt="emojis" class="img-responsive" src="https://cavaunpeu.github.io/images/emojis.png"/></p>
<h2>Model emojis</h2>
<div class="highlight"><pre><span></span><span class="n">EMBEDDING_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">FILTER_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
</pre></div>
<h3>Variational layer</h3>
<p>This is taken from a previous post of mine, <a href="https://cavaunpeu.github.io/2017/05/08/transfer-learning-flight-delay-prediction/">Transfer Learning for Flight Delay Prediction via Variational Autoencoders</a>.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VariationalLayer</span><span class="p">(</span><span class="n">KerasLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">epsilon_std</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="sd">'''A custom "variational" Keras layer that completes the</span>
<span class="sd">        variational autoencoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            embedding_dim : The desired number of latent dimensions in our</span>
<span class="sd">                embedding space.</span>
<span class="sd">        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_std</span> <span class="o">=</span> <span class="n">epsilon_std</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'glorot_normal'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">'z_mean_weights'</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'zero'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">'z_mean_bias'</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'glorot_normal'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">'z_log_var_weights'</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,),</span>
            <span class="n">initializer</span><span class="o">=</span><span class="s1">'zero'</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">'z_log_var_bias'</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z_mean</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_mean_bias</span>
        <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_log_var_bias</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">),</span>
            <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
            <span class="n">stddev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_std</span>
        <span class="p">)</span>

        <span class="n">kl_loss_numerator</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">z_log_var</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kl_loss_numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z_mean</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">):</span>
        <span class="n">base_loss</span> <span class="o">=</span> <span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">)</span>
        <span class="n">base_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">base_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">base_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span>

    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,)</span>
</pre></div>
<h3>Autoencoder</h3>
<div class="highlight"><pre><span></span><span class="c1"># encoder</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">EMOJI_SHAPE</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'original'</span><span class="p">)</span>

<span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">FILTER_SIZE</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">original</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">original</span><span class="p">)</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">FILTER_SIZE</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">conv</span><span class="p">)</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">FILTER_SIZE</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">conv</span><span class="p">)</span>

<span class="n">flat</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">conv</span><span class="p">)</span>
<span class="n">variational_layer</span> <span class="o">=</span> <span class="n">VariationalLayer</span><span class="p">(</span><span class="n">EMBEDDING_SIZE</span><span class="p">)</span>
<span class="n">variational_params</span> <span class="o">=</span> <span class="n">variational_layer</span><span class="p">(</span><span class="n">flat</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">original</span><span class="p">],</span> <span class="p">[</span><span class="n">variational_params</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'encoder'</span><span class="p">)</span>

<span class="c1"># decoder</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">EMBEDDING_SIZE</span><span class="p">,))</span>

<span class="n">upsample</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">EMOJI_SHAPE</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">reshape</span> <span class="o">=</span> <span class="n">Reshape</span><span class="p">(</span><span class="n">EMOJI_SHAPE</span><span class="p">)(</span><span class="n">upsample</span><span class="p">)</span>

<span class="n">deconv</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">FILTER_SIZE</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)(</span><span class="n">reshape</span><span class="p">)</span>
<span class="n">deconv</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">FILTER_SIZE</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">deconv</span><span class="p">)</span>
<span class="n">deconv</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">FILTER_SIZE</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">deconv</span><span class="p">)</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="o">.</span><span class="mi">8</span><span class="p">)(</span><span class="n">deconv</span><span class="p">)</span>
<span class="n">reconstructed</span> <span class="o">=</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">N_CHANNELS</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)(</span><span class="n">dropout</span><span class="p">)</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoded</span><span class="p">],</span> <span class="p">[</span><span class="n">reconstructed</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'decoder'</span><span class="p">)</span>

<span class="c1"># end-to-end</span>
<span class="n">encoder_decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">original</span><span class="p">],</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">([</span><span class="n">original</span><span class="p">])))</span>
</pre></div>
<p>The full model <code>encoder_decoder</code> is composed of separate models <code>encoder</code> and <code>decoder</code>. Training the former will implicitly train the latter two; they are available for our use thereafter.</p>
<p>The above architecture takes inspiration from <a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py">Keras</a>, <a href="https://github.com/blei-lab/edward/blob/master/examples/vae_convolutional.py">Edward</a> and the GDGS (gradient descent by grad student) method by as discussed by <a href="https://www.reddit.com/r/MachineLearning/comments/6hso7g/d_how_do_people_come_up_with_all_these_crazy_deep/">Brudaks on Reddit</a>:</p>
<blockquote>
<p>A popular method for designing deep learning architectures is GDGS (gradient descent by grad student).
This is an iterative approach, where you start with a straightforward baseline architecture (or possibly an earlier SOTA), measure its effectiveness; apply various modifications (e.g. add a highway connection here or there), see what works and what does not (i.e. where the gradient is pointing) and iterate further on from there in that direction until you reach a (local?) optimum.</p>
</blockquote>
<p>I'm not a grad student but I think it still plays.</p>
<h3>Fit model</h3>
<div class="highlight"><pre><span></span><span class="n">encoder_decoder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="o">.</span><span class="mo">003</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="n">variational_layer</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

<span class="n">encoder_decoder_fit</span> <span class="o">=</span> <span class="n">encoder_decoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
<h2>Generate emojis</h2>
<p>As promised we'll generate emojis. Again, latent codes are distributed as a (16-dimensional) Gaussian; to generate, we'll simply take samples thereof and feed them to our <code>decoder</code>.</p>
<p>While scanning a 16-dimensional hypercube, i.e. taking (evenly-spaced, usually) samples from our latent space, is a few lines of Numpy, visualizing a 16-dimensional grid is impractical. In solution, we'll work on a 2-dimensional grid while treating subsets of our latent space as homogenous.</p>
<p>For example, if our 2-D sample were <code>(0, 1)</code>, we could posit 16-D samples as:</p>
<div class="highlight"><pre><span></span>A. `(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1)`
B. `(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1)`
C. `(0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1)`
</pre></div>
<p>Then, if another sample were <code>(2, 3.5)</code>, we could posit 16-D samples as:</p>
<div class="highlight"><pre><span></span>A. `(2, 2, 2, 2, 2, 2, 2, 2, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5)`
B. `(2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5)`
C. `(2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5)`
</pre></div>
<p>There is no math here: I'm just creating 16-element lists in different ways. We'll then plot "A-lists," "B-lists," etc. separately.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compose_code_A</span><span class="p">(</span><span class="n">coord_1</span><span class="p">,</span> <span class="n">coord_2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">8</span> <span class="o">*</span> <span class="p">[</span><span class="n">coord_1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">8</span> <span class="o">*</span> <span class="p">[</span><span class="n">coord_2</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">compose_code_B</span><span class="p">(</span><span class="n">coord_1</span><span class="p">,</span> <span class="n">coord_2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">8</span> <span class="o">*</span> <span class="p">[</span><span class="n">coord_1</span><span class="p">,</span> <span class="n">coord_2</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">compose_code_C</span><span class="p">(</span><span class="n">coord_1</span><span class="p">,</span> <span class="n">coord_2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">[</span><span class="n">coord_1</span><span class="p">,</span> <span class="n">coord_1</span><span class="p">,</span> <span class="n">coord_2</span><span class="p">,</span> <span class="n">coord_2</span><span class="p">]</span>


<span class="n">ticks</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ticks</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">plot_generated_emojis</span><span class="p">(</span><span class="n">compose_code_A</span><span class="p">)</span>
</pre></div>
<p><img alt="generated emojis A" class="img-responsive" src="https://cavaunpeu.github.io/figures/generated_emojis_A.png"/></p>
<div class="highlight"><pre><span></span><span class="n">plot_generated_emojis</span><span class="p">(</span><span class="n">compose_code_B</span><span class="p">)</span>
</pre></div>
<p><img alt="generated emojis B" class="img-responsive" src="https://cavaunpeu.github.io/figures/generated_emojis_B.png"/></p>
<div class="highlight"><pre><span></span><span class="n">plot_generated_emojis</span><span class="p">(</span><span class="n">compose_code_C</span><span class="p">)</span>
</pre></div>
<p><img alt="generated emojis C" class="img-responsive" src="https://cavaunpeu.github.io/figures/generated_emojis_C.png"/></p>
<p>As our emojis live in a continuous latent space we can observe the smoothness of the transition from one to the next.</p>
<p>The generated emojis have the makings of maybe some devils, maybe some bubbles, maybe some hearts, maybe some fish. I doubt they'll be featured on your cell phone's keyboard anytime soon.</p>
<h2>Text-question, emoji-answer</h2>
<p>I spent a while looking for an adequate dataset to no avail. (Most Twitter datasets are not open-source, I requested my own tweets days ago and continue to wait, etc.) As such, I'm working with the <a href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment">Twitter US Airline Sentiment</a> dataset: tweets are labeled as <code>positive</code>, <code>neutral</code>, <code>negative</code> which I've mapped to üéâ, üòà and üò°.</p>
<h3>Contrastive loss</h3>
<p>We've thus far discussed the SQAM. Our final model will make use of two SQAM's in parallel, as follows:</p>
<ol>
<li>Receive <code>(question, correct_answer, incorrect_answer)</code> triplets as input.</li>
<li>Compute the cosine similarity between <code>question</code>, <code>correct_answer</code> via <code>SQAM_1</code> ‚Äî <code>correct_cos_sim</code>.</li>
<li>Compute the cosine similarity between <code>question</code>, <code>incorrect_answer</code> via <code>SQAM_2</code> ‚Äî <code>incorrect_cos_sim</code>.</li>
</ol>
<p>The model is trained to minimize the following: <code>max(0, margin - correct_cos_sim + incorrect_cos_sim)</code>, a variant of the <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a>. This ensures that <code>(question, correct_answer)</code> pairs have a higher cosine similarity than <code>(question, incorrect_answer)</code> pairs, mediated by <code>margin</code>. Note that this function is differentiable: it is simply a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>.</p>
<h3>Architecture</h3>
<p>A single SQAM receives two inputs: a <code>question</code> ‚Äî a max-padded sequence of token ids ‚Äî and an <code>answer</code> ‚Äî an emoji's 16-D latent code.</p>
<p>To process the <code>question</code> we employ the following steps, i.e. network layers:</p>
<ol>
<li>Select the <a href="https://nlp.stanford.edu/projects/glove/">pre-trained-with-Glove</a> 100-D embedding for each token id. This gives a matrix of size <code>(MAX_QUESTION_LEN, GLOVE_EMBEDDING_DIM)</code>.</li>
<li>
<p>Pass the result through a bidirectional LSTM ‚Äî (apparently) key to current <a href="https://explosion.ai/blog/deep-learning-formula-nlp">state</a>-of-the-<a href="https://www.youtube.com/watch?v=nFCxTtBqF5U">art</a> results in a variety of NLP tasks. This can be broken down as follows:</p>
<ul>
<li>Initialize two matrices of size <code>(MAX_QUESTION_LEN, LSTM_HIDDEN_STATE_SIZE)</code>: <code>forward_matrix</code> and <code>backward_matrix</code>.</li>
<li>Pass the sequence of token ids through an LSTM and return all hidden states. The first hidden state is a function of, i.e. is computed using, the first token id's embedding; place it in the first row of <code>forward_matrix</code>. The second hidden state is a function of the first and second token-id embeddings; place it in the second row of <code>forward_matrix</code>. The third hidden state is a function of the first and second and third token-id embeddings, and so forth.</li>
<li>Do the same thing but pass the sequence to the LSTM in reverse order. Place the first hidden state in the <em>last</em> row of <code>backward_matrix</code>, the second hidden state in the <em>second-to-last</em> row of <code>backward_matrix</code>, etc.</li>
<li>Concatenate <code>forward_matrix</code> and <code>backward_matrix</code> into a single matrix of size <code>(MAX_QUESTION_LEN, 2 * LSTM_HIDDEN_STATE_SIZE)</code>.</li>
</ul>
</li>
<li>
<p><a href="https://keras.io/layers/pooling/#maxpooling1d">Max-pool</a>.</p>
</li>
<li>Flatten.</li>
<li>Dense layer with ReLU activations, down to 10 dimensions.</li>
</ol>
<p>To process the <code>answer</code> we employ the following steps:</p>
<ol>
<li>Dense layer with ReLU activations.</li>
<li>Dense layer with ReLU activations, down to 10 dimensions.</li>
</ol>
<p>Now of equal size, we further process our <code>question</code> and <code>answer</code> with a <em>single</em> set of dense layers ‚Äî the key difference between a SQAM and (the neural-network formulation of) other canonical <code>(user, item)</code> recommendation algorithms. The last of these layers employs <code>tanh</code> activations as suggested in Feng et al. (2015).</p>
<p>Finally, we compute the cosine similarity between the resulting embeddings.</p>
<h2>Prepare questions, answers</h2>
<h3>Import tweets</h3>
<div class="highlight"><pre><span></span><span class="n">tweets_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'data/tweets.csv'</span><span class="p">)[[</span><span class="s1">'text'</span><span class="p">,</span> <span class="s1">'airline_sentiment'</span><span class="p">]]</span>\
    <span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">tweets_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="table-hover table table-striped dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>index</th>
<th>text</th>
<th>airline_sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>2076</td>
<td>@united that's not an apology. Say it.</td>
<td>negative</td>
</tr>
<tr>
<th>1</th>
<td>7534</td>
<td>@JetBlue letting me down in San Fran. No Media...</td>
<td>negative</td>
</tr>
<tr>
<th>2</th>
<td>14441</td>
<td>@AmericanAir where do I look for cabin crew va...</td>
<td>neutral</td>
</tr>
<tr>
<th>3</th>
<td>13130</td>
<td>@AmericanAir just sad that even after spending...</td>
<td>negative</td>
</tr>
<tr>
<th>4</th>
<td>3764</td>
<td>@united What's up with the reduction in E+ on ...</td>
<td>negative</td>
</tr>
</tbody>
</table>
</div>
<h3>Embed answers into 16-D latent space</h3>
<p>Additionally, scale the latent codes; these will be fed to our network as input.</p>
<div class="highlight"><pre><span></span><span class="c1"># embed</span>
<span class="n">sentiment_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">emojis_dict</span><span class="p">[</span><span class="s1">'1f389.png'</span><span class="p">],</span> <span class="n">emojis_dict</span><span class="p">[</span><span class="s1">'1f608.png'</span><span class="p">],</span> <span class="n">emojis_dict</span><span class="p">[</span><span class="s1">'1f621.png'</span><span class="p">]])</span>
<span class="n">sentiment_embeddings</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sentiment_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="c1"># scale</span>
<span class="n">sentiment_embeddings</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">sentiment_embeddings</span><span class="p">)</span>

<span class="c1"># build vectors of correct, incorrect answers</span>
<span class="n">embedding_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'positive'</span><span class="p">:</span> <span class="n">sentiment_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">'neutral'</span><span class="p">:</span> <span class="n">sentiment_embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">'negative'</span><span class="p">:</span> <span class="n">sentiment_embeddings</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">incorrect_answers</span><span class="p">,</span> <span class="n">correct_answers</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">sentiments</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span> <span class="n">embedding_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="p">)</span>
<span class="k">for</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="n">tweets_df</span><span class="p">[</span><span class="s1">'airline_sentiment'</span><span class="p">]:</span>
    <span class="n">correct_answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">embedding_map</span><span class="p">[</span><span class="n">sentiment</span><span class="p">]</span> <span class="p">)</span>
    <span class="n">incorrect_sentiment</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sentiments</span> <span class="o">-</span> <span class="p">{</span><span class="n">sentiment</span><span class="p">},</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">incorrect_answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">embedding_map</span><span class="p">[</span><span class="n">incorrect_sentiment</span><span class="p">]</span> <span class="p">)</span>


<span class="n">questions</span> <span class="o">=</span> <span class="n">tweets_df</span><span class="p">[</span><span class="s1">'text'</span><span class="p">]</span>
<span class="n">correct_answers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">correct_answers</span><span class="p">)</span>
<span class="n">incorrect_answers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">incorrect_answers</span><span class="p">)</span>
</pre></div>
<p>We've now built (only) one <code>(question, correct_answer, incorrect_answer)</code> training triplet for each ground-truth <code>(question, correct_answer)</code>. In practice, we should likely have many more, i.e. <code>(question, correct_answer, incorrect_answer_1), (question, correct_answer, incorrect_answer_2), ..., (question, correct_answer, incorrect_answer_n)</code>.</p>
<h3>Construct sequences of token ids</h3>
<div class="highlight"><pre><span></span><span class="n">MAX_QUESTION_LEN</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span>
<span class="n">question_seqs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span>
<span class="n">question_seqs</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">question_seqs</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_QUESTION_LEN</span><span class="p">)</span>
</pre></div>
<h3>Split data into train, validation sets</h3>
<p>NB: We don't actually have <code>y</code> values: we pass <code>(question, correct_answer, incorrect_answer)</code> triplets to our network and try to minimize <code>max(0, margin - correct_cos_sim + incorrect_cos_sim)</code>. Notwithstanding, Keras requires that we pass both <code>x</code> and <code>y</code> (as Numpy arrays); we pass the latter as a vector of 0's.</p>
<div class="highlight"><pre><span></span><span class="n">train_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span><span class="n">question_seqs</span><span class="p">)</span> <span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.8</span>

<span class="n">questions_train</span> <span class="o">=</span> <span class="n">question_seqs</span><span class="p">[</span><span class="n">train_mask</span><span class="p">]</span>
<span class="n">correct_answers_train</span> <span class="o">=</span> <span class="n">correct_answers</span><span class="p">[</span><span class="n">train_mask</span><span class="p">]</span>
<span class="n">incorrect_answers_train</span> <span class="o">=</span> <span class="n">incorrect_answers</span><span class="p">[</span><span class="n">train_mask</span><span class="p">]</span>

<span class="n">questions_val</span> <span class="o">=</span> <span class="n">question_seqs</span><span class="p">[</span><span class="o">~</span><span class="n">train_mask</span><span class="p">]</span>
<span class="n">correct_answers_val</span> <span class="o">=</span> <span class="n">correct_answers</span><span class="p">[</span><span class="o">~</span><span class="n">train_mask</span><span class="p">]</span>
<span class="n">incorrect_answers_val</span> <span class="o">=</span> <span class="n">incorrect_answers</span><span class="p">[</span><span class="o">~</span><span class="n">train_mask</span><span class="p">]</span>

<span class="n">y_train_dummy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">questions_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_val_dummy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">questions_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
<div class="highlight"><pre><span></span>Dataset sizes:
    questions_train:         (4079, 20)
    correct_answers_train:   (4079, 16)
    incorrect_answers_train: (4079, 16)
    questions_val:           (921, 20)
    correct_answers_val:     (921, 16)
    incorrect_answers_val:   (921, 16)
</pre></div>
<h3>Build embedding layer from Glove vectors</h3>
<div class="highlight"><pre><span></span><span class="n">GLOVE_EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># 1. Load Glove embeddings</span>

<span class="c1"># 2. Build embeddings matrix</span>

<span class="c1"># 3. Build Keras embedding layer</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="o">=</span><span class="n">GLOVE_EMBEDDING_DIM</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span>
    <span class="n">input_length</span><span class="o">=</span><span class="n">MAX_QUESTION_LEN</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</pre></div>
<h3>Build Siamese question-answer model (SQAM)</h3>
<p>GDGS architecture, ‚úåÔ∏è.</p>
<div class="highlight"><pre><span></span><span class="n">LSTM_HIDDEN_STATE_SIZE</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># question</span>
<span class="n">question</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_QUESTION_LEN</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>

<span class="n">question_embedding</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">biLSTM</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">LSTM_HIDDEN_STATE_SIZE</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))(</span><span class="n">question_embedding</span><span class="p">)</span>
<span class="n">max_pool</span> <span class="o">=</span> <span class="n">MaxPool1D</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">biLSTM</span><span class="p">)</span>
<span class="n">flat</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">max_pool</span><span class="p">)</span>
<span class="n">dense_question</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">flat</span><span class="p">)</span>

<span class="c1"># answer</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">EMBEDDING_SIZE</span><span class="p">,))</span>
<span class="n">dense_answer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">answer</span><span class="p">)</span>
<span class="n">dense_answer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">answer</span><span class="p">)</span>

<span class="c1"># combine</span>
<span class="n">shared_dense_1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)</span>
<span class="n">shared_dense_2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)</span>
<span class="n">shared_dense_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">)</span>

<span class="n">dense_answer</span> <span class="o">=</span> <span class="n">shared_dense_1</span><span class="p">(</span><span class="n">dense_answer</span><span class="p">)</span>
<span class="n">dense_question</span> <span class="o">=</span> <span class="n">shared_dense_1</span><span class="p">(</span><span class="n">dense_question</span><span class="p">)</span>

<span class="n">dense_answer</span> <span class="o">=</span> <span class="n">shared_dense_2</span><span class="p">(</span><span class="n">dense_answer</span><span class="p">)</span>
<span class="n">dense_question</span> <span class="o">=</span> <span class="n">shared_dense_2</span><span class="p">(</span><span class="n">dense_question</span><span class="p">)</span>

<span class="n">dense_answer</span> <span class="o">=</span> <span class="n">shared_dense_3</span><span class="p">(</span><span class="n">dense_answer</span><span class="p">)</span>
<span class="n">dense_question</span> <span class="o">=</span> <span class="n">shared_dense_3</span><span class="p">(</span><span class="n">dense_question</span><span class="p">)</span>

<span class="c1"># compute cosine sim, a normalized dot product</span>
<span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">dot</span><span class="p">([</span><span class="n">dense_question</span><span class="p">,</span> <span class="n">dense_answer</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">axes</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># model</span>
<span class="n">qa_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">],</span> <span class="p">[</span><span class="n">cosine_sim</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'qa_model'</span><span class="p">)</span>
</pre></div>
<p><img alt="qa model" class="img-responsive" src="https://cavaunpeu.github.io/figures/qa_model.png"/></p>
<h3>Build contrastive model</h3>
<p>Two Siamese networks, trained jointly so as to minimize the hinge loss of their respective outputs.</p>
<div class="highlight"><pre><span></span><span class="c1"># contrastive model</span>
<span class="n">correct_answer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">EMBEDDING_SIZE</span><span class="p">,))</span>
<span class="n">incorrect_answer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">EMBEDDING_SIZE</span><span class="p">,))</span>
<span class="n">correct_cos_sim</span> <span class="o">=</span> <span class="n">qa_model</span><span class="p">([</span><span class="n">question</span><span class="p">,</span> <span class="n">correct_answer</span><span class="p">])</span>
<span class="n">incorrect_cos_sim</span> <span class="o">=</span> <span class="n">qa_model</span><span class="p">([</span><span class="n">question</span><span class="p">,</span> <span class="n">incorrect_answer</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">cos_sims</span><span class="p">,</span> <span class="n">margin</span><span class="o">=.</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">incorrect</span> <span class="o">=</span> <span class="n">cos_sims</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">margin</span> <span class="o">-</span> <span class="n">correct</span> <span class="o">+</span> <span class="n">incorrect</span><span class="p">)</span>

<span class="n">contrastive_loss</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="n">hinge_loss</span><span class="p">)([</span><span class="n">correct_cos_sim</span><span class="p">,</span> <span class="n">incorrect_cos_sim</span><span class="p">])</span>

<span class="c1"># model</span>
<span class="n">contrastive_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">question</span><span class="p">,</span> <span class="n">correct_answer</span><span class="p">,</span> <span class="n">incorrect_answer</span><span class="p">],</span> <span class="p">[</span><span class="n">contrastive_loss</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'contrastive_model'</span><span class="p">)</span>
</pre></div>
<h3>Build prediction model</h3>
<p>This is what we'll use to compute the cosine similarity of novel <code>(question, answer)</code> pairs.</p>
<div class="highlight"><pre><span></span><span class="n">prediction_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">],</span> <span class="n">qa_model</span><span class="p">([</span><span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'prediction_model'</span><span class="p">)</span>
</pre></div>
<h3>Fit contrastive model</h3>
<p>Fitting <code>contrastive_model</code> will implicitly fit <code>prediction_model</code> as well, so long as the latter has been compiled.</p>
<div class="highlight"><pre><span></span><span class="c1"># compile</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="o">.</span><span class="mo">001</span><span class="p">,</span> <span class="n">clipnorm</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="n">contrastive_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="k">lambda</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">prediction_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="k">lambda</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="c1"># fit</span>
<span class="n">contrastive_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">questions_train</span><span class="p">,</span> <span class="n">correct_answers_train</span><span class="p">,</span> <span class="n">incorrect_answers_train</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y_train_dummy</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">questions_val</span><span class="p">,</span> <span class="n">correct_answers_val</span><span class="p">,</span> <span class="n">incorrect_answers_val</span><span class="p">],</span> <span class="n">y_val_dummy</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>Train on 4089 samples, validate on 911 samples
Epoch 1/3
4089/4089 [==============================] - 18s - loss: 0.1069 - val_loss: 0.0929
Epoch 2/3
4089/4089 [==============================] - 14s - loss: 0.0796 - val_loss: 0.0822
Epoch 3/3
4089/4089 [==============================] - 14s - loss: 0.0675 - val_loss: 0.0828
</pre></div>
<h3>Predict on new tweets</h3>
<blockquote>
<ol>
<li>
<p>"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous"</p>
</li>
<li>
<p>"@usairways I've called for 3 days and can't get thru. is there some secret method i can use that doesn't result in you hanging up on me?"</p>
</li>
<li>
<p>"@AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!"</p>
</li>
</ol>
</blockquote>
<div class="highlight"><pre><span></span><span class="n">new_questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous"</span><span class="p">,</span>
    <span class="s2">"@usairways I've called for 3 days and can't get thru. is there some secret method i can use that doesn't result in you hanging up on me?"</span><span class="p">,</span>
    <span class="s2">"@AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!"</span>
<span class="p">]</span>

<span class="n">new_questions_seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">new_questions</span><span class="p">)</span>
<span class="n">new_questions_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">new_questions_seq</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_QUESTION_LEN</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">n_questions</span><span class="p">,</span> <span class="n">n_sentiments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_questions_seq</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentiment_embeddings</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">new_questions_seq</span><span class="p">,</span> <span class="n">repeats</span><span class="o">=</span><span class="n">n_sentiments</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">sentiment_embeddings</span><span class="p">,</span> <span class="p">(</span><span class="n">n_questions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">prediction_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">a</span><span class="p">])</span>
</pre></div>
<h3>Tweet #1</h3>
<div class="highlight"><pre><span></span><span class="n">positive_pred</span><span class="p">,</span> <span class="n">neutral_pred</span><span class="p">,</span> <span class="n">negative_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Predictions:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üéâ (Positive): {positive_pred[0]:0.5}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üòà (Neutral) : {neutral_pred[0]:0.5}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üò° (Negative): {negative_pred[0]:0.5}'</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>Predictions:
    üéâ (Positive): 0.51141
    üòà (Neutral) : 0.56273
    üò° (Negative): 0.9728
</pre></div>
<h3>Tweet #2</h3>
<div class="highlight"><pre><span></span><span class="n">positive_pred</span><span class="p">,</span> <span class="n">neutral_pred</span><span class="p">,</span> <span class="n">negative_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Predictions:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üéâ (Positive): {positive_pred[0]:0.5}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üòà (Neutral) : {neutral_pred[0]:0.5}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üò° (Negative): {negative_pred[0]:0.5}'</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>Predictions:
    üéâ (Positive): 0.41422
    üòà (Neutral) : 0.61587
    üò° (Negative): 0.99161
</pre></div>
<h3>Tweet #3</h3>
<div class="highlight"><pre><span></span><span class="n">positive_pred</span><span class="p">,</span> <span class="n">neutral_pred</span><span class="p">,</span> <span class="n">negative_pred</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">6</span><span class="p">:</span><span class="mi">9</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Predictions:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üéâ (Positive): {positive_pred[0]:0.5}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üòà (Neutral) : {neutral_pred[0]:0.5}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'    üò° (Negative): {negative_pred[0]:0.5}'</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span>Predictions:
    üéâ (Positive): 0.87107
    üòà (Neutral) : 0.46741
    üò° (Negative): 0.73435
</pre></div>
<h3>Additionally, we can predict on the full set of emojis</h3>
<p>Some emoji embeddings contain <code>np.inf</code> values, unfortunately. We could likely mitigate this by further tweaking the hyperparameters of our autoencoder.</p>
<div class="highlight"><pre><span></span><span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">emojis</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'{100 * inf_mask.mean():.3}</span><span class="si">% o</span><span class="s1">f values are `np.inf`.'</span><span class="p">)</span>

<span class="n">all_embeddings</span> <span class="o">=</span> <span class="n">all_embeddings</span><span class="p">[</span><span class="o">~</span><span class="n">inf_mask</span><span class="p">]</span>
</pre></div>
<div class="highlight"><pre><span></span>4.15% of values are `np.inf`.
</pre></div>
<div class="highlight"><pre><span></span><span class="n">n_questions</span><span class="p">,</span> <span class="n">n_sentiments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_questions_seq</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">new_questions_seq</span><span class="p">,</span> <span class="n">repeats</span><span class="o">=</span><span class="n">n_sentiments</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">all_embeddings</span><span class="p">,</span> <span class="p">(</span><span class="n">n_questions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">prediction_model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">a</span><span class="p">])</span>
</pre></div>
<h3>Tweet #1</h3>
<div class="highlight"><pre><span></span><span class="n">preds_1</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[:</span><span class="n">n_sentiments</span><span class="p">]</span>
<span class="n">top_5_matches</span> <span class="o">=</span> <span class="n">extract_top_5_argmax</span><span class="p">(</span><span class="n">preds_1</span><span class="p">)</span>
<span class="n">display_top_5_results</span><span class="p">(</span><span class="n">top_5_matches</span><span class="p">)</span>
</pre></div>
<p><img alt="predicted tweets 1" class="img-responsive" src="https://cavaunpeu.github.io/images/predicted_tweets_1.png"/></p>
<h3>Tweet #2</h3>
<div class="highlight"><pre><span></span><span class="n">preds_2</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="n">n_sentiments</span><span class="p">:</span><span class="mi">2</span><span class="o">*</span><span class="n">n_sentiments</span><span class="p">]</span>
<span class="n">top_5_matches</span> <span class="o">=</span> <span class="n">extract_top_5_argmax</span><span class="p">(</span><span class="n">preds_2</span><span class="p">)</span>
<span class="n">display_top_5_results</span><span class="p">(</span><span class="n">top_5_matches</span><span class="p">)</span>
</pre></div>
<p><img alt="predicted tweets 2" class="img-responsive" src="https://cavaunpeu.github.io/images/predicted_tweets_2.png"/></p>
<h3>Tweet #3</h3>
<div class="highlight"><pre><span></span><span class="n">preds_3</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">n_sentiments</span><span class="p">:]</span>
<span class="n">top_5_matches</span> <span class="o">=</span> <span class="n">extract_top_5_argmax</span><span class="p">(</span><span class="n">preds_3</span><span class="p">)</span>
<span class="n">display_top_5_results</span><span class="p">(</span><span class="n">top_5_matches</span><span class="p">)</span>
</pre></div>
<p><img alt="predicted tweets 3" class="img-responsive" src="https://cavaunpeu.github.io/images/predicted_tweets_3.png"/></p>
<p>Not particularly useful. These emojis have 0 notion of sentiment, though: the model is simply predicting on their (pixel-based) latent codes.</p>
<h2>Future work</h2>
<p>In this work, we trained a convolutional variational autoencoder to model the distribution of emojis. Next, we trained a Siamese question-answer model to answer text questions with emoji answers. Finally, we were able to use the latter to predict on novel emojis from the former.</p>
<p>Moving forward, I see a few logical steps:</p>
<ul>
<li>Use emoji embeddings that are conscious of sentiment ‚Äî likely trained via a different network altogether. This way, we could make more meaningful (sentiment-based) predictions on novel emojis.</li>
<li>Predict on emojis generated from the autoencoder.</li>
<li>Add 1-D convolutions to the text side of the SQAM.</li>
<li>Add an <a href="https://www.quora.com/What-is-attention-in-the-context-of-deep-learning">"attention"</a> mechanism ‚Äî the one component missing from the <a href="https://explosion.ai/blog/deep-learning-formula-nlp">"embed, encode, attend, predict"</a> dynamic quartet of modern NLP.</li>
<li>Improve the stability of our autoencoder so as to not produce embeddings containing <code>np.inf</code>.</li>
</ul>
<p>Sincere thanks for reading, and emojis ü§ò.</p>
<h2>Code</h2>
<p>The <a href="https://github.com/cavaunpeu/neurally-embedded-emojis">repository</a> and <a href="http://nbviewer.jupyter.org/github/cavaunpeu/neurally-embedded-emojis/blob/master/neurally-embedded-emojis.ipynb">rendered notebook</a> for this project can be found at their respective links.</p>
<h2>References</h2>
<ul>
<li><a href="http://ben.bolte.cc/blog/2016/language.html">Deep Language Modeling for Question Answering using Keras</a></li>
<li><a href="https://arxiv.org/pdf/1508.01585v2.pdf">Applying Deep Learning To Answer Selection: A Study And An Open Task</a></li>
<li><a href="https://arxiv.org/pdf/1511.04108.pdf">LSTM Based Deep Learning Models For Non-Factoid Answer Selection</a></li>
<li><a href="https://explosion.ai/blog/deep-learning-formula-nlp">Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models</a></li>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py">Keras Examples - Convolutional Variational Autoencoder</a></li>
<li><a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Introducing Variational Autoencoders (in Prose and Code)</a></li>
<li><a href="http://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html">Under the Hood of the Variational Autoencoder (in Prose and Code)</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="willwolf_">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'willwolf';
    var disqus_title = 'Neurally Embedded Emojis';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</div> </div>
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1" id="footer-wrapper">
				<div id="social-links">
					<h4>
						Social:
						<a href="http://twitter.com/willwolf_"><i class="fa fa-twitter" aria-hidden="true"></i></a>
						<a href="http://github.com/cavaunpeu"><i class="fa fa-github" aria-hidden="true"></i></a>
						<a href="http://linkedin.com/in/williamabrwolf"><i class="fa fa-linkedin-square" aria-hidden="true"></i></a>
						<a href="mailto:williamabrwolf@gmail.com"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>
					</h4>
				</div>
				<div id="travel-blog">
					<h4>
						Links:
						<a href="http://willtravellife.com">Travel Blog</a>, <a href="https://github.com/cavaunpeu/willwolf.io-source">Source Code</a>
					</h4>
				</div>
			</div>
		</div>
	</div>
</footer><div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Will Wolf 2017</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div><!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-97412095-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>