<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>will wolf</title>
	<meta name="description" content="data science things and thoughts on the world">
	<meta name="author" content="Will Wolf">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Favicon -->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="https://cavaunpeu.github.io/theme/html5.js"></script>
		<![endif]-->

	<!-- Atom Feed -->

	<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@willwolf_">
  <meta name="twitter:creator" content="@willwolf_">
  <meta name="twitter:domain" content="https://cavaunpeu.github.io">
    <meta property="twitter:title" content="So You Want to Implement a Custom Loss Function?"/>
    <meta property="twitter:description" content="Implementing custom loss functions in Python using autograd."/>
    <meta property="twitter:image" content="https://cavaunpeu.github.io/figures/y_equals_1.png"/>

	<!-- CSS -->
	<link href="https://cavaunpeu.github.io/theme/css/ipython.css?v={12345}" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.3.7/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/local.css?v={12345}" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/pygments.css?v={12345}" rel="stylesheet">
	<link href="https://cavaunpeu.github.io/theme/css/main.css?v={12345}" rel="stylesheet">
</head><body>
<div class="container">
<div class="page-header">
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <h1><a id="site-title" href="https://cavaunpeu.github.io/">will wolf</a></h1>
          <h4 id="site-subtitle">data science things and thoughts on the world</h4>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav navbar-right">
              <li><a href="https://cavaunpeu.github.io/about/" title="About"><span class="glyphicon glyphicon-user"></span> About</a></li>
              <li><a href="https://cavaunpeu.github.io/cv/" title="CV"><span class="glyphicon glyphicon-folder-open"></span> CV</a></li>
              <li><a href="https://cavaunpeu.github.io/archive/" title="Archive"><span class="glyphicon glyphicon-th-list"></span> Archive</a></li>
              <li><a href="https://cavaunpeu.github.io/feeds/all.atom.xml" title="willwolf.io Atom feed"><span class="icon-rss"></span> RSS</a></li>
          <li class="dropdown">
            <a class="dropdown-toggle" data-toggle="dropdown" href="https://cavaunpeu.github.io" title=English id="activeLanguage"><span class="glyphicon glyphicon-flag"></span>EN<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li>
                  <a id="inactiveLanguage" href="https://cavaunpeu.github.io/es/" title=EspaÃ±ol>ES</a>
                </li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</div>	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">So You Want to Implement a Custom Loss Function?</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Will Wolf</h4>
		</span>
		<time datetime="2015-11-18T02:12:00-05:00" itemprop="datePublished">November 18, 2015</time>
	</div>
	<div itemprop="articleBody" class="article-body"><p>It's often not so hard.</p>
<p>My venerable boss recently took a trip to Amsterdam. As we live in New York City, he needed to board a plane. Days before, we discussed the asymmetric risk of airport punctuality: "if I get to the gate early, it's really not so bad; if I arrive too late and miss my flight, it really, really sucks." When deciding just when to leave the house - well - machine learning can help.</p>
<p>Let's assume we have a 4-feature dataset, <em>X</em>. In order, each columns denotes: a binary "holiday/no-holiday", a binary "weekend/no-weekend", a normalized continuous value dictating "how does the traffic on my route compare to the average day of traffic on this route", and the number of seats on the plane. Each observation is paired with a response, y, where <span class="math">\(y = 0\)</span> indicates the flight will leave on time, and <span class="math">\(y = 1\)</span> indicates the flight will leave later (meaning we can arrive later ourselves).</p>
<p>As mentioned, missing a plane is bad. You incur stress. You have to book another ticket, which is often inconveniently expensive. You have to eat your next 2 meals in the airport, which is a whole other bag of misery. If we want to train a learning algorithm to predict flight delay - and therefore inform us as to when to leave the house - we'll have to teach it just how bad the Terminal 3 beef and broccoli really is.</p>
<p>To train a supervised learning algorithm, we need a few things.</p>
<ol>
<li><strong>Some training data.</strong></li>
<li><strong>A model.</strong> Let's choose logistic regression. It's simple, deterministic, and interpretable.</li>
<li><strong>A loss function</strong> - also known as a cost function - which quantitatively answers the following: <em>"The real label was 1, but I predicted 0: is that bad?"</em> Answer: <em>"Yeah. That's bad. That's .. 500 bad."</em> Many supervised algorithms come with standard loss functions in tow. SVM likes the <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a>. Logistic regression likes <a href="https://www.kaggle.com/wiki/MultiClassLogLoss">log loss</a>, or <a href="https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function">0-1 loss</a>. Because our loss is asymmetric - an incorrect answer is more bad than a correct answer is good - we're going to create our own.</li>
<li><strong>A way to optimize our loss function.</strong> This is just a fancy way of saying: "Look. We have these 4 factors that advise us as to whether or not that airplane is going to take off on time. And we saw what happened in the past. So what we really want to know is: if it's a holiday, is our plane more or less likely to stay on schedule? How about a weekend? Does the traffic have any impact? What about the size of the plane? Finally, once I know this - once I know how each of my variables relates to the thing I'm trying to predict - I want to keep the real world in mind: 'the plane left on time, but I showed up late: is that bad?'"</li>
</ol>
<p>To do this, we often employ an algorithm called gradient descent (and its variants), which is quick, effective, and easy to understand. Gradient descent requires we take the gradient - or derivative - of our loss function. With most typical loss functions (hinge loss, least squares loss, etc.), we can easily differentiate with a pencil and paper. With more complex loss functions, we often can't. Why not get a computer to do it for us, so we can move onto the fun part of actually fitting our model?</p>
<p>Introducing <a href="https://github.com/HIPS/autograd">autograd</a>.</p>
<p>Autograd is a pure Python library that "efficiently computes derivatives of numpy code" via automatic differentiation. Automatic differentiation exploits the calculus chain rule to differentiate our functions. In fact, for the purposes of this post, the implementation details aren't that important. What is important, though, is how we can use it: with autograd, obtaining the gradient of your custom loss function is as easy as <code>custom_gradient = grad(custom_loss_function)</code>. It's really that simple.</p>
<p>Let's return to our airplane. We have some data - with each column encoding the 4 features described above - and we have our corresponding target. (In addition, we initialize our weights to 0, and define an epsilon with which to clip our predictions in (0, 1)).</p>
<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.3213</span><span class="p">,</span> <span class="mf">0.4856</span><span class="p">,</span> <span class="mf">0.2995</span><span class="p">,</span> <span class="mf">2.5044</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.3005</span><span class="p">,</span> <span class="mf">0.4757</span><span class="p">,</span> <span class="mf">0.2974</span><span class="p">,</span> <span class="mf">2.4691</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.5638</span><span class="p">,</span> <span class="mf">0.8005</span><span class="p">,</span> <span class="mf">0.3381</span><span class="p">,</span> <span class="mf">2.3102</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.5281</span><span class="p">,</span> <span class="mf">0.6542</span><span class="p">,</span> <span class="mf">0.3129</span><span class="p">,</span> <span class="mf">2.1298</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.3221</span><span class="p">,</span> <span class="mf">0.5126</span><span class="p">,</span> <span class="mf">0.3085</span><span class="p">,</span> <span class="mf">2.6147</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.3055</span><span class="p">,</span> <span class="mf">0.4885</span><span class="p">,</span> <span class="mf">0.289</span> <span class="p">,</span> <span class="mf">2.4957</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.3276</span><span class="p">,</span> <span class="mf">0.5185</span><span class="p">,</span> <span class="mf">0.3218</span><span class="p">,</span> <span class="mf">2.6013</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.5313</span><span class="p">,</span> <span class="mf">0.7028</span><span class="p">,</span> <span class="mf">0.3266</span><span class="p">,</span> <span class="mf">2.1543</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.4728</span><span class="p">,</span> <span class="mf">0.6399</span><span class="p">,</span> <span class="mf">0.3062</span><span class="p">,</span> <span class="mf">2.0597</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.3221</span><span class="p">,</span> <span class="mf">0.5126</span><span class="p">,</span> <span class="mf">0.3085</span><span class="p">,</span> <span class="mf">2.6147</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">])</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-15</span>
</pre></div>
<p>We have our model:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">wTx</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
   <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
   <span class="k">return</span> <span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">logistic_predictions</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
   <span class="n">predictions</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">wTx</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
   <span class="k">return</span> <span class="n">predictions</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">eps</span><span class="p">)</span>
</pre></div>
<p>And we define our loss function. Here, I've decided on a variant of log loss, with the latter logarithmic term exponentiated and the sign reversed. In English, it says: "If the flight actually leaves on time, and we predict that it leaves on time (and leave our house early enough), then all is right with the world. However, if we instead predict that it will be delayed (and leave our house 30 minutes later), then we'll be stressed, -$1000, and dining on rubber for the following few hours.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">custom_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y_predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">custom_loss_given_weights</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">y_predicted</span> <span class="o">=</span> <span class="n">logistic_predictions</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">custom_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
</pre></div>
<p>Let's see what this function looks like, for each of <span class="math">\(y = 1\)</span> and <span class="math">\(y = 0\)</span>, and varying values ofÂ y_hat.</p>
<p><img alt="y_equals_1" class="img-responsive" src="https://cavaunpeu.github.io/figures/y_equals_1.png"/></p>
<p><img alt="y_equals_0" class="img-responsive" src="https://cavaunpeu.github.io/figures/y_equals_0.png"/></p>
<p>WhenÂ <span class="math">\(y = 1\)</span>, our cost is that of the typical logarithmic loss. At worst, our cost is <span class="math">\(\approx 5\)</span>. However, whenÂ <span class="math">\(y = 0\)</span>, the dynamic is different: if we guess <span class="math">\(0.2\)</span>, it's not so bad; if we guess <span class="math">\(0.6\)</span> it's not so bad; if we guess <span class="math">\(&gt; 0.8\)</span>, it's really, really bad. <span class="math">\(\approx 25\)</span> bad. Finally, we compute our gradient. As easy as:</p>
<div class="highlight"><pre><span></span><span class="n">gradient</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">custom_loss_given_weights</span><span class="p">)</span>
</pre></div>
<p>And, lastly, apply basic gradient descent:</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">*</span> <span class="o">.</span><span class="mo">01</span>
</pre></div>
<p>That's it. Implementing a custom loss function into your machine learning model with autograd is as easy as "call the <code>grad</code> function."</p>
<p>Why don't people use this more often?</p>
<hr/>
<p>Code:</p>
<p>All code used in this post can be found in a Jupyter <a href="http://nbviewer.ipython.org/github/cavaunpeu/automatic-differentiation/blob/master/automatic_differentiation.ipynb">notebook</a> on myÂ <a href="https://github.com/cavaunpeu">GitHub</a>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="willwolf_">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'willwolf';
    var disqus_title = 'So You Want to Implement a Custom Loss Function?';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</div> </div>
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-10 col-md-offset-1" id="footer-wrapper">
				<div id="social-links">
					<h4>
						Social:
						<a href="http://twitter.com/willwolf_"><i class="fa fa-twitter" aria-hidden="true"></i></a>
						<a href="http://github.com/cavaunpeu"><i class="fa fa-github" aria-hidden="true"></i></a>
						<a href="http://linkedin.com/in/williamabrwolf"><i class="fa fa-linkedin-square" aria-hidden="true"></i></a>
						<a href="mailto:williamabrwolf@gmail.com"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>
					</h4>
				</div>
				<div id="travel-blog">
					<h4>
						Links:
						<a href="http://willtravellife.com">Travel Blog</a>, <a href="https://github.com/cavaunpeu/willwolf.io-source">Source Code</a>
					</h4>
				</div>
			</div>
		</div>
	</div>
</footer><div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Will Wolf 2017</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div><!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-97412095-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>