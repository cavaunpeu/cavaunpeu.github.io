<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>will wolf</title><link href="https://willwolf.io/es/" rel="alternate"></link><link href="https://willwolf.io/feeds/all.atom.xml" rel="self"></link><id>https://willwolf.io/es/</id><updated>2017-03-22T19:56:00-04:00</updated><entry><title>Docker y Kaggle con Enrique y Beto</title><link href="https://willwolf.io/es/2017/03/22/docker-y-kaggle-con-enrique-y-beto/" rel="alternate"></link><published>2017-03-22T19:56:00-04:00</published><updated>2017-03-22T19:56:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-03-22:/es/2017/03/22/docker-y-kaggle-con-enrique-y-beto/</id><summary type="html">&lt;p&gt;Este post tiene como objetivo familiarizarlos con lo que es Docker, por qué y cómo usarlo para Kaggle.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Este post tiene como objetivo familiarizarlos con lo que es Docker, por qué y cómo usarlo para Kaggle. Para hacer las cosas más simples, hablaremos principalmente de Plaza Sésamo y pasteles en lugar de computadoras y datos.&lt;/p&gt;
&lt;p&gt;Una mañana de lunes, Enrique sale de debajo de su cobija rayada, pone los dos pies en el piso y abre la ventana de su cuarto. Echa un vistazo hacia la metrópoli llena de galletas y muñecos de peluche, endereza el cuello de su suéter color banana, suelta un fuerte bostezo matutino y exclama: "Hoy, voy a preparar pasteles para mi querido compañero Beto."&lt;/p&gt;
&lt;p&gt;&lt;img alt="ernie and bert" src="https://willwolf.io/es/../images/ernie_and_bert.png"&gt;&lt;/p&gt;
&lt;p&gt;Por mala suerte, Enrique nunca ha hecho pasteles antes. ¡Pero no importa! Se precipita con prisa hacia la cocina, toma un libro de cocina, organiza los ingredientes y prende su hornito Easy-Bake. "Lo pongo a prueba igual. Prepararé el mejor pastel jamás hecho por todos los muñecos de peluche. Y una vez que el resultado me complazca, haré 50 más," grita.&lt;/p&gt;
&lt;p&gt;Horas después, su trabajo termina: su pastel - tres pisos de "mini-pasteles" sabor arándano, fresa y tocino - es simplemente la mejor cosa que haya probado alguna vez en su vida. "¡Mucho mejor que cualquier cosa que ese fraude Cookie Monster haya probado jamás!" dice. Emocionado, Enrique toma una pausa en su cocina ya demasiada sucia para admirar el resultado. Piensa en Beto y se pregunta qué tan rápido se puede entregar el regalo. "Ya que he horneado el pastel perfecto, solo me toca hacer 50 más. ¿Debe ser fácil, verdad?"&lt;/p&gt;
&lt;p&gt;Enrique se da vuelta para mirar su Easy-Bake. "Pues, esa vaina solo cocina un pastel a la vez. ¡Tardaría días en hacer 50 a ese ritmo!" Aún de buen humor, corre a la panadería del pueblo para pedir prestado su horno - ese más grande que el suyo. Se lo dan en seguida y Enrique se pone a cocinar.&lt;/p&gt;
&lt;p&gt;Desafortunadamente, mientras está mezclando los ingredientes empieza a tener problemas con las herramientas de la panadería. El mezclador eléctrico se rompe. El cuchillo no corta las fresas de la manera correcta. Las tazas de medir tienen tamaños sutilmente distintos. Enrique se estresa. Pensaba que estaba al punto de terminar, pero se da cuenta de repente de que en realidad acaba de empezar. Aunque vino con la receta exacta en mano, se nota que ahora está usando herramientas diferentes en una cocina extraña, en un ambiente nuevo. "¿No puede un muñeco de peluche cocinar un solo pastel en su horno pequeño, luego traer la receta y los ingredientes a un horno más grande y ahí fabricarlos rápidamente a escala masiva? ¿Por qué tiene que ser tan complicado esto?"&lt;/p&gt;
&lt;h2&gt;Presentando Docker&lt;/h2&gt;
&lt;p&gt;Con tristeza y desesperación, Enrique camina al puerto para aclarar su mente. Allí, se encuentra con cientos de contenedores azules y blancos del tamaño de camionetas y se le ocurre una idea divertida: "¿Qué tal si cocino allí? Moveré todas mis herramientas dentro del contenedor - la tabla de cortar, el cuchillo, el mezclador, los utensilios - y escribiré la receta en el muro interior. La única cosa que faltará sería el horno, pero eso se obtiene en todas partes. Así, usando el horno en mi casa, puedo continuar horneando un pastel a la vez como siempre; por el contrario, usando el horno de la panadería puedo cocinar de capacidad aumentada. Listo. Enrique agarra el primer contenedor que ve y corre a casa para llenarlo de pastel."&lt;/p&gt;
&lt;p&gt;Después de escribir la receta en el muro interior del contenedor, Enrique se da cuenta de que lo que quiere traer a la panadería tiene que ser ligero. Si no, ¡no lo podrá llevar! Por lo tanto, en lugar de físicamente llevar sus herramientas - el cuchillo, el mezclador, etc. - simplemente escribe los nombres y números de estos productos además de instrucciones para adquirirlos por fuera. De la misma manera, en lugar de encerrar los ingredientes mismos, espera que estén disponibles en la panadería una vez que llegue. Así, cuando la receta diga "echa 3 cucharadas de azúcar del gabinete," el azúcar ya estará puesto en el gabinete mismo.&lt;/p&gt;
&lt;h2&gt;Presentando Kaggle&lt;/h2&gt;
&lt;p&gt;Hornear pasteles en Plaza Sésamo es una metáfora de construir modelos para Kaggle. Típicamente, construimos prototipos sencillos en nuestra entorno local y luego alquilamos una máquina más poderosa ubicada en alguna granja en Virginia para hacer el trabajo pesado en el sentido computacional. En las competencias de Kaggle, el problema inicial de Enrique es demasiado común: aún después de lograr conseguir un mezclador eléctrico, tazas de medir, etc. que se parecen a los suyos - esto es, aún después de instalar todas aquellas librerías en el servidor remoto que teníamos en el local - los entornos aún no eran idénticos y algunos problemas surgen en seguida. Los contenedores de Docker resuelven este problema: si podemos lograr hornear pasteles una sola vez en nuestra cocina, podemos así rehacerlos de manera determinista &lt;em&gt;n&lt;/em&gt; veces en cualquier cocina de fuera - y preferiblemente en una con un horno mucho más poderoso que el nuestro.&lt;/p&gt;
&lt;h2&gt;Con ustedes, los servidores remotos&lt;/h2&gt;
&lt;p&gt;Un servidor remoto es la panadería: es una computadora como la nuestra, pero que puede procesar datos más rápido y en cantidades más grandes. En otras palabras, es una cocina con un horno más grande.&lt;/p&gt;
&lt;h2&gt;Utensilios de cocina y los ingredientes&lt;/h2&gt;
&lt;p&gt;En lugar de incluir utensilios de cocina en nuestro contenedor simplemente pormenorizamos cuáles necesitamos y cómo adquirirlos. Para una competencia de Kaggle, esto es igual a instalar las librerías - pandas, scikit-learn, etc. - necesarias para la tarea a mano. Una vez más, no tenemos que incluir estas librerías en nuestro contenedor, sino disponer de instrucciones para dónde y cómo instalarlas. En la práctica, esto se manifiesta como un &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; en nuestro &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;En lugar de incluir los ingredientes en nuestro contenedor asumimos que estarán disponibles en la panadería anfitriona. Esto es un poco más complicado de lo que suena por las siguientes razones:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Nuestra panadería anfitriona está a un par de cuadras de nuestra casa. Si queremos que estén disponibles los ingredientes en esa panadería, tenemos que traerlos ahí físicamente de alguna manera.&lt;/li&gt;
&lt;li&gt;Aún después de traerlos físicamente a la panadería, el cocinar que resulte dentro del contenedor estará aislado del resto de la panadería misma: la única cosa con la que se conecta exteriormente es su horno.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Para una competencia de Kaggle, ¿cómo hacemos que los datos locales sean utilizables &lt;em&gt;dentro del contenedor, hospedado en un servidor remoto&lt;/em&gt;?&lt;/p&gt;
&lt;h3&gt;Los "Docker Volumes"&lt;/h3&gt;
&lt;p&gt;Los "Docker Volumes" permiten que datos sean compartidos entre una carpeta dentro de un contenedor y una carpeta en el sistema de archivo local del servidor hospedando ese contenedor. Esto es igual a lo que ocurre cuando:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Enrique trae sus ingredientes a la panadería, junto con (pero no dentro de) su contenedor.&lt;/li&gt;
&lt;li&gt;Al llegar, pone un tarro de azúcar en un cubo azul en la esquina de la sala.&lt;/li&gt;
&lt;li&gt;Se estipula que, al comenzar a hornear dentro del contenedor de la la panadería, los ingredientes se deberían compartir entre el cubo azul en la esquina de la sala y el gabinete. Así, cuando la receta diga "agarra un tarro de azúcar del gabinete," Enrique puede extender la mano hacia el gabinete dentro del contenedor y recuperar el tarro de azúcar del cubo azul en la esquina de la panadería. Recuerden: el contenedor no vino con ningún ingrediente empacado por dentro; el gabinete hubiera estado vacío por la misma razón.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Trayendo el contenedor a la panadería es igual a un sencillo &lt;code&gt;docker run&lt;/code&gt; con el servidor remoto como el &lt;code&gt;docker-machine&lt;/code&gt;. Trayendo los ingredientes a la panadería, esto es colocando datos en el sistema de archivo local del servidor remoto, es mucho menos "sexy." En el sentido más simple, es igual a usar &lt;code&gt;scp&lt;/code&gt; or &lt;code&gt;rsync&lt;/code&gt; para transferir un archivo del entorno local al servidor remoto hasta usar &lt;code&gt;curl&lt;/code&gt; para descargar un archivo directamente en el servidor remoto mismo.&lt;/p&gt;
&lt;p&gt;En la práctica, esto se ve generalmente así:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;docker&lt;/span&gt;
&lt;span class="x"&gt;    --tlsverify&lt;/span&gt;
&lt;span class="x"&gt;    --tlscacert=&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HOME&lt;/span&gt;&lt;span class="x"&gt;/.docker/machine/certs/ca.pem&amp;quot;&lt;/span&gt;
&lt;span class="x"&gt;    --tlscert=&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HOME&lt;/span&gt;&lt;span class="x"&gt;/.docker/machine/certs/cert.pem&amp;quot;&lt;/span&gt;
&lt;span class="x"&gt;    --tlskey=&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HOME&lt;/span&gt;&lt;span class="x"&gt;/.docker/machine/certs/key.pem&amp;quot; -H=tcp://12.34.56:78&lt;/span&gt;
&lt;span class="x"&gt;run&lt;/span&gt;
&lt;span class="x"&gt;    --rm&lt;/span&gt;
&lt;span class="x"&gt;    -i&lt;/span&gt;
&lt;span class="x"&gt;    -v&lt;/span&gt;
&lt;span class="x"&gt;    /data:/data kaggle-contest build_model.sh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Utensilios de cocina que no se compran en la tienda&lt;/h2&gt;
&lt;p&gt;Para hacer su pastel, Enrique usó una tabla de cortar única en el mundo que Beto hizo a mano para él. ¿Cómo puede usarla en la panadería? En términos de Kaggle: ¿cómo puedo usar una librería en mi proyecto que no está disponible en un repositorio de paquetes público (una que construí yo mismo)?&lt;/p&gt;
&lt;p&gt;Para esto, no existe una "fórmula secreta." Con la tabla de cortar/librería, podemos:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Incluirla en el contenedor y aguantar el peso extra.&lt;/li&gt;
&lt;li&gt;Tratarla como ingrediente, traerla a la panadería y accederla vía un "Docker Volume."&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Feliz cocinada&lt;/h2&gt;
&lt;p&gt;Trasladar tu entorno local al interior de un contenedor de Docker, y/o "Dockerizar" este entorno una vez que estés listo para usar un servidor remoto para hacer el trabajo pesado, asegurará que solo tendrás que averiguar cómo hacer el pastel una sola vez. Haz tus prototipos localmente, luego enviarlos sin estrés a la panadería para la producción de escala masiva.&lt;/p&gt;
&lt;p&gt;¡Muy buen provecho!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Recursos adicionales:&lt;/p&gt;
&lt;p&gt;Aquí dejamos dos recursos adicionales que creo útiles para aprender
sobre Docker para Kaggle:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://speakerdeck.com/smly/workflow-serialization-and-docker-for-kaggle"&gt;Workflow, Serialization &amp;amp; Docker for Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2016/02/05/how-to-get-started-with-data-science-in-containers/"&gt;How to get started with data science in containers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content></entry><entry><title>Docker and Kaggle with Ernie and Bert</title><link href="https://willwolf.io/es/../2016/11/22/docker-and-kaggle-with-ernie-and-bert/" rel="alternate"></link><published>2016-11-22T13:39:00-05:00</published><updated>2016-11-22T13:39:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2016-11-22:/es/../2016/11/22/docker-and-kaggle-with-ernie-and-bert/</id><summary type="html">&lt;p&gt;An introduction to what Docker is and why and how to use it for Kaggle.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is meant to serve as an introduction to what Docker is and why and how to use it for Kaggle. For simplicity, we will primarily speak about Sesame Street and cupcakes in lieu of computers and data.&lt;/p&gt;
&lt;p&gt;One Monday morning, Ernie from the 'Street climbs out from under his red-and-blue pinstriped covers, puts both feet on the ground and opens his bedroom window. He stares out into a bustling metropolis of cookies and fur, straightens his banana-yellow turtleneck, lets out a deep, vigorous, crescent-shaped morning yawn and exclaims aloud: "Today, I'm going to make cupcakes for my dear friend Bert."&lt;/p&gt;
&lt;p&gt;&lt;img alt="ernie and bert" src="https://willwolf.io/es/../images/ernie_and_bert.png"&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, Ernie has never made cupcakes before. But no matter! He darts hastily to the kitchen, pulls out a cookbook, organizes the ingredients and turns on his small Easy-Bake oven. "I'll experiment here. I'll make the greatest cupcake known to all stuffed-animal-kind. And when I'm happy with the result, I'll make 50 more," he shouts.&lt;/p&gt;
&lt;p&gt;Hours later, Ernie's work is done: his cupcake - a 3-story stack of blueberry, strawberry and bacon-flavored sub-cakes - is the single best thing he's ever tasted. Far better than anything that fraud Cookie Monster had ever tried! Ernie is thrilled, and sits back in his now-filthy kitchen to admire the result. He thinks to Bert, and wonders how just quickly he can deliver his gift. "Now that I've baked the perfect cupcake, I'll just need to bake 50 more. This shouldn't be that hard. Right?"&lt;/p&gt;
&lt;p&gt;Ernie spins around to look at this Easy-Bake. "Well, that thing only bakes one cupcake at a time. At that rate, 50 would take me days!" Spirits still high, he runs to the local bakery and asks to use their oven - this one much larger. They happily oblige, and Ernie starts baking right away.&lt;/p&gt;
&lt;p&gt;Unfortunately, as he's mixing the ingredients he starts to have problems. The electric mixer breaks. The knife doesn't quite cut the strawberries in just the right way. The measuring cups have an ever-so-slightly different size. Ernie starts to stress. He thought he was at the finish line, but now realizes that he's really just at the start. While Ernie came equipped with the recipe to bake the cupcakes, he notes that he's now using all new tools in a completely new kitchen under completely different circumstances. "Can't a stuffed animal just bake a single cupcake in his small oven, then bring the recipe and ingredients to a bigger oven and bake a bunch more? Why does this need to be so complicated?"&lt;/p&gt;
&lt;h2&gt;Enter Docker&lt;/h2&gt;
&lt;p&gt;In sadness and despair, Ernie wanders to the seaport to clear his mind. There, he comes across hundreds of blue and white, SUV-sized shipping containers and gets a funny idea: "What if I did my baking in there? I'll move all of my tools inside - the cutting board, the knife, the mixer, the utensils - and write the recipe on the inner wall. The only thing missing will be the oven, but I can get that anywhere. That way, using the oven &lt;em&gt;chez moi,&lt;/em&gt; I can continue to bake one cupcake at a time; conversely, using the oven at the bakery I can bake a whole lot more. Perfect. Ernie grabs the first container he sees and races home to pack it full.&lt;/p&gt;
&lt;p&gt;After writing the ingredients on the container's inner wall, Ernie realizes that if he's going to bring this container to the bakery, it better be light. If not, he won't be able to carry it! Therefore, instead of actually including his tools - the knife, the mixer, etc. - he simply writes down the names and numbers of these products and instructions as to where they can be acquired. Similarly, instead of including the actual ingredients for the cupcakes, he expects them to be available at the bakery itself. Then, when the recipe says "take 3 tablespoons of sugar from the cupboard," that sugar will have already been placed in the cupboard itself.&lt;/p&gt;
&lt;h2&gt;Enter Kaggle&lt;/h2&gt;
&lt;p&gt;Baking cupcakes on Sesame Street is a metaphor for building models for Kaggle. Typically, we build small prototypes on our local machine, then temporarily rent a more powerful machine sitting on a farm somewhere in Virginia to do the heavy lifting. In Kaggle competitions, Ernie's initial problem is all too common: even after finding an electric mixer, measuring cups, etc. comparable to his own - i.e. even after installing all those libraries on our remote machine that we had on our local - the environments still weren't quite the same and problems therein arose. Docker containers solve this problem: if we can bake our cupcake once in our kitchen, we can deterministically re-bake it &lt;em&gt;n&lt;/em&gt; times in any kitchen - and preferably one with an oven much more powerful than our own.&lt;/p&gt;
&lt;h2&gt;Enter remote instances&lt;/h2&gt;
&lt;p&gt;A remote instance is the bakery: it is a computer, like ours, that can process data faster and in larger quantities. In other words, it is a kitchen with a much bigger oven.&lt;/p&gt;
&lt;h2&gt;Cooking utensils and ingredients&lt;/h2&gt;
&lt;p&gt;In lieu of including cooking utensils in our container we merely specify which utensils we need and how to acquire them. For a Kaggle competition, this is akin to installing the libraries - pandas, scikit-learn, etc. - necessary for the task at hand. Once more, we do not include these libraries in our container, but instead provide instructions as to where and how to install them. In practice, this often looks like a &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; in our &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In lieu of including ingredients in our container we merely assume they'll be available in our host bakery. This is a bit trickier than it sounds for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our host bakery is several blocks from our home. If we want ingredients to be available in that bakery, we're going to need to physically carry them there in some sense. 2. Even after physically bringing ingredients to the bakery, they still won't be immediately available inside the container. Remember, after bringing our container to the bakery, the cooking that transpires within the container is isolated from the rest of the bakery itself; it interfaces only with the bakery's oven.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a Kaggle competition, how do we make local data available &lt;em&gt;within the container&lt;/em&gt;, &lt;em&gt;on a remote machine?&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Docker Volumes&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://boxboat.com/2016/06/18/docker-data-containers-and-named-volumes/"&gt;Docker Volumes&lt;/a&gt; allow data to be shared between a directory inside of a container and a directory in the local file system of the machine hosting that container. This is akin to Ernie:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Carrying his ingredients to the bakery, along with (but not inside) his container.&lt;/li&gt;
&lt;li&gt;Upon arrival, placing a jar of sugar in a blue bucket in the corner of the room.&lt;/li&gt;
&lt;li&gt;Stipulating that, upon beginning to bake inside of the container at the bakery, ingredients should be shared between the blue bucket in the corner of the room and the cupboard. That way, when the recipe says "get a jar of sugar from the cupboard," Ernie can reach into the cupboard inside of the container and retrieve the jar of sugar &lt;em&gt;from the blue bucket sitting in the corner of the bakery.&lt;/em&gt; Remember: the container did not ship with any ingredients inside; the cupboard, therefore, would have itself been empty.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Carrying the container to the bakery is akin to a simple &lt;code&gt;docker run&lt;/code&gt; onto the remote machine. Carrying ingredients to the bakery, i.e. placing a data file on the local file system of the remote machine, is much less sexy. In the simplest sense, this is akin to using &lt;code&gt;scp&lt;/code&gt; or &lt;code&gt;rsync&lt;/code&gt; to transfer a file from the local machine to the remote, or even using &lt;code&gt;curl&lt;/code&gt; to download a file directly onto the remote machine itself.&lt;/p&gt;
&lt;p&gt;In practice, this often looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;docker&lt;/span&gt;
&lt;span class="x"&gt;    --tlsverify&lt;/span&gt;
&lt;span class="x"&gt;    --tlscacert=&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HOME&lt;/span&gt;&lt;span class="x"&gt;/.docker/machine/certs/ca.pem&amp;quot;&lt;/span&gt;
&lt;span class="x"&gt;    --tlscert=&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HOME&lt;/span&gt;&lt;span class="x"&gt;/.docker/machine/certs/cert.pem&amp;quot;&lt;/span&gt;
&lt;span class="x"&gt;    --tlskey=&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HOME&lt;/span&gt;&lt;span class="x"&gt;/.docker/machine/certs/key.pem&amp;quot; -H=tcp://12.34.56:78&lt;/span&gt;
&lt;span class="x"&gt;run&lt;/span&gt;
&lt;span class="x"&gt;    --rm&lt;/span&gt;
&lt;span class="x"&gt;    -i&lt;/span&gt;
&lt;span class="x"&gt;    -v&lt;/span&gt;
&lt;span class="x"&gt;    /data:/data kaggle-contest build_model.sh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Cooking tools that you can't buy at the store&lt;/h2&gt;
&lt;p&gt;To bake his cupcake, Ernie used a one-of-a-kind cutting board that Bert had hand-molded for him. How can he use this at the bakery? In Kaggle terms: how can I use a library in my project that is not available on a public package repository (i.e. one that I built myself)?&lt;/p&gt;
&lt;p&gt;To this end, there's really no secret sauce. With the cutting board/library, we can either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Include it in our container and deal with the extra weight.&lt;/li&gt;
&lt;li&gt;Treat it as an ingredient, carry it to the bakery, and access it via a Docker Volume.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Happy cooking&lt;/h2&gt;
&lt;p&gt;Moving your local development inside of a Docker container, and/or Dockerizing this local environment once you're ready to use a remote resource to do the heavier lifting, will ensure you only have to figure out how to bake the cupcake once. Prototype locally, then send stress-free to the bakery for mass production.&lt;/p&gt;
&lt;p&gt;Happy cooking.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Additional resources:&lt;/p&gt;
&lt;p&gt;Here's two resources I found very helpful when learning about Docker for Kaggle:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://speakerdeck.com/smly/workflow-serialization-and-docker-for-kaggle"&gt;Workflow, Serialization &amp;amp; Docker for Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2016/02/05/how-to-get-started-with-data-science-in-containers/"&gt;How to get started with data science in containers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content></entry><entry><title>Gradientes de Recurrent Neural Networks y Lo Que Aprendí Derivándolos</title><link href="https://willwolf.io/es/2016/10/18/gradientes-de-recurrent-neural-networks-y-lo-que-aprendi-derivandolos/" rel="alternate"></link><published>2016-10-18T14:00:00-04:00</published><updated>2016-10-18T14:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2016-10-18:/es/2016/10/18/gradientes-de-recurrent-neural-networks-y-lo-que-aprendi-derivandolos/</id><summary type="html">&lt;p&gt;Gradientes de recurrent neural networks a mano.&lt;/p&gt;</summary><content type="html">&lt;p&gt;He pasado la mayoría de la última semana construyendo recurrent neural networks a mano. Estoy tomando el curso de &lt;a href="https://www.udacity.com/course/deep-learning--ud730"&gt;Udacity Deep Learning&lt;/a&gt;, y, llegando al contenido sobre RNN's y LSTM's, decidí construir algunos de ellos desde cero yo mismo.&lt;/p&gt;
&lt;h3&gt;¿Qué es un RNN?&lt;/h3&gt;
&lt;p&gt;Por afuera, las recurrent neural networks se diferencian del feedforward neural network típico por el hecho de que pueden ingerir &lt;em&gt;una secuencia&lt;/em&gt; de input en lugar de un sólo input de largo fijo. Concretamente, imagínense que estamos entrenando un modelo de clasificación con un puñado de tuits. Para codificar dichos tuits en el espacio vectorial, creamos un modelo de bag-of-words con un vocabulario de 3 palabras distintas. En el neural network clásico, esto implica un "input layer" con un tamaño de 3; un input podría ser &lt;span class="math"&gt;\([4, 9, 3]\)&lt;/span&gt;, o &lt;span class="math"&gt;\([1, 0, 5]\)&lt;/span&gt;, o &lt;span class="math"&gt;\([0, 0, 6]\)&lt;/span&gt;, por ejemplo. En el caso del recurrent neural network, nuestro input layer tiene el mismo tamaño de 3, pero en lugar de un sólo input, le podemos alimentar una secuencia de inputs de tamaño 3 de cualquier largo. Como ejemplo, un input podría ser &lt;span class="math"&gt;\([[1, 8, 5], [2, 2, 4]]\)&lt;/span&gt;, o &lt;span class="math"&gt;\([[6, 7, 3], [6, 2, 4], [9, 17, 5]]\)&lt;/span&gt;, o &lt;span class="math"&gt;\([[2, 3, 0], [1, 1, 7], [5, 5, 3], [8, 18, 4]]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;En su interior, las recurrent neural networks tienen un mecanismo feedforward diferente del neural network típico. Además, cada input en nuestra secuencia se procesa individual y cronológicamente: el primer input es procesado, luego el segundo, hasta procesar el último. Por fin, después de procesar todos los inputs, computamos algunos gradientes y actualizamos los parámetros (weights) de la red. Tal como en los feedforward networks, lo hacemos con backpropagation. Al contrario, ya nos toca propagarles los errores a cada parámetro en cada etapa del tiempo. Dicho de otra manera, nos toca calcular gradientes con respecto a: el estado del mundo al procesar nuestro primer input, el estado del mundo al procesar nuestro segundo input, hasta el en el que procesamos nuestro último input. Este algoritmo se llama &lt;a href="https://en.wikipedia.org/wiki/Backpropagation_through_time"&gt;Backpropagation Through Time&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Otros Recursos, Mis Frustraciones&lt;/h3&gt;
&lt;p&gt;Existen bastantes recursos para entender cómo calcular los gradientes usando el Backpropagation Through Time. En mi opinión, &lt;a href="https://www.existor.com/en/ml-rnn.html"&gt;Recurrent Neural Networks Maths&lt;/a&gt; es el más comprehensivo en un sentido matemático, mientras &lt;a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"&gt;Recurrent Neural Networks Tutorial Part 3&lt;/a&gt; es más conciso pero igual de claro. Finalmente, está &lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;Minimal character-level language model&lt;/a&gt; por Andrej Karpathy, acompañando su &lt;a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;blog post&lt;/a&gt; excelente sobre la teoría y el uso general de los RNN's, que al inicio me costó mucho entender.&lt;/p&gt;
&lt;p&gt;En todos los posts, pienso que los autores desafortunadamente no aclaran muy bien la línea divisoria entre la derivación de los gradientes y su implementación (eficiente) en código, o por lo menos brincan demasiado rápido del primero al segundo. Definen variables como  &lt;code&gt;dbnext&lt;/code&gt;,  &lt;code&gt;delta_t&lt;/code&gt;, y &lt;span class="math"&gt;\(e_{hi}^{2f3}\)&lt;/span&gt; sin explicar cabalmente su significado en los gradientes analíticos mismos. Como ejemplo, el primer post incluye la siguiente sección:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}} =
e^{t=2f2}_{hi} \frac{\partial h^{t=2}_i}{\partial z^{t=2}_{hi}} \frac{\partial z^{t=2}_{hi}}{\partial w^{xh}_{mi}} +
e^{t=1f2}_{hi} \frac{\partial h^{t=1}_i} {\partial z^{t=1}_{hi}} \frac{\partial z^{t=1}_{hi}}{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hasta ahora, no está hablando sino de los gradientes analíticos. A continuación, alude a la implementación-en-código que sigue.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So the thing to note is that we can delay adding in the backward propagated errors until we get further into the loop. In other words, we can initially compute the derivatives of &lt;em&gt;J&lt;/em&gt; with respect to the third unrolled network with only the first term:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=3}}{\partial w^{xh}_{mi}} =
e^{t=3f3}_{hi} \frac{\partial h^{t=3}_i}{\partial z^{t=3}_{hi}} \frac{\partial z^{t=3}_{hi}}{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;And then add in the other term only when we get to the second unrolled network:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}} =
(e^{t=2f3}_{hi} + e^{t=2f2}_{hi}) \frac{\partial h^{t=2}_i}{\partial z^{t=2}_{hi}}
\frac{\partial z^{t=2}_{hi}}
{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Noten las definiciones opuestas de la variable &lt;span class="math"&gt;\(\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}}\)&lt;/span&gt;. Hasta donde yo sé, la segunda es, sin hacerle caso a algún posible código, categóricamente falsa. Dicho eso, creo que el autor está simplemente ofreciendo una definición alternativa para esta cantidad en cuanto a un atajo pequeño que luego tome.&lt;/p&gt;
&lt;p&gt;Sobre decir que estas ambigüedades hacen que todo se vuelva muy emocional, muy rápido. Me dejaron confundido durante dos días. Por lo tanto, el objetivo de este post es derivar los gradientes de un recurrent neural network desde cero, y clarificar enfáticamente que cualquier atajo de implementación que siga no es nada más que ese mismo, y que no tiene nada que ver con la definición analítica del gradiente correspondiente. En otras palabras, si puedes derivar los gradientes, has ganado. Escribe un test unitario, implementa dichos gradientes de la manera más cruda posible, velo pasar, y enseguida te darás cuenta de que puedes hacer tu código mucho más eficiente con muy poco esfuerzo. A esa altura, todos los "atajos" que tomen los autores ya mencionados te van a parecer absolutamente obvios.&lt;/p&gt;
&lt;h3&gt;Backpropagation Through Time&lt;/h3&gt;
&lt;p&gt;En el caso más simple, asumamos que nuestra red tiene 3 capas, y tan sólo 3 parámetros para optimizar: &lt;span class="math"&gt;\(\mathbf{W^{xh}}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; y &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt;. Las ecuaciones principales son las siguientes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{z_t} = \mathbf{W^{xh}}\mathbf{x} + \mathbf{W^{hh}}\mathbf{h_{t-1}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{h_t} = \tanh(\mathbf{z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{y_t} = \mathbf{W^{hy}}\mathbf{h_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{p_t} = \text{softmax}(\mathbf{y_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{J_t} = \text{crossentropy}(\mathbf{p_t}, \mathbf{\text{labels}})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;He escrito "softmax" y "cross-entropy" por cuestiones de claridad: antes de emprender lo siguiente, es crucial entender lo que hacen y cómo calcular sus gradientes a mano.&lt;/p&gt;
&lt;p&gt;Antes de avanzar, damos la definición de una derivada parcial misma:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Una derivada parcial, &lt;span class="math"&gt;\(\frac{\partial y}{\partial x}\)&lt;/span&gt; por ejemplo, nos dice cuánto crece &lt;span class="math"&gt;\(y\)&lt;/span&gt; a consecuencia de un cambio en &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nuestro costo &lt;span class="math"&gt;\(\mathbf{J_t}\)&lt;/span&gt; es el costo &lt;em&gt;total&lt;/em&gt; (no el costo promedio) de una cierta secuencia de inputs. Por eso, un cambio de una unidad en &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; impacta a &lt;span class="math"&gt;\(\mathbf{J_1}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{J_2}\)&lt;/span&gt; y &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; por separado. En consecuencia, nuestro gradiente equivale a la suma de los gradientes respectivos en cada etapa de tiempo &lt;span class="math"&gt;\(t\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{hy}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{hy}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{hy}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{hh}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{hh}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{hh}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{hh}}}\\
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{xh}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{xh}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{xh}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{xh}}}$$&lt;/div&gt;
&lt;p&gt;Tomémoslo pasa a paso.&lt;/p&gt;
&lt;h3&gt;Derivadas Algebraicas&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Empezando con&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}}\)&lt;/span&gt;, notamos que un cambio en &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; impacta a &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; sólo cuando &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, y no a ninguna otra cantidad. Sigue que:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}\frac{\partial \mathbf{y_3}}{\partial \mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}\frac{\partial \mathbf{y_2}}{\partial \mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}\frac{\partial \mathbf{y_1}}{\partial \mathbf{W^{hy}}}
$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Empezando con &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;, un cambio en &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; impacta a nuestro costo en &lt;em&gt;3 momentos distintos: &lt;/em&gt;por primera vez al calcular el valor de &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;; por segunda vez al calcular el valor de &lt;span class="math"&gt;\(\mathbf{h_2}\)&lt;/span&gt;, que está condicionado a &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;; por tercera vez al calcular &lt;span class="math"&gt;\(\mathbf{h_3}\)&lt;/span&gt;, que está condicionado a &lt;span class="math"&gt;\(\mathbf{h_2}\)&lt;/span&gt;, que está condicionado a &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;En términos más generales, un cambio en &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; impacta al costo &lt;span class="math"&gt;\(\mathbf{J_t}\)&lt;/span&gt; en &lt;span class="math"&gt;\(t\)&lt;/span&gt; momentos distintos. Sigue que:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}} =
\sum\limits_{k=0}^{t} \frac{\partial \mathbf{J_t}}{\partial
\mathbf{h_t}} \frac{\partial \mathbf{h_t}}{\partial
\mathbf{h_k}} \frac{\partial \mathbf{h_k}}{\partial
\mathbf{z_k}} \frac{\partial \mathbf{z_k}}{\partial
\mathbf{W^{hh}}}
$$&lt;/div&gt;
&lt;p&gt;Con esta definición, calculamos nuestras gradientes como:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}
\end{align*}
$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Análogamente:&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}} =
\sum\limits_{k=0}^{t} \frac{\partial \mathbf{J_t}}{\partial
\mathbf{h_t}} \frac{\partial \mathbf{h_t}}{\partial
\mathbf{h_k}} \frac{\partial \mathbf{h_k}}{\partial
\mathbf{z_k}} \frac{\partial \mathbf{z_k}}{\partial
\mathbf{W^{xh}}}$$&lt;/div&gt;
&lt;p&gt;Así que:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Derivadas Analíticas&lt;/h3&gt;
&lt;p&gt;Finalmente, insertamos las derivadas parciales individuales para llegar a los gradientes finales con lo siguiente en mano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial y} = \mathbf{p_t} - \mathbf{\text{labels}_t}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\mathbf{\text{labels}_t}\)&lt;/span&gt; is a one-hot vector of the correct answer at a given time-step &lt;span class="math"&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}} = (\mathbf{p_t} - \mathbf{\text{labels}_t})\mathbf{h_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{h_t}} = (\mathbf{p_t} - \mathbf{\text{labels}_t})\mathbf{W^{hy}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{h_t}}{\partial \mathbf{z_t}} = 1 - \tanh^2(\mathbf{z_t}) = 1 - \mathbf{h_t}^2\)&lt;/span&gt;, as &lt;span class="math"&gt;\(\mathbf{h_t} = \tanh(\mathbf{z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{z_t}}{\mathbf{h_{t-1}}} = \mathbf{W^{hh}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{z_t}}{\partial \mathbf{W^{xh}}} = \mathbf{x_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{z_t}{\partial \mathbf{W^{hh}}} = \mathbf{h_{t-1}}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A esta altura, has terminado: has calculado tus gradientes, y entiendes bien el algoritmo de Backpropagation Through Time. De aquí en adelante, lo único que queda es escribir algunos for-loops.&lt;/p&gt;
&lt;h3&gt;Atajos de Implementación&lt;/h3&gt;
&lt;p&gt;Al calcular le gradiente de, por ejemplo, &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, se nota de inmediato que necesitamos acceso a los labels de &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, &lt;span class="math"&gt;\(t=2\)&lt;/span&gt; y &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Para &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, necesitamos acceso a los labels de &lt;span class="math"&gt;\(t=2\)&lt;/span&gt; y &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Por fin, para &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, necesitamos los labels de &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Naturalmente, nos preguntamos cómo podemos hacer este proceso más eficiente: por ejemplo, para calcular &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, ¿qué tal sólo calcular las partes de &lt;span class="math"&gt;\(t=3\)&lt;/span&gt; a &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, y agregarle el resto en los pasos del tiempo que siguen? En lugar de profundizar, te los dejo a ustedes: esta parte es trivial en el fondo, un buen ejercicio para el practicante, y al acabar vas a descubrir de repente que tu código se parece bastante al de los recursos arriba.&lt;/p&gt;
&lt;h3&gt;Aprendizajes del Proceso&lt;/h3&gt;
&lt;p&gt;Mediante este proceso, aprendí varias cosas claves:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Al querer implementar un neural network desde cero, deriva las gradientes a mano al inicio. &lt;em&gt;Esto hace que todo salga mucho más fácil.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Usa más el lápiz y papel antes de siquiera escribir una sola linea de código. No dan miedo y tienen absolutamente su función.&lt;/li&gt;
&lt;li&gt;El "chain rule" queda simple y claro. Si una derivada parece estar fuera de esta dificultad general, es probable que haya otro detalle importante que te falta reconocer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Felices RNN's.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Referencias claves para este artículo se nombran:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"&gt;Recurrent Neural Networks Tutorial Part 2 Implementing A Rnn With Python Numpy And Theano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"&gt;Recurrent Neural Networks Tutorial Part 3 Backpropagation Through Time And Vanishing Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.existor.com/en/ml-rnn.html"&gt;Machine Learning - Recurrent Neural Networks Maths&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Recurrent Neural Network Gradients, and Lessons Learned Therein</title><link href="https://willwolf.io/es/../2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/" rel="alternate"></link><published>2016-10-18T14:00:00-04:00</published><updated>2016-10-18T14:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2016-10-18:/es/../2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/</id><summary type="html">&lt;p&gt;Recurrent neural network gradients by hand.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've spent the last week hand-rolling recurrent neural networks. I'm currently taking Udacity's Deep Learning &lt;a href="https://www.udacity.com/course/deep-learning--ud730"&gt;course&lt;/a&gt;, and arriving at the section on RNN's and LSTM's, I decided to build a few for myself.&lt;/p&gt;
&lt;h3&gt;What are RNN's?&lt;/h3&gt;
&lt;p&gt;On the outside, recurrent neural networks differ from typical, feedforward neural networks in that they take a &lt;em&gt;sequence&lt;/em&gt; of input instead of an input of fixed length. Concretely, imagine we are training a sentiment classifier on a bunch of tweets. To embed these tweets in vector space, we create a bag-of-words model with vocabulary size 3. In a typical neural network, this implies an input layer of size 3; an input could be &lt;span class="math"&gt;\([4, 9, 3]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([1, 0, 5]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([0, 0, 6]\)&lt;/span&gt;, for example. In a recurrent neural network, our input layer has the same size 3, but instead of just a single size-3 input, we can feed it a sequence of size-3 inputs of any length. For example, an input could be &lt;span class="math"&gt;\([[1, 8, 5], [2, 2, 4]]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([[6, 7, 3], [6, 2, 4], [9, 17, 5]]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([[2, 3, 0], [1, 1, 7], [5, 5, 3], [8, 18, 4]]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the inside, recurrent neural networks have a different feedforward mechanism than typical neural networks. In addition, each input in our sequence of inputs is processed individually and chronologically: the first input is fed forward, then the second, and so on. Finally, after all inputs have been fed forward, we compute some gradients and update our weights. Like in feedforward networks, we also use backpropagation. However, we must now backpropagate errors to our parameters at every step in time. In other words, we must compute gradients with respect to: the state of the world when we fed our first input forward, the state of the world when we fed our second input forward, and up until the state of the world when we fed our last input forward. This algorithm is called &lt;a href="https://en.wikipedia.org/wiki/Backpropagation_through_time"&gt;Backpropagation Through Time&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Other Resources, My Frustrations&lt;/h3&gt;
&lt;p&gt;There are many resources for understanding how to compute gradients using Backpropagation Through Time. In my view, &lt;a href="https://www.existor.com/en/ml-rnn.html"&gt;Recurrent Neural Networks Maths&lt;/a&gt; is the most mathematically comprehensive, while &lt;a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"&gt;Recurrent Neural Networks Tutorial Part 3&lt;/a&gt; is more concise yet equally clear. Finally, there exists Andrej Karpathy's &lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;Minimal character-level language model&lt;/a&gt;, accompanying his excellent &lt;a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;blog post&lt;/a&gt; on the general theory and use of RNN's, which I initially found convoluted and hard to understand.&lt;/p&gt;
&lt;p&gt;In all posts, I think the authors unfortunately blur the line between the derivation of the gradients and their (efficient) implementation in code, or at the very least jump too quickly from one to another. They define variables like &lt;code&gt;dbnext&lt;/code&gt;,  &lt;code&gt;delta_t&lt;/code&gt;, and &lt;span class="math"&gt;\(e_{hi}^{2f3}\)&lt;/span&gt; without thoroughly explaining their place in the analytical gradients themselves. As one example, the first post includes the snippet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}} =
e^{t=2f2}_{hi} \frac{\partial h^{t=2}_i}{\partial z^{t=2}_{hi}} \frac{\partial z^{t=2}_{hi}}{\partial w^{xh}_{mi}} +
e^{t=1f2}_{hi} \frac{\partial h^{t=1}_i} {\partial z^{t=1}_{hi}} \frac{\partial z^{t=1}_{hi}}{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So far, he's just talking about analytical gradients. Next, he gives hint to the implementation-in-code that follows.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So the thing to note is that we can delay adding in the backward propagated errors until we get further into the loop. In other words, we can initially compute the derivatives of &lt;em&gt;J&lt;/em&gt; with respect to the third unrolled network with only the first term:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=3}}{\partial w^{xh}_{mi}} =
e^{t=3f3}_{hi} \frac{\partial h^{t=3}_i}{\partial z^{t=3}_{hi}} \frac{\partial z^{t=3}_{hi}}{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;And then add in the other term only when we get to the second unrolled
network:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}} =
(e^{t=2f3}_{hi} + e^{t=2f2}_{hi}) \frac{\partial h^{t=2}_i}{\partial z^{t=2}_{hi}}
\frac{\partial z^{t=2}_{hi}}
{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note the opposing definitions of the variable &lt;span class="math"&gt;\(\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}}\)&lt;/span&gt;. As far as I know, the latter is, in a vacuum, categorically false. This said, I believe the author is simply providing an alternative definition of this quantity in line with a computational shortcut he later takes.&lt;/p&gt;
&lt;p&gt;Of course, these ambiguities become very emotional, very quickly. I myself was confused for two days. As such, the aim of this post is to derive recurrent neural network gradients from scratch, and emphatically clarify that all implementation "shortcuts" thereafter are nothing more than just that, with no real bearing on the analytical gradients themselves. In other words, if you can derive the gradients, you win. Write a unit test, code these gradients in the crudest way you can, watch your test pass, and then immediately realize that your code can be made more efficient. At this point, all "shortcuts" that the above authors (and myself, now, as well) take in their code will make perfect sense.&lt;/p&gt;
&lt;h3&gt;Backpropagation Through Time&lt;/h3&gt;
&lt;p&gt;In the simplest case, let's assume our network has 3 layers, and just 3 parameters to optimize: &lt;span class="math"&gt;\(\mathbf{W^{xh}}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt;. The foundational equations of this network are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{z_t} = \mathbf{W^{xh}}\mathbf{x} + \mathbf{W^{hh}}\mathbf{h_{t-1}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{h_t} = \tanh(\mathbf{z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{y_t} = \mathbf{W^{hy}}\mathbf{h_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{p_t} = \text{softmax}(\mathbf{y_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{J_t} = \text{crossentropy}(\mathbf{p_t},
    \mathbf{\text{labels}_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I've written "softmax" and "cross-entropy" for clarity: before tackling the math below, it is important to understand what they do, and how to derive their gradients by hand.&lt;/p&gt;
&lt;p&gt;Before moving forward, let's restate the definition of a partial derivative itself.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A partial derivative, for example &lt;span class="math"&gt;\(\frac{\partial y}{\partial x}\)&lt;/span&gt;, measures how much &lt;span class="math"&gt;\(y\)&lt;/span&gt; increases with every 1-unit increase in &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our cost &lt;span class="math"&gt;\(\mathbf{J_t}\)&lt;/span&gt; is the &lt;em&gt;total&lt;/em&gt; &lt;em&gt;cost&lt;/em&gt; (i.e., not the average cost) of a given sequence of inputs. As such, a 1-unit increase in &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; will impact each of &lt;span class="math"&gt;\(\mathbf{J_1}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{J_2}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; individually. Therefore, our gradient is equal to the sum of the respective gradients at each time step &lt;span class="math"&gt;\(t\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{hy}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{hy}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{hy}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{hh}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{hh}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{hh}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{hh}}}\\
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{xh}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{xh}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{xh}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{xh}}}
$$&lt;/div&gt;
&lt;p&gt;Let's take this piece by piece.&lt;/p&gt;
&lt;h3&gt;Algebraic Derivatives&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Starting with &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}}\)&lt;/span&gt;, we note that a change in &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; will only impact &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;: &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; plays no role in computing the value of anything other than &lt;span class="math"&gt;\(\mathbf{y_3}\)&lt;/span&gt;. Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{W^{hy}}}\\
$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Starting with &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;, a change in &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; will impact our cost &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; in &lt;em&gt;3 separate ways: &lt;/em&gt;once, when computing the value of &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;; once, when computing the value of &lt;span class="math"&gt;\(\mathbf{h_2}\)&lt;/span&gt;, which depends on &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;; once, when computing the value of &lt;span class="math"&gt;\(\mathbf{h_3}\)&lt;/span&gt;, which depends on &lt;span class="math"&gt;\(\mathbf{h_2}\)&lt;/span&gt;, which depends on &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;More generally, a change in &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; will impact our cost &lt;span class="math"&gt;\(\mathbf{J_t}\)&lt;/span&gt; on &lt;span class="math"&gt;\(t\)&lt;/span&gt; separate occasions. Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}} =
\sum\limits_{k=0}^{t} \frac{\partial \mathbf{J_t}}{\partial
\mathbf{h_t}} \frac{\partial \mathbf{h_t}}{\partial
\mathbf{h_k}} \frac{\partial \mathbf{h_k}}{\partial
\mathbf{z_k}} \frac{\partial \mathbf{z_k}}{\partial
\mathbf{W^{hh}}}
$$&lt;/div&gt;
&lt;p&gt;Then, with this definition, we compute our individual gradients as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}
\end{align*}
$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Similarly:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}} =
\sum\limits_{k=0}^{t} \frac{\partial \mathbf{J_t}}{\partial
\mathbf{h_t}} \frac{\partial \mathbf{h_t}}{\partial
\mathbf{h_k}} \frac{\partial \mathbf{h_k}}{\partial
\mathbf{z_k}} \frac{\partial \mathbf{z_k}}{\partial
\mathbf{W^{xh}}}
$$&lt;/div&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Analytical Derivatives&lt;/h3&gt;
&lt;p&gt;Finally, we plug in the individual partial derivates to compute our final gradients, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{y_t}} = \mathbf{p_t} - \mathbf{\text{labels}_t}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\mathbf{\text{labels}_t}\)&lt;/span&gt; is a one-hot vector of the correct answer at a given time-step &lt;span class="math"&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}} = (\mathbf{p_t} - \mathbf{\text{labels}_t})\mathbf{h_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{h_t}} = (\mathbf{p_t} - \mathbf{\text{labels}_t})\mathbf{W^{hy}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{h_t}}{\partial \mathbf{z_t}} = 1 - \tanh^2(\mathbf{z_t}) = 1 - \mathbf{h_t}^2\)&lt;/span&gt;, as &lt;span class="math"&gt;\(\mathbf{h_t} = \tanh(\mathbf{z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{z_t}}{\mathbf{h_{t-1}}} = \mathbf{W^{hh}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{z_t}}{\partial \mathbf{W^{xh}}} = \mathbf{x_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{z_t}{\partial \mathbf{W^{hh}}} = \mathbf{h_{t-1}}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, you're done: you've computed your gradients, and you understand Backpropagation Through Time. From this point forward, all that's left is writing some for-loops.&lt;/p&gt;
&lt;h3&gt;Implementation Shortcuts&lt;/h3&gt;
&lt;p&gt;As you'll readily note, when computing the gradient for, for example, &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, we'll need access to our labels at time-steps &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, &lt;span class="math"&gt;\(t=2\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. For &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, we'll need our labels at time-steps &lt;span class="math"&gt;\(t=2\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Finally, for &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, we'll need our labels at just &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Naturally, we look to make this efficient: for, for example, &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, how about just compute the &lt;span class="math"&gt;\(t=3\)&lt;/span&gt; parts at &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, and add in the rest at &lt;span class="math"&gt;\(t=2\)&lt;/span&gt;? Instead of explaining further, I leave this step to you: it is ultimately trivial, a good exercise, and when you're finished, you'll find that your code readily resembles much of that written in the above resources.&lt;/p&gt;
&lt;h3&gt;Lessons Learned&lt;/h3&gt;
&lt;p&gt;Throughout this process, I learned a few lessons.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When implementing neural networks from scratch, derive gradients by hand at the outset. &lt;em&gt;This makes thing so much easier.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Turn more readily to your pencil and paper before writing a single line of code. They are not scary and they absolutely have their place.&lt;/li&gt;
&lt;li&gt;The chain rule remains simple and clear. If a derivative seems to "supercede" the general difficulty of the chain rule, there's probably something else you're missing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Happy RNN's.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Key references for this article include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"&gt;Recurrent Neural Networks Tutorial Part 2 Implementing A Rnn With Python Numpy And Theano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"&gt;Recurrent Neural Networks Tutorial Part 3 Backpropagation Through Time And Vanishing Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.existor.com/en/ml-rnn.html"&gt;Machine Learning - Recurrent Neural Networks Maths&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Simulating the Colombian Peace Vote: Did the "No" Really Win?</title><link href="https://willwolf.io/es/../2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win/" rel="alternate"></link><published>2016-10-12T14:58:00-04:00</published><updated>2016-10-12T14:58:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2016-10-12:/es/../2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win/</id><summary type="html">&lt;p&gt;A post-mortem statistical simulation of the &lt;a href="https://en.wikipedia.org/wiki/Colombian_peace_agreement_referendum,_2016"&gt;2016 Colombian plebiscite&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;On October 2nd, 2016, I watched in awe as Colombia's national plebiscite for its just-signed peace accord narrowly failed. For the following week, I brooded over the result: the disinformation campaign, Uribe's antics, and just how good the &lt;a href="https://www.youtube.com/playlist?list=PLa28R7QEiMblKeZ_OlZ_XfjjxjfeIhpuL"&gt;deal&lt;/a&gt; really seemed to be. Two days ago, I chanced upon this &lt;a href="https://theconversation.com/colombia-did-not-vote-no-in-its-peace-referendum-what-the-statistics-reveal-66471"&gt;post&lt;/a&gt;, which reminds us that the razor-thin margin - 6,431,376 "No" vs. 6,377,482 "Yes" - is not particularly convincing, nor, as it happens, immune to human error.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;And as with all manual voting systems, one cannot rule out at least some degree of misclassification of papers on some scale, no matter how small. We know of no evidence of cheating, and Colombia is to be lauded for the seriousness of its referendum process, but the distinction between intentional and unintentional misclassification by individual counters can occasionally become blurred in practice.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, it was humans - tired humans - counting ballots by hand.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The technology of tired humans sorting pieces of paper into four stacks is, at best, crude. As a large research literature has made clear, we can reasonably assume that even well-rested people would have made mistakes with between 0.5% and 1% of the ballots. On this estimate, about 65,000-130,000 votes would have been unintentionally misclassified. It means the number of innocent counting errors could easily be substantially larger than the 53,894 yes-no difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Is it possible that the majority wanted "Yes" and still happened to lose?&lt;/p&gt;
&lt;p&gt;&lt;img alt="plebiscite vote" src="https://willwolf.io/es/../images/colombian_plebiscite_vote.jpg"&gt;&lt;/p&gt;
&lt;p&gt;To answer this question, we can frame the vote as a simple statistical process and ask: "if we were to re-hold the vote many more times, how often would the 'Yes' vote actually win?"&lt;/p&gt;
&lt;p&gt;Should we choose, we could pursue this result analytically, i.e. solve the problem with a pencil and paper. This get messy quickly. Instead, we'll disregard closed-form theory and run a basic simulation; &lt;a href="https://speakerdeck.com/jakevdp/statistics-for-hackers"&gt;"if you can write a for-loop, you can do statistics."&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We'll frame our problem as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(V_t=13,066,047\)&lt;/span&gt; voters arrive to the polls.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{\text{yes}}\%\)&lt;/span&gt; of them intend to vote "Yes", &lt;span class="math"&gt;\((1-p_{\text{yes}})\%\)&lt;/span&gt; of them intend to vote "No."&lt;/li&gt;
&lt;li&gt;Each voter casts an invalid (unmarked or void) ballot with probability &lt;span class="math"&gt;\(p_{\text{invalid}}\%\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Of the valid ballots, the poll workers misclassify the vote with probability &lt;span class="math"&gt;\(p_{\text{misclassification}}\%\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Majority vote wins.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;YES_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6377482&lt;/span&gt;
&lt;span class="n"&gt;NO_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6431376&lt;/span&gt;
&lt;span class="n"&gt;UNMARKED_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;86243&lt;/span&gt;
&lt;span class="n"&gt;NULL_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;170946&lt;/span&gt;

&lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;YES_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;NO_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;UNMARKED_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;NULL_BALLOTS&lt;/span&gt;
&lt;span class="n"&gt;P_INVALID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;02&lt;/span&gt;
&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt;
&lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In each trial, we assume a true, underlying &lt;span class="math"&gt;\(p_{\text{yes}}\%\)&lt;/span&gt; for the voting populace. For example, if &lt;span class="math"&gt;\(p_{\text{yes}}\)&lt;/span&gt; is .48, we will have &lt;span class="math"&gt;\(V_t * p_{\text{yes}}\)&lt;/span&gt; individuals intending to vote "Yes," and &lt;span class="math"&gt;\(V_t * (1-p_{\text{yes}})\)&lt;/span&gt; voters intending to vote "No." We assume these values to be static: they are not generated by a random process.&lt;/p&gt;
&lt;p&gt;Next, each voter casts an invalid ballot with probability &lt;span class="math"&gt;\(p_{\text{invalid}}\)&lt;/span&gt;, which we model as a Binomial random variable. Each remaining, valid ballot is then misclassified with probability &lt;span class="math"&gt;\(p_{\text{misclassification}}\)&lt;/span&gt;. Finally, the tallies of "Yes" and "No" votes are counted, and the percentage of "Yes" votes is returned.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_votes&lt;/span&gt;

    &lt;span class="n"&gt;yes_votes_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;yes_votes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;no_votes_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;no_votes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;invalid_ballots_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;yes_votes_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_INVALID&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;invalid_ballots_no&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;no_votes_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_INVALID&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;valid_yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yes_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;invalid_ballots_yes&lt;/span&gt;
    &lt;span class="n"&gt;valid_no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;invalid_ballots_no&lt;/span&gt;

    &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_yes_votes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;no_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valid_yes_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt;

    &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_no_votes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;yes_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valid_no_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt;

    &lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_no_voters&lt;/span&gt;
    &lt;span class="n"&gt;tallied_no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_yes_voters&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tallied_no_votes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's try this out for varying values of &lt;span class="math"&gt;\(p_{\text{yes}}\)&lt;/span&gt;. To start, if the true, underlying percentage of "Yes" voters were 51%, how often would the "No" vote still win?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's comforting. Given our assumptions, if 51% of the Colombian people arrived at the polls intending to vote "Yes," the "No" vote would have nonetheless won in 0 of 100,000 trials. So, how close can we get before we start seeing backwards results?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;probability_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
    &lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;proportion_of_trials_won_by_no&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p_yes: {:1.6f}% | no_win_percentage: {:1.3f}%&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;proportion_of_trials_won_by_no&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;60.000000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;51.000000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.100000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.010000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.191&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.001000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.688&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.000100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;48.791&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.000010&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.063&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our first frustration comes at &lt;span class="math"&gt;\(p_{\text{yes}} = .5001\)&lt;/span&gt;: if &lt;span class="math"&gt;\(V_t * p_{\text{yes}} = 13,066,047 * .5001 \approx 6,534,330\)&lt;/span&gt; voters wanted "Yes" vs. &lt;span class="math"&gt;\(\approx 6,531,716\)&lt;/span&gt; who wanted "No," the "No" vote would have still won &lt;span class="math"&gt;\(0.191\%\)&lt;/span&gt; of the time. Again, this reversal derives from human error: both on the part of the voter in casting an invalid ballot, and on the part of the the poll-worker incorrectly classifying that ballot by hand.&lt;/p&gt;
&lt;p&gt;As we move further down, the results get tighter. At &lt;span class="math"&gt;\(p_{\text{yes}} = .50001\)&lt;/span&gt;, the "Yes" vote can only be expected to have won &lt;span class="math"&gt;\(1 - .38688 = 61.312\%\)&lt;/span&gt; of the time. Finally, at &lt;span class="math"&gt;\(p_{\text{yes}} = .5000001\)&lt;/span&gt; (which, keep in mind, implies an "I intend to vote 'Yes'" vs. "I intend to vote 'No'" differential of just &lt;span class="math"&gt;\(13,066,047 * (p_{\text{yes}} - (1 - p_{\text{yes}})) \approx 3\)&lt;/span&gt; voters), the "No" vote actually wins the &lt;em&gt;majority&lt;/em&gt; of the 100,000 hypothetical trials. At that point, we're really just flipping coins.&lt;/p&gt;
&lt;p&gt;In summary, as the authors of the above post suggest, it would be statistically irresponsible to claim a definitive win for the "No." Conversely, the true, underlying margin does prove to be extremely tight: maybe a majority vote just isn't the best way to handle these issues after all.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/colombia-vote-simulation/blob/master/colombia-vote-simulation.ipynb"&gt;notebook&lt;/a&gt; and &lt;a href="https://github.com/cavaunpeu/colombia-vote-simulation"&gt;repo&lt;/a&gt; for the analysis can be found here.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Key references include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://theconversation.com/colombia-did-not-vote-no-in-its-peace-referendum-what-the-statistics-reveal-66471"&gt;Colombia did not vote ‘no’ in its peace referendum – what the statistics reveal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://andrewgelman.com/2016/10/04/did-colombia-really-vote-no-in-that-peace-referendum/"&gt;Did Colombia really vote no in that peace referendum?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://plebiscito.registraduria.gov.co/99PL/DPLZZZZZZZZZZZZZZZZZ_L1.htm"&gt;Plebiscito Site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry><entry><title>Simulación Estadística del Plebiscito Colombiano: ¿Realmente Ganaron Los del "No?"</title><link href="https://willwolf.io/es/2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win-2/" rel="alternate"></link><published>2016-10-12T11:53:00-04:00</published><updated>2016-10-12T11:53:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2016-10-12:/es/2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win-2/</id><summary type="html">&lt;p&gt;Una simulación estadística del &lt;a href="https://en.wikipedia.org/wiki/Colombian_peace_agreement_referendum,_2016"&gt;plebiscito colombiano de 2016&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;El 2 de octubre del 2016, observé con terror como los colombianos votaron por el "No" en su plebiscito nacional para decretar el recién firmado acuerdo de paz. En la siguiente semana, medité sobre el resultado y las cosas que hubieran sucedido: la gran campaña de desinformación, las payasadas de Uribe y lo súper bueno que realmente parecía el &lt;a href="https://www.youtube.com/playlist?list=PLa28R7QEiMblKeZ_OlZ_XfjjxjfeIhpuL"&gt;acuerdo&lt;/a&gt; mismo. Hace dos días, me topé por casualidad con este &lt;a href="https://theconversation.com/colombia-did-not-vote-no-in-its-peace-referendum-what-the-statistics-reveal-66471"&gt;post&lt;/a&gt;, que nos pide recordar que el margen escaso con el que ganó el "No" - 6,431,376 vs. 6,377,482 - no es particularmente convincente, ni, en realidad, tan decisivo frente al error humano.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;And as with all manual voting systems, one cannot rule out at least some degree of misclassification of papers on some scale, no matter how small. We know of no evidence of cheating, and Colombia is to be lauded for the seriousness of its referendum process, but the distinction between intentional and unintentional misclassification by individual counters can occasionally become blurred in practice.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;En otras palabras, fueron seres humanos - seres humanos agotados - contando los votos a mano.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The technology of tired humans sorting pieces of paper into four stacks is, at best, crude. As a large research literature has made clear, we can reasonably assume that even well-rested people would have made mistakes with between 0.5% and 1% of the ballots. On this estimate, about 65,000-130,000 votes would have been unintentionally misclassified. It means the number of innocent counting errors could easily be substantially larger than the 53,894 yes-no difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;¿Sería posible que la mayoría quería el "Sí" y sin embargo perdió igual?&lt;/p&gt;
&lt;p&gt;&lt;img alt="plebiscite vote" src="https://willwolf.io/es/../images/colombian_plebiscite_vote.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Para investigarlo, podemos formular el proceso del voto como un sencillo proceso estadístico y preguntarse: "Si repitiéramos el plebiscito muchas más veces, ¿con qué frecuencia ganaría el 'Sí' de verdad?"&lt;/p&gt;
&lt;p&gt;Si queremos, podemos analizar el problema desde un lado analítico, que es decir resolverlo a mano con un lápiz y papel. Esto se pone complicado rápido. Más bien, nos negamos de hacerle tanto caso a la teoría y corremos una simulación básica en su lugar; &lt;a href="https://speakerdeck.com/jakevdp/statistics-for-hackers"&gt;"si sabes escribir un for-loop, puedes tú hacer los estadísticos."&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Formulemos el problem así:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(V_t=13,066,047\)&lt;/span&gt; votantes llegan a votar.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{\text{yes}}\%\)&lt;/span&gt; de los votantes tienen la intención de votar por el "Sí", mientras el resto &lt;span class="math"&gt;\((1-p_{\text{yes}})\%\)&lt;/span&gt; tienen la intención de votar por el "No."&lt;/li&gt;
&lt;li&gt;Cada persona invalida su voto (por dejarlo no marcado o nulo) con una probabilidad de &lt;span class="math"&gt;\(p_{\text{invalid}}\%\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Cada voto válido se pone en la urna incorrecta con una probabilidad de &lt;span class="math"&gt;\(p_{\text{misclassification}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;El voto mayoritario gana.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;YES_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6377482&lt;/span&gt;
&lt;span class="n"&gt;NO_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6431376&lt;/span&gt;
&lt;span class="n"&gt;UNMARKED_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;86243&lt;/span&gt;
&lt;span class="n"&gt;NULL_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;170946&lt;/span&gt;

&lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;YES_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;NO_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;UNMARKED_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;NULL_BALLOTS&lt;/span&gt;
&lt;span class="n"&gt;P_INVALID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;02&lt;/span&gt;
&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt;
&lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En cada prueba, asumimos una proporción verdadera y subyacente &lt;span class="math"&gt;\(p_{\text{yes}}\%\)&lt;/span&gt; de gente que vota por el "Sí." Por ejemplo, si le damos .48 al &lt;span class="math"&gt;\(p_{\text{yes}}\)&lt;/span&gt;, tendremos &lt;span class="math"&gt;\(V_t * p_{\text{yes}}\)&lt;/span&gt; individuos con la intención de votar por el "Sí," y los demás &lt;span class="math"&gt;\(V_t * (1-p_{\text{yes}})\)&lt;/span&gt; por el "No." Asumimos que estos valores son estáticos: no son generados por un proceso random.&lt;/p&gt;
&lt;p&gt;A continuación, cada person entrega un voto inválido con probabilidad &lt;span class="math"&gt;\(p_{\text{invalid}}\)&lt;/span&gt;, que modelamos como una Binomial random variable. Los votos válidos que quedan se ponen en la urna equivocada, también modelado con un proceso Binomial. Por fin, el número de votos por el "Sí" y por el "No" se cuentan, y el porcentaje de los que pertenecen al "Sí" se entrega.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_votes&lt;/span&gt;

    &lt;span class="n"&gt;yes_votes_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;yes_votes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;no_votes_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;no_votes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;invalid_ballots_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;yes_votes_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_INVALID&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;invalid_ballots_no&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;no_votes_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_INVALID&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;valid_yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yes_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;invalid_ballots_yes&lt;/span&gt;
    &lt;span class="n"&gt;valid_no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;invalid_ballots_no&lt;/span&gt;

    &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_yes_votes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;no_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valid_yes_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt;

    &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_no_votes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;yes_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valid_no_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt;

    &lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_no_voters&lt;/span&gt;
    &lt;span class="n"&gt;tallied_no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_yes_voters&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tallied_no_votes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Intentémoslo para valores diferentes de &lt;span class="math"&gt;\(p_{\text{yes}}\)&lt;/span&gt;. Para empezar, si el porcentaje verdadero y subyacente de los que querían el "Sí" fuera 51%, ¿con qué frecuencia ganaría el "No?"&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Un resultado lógico. Dadas nuestras suposiciones, si 51% de los colombianos tuviera la intención de votar por el "Sí," el "No" hubiera ganado igual en 0 de 100,000 pruebas. Pues, la pregunta es la siguiente: ¿cuánto nos podemos acercar a la linea divisoria antes de empezar a ver resultados que no son representativos?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;probability_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
    &lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;proportion_of_trials_won_by_no&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p_yes: {:1.6f}% | no_win_percentage: {:1.3f}%&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;proportion_of_trials_won_by_no&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;60.000000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;51.000000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.100000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.010000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.191&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.001000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.688&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.000100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;48.791&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.000010&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.063&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La primera frustración llega cuando &lt;span class="math"&gt;\(p_{\text{yes}} = .5001\)&lt;/span&gt;: si &lt;span class="math"&gt;\(V_t * p_{\text{yes}} = 13,066,047 * .5001 \approx 6,534,330\)&lt;/span&gt; votantes quisieran el "Sí" vs. &lt;span class="math"&gt;\(\approx 6,531,716\)&lt;/span&gt; el "No," el "No" hubiera ganado igual el &lt;span class="math"&gt;\(0.191\%\)&lt;/span&gt; del tiempo. Otra vez, este resultado cuenta con el error humano: tanto por parte del votante en producir un voto inválido, como el del personal en introducir por accidente dicho voto en la urna equivocada.&lt;/p&gt;
&lt;p&gt;Mientras continuamos con la lista, los resultados se tornan más variables. Al &lt;span class="math"&gt;\(p_{\text{yes}} = .50001\)&lt;/span&gt;, se puede esperar que gane el "Sí" en tan sólo el &lt;span class="math"&gt;\(1 - .38688 = 61.312\%\)&lt;/span&gt; del tiempo. Por fin, al &lt;span class="math"&gt;\(p_{\text{yes}} = .5000001\)&lt;/span&gt; (que significa, tengan en cuenta, que existe una diferencia de personas que tenían la intención de votar por el "Sí" vs. por el "No" de tan sólo &lt;span class="math"&gt;\(13,066,047 * (p_{\text{yes}} - (1 - p_{\text{yes}})) \approx 3\)&lt;/span&gt;), el "No" aún hubiera ganado de veras en la &lt;em&gt;mayoría&lt;/em&gt; de las pruebas hipotéticas. En ese caso, no estamos haciendo nada más que lanzar monedas.&lt;/p&gt;
&lt;p&gt;En resumen, como dicen los autores del post mencionado, sería estadísticamente irresponsable aducir una victoria definitiva para el "No." De otra manera, el margen verdadera y subyacente parece súper escaso de verdad: al fin del día, quizás un voto mayoritario no sea la mejor forma para resolver esta clase de problemas.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Código:&lt;/p&gt;
&lt;p&gt;Pueden encontrar el &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/colombia-vote-simulation/blob/master/colombia-vote-simulation.ipynb"&gt;notebook&lt;/a&gt; y &lt;a href="https://github.com/cavaunpeu/colombia-vote-simulation"&gt;repo&lt;/a&gt; para este análisis aquí. &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Referencias claves incluyen:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://theconversation.com/colombia-did-not-vote-no-in-its-peace-referendum-what-the-statistics-reveal-66471"&gt;Colombia did not vote ‘no’ in its peace referendum – what the statistics reveal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://andrewgelman.com/2016/10/04/did-colombia-really-vote-no-in-that-peace-referendum/"&gt;Did Colombia really vote no in that peace referendum?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://plebiscito.registraduria.gov.co/99PL/DPLZZZZZZZZZZZZZZZZZ_L1.htm"&gt;Plebiscito Site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Gracias a Daniela Fleishman por su edición.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry></feed>