<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>will wolf</title>
	<meta name="description" content="writings on machine learning, crypto, geopolitics, life">
	<meta name="author" content="Will Wolf">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Favicon -->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="https://willwolf.io/theme/html5.js"></script>
		<![endif]-->

	<!-- Atom Feed -->

	<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@willwolf_">
  <meta name="twitter:creator" content="@willwolf_">
  <meta name="twitter:domain" content="https://willwolf.io">
    <meta property="twitter:title" content="Deriving Mean-Field Variational Bayes"/>
    <meta property="twitter:description" content="A detailed derivation of Mean-Field Variational Bayes, its connection to Expectation-Maximization, and its implicit motivation for the &#34;black-box variational inference&#34; methods born in recent years."/>
    <meta property="twitter:image" content="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-1.png"/>

	<!-- CSS -->
	<link href="https://willwolf.io/theme/css/ipython.css?v={12345}" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
	<link href='https://fonts.googleapis.com/css?family=Berkshire Swash' rel='stylesheet' type='text/css'>
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.3.7/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/local.css?v={12345}" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/pygments.css?v={12345}" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/main.css?v={12345}" rel="stylesheet">
</head><body>
<div class="container">
<div class="page-header">
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <h1><a id="site-title" href="https://willwolf.io/">will wolf</a></h1>
            <h4 id="site-subtitle-with-links">writings on <a id="sitesubtitle-machine-learning" href="/machine-learning">machine learning</a>, <a id="sitesubtitle-crypto" href="/crypto">crypto</a>, <a id="sitesubtitle-geopolitics" href="/geopolitics">geopolitics</a>, <a id="sitesubtitle-life" href="/life">life</a></h4>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav navbar-right">
              <li><a href="https://willwolf.io/about/" title="About">About</a></li>
              <li><a href="https://willwolf.io/consulting/" title="Consulting"></span> Consulting</a></li>
              <li><a href="https://willwolf.io/books/" title="Books">Books</a></li>
          <li>
            <button id="subscribeButton">
              <a href="https://willwolf.io/subscribe/" title="Get new posts by email">Subscribe</a>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</div>	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">Deriving Mean-Field Variational Bayes</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Will Wolf</h4>
		</span>
		<time datetime="2018-11-23T10:00:00-05:00" itemprop="datePublished">November 23, 2018</time>
	</div>
	<div itemprop="articleBody" class="article-body"><p>"Mean-Field Variational Bayes" (MFVB), is similar to <a href="https://willwolf.io/2018/11/11/em-for-lda/">expectation-maximization</a> (EM) yet distinct in two key ways:</p>
<ol>
<li>We do not minimize <span class="math">\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)</span>, i.e. perform the E-step, as [in the problems in which we employ mean-field] the posterior distribution <span class="math">\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)</span> "is too complex to work with,"™ i.e. it has no analytical form.</li>
<li>Our variational distribution <span class="math">\(q(\mathbf{Z})\)</span> is a <em>factorized distribution</em>, i.e.</li>
</ol>
<div class="math">$$
q(\mathbf{Z}) = \prod\limits_i^{M} q_i(\mathbf{Z}_i)
$$</div>
<p>for all latent variables <span class="math">\(\mathbf{Z}_i\)</span>.</p>
<p>Briefly, factorized distributions are cheap to compute: if each <span class="math">\(q_i(\mathbf{Z}_i)\)</span> is Gaussian, <span class="math">\(q(\mathbf{Z})\)</span> requires optimization of <span class="math">\(2M\)</span> parameters (a mean and a variance for each factor); conversely, a non-factorized <span class="math">\(q(\mathbf{Z}) = \text{Normal}(\mu, \Sigma)\)</span> would require optimization of <span class="math">\(M\)</span> parameters for the mean and <span class="math">\(\frac{M^2 + M}{2}\)</span> parameters for the covariance. Following intuition, this gain in computational efficiency comes at the cost of decreased accuracy in approximating the true posterior over latent variables.</p>
<h2>So, what is it?</h2>
<p>Mean-field Variational Bayes is an iterative maximization of the ELBO. More precisely, it is an iterative M-step with respect to the variational factors <span class="math">\(q_i(\mathbf{Z}_i)\)</span>.</p>
<p>In the simplest case, we posit a variational factor over every latent variable, <em>as well as every parameter</em>. In other words, as compared to the log-marginal decomposition in EM, <span class="math">\(\theta\)</span> is absorbed into <span class="math">\(\mathbf{Z}\)</span>.</p>
<div class="math">$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\quad \text{(EM)}
$$</div>
<p>becomes</p>
<div class="math">$$
\log{p(\mathbf{X})} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\quad \text{(MFVB)}
$$</div>
<p>From there, we simply maximize the ELBO, i.e. <span class="math">\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]\)</span>, by <em>iteratively maximizing with respect to each variational factor <span class="math">\(q_i(\mathbf{Z}_i)\)</span></em> in turn.</p>
<h2>What's this do?</h2>
<p>Curiously, we note that <span class="math">\(\log{p(\mathbf{X})}\)</span> is a <em>fixed quantity</em> with respect to <span class="math">\(q(\mathbf{Z})\)</span>: updating our variational factors <em>will not change</em> the marginal log-likelihood of our data.</p>
<p>This said, we note that the ELBO and <span class="math">\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\)</span> trade off linearly: when one goes up by <span class="math">\(\Delta\)</span>, the other goes down by <span class="math">\(\Delta\)</span>.</p>
<p>As such, (iteratively) maximizing the ELBO in MFVB is akin to minimizing the divergence between the true posterior over the latent variables given data and our factorized variational approximation thereof.</p>
<h2>Derivation</h2>
<p>So, what do these updates look like?</p>
<p>First, let's break the ELBO into its two main components:</p>
<div class="math">$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;= \int{q(\mathbf{Z})\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}}d\mathbf{Z}\\
&amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z} - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;= A + B
\end{align*}
$$</div>
<p>Next, rewrite this expression in a way that isolates a single variational factor <span class="math">\(q_j(\mathbf{Z}_j)\)</span>, i.e. the factor with respect to which we'd like to maximize the ELBO in a given iteration.</p>
<h2>Expanding the first term</h2>
<div class="math">$$
\begin{align*}
A
&amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}d\mathbf{Z}}\\
&amp;= \int{\prod\limits_{i}q_i(\mathbf{Z}_i)\log{p(\mathbf{X, Z})}d\mathbf{Z}_i}\\
&amp;= \int{q_j(\mathbf{Z}_j)\bigg[\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i\bigg]}d\mathbf{Z}_j\\
&amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j}\\
\end{align*}
$$</div>
<p>Following Bishop<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>'s derivation, we've introduced the notation:</p>
<div class="math">$$
\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$</div>
<p>A few things to note, and in case this looks strange:</p>
<ul>
<li>Were the left-hand side to read <span class="math">\(\int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z}\)</span>, this would look like the perfectly vanilla expectation <span class="math">\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)</span>.</li>
<li>An expectation maps a function <span class="math">\(f\)</span>, e.g. <span class="math">\(\log{p(\mathbf{X, Z})}\)</span>, to a single real number. As our expression reads <span class="math">\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)</span> as opposed to <span class="math">\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)</span>, we're conspicuously unable to integrate over the remaining factor <span class="math">\(q_j(\mathbf{Z}_j)\)</span></li>
<li><strong>As such, <span class="math">\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)</span> gives a function of the value of <span class="math">\(\mathbf{Z}_j\)</span></strong> which itself maps to the aforementioned real number.</li>
</ul>
<p>To further illustrate, let's employ some toy Python code:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Suppose `Z = [Z_0, Z_1, Z_2]`, with corresponding (discrete) variational distributions `q_0`, `q_1`, `q_2`</span>

<span class="n">q_0</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">),</span>  <span class="c1"># q_0(1) = .2</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">q_1</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">.3</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">.3</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">.4</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">q_2</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">.7</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">.2</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">dists</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_0</span><span class="p">,</span> <span class="n">q_1</span><span class="p">,</span> <span class="n">q_2</span><span class="p">)</span>

<span class="c1"># Next, suppose we'd like to isolate Z_2</span>
<span class="n">j</span> <span class="o">=</span> <span class="mi">2</span>
</code></pre></div>
<p><span class="math">\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)</span>, written <code>E_i_neq_j_log_p_X_Z</code> below, can be computed as:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">E_i_neq_j_log_p_X_Z</span><span class="p">(</span><span class="n">Z_j</span><span class="p">):</span>

    <span class="n">E</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">Z_i_neq_j_dists</span> <span class="o">=</span> <span class="p">[</span><span class="n">dist</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dists</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">comb</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">Z_i_neq_j_dists</span><span class="p">):</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">comb</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">comb</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dists</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">Z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Z_j</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">Z_i</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">comb</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">Z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Z_i</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">prob</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">prob</span> <span class="o">=</span> <span class="n">p</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">prob</span> <span class="o">*=</span> <span class="n">p</span>
        <span class="n">E</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">ln_p_X_Z</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">E</span>
</code></pre></div>
<ul>
<li>Continuing with our notes, it was not immediately obvious to me how and why we're able to introduce a second integral sign on line 3 of the derivation above. Notwithstanding, the reason is quite simple; a simple exercise of nested for-loops is illustrative.</li>
</ul>
<p>Before beginning, we remind the definition of an integral in code. In its simplest example, <span class="math">\(\int{ydx}\)</span> can be written as:</p>
<div class="highlight"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lower_lim</span><span class="p">,</span> <span class="n">upper_lim</span><span class="p">,</span> <span class="n">n_ticks</span><span class="p">)</span>

<span class="n">integral</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">y</span> <span class="o">*</span> <span class="n">dx</span> <span class="k">for</span> <span class="n">dx</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

<span class="c1"># ...where `n_ticks` approaches infinity.</span>
</code></pre></div>
<p>With this in mind, the following confirms the self-evidence of the second integral sign:</p>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>


<span class="k">def</span><span class="w"> </span><span class="nf">ln_p_X_Z</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="n">Z</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># some dummy expression</span>


<span class="c1"># Line 2 of `Expanding the first term`</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">Z_0</span> <span class="ow">in</span> <span class="n">q_0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">Z_1</span> <span class="ow">in</span> <span class="n">q_1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">Z_2</span> <span class="ow">in</span> <span class="n">q_2</span><span class="p">:</span>
            <span class="n">val_z_0</span><span class="p">,</span> <span class="n">prob_z_0</span> <span class="o">=</span> <span class="n">Z_0</span>
            <span class="n">val_z_1</span><span class="p">,</span> <span class="n">prob_z_1</span> <span class="o">=</span> <span class="n">Z_1</span>
            <span class="n">val_z_2</span><span class="p">,</span> <span class="n">prob_z_2</span> <span class="o">=</span> <span class="n">Z_2</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">val_z_0</span><span class="p">,</span> <span class="n">val_z_1</span><span class="p">,</span> <span class="n">val_z_2</span><span class="p">])</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">prob_z_0</span> <span class="o">*</span> <span class="n">prob_z_1</span> <span class="o">*</span> <span class="n">prob_z_2</span> <span class="o">*</span> <span class="n">ln_p_X_Z</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>

<span class="n">TOTAL</span> <span class="o">=</span> <span class="n">total</span>


<span class="c1"># Line 3 of `Expanding the first term`</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">Z_0</span> <span class="ow">in</span> <span class="n">q_0</span><span class="p">:</span>
    <span class="n">_total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">val_z_0</span><span class="p">,</span> <span class="n">prob_z_0</span> <span class="o">=</span> <span class="n">Z_0</span>
    <span class="k">for</span> <span class="n">Z_1</span> <span class="ow">in</span> <span class="n">q_1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">Z_2</span> <span class="ow">in</span> <span class="n">q_2</span><span class="p">:</span>
            <span class="n">val_z_1</span><span class="p">,</span> <span class="n">prob_z_1</span> <span class="o">=</span> <span class="n">Z_1</span>
            <span class="n">val_z_2</span><span class="p">,</span> <span class="n">prob_z_2</span> <span class="o">=</span> <span class="n">Z_2</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">val_z_0</span><span class="p">,</span> <span class="n">val_z_1</span><span class="p">,</span> <span class="n">val_z_2</span><span class="p">])</span>
            <span class="n">_total</span> <span class="o">+=</span> <span class="n">prob_z_1</span> <span class="o">*</span> <span class="n">prob_z_2</span> <span class="o">*</span> <span class="n">ln_p_X_Z</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">prob_z_0</span> <span class="o">*</span> <span class="n">_total</span>


<span class="k">assert</span> <span class="n">total</span> <span class="o">==</span> <span class="n">TOTAL</span>
</code></pre></div>
<p>In effect, isolating <span class="math">\(q_j(\mathbf{Z}_j)\)</span> is akin to the penultimate line <code>total += prob_z_0 * _total</code>, i.e. multiplying <span class="math">\(q_j(\mathbf{Z}_j)\)</span> by an intermediate summation <code>_total</code>.  Therefore, the second integral sign is akin to <code>_total += prob_z_1 * prob_z_2 * ln_p_X_Z(X, Z)</code>, i.e. the computation of this intermediate summation itself.</p>
<p>More succinctly, a multi-dimensional integral can be thought of as a nested-for-loop which commutes a global sum. Herein, we are free to compute intermediate sums at will.</p>
<h2>Expanding the second term</h2>
<p>Next, let's expand <span class="math">\(B\)</span>. We note that this is the entropy of the full variational distribution <span class="math">\(q(\mathbf{Z})\)</span>.</p>
<div class="math">$$
\begin{align*}
B
&amp;= - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\prod\limits_{i}q_i(\mathbf{Z}_i)}\bigg]\\
&amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)} + \sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q_{i \neq j}(\mathbf{Z}_i)}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] + \text{const}\\
&amp;= - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$</div>
<p>As we'll be maximizing w.r.t. just <span class="math">\(q_j(\mathbf{Z}_j)\)</span>, we can set all terms that don't include this factor to constants.</p>
<h2>Putting it back together</h2>
<div class="math">$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;= A + B\\
&amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$</div>
<h2>One final pseudonym</h2>
<p>Were we able to replace the expectation in <span class="math">\(A\)</span> with the <span class="math">\(\log\)</span> of some density <span class="math">\(D\)</span>, i.e.</p>
<div class="math">$$
= \int{q_j(\mathbf{Z}_j){ \log{D} }\ d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}
$$</div>
<p><span class="math">\(A + B\)</span> could be rewritten as <span class="math">\(-\text{KL}(q_j(\mathbf{Z}_j)\Vert D)\)</span>.</p>
<p>Acknowledging that <span class="math">\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)</span> is an unnormalized log-likelihood written as a function of <span class="math">\(\mathbf{Z}_j\)</span>, we temporarily rewrite it as:</p>
<div class="math">$$
\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] = \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j})
$$</div>
<p>As such:</p>
<div class="math">$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;= \int{q_j(\mathbf{Z}_j){ \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j}) }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
&amp;= \int{q_j(\mathbf{Z}_j){ \log{\frac{\tilde{p}(\mathbf{X}, \mathbf{Z}_j)}{q_j(\mathbf{Z}_j)}} }d\mathbf{Z}_j} + \text{const}\\
&amp;= - \text{KL}\big(q_j(\mathbf{Z}_j)\Vert \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\big) + \text{const}\\
\end{align*}
$$</div>
<p>Finally, per this expression, the ELBO reaches its minimum when:</p>
<div class="math">$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;= \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\\
&amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}
\end{align*}
$$</div>
<p>Or equivalently:</p>
<div class="math">$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$</div>
<p><strong>Summing up:</strong></p>
<ul>
<li>Iteratively minimizing the divergence between <span class="math">\(q_j(\mathbf{Z}_j)\)</span> and <span class="math">\(\tilde{p}(\mathbf{X}, \mathbf{Z}_j)\)</span> for all factors <span class="math">\(j\)</span> is our mechanism for maximizing the ELBO</li>
<li>In turn, maximizing the ELBO is our mechanism for minimizing the KL divergence between the full factorized posterior <span class="math">\(q(\mathbf{Z})\)</span> and the true posterior <span class="math">\(p(\mathbf{Z}\vert\mathbf{X})\)</span>.</li>
</ul>
<p>Finally, as the optimal density <span class="math">\(q_j(\mathbf{Z}_j)\)</span> relies on those of <span class="math">\(q_{i \neq j}(\mathbf{Z}_{i})\)</span>, this optimization algorithm is necessarily <em>iterative</em>.</p>
<h2>Normalization constant</h2>
<p>Nearing the end, we note that <span class="math">\(q_j(\mathbf{Z}_j) = \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}\)</span> is not necessarily a normalized density (over <span class="math">\(\mathbf{Z}_j\)</span>). "By inspection," we compute:</p>
<div class="math">$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;= \frac{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}}{\int{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}d\mathbf{Z}_j}}\\
&amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)} + \text{const}\\
\end{align*}
$$</div>
<h2>How to actually employ this thing</h2>
<p>First, plug in values for the right-hand side of:</p>
<div class="math">$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$</div>
<p>Then, attempt to rearrange this expression such that:</p>
<p>Once exponentiated, giving <span class="math">\(\exp{\big(\log{q_j(\mathbf{Z}_j)}\big)} = q_j(\mathbf{Z}_j)\)</span>, we are left with something that, once normalized (by inspection), resembles a known density function (e.g. a Gaussian, a Gamma, etc.).</p>
<p>NB: This may require significant computation.</p>
<h1>Approximating a Gaussian</h1>
<p>Here, we'll approximate a 2D multivariate Gaussian with a factorized mean-field approximation.</p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-1.png"/></p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-2.png"/></p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-3.png"/></p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-4.png"/></p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-5.png"/></p>
<h1>Summing up</h1>
<p>Mean-Field Variational Bayes is an iterative optimization algorithm for maximizing a lower-bound of the marginal likelihood of some data <span class="math">\(\mathbf{X}\)</span> under a given model with latent variables <span class="math">\(\mathbf{Z}\)</span>. It accomplishes this task by positing a factorized variational distribution over all latent variables <span class="math">\(\mathbf{Z}\)</span> and parameters <span class="math">\(\theta\)</span>, then computes, <em>analytically</em>, the algebraic forms and parameters of each factor which maximize this bound.</p>
<p>In practice, this process can be cumbersome and labor-intensive. As such, in recent years, "black-box variational inference" techniques were born, which <em>fix</em> the forms of each factor <span class="math">\(q_j(\mathbf{Z}_j)\)</span>, then optimize its parameters via gradient descent.</p>
<h2>References</h2>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p>C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. <a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="willwolf_">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'willwolf';
    var disqus_title = 'Deriving Mean-Field Variational Bayes';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</div> </div>
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-11 col-md-offset-1" id="footer-wrapper">
				<div class="col-md-3">
					<div id="social-links">
						<h4>
							Contact
						</h4>
						<li><a href="mailto:williamabrwolf@gmail.com">Email</a></li>
							<li><a href="http://twitter.com/willwolf_">Twitter</a></li>
							<li><a href="http://linkedin.com/in/williamabrwolf">LinkedIn</a></li>
							<li><a href="http://calendly.com/willwolf">Calendly</a></li>
					</div>
				</div>
				<div class="col-md-3">
					<div id="other-links">
						<h4>
							Links
						</h4>
						<ul>
							<li><a href="https://tinyletter.com/willwolf">Newsletter</a></li>
							<li><a href="http://willtravellife.com">Travel Blog</a></li>
							<li><a href="http://github.com/cavaunpeu">Github</a></li>
							<li><a href="https://github.com/cavaunpeu/willwolf.io-source">Source Code</a></li>
						</ul>
					</div>
				</div>
				<div class="col-md-3">
				  <div id="categories">
				    <h4>
				      Categories
				    </h4>
				    <ul>
				      <li><a href="https://willwolf.io/crypto/">crypto</li>
				      <li><a href="https://willwolf.io/geopolitics/">geopolitics</li>
				      <li><a href="https://willwolf.io/life/">life</li>
				      <li><a href="https://willwolf.io/machine-learning/">machine-learning</li>
				    </ul>
				  </div>
				</div>
				<div class="col-md-3">
				  <div id="pages">
				    <h4>
				      Pages
				    </h4>
				    <ul>
				      <li><a href="https://willwolf.io/archive/" title="Archive">Archive</a></li>
				      <li><a href="https://willwolf.io/WillWolfResume.pdf" title="Résumé">Résumé</a></li>
				    </ul>
				  </div>
				</div>
			</div>
		</div>
	</div>
</footer><div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Will Wolf 2020</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div><!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-97412095-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>