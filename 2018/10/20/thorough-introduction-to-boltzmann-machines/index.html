<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>will wolf</title>
	<meta name="description" content="writings on machine learning, crypto, geopolitics, life">
	<meta name="author" content="Will Wolf">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Favicon -->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="https://willwolf.io/theme/html5.js"></script>
		<![endif]-->

	<!-- Atom Feed -->

	<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@willwolf_">
  <meta name="twitter:creator" content="@willwolf_">
  <meta name="twitter:domain" content="https://willwolf.io">
    <meta property="twitter:title" content="A Thorough Introduction to Boltzmann Machines"/>
    <meta property="twitter:description" content="A pedantic walk through Boltzmann machines, with focus on the computational thorn-in-side of the partition function."/>
    <meta property="twitter:image" content="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_23_1.png"/>

	<!-- CSS -->
	<link href="https://willwolf.io/theme/css/ipython.css?v={12345}" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
	<link href='https://fonts.googleapis.com/css?family=Berkshire Swash' rel='stylesheet' type='text/css'>
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.3.7/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/local.css?v={12345}" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/pygments.css?v={12345}" rel="stylesheet">
	<link href="https://willwolf.io/theme/css/main.css?v={12345}" rel="stylesheet">
</head><body>
<div class="container">
<div class="page-header">
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <h1><a id="site-title" href="https://willwolf.io/">will wolf</a></h1>
            <h4 id="site-subtitle-with-links">writings on <a id="sitesubtitle-machine-learning" href="/machine-learning">machine learning</a>, <a id="sitesubtitle-crypto" href="/crypto">crypto</a>, <a id="sitesubtitle-geopolitics" href="/geopolitics">geopolitics</a>, <a id="sitesubtitle-life" href="/life">life</a></h4>
      </div>
      <div class="collapse navbar-collapse" id="myNavbar">
        <ul class="nav navbar-nav navbar-right">
              <li><a href="https://willwolf.io/about/" title="About">About</a></li>
              <li><a href="https://willwolf.io/consulting/" title="Consulting"></span> Consulting</a></li>
              <li><a href="https://willwolf.io/books/" title="Books">Books</a></li>
          <li>
            <button id="subscribeButton">
              <a href="https://willwolf.io/subscribe/" title="Get new posts by email">Subscribe</a>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</div>	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">A Thorough Introduction to Boltzmann Machines</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person">
			<h4 itemprop="name">Will Wolf</h4>
		</span>
		<time datetime="2018-10-20T14:00:00-04:00" itemprop="datePublished">October 20, 2018</time>
	</div>
	<div itemprop="articleBody" class="article-body"><p>The principal task of machine learning is to fit a model to some data. In programming terms, this model is an object with two methods:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div>
<h2>Likelihood</h2>
<p>How likely is the query point <span class="math">\(x\)</span> under our model? In other words, how likely was it that our model produced <span class="math">\(x\)</span>?</p>
<p>Note: The likelihood gives a value proportional to a valid probability, but is not necessarily a valid probability itself.</p>
<h2>Sample</h2>
<p>Draw a sample datum <span class="math">\(x\)</span> from the model.</p>
<h2>Notation</h2>
<p>Canonically, we denote an instance of our <code>Model</code> in mathematical syntax as follows:</p>
<div class="math">$$
x \sim p(x)
$$</div>
<p>Again, this simple notation implies two powerful methods: that we can evaluate the <code>likelihood</code> of having observed <span class="math">\(x\)</span> under our model <span class="math">\(p\)</span>, and that we can <code>sample</code> a new value <span class="math">\(x\)</span> from our model <span class="math">\(p\)</span>.</p>
<p>Often, we work instead with <em>conditional</em> models, e.g. <span class="math">\(y \sim p(y\vert x)\)</span>, in classification and regression tasks. The <code>likelihood</code> and <code>sample</code> methods apply all the same.</p>
<h2>Boltzmann machines</h2>
<p>A Boltzmann machine is one of the simplest mechanisms for modeling <span class="math">\(p(x)\)</span>. It is an undirected graphical model where every dimension <span class="math">\(x_i\)</span> of a given observation <span class="math">\(x\)</span> influences every other dimension. As such, we might use it to model data which we believe to exhibit this property, e.g. an image (where intuitively, pixel values influence neighboring pixel values). For <span class="math">\(x \in R^3\)</span>, our model would look as follows:</p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/boltzmann-machine.svg"/></p>
<p>For <span class="math">\(x \in R^n\)</span>, a given node <span class="math">\(x_i\)</span> would have <span class="math">\(n - 1\)</span> outgoing connections in totalâ€”one to every other node <span class="math">\(x_j\ \forall\ j \neq i\)</span>.</p>
<p>Finally, a Boltzmann machine strictly operates on <em>binary</em> data. This keeps things simple.</p>
<h2>Computing the likelihood</h2>
<p>A Boltzmann machines admits the following formula for computing the <code>likelihood</code> of data points <span class="math">\(x^{(1)}, ..., x^{(n)}\)</span>:</p>
<div class="math">$$
H(x) = \sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i
$$</div>
<div class="math">$$
p(x) = \frac{\exp{(H(x))}}{Z}
$$</div>
<div class="math">$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{i=1}^n p(x^{(i)})
$$</div>
<p>Note:</p>
<ul>
<li>Since our weights can be negative, <span class="math">\(H(x)\)</span> can be negative. Since our likelihood is proportional to a valid probability, we'd prefer it to be non-negative.</li>
<li>To enforce this constraint, we exponentiate <span class="math">\(H(x)\)</span> in the second equation.</li>
<li>To normalize, we divide by the normalization constant <span class="math">\(Z\)</span>, i.e. the sum of the likelihoods of all possible values of <span class="math">\(x\)</span>.</li>
</ul>
<h2>Computing the partition function by hand</h2>
<p>In the case of 2-dimensional binary datum <span class="math">\(x\)</span>, there are 4 possible "configurations": <span class="math">\([0, 0], [0, 1], [1, 0], [1, 1]\)</span>. As such, to compute the likelihood of one of these configurations, e.g.</p>
<div class="math">$$
p([1, 0]) = \frac{\exp{(H([1, 0]))}}{\exp{(H([0, 0]))} + \exp{(H([0, 1]))} + \exp{(H([1, 0]))} + \exp{(H([1, 1]))}}
$$</div>
<p>we see that the normalization constant <span class="math">\(Z\)</span> is a sum of 4 terms.</p>
<p>More generally, given <span class="math">\(d\)</span>-dimensional <span class="math">\(x\)</span>, where each <span class="math">\(x_i\)</span> can assume one of <span class="math">\(v\)</span> distinct values, computing <span class="math">\(p(x)\)</span> implies a summation over <span class="math">\(v^d\)</span> terms. <strong>With a non-trivially large <span class="math">\(v\)</span> or <span class="math">\(d\)</span> this becomes intractable to compute.</strong></p>
<p>Below, we'll demonstrate how "tractability," i.e. "can we actually compute <span class="math">\(Z\)</span> before the end of the universe?" changes with varying <span class="math">\(d\)</span> for our Boltzmann machine (of <span class="math">\(v = 2\)</span>).</p>
<h2>The likelihood function in code</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_unnormalized_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_H</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_H</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_combinations</span><span class="p">:</span>
        <span class="n">h</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">h</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">@</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">h</span>

<span class="k">def</span><span class="w"> </span><span class="nf">likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    :param x: a vector of shape (n_units,) or (n, n_units),</span>
<span class="sd">        where the latter is a matrix of multiple data points</span>
<span class="sd">        for which to compute the joint likelihood.</span>
<span class="sd">    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_units</span><span class="p">:</span>
        <span class="k">raise</span><span class="p">(</span><span class="s1">'Please pass 1 or more points of `n_units` dimensions'</span><span class="p">)</span>

    <span class="c1"># compute unnormalized likelihoods</span>
    <span class="n">multiple_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">multiple_samples</span><span class="p">:</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_unnormalized_likelihood</span><span class="p">(</span><span class="n">point</span><span class="p">)</span> <span class="k">for</span> <span class="n">point</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">likelihood</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_unnormalized_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>

    <span class="c1"># compute partition function</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_unnormalized_likelihood</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_configs</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">log</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lik</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="k">for</span> <span class="n">lik</span> <span class="ow">in</span> <span class="n">likelihood</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">reduce</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">,</span> <span class="p">[</span><span class="n">lik</span> <span class="o">/</span> <span class="n">Z</span> <span class="k">for</span> <span class="n">lik</span> <span class="ow">in</span> <span class="n">likelihood</span><span class="p">])</span>
</code></pre></div>
<p>NB: In mathematical Python code, for-loops are bad; we should prefer <code>numpy</code> instead. Nevertheless, I've used for-loops here because they are easier to read.</p>
<p>This code block is longer than you might expect because it includes a few supplementary behaviors, namely:</p>
<ul>
<li>Computing the likelihood of one or more points</li>
<li>Avoiding redundant computation of <code>Z</code></li>
<li>Optionally computing the log-likelihood</li>
</ul>
<h2>Training the model</h2>
<p>At the outset, the parameters <code>self.weights</code> and <code>self.biases</code> of our model are initialized at random. Trivially, such that the values returned by <code>likelihood</code> and <code>sample</code> are useful, we must first update these parameters by fitting this model to observed data.</p>
<p>To do so, we will employ the principal of maximum likelihood: compute the parameters that make the observed data maximally likely under the model, via gradient ascent.</p>
<h2>Gradients</h2>
<p>Since our model is simple, we can derive exact gradients by hand. We will work with the log-likelihood instead of the true likelihood to avoid issues of computational underflow. Below, we simplify this expression, then compute its various gradients.</p>
<h3><span class="math">\(\log{\mathcal{L}}\)</span></h3>
<div class="math">$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{k=1}^n \frac{\exp{(H(x^{(k)})}}{Z}
$$</div>
<div class="math">$$
\begin{align*}
\log{\mathcal{L}(x^{(1)}, ..., x^{(n)})}
&amp;= \sum\limits_{k=1}^n \log{\frac{\exp{(H(x^{(k)})}}{Z}}\\
&amp;= \sum\limits_{k=1}^n \log{\big(\exp{(H(x^{(k)})}\big)} - \log{Z}\\
&amp;= \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
\end{align*}
$$</div>
<p>This gives the total likelihood. Our aim is to maximize the expected likelihood with respect to the data generating distribution.</p>
<h3>Expected likelihood</h3>
<div class="math">$$
\begin{align*}
\mathop{\mathbb{E}}_{x \sim p_{\text{data}}}\big[ \mathcal{L}(x) \big]
&amp;= \sum\limits_{k=1}^N p_{\text{data}}(x = x^{(k)}) \mathcal{L(x^{(k)})}\\
&amp;\approx \sum\limits_{k=1}^N \frac{1}{N} \mathcal{L(x^{(k)})}\\
&amp;= \frac{1}{N} \sum\limits_{k=1}^N  \mathcal{L(x^{(k)})}\\
\end{align*}
$$</div>
<p>In other words, we wish to maximize the average likelihood of our data under the model. Henceforth, we will refer to this quantity as <span class="math">\(\mathcal{L}\)</span>, i.e. <span class="math">\(\mathcal{L} = \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}\)</span>.</p>
<p>Now, deriving the gradient with respect to our weights:</p>
<h3><span class="math">\(\nabla_{w_{i, j}}\log{\mathcal{L}}\)</span>:</h3>
<div class="math">$$
\begin{align*}
\nabla_{w_{i, j}} \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
&amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)}) - \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \log{Z}
\end{align*}
$$</div>
<h3>First term:</h3>
<div class="math">$$
\begin{align*}
\frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)})
&amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \bigg[ \sum\limits_{i \neq j} w_{i, j} x_i^{(k)} x_j^{(k)} + \sum\limits_i b_i x_i^{(k)} \bigg]\\
&amp;= \frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\\
&amp;\approx \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j]
\end{align*}
$$</div>
<h3>Second term:</h3>
<p>NB: <span class="math">\(\sum\limits_{\mathcal{x}}\)</span> implies a summation over all <span class="math">\(v^d\)</span> possible configurations of <span class="math">\(x\)</span>.</p>
<div class="math">$$
\begin{align*}
\nabla_{w_{i, j}} \log{Z}
&amp;= \nabla_{w_{i, j}} \log{\sum\limits_{\mathcal{x}}} \exp{(H(x))}\\
&amp;= \frac{1}{\sum\limits_{\mathcal{x}} \exp{(H(x))}} \nabla_{w_{i, j}} \sum\limits_{\mathcal{x}} \exp{(H(x))}\\
&amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \nabla_{w_{i, j}} \exp{(H(x))}\\
&amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \exp{(H(x))} \nabla_{w_{i, j}} H(x)\\
&amp;= \sum\limits_{\mathcal{x}} \frac{\exp{(H(x))}}{Z} \nabla_{w_{i, j}} H(x)\\
&amp;= \sum\limits_{\mathcal{x}} p(x) \nabla_{w_{i, j}} H(x)\\
&amp;= \sum\limits_{\mathcal{x}} p(x) [x_i  x_j]\\
&amp;= \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
\end{align*}
$$</div>
<h3>Putting it back together</h3>
<p>Combining these constituent parts, we arrive at the following formula:</p>
<div class="math">$$
\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
$$</div>
<p>Finally, following the same logic, we derive the exact gradient with respect to our biases:</p>
<div class="math">$$
\nabla_{b_i}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$</div>
<p>The first and second terms of each gradient are called, respectively, <strong>the positive and negative phases.</strong></p>
<h2>Computing the positive phase</h2>
<p>In the following toy example, our data are small: we can compute the positive phase using all of the training data, i.e. <span class="math">\(\frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\)</span>. Were our data bigger, we could approximate this expectation with a mini-batch of training data (much like SGD).</p>
<h2>Computing the negative phase</h2>
<p>Again, this term asks us to compute then sum the log-likelihood over every possible data configuration in the support of our model, which is <span class="math">\(O(v^d)\)</span>. <strong>With non-trivially large <span class="math">\(v\)</span> or <span class="math">\(d\)</span>, this becomes intractable to compute.</strong></p>
<p>Below, we'll begin our toy example by computing the true negative-phase, <span class="math">\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)</span>, with varying data dimensionalities <span class="math">\(d\)</span>. Then, once this computation becomes slow, we'll look to approximate this expectation later on.</p>
<h2>Parameter updates in code</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">update_parameters_with_true_negative_phase</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">)</span>
    <span class="n">model_distribution</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">config</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">config</span><span class="p">))</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">all_configs</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">var_combinations</span><span class="p">:</span>
        <span class="c1"># positive phase</span>
        <span class="n">positive_phase</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># negative phase</span>
        <span class="n">negative_phase</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">config</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">likelihood</span> <span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">model_distribution</span><span class="p">])</span>

        <span class="c1"># update weights</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">biases</span><span class="p">):</span>
        <span class="c1"># positive phase</span>
        <span class="n">positive_phase</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># negative phase</span>
        <span class="n">negative_phase</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">config</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">likelihood</span> <span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">model_distribution</span><span class="p">])</span>

        <span class="c1"># update biases</span>
        <span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">biases</span><span class="p">)</span>
</code></pre></div>
<h2>Train model, visualize model distribution</h2>
<p>Finally, we're ready to train. Using the true negative phase, let's train our model for 100 epochs with <span class="math">\(d=3\)</span> then visualize results.</p>
<div class="highlight"><pre><span></span><code><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">.01</span>


<span class="k">def</span><span class="w"> </span><span class="nf">reset_data_and_parameters</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">.8</span><span class="p">,</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.5</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Generate training data, weights, biases, and a list of all data configurations</span>
<span class="sd">    in our model's support.</span>

<span class="sd">    In addition, generate a list of tuples of the indices of adjacent nodes, which</span>
<span class="sd">    we'll use to update parameters without duplication.</span>

<span class="sd">    For example, with `n_units=3`, we generate a matrix of weights with shape (3, 3);</span>
<span class="sd">    however, there are only 3 distinct weights in this matrix that we'll actually</span>
<span class="sd">    want to update: those connecting Node 0 --&gt; Node 1, Node 1 --&gt; Node 2, and</span>
<span class="sd">    Node 0 --&gt; Node 2. This function returns a list containing these tuples</span>
<span class="sd">    named `var_combinations`.</span>

<span class="sd">    :param n_units: the dimensionality of our data `d`</span>
<span class="sd">    :param n_obs: the number of observations in our training set</span>
<span class="sd">    :param p: a vector of the probabilities of observing a 1 in each index</span>
<span class="sd">        of the training data. The length of this vector must equal `n_units`</span>

<span class="sd">    :return: weights, biases, var_combinations, all_configs, data</span>
<span class="sd">    """</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># initialize data</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_units</span><span class="p">))</span>

    <span class="c1"># initialize parameters</span>
    <span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_units</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_units</span><span class="p">,</span> <span class="n">n_units</span><span class="p">)</span>

    <span class="c1"># a few other pieces we'll need</span>
    <span class="n">var_combinations</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_units</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">all_configs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">repeat</span><span class="o">=</span><span class="n">n_units</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">,</span> <span class="n">data</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_combinations</span> <span class="o">=</span> <span class="n">var_combinations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_configs</span> <span class="o">=</span> <span class="n">all_configs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_units</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_inv_logit</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_unnormalized_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_H</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_H</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_combinations</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">h</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">@</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        :param x: a vector of shape (n_units,) or (n, n_units),</span>
<span class="sd">            where the latter is a matrix of multiple data points</span>
<span class="sd">            for which to compute the joint likelihood.</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_units</span><span class="p">:</span>
            <span class="k">raise</span><span class="p">(</span><span class="s1">'Please pass 1 or more points of `n_units` dimensions'</span><span class="p">)</span>

        <span class="c1"># compute unnormalized likelihoods</span>
        <span class="n">multiple_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">multiple_samples</span><span class="p">:</span>
            <span class="n">likelihood</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_unnormalized_likelihood</span><span class="p">(</span><span class="n">point</span><span class="p">)</span> <span class="k">for</span> <span class="n">point</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">likelihood</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_unnormalized_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>

        <span class="c1"># compute partition function</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_unnormalized_likelihood</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_configs</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">log</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lik</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="k">for</span> <span class="n">lik</span> <span class="ow">in</span> <span class="n">likelihood</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">reduce</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">,</span> <span class="p">[</span><span class="n">lik</span> <span class="o">/</span> <span class="n">Z</span> <span class="k">for</span> <span class="n">lik</span> <span class="ow">in</span> <span class="n">likelihood</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">init_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">burn_in</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">every_n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>

        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">burn_in</span> <span class="o">&gt;</span> <span class="n">n_samples</span><span class="p">:</span>
            <span class="k">raise</span><span class="p">(</span><span class="s2">"Can't burn in for more samples than there are in the chain"</span><span class="p">)</span>

        <span class="n">init_sample</span> <span class="o">=</span> <span class="n">init_sample</span> <span class="ow">or</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_sample</span><span class="p">]</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_gibbs_step</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">sample</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span> <span class="k">if</span> <span class="n">j</span> <span class="o">!=</span> <span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inv_logit</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># make copy</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
                <span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_gibbs_step</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">sample</span> <span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sample</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">burn_in</span><span class="p">:])</span> <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">every_n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">])</span>


<span class="k">def</span><span class="w"> </span><span class="nf">update_parameters_with_true_negative_phase</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">)</span>
    <span class="n">model_distribution</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">config</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">config</span><span class="p">))</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">all_configs</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">var_combinations</span><span class="p">:</span>
        <span class="c1"># positive phase</span>
        <span class="n">positive_phase</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># negative phase</span>
        <span class="n">negative_phase</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">config</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">likelihood</span> <span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">model_distribution</span><span class="p">])</span>

        <span class="c1"># update weights</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">biases</span><span class="p">):</span>
        <span class="c1"># positive phase</span>
        <span class="n">positive_phase</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># negative phase</span>
        <span class="n">negative_phase</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">config</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">likelihood</span> <span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">model_distribution</span><span class="p">])</span>

        <span class="c1"># update biases</span>
        <span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">biases</span><span class="p">)</span>
</code></pre></div>
<h2>Train</h2>
<div class="highlight"><pre><span></span><code><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">reset_data_and_parameters</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">.8</span><span class="p">,</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.5</span><span class="p">])</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">update_parameters_with_true_negative_phase</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">lik</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">)</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch: </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s1">2</span><span class="si">}</span><span class="s1"> | Likelihood: </span><span class="si">{</span><span class="n">lik</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">Epoch</span><span class="o">:</span><span class="w">  </span><span class="mi">0</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">209.63758306786653</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">162.04280784271083</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">20</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">160.49961381649555</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">30</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">159.79539070373576</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">40</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">159.2853717231018</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">50</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">158.90186293631422</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">60</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">158.6084020645482</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">70</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">158.38094343579155</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">80</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">158.20287017780586</span>
<span class="n">Epoch</span><span class="o">:</span><span class="w"> </span><span class="mi">90</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Likelihood</span><span class="o">:</span><span class="w"> </span><span class="o">-</span><span class="mf">158.06232196551673</span>
</code></pre></div>
<h2>Visualize samples</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">plot_n_samples</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    NB: We add some jitter to the points so as to better visualize density in a given corner of the model.</span>
<span class="sd">    """</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">'3d'</span><span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

    <span class="n">x</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="mf">.05</span>
    <span class="n">y</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="mf">.05</span>
    <span class="n">z</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">z</span><span class="p">))</span> <span class="o">*</span> <span class="mf">.05</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Node 0'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Node 1'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">'Node 2'</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">plot_n_samples</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s1"> Samples from Model'</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_7_0.png"/></p>
<p>The plot roughly matches the data-generating distribution: most points assume values of either <span class="math">\([1, 0, 1]\)</span>, or <span class="math">\([1, 0, 0]\)</span> (given <span class="math">\(p=[.8, .1, .5]\)</span>).</p>
<h2>Sampling, via Gibbs</h2>
<p>The second, final method we need to implement is <code>sample</code>. In a Boltzmann machine, we typically do this via <a href="http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf">Gibbs sampling</a>.</p>
<p>To effectuate this sampling scheme, we'll need a model of each data dimension conditional on the other data dimensions. For example, for <span class="math">\(d=3\)</span>, we'll need to define:</p>
<ul>
<li><span class="math">\(p(x_0\vert x_1, x_2)\)</span></li>
<li><span class="math">\(p(x_1\vert x_0, x_2)\)</span></li>
<li><span class="math">\(p(x_2\vert x_0, x_1)\)</span></li>
</ul>
<p>Given that each dimension must assume a 0 or a 1, the above 3 models must necessarily return the probability of observing a 1 (where 1 minus this value gives the probability of observing a 0).</p>
<p>Let's derive these models using the workhorse axiom of conditional probability, starting with the first:</p>
<div class="math">$$
\begin{align*}
p(x_0 = 1\vert x_1, x_2)
&amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_1, x_2)}\\
&amp;= \frac{p(x_0 = 1, x_1, x_2)}{\sum\limits_{x_0 \in [0, 1]} p(x_0, x_1, x_2)}\\
&amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_0 = 0, x_1, x_2) + p(x_0 = 1, x_1, x_2)}\\
&amp;= \frac{1}{1 + \frac{p(x_0 = 0, x_1, x_2)}{p(x_0 = 1, x_1, x_2)}}\\
&amp;= \frac{1}{1 + \frac{\exp{(H(x_0 = 0, x_1, x_2)))}}{\exp{(H(x_0 = 1, x_1, x_2)))}}}\\
&amp;= \frac{1}{1 + \exp{(H(x_0 = 0, x_1, x_2) - H(x_0 = 1, x_1, x_2))}}\\
&amp;= \frac{1}{1 + \exp{(\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i - (\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i))}}\\
&amp;= \frac{1}{1 + \exp{(-\sum\limits_{j \neq i = 0} w_{i, j} x_j - b_i)}}\\
&amp;= \sigma\bigg(\sum\limits_{j \neq i = 0} w_{i, j} x_j + b_i\bigg)\\
\end{align*}
$$</div>
<p>Pleasantly enough, this model resolves to a simple Binomial GLM, i.e. logistic regression, involving only its neighboring units and the weights that connect them.</p>
<p>With the requisite conditionals in hand, let's run this chain and compare it with our (trained) model's true probability distribution.</p>
<h2>True probability distribution</h2>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">)</span>

<span class="n">distribution</span> <span class="o">=</span> <span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">config</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">config</span><span class="p">))</span> <span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">all_configs</span><span class="p">]</span>
<span class="k">assert</span> <span class="nb">sum</span><span class="p">([</span><span class="n">likelihood</span> <span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">distribution</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">distribution</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">likelihood</span><span class="si">:</span><span class="s1">.4</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[0, 0, 0]: 0.07327
[0, 0, 1]: 0.09227
[0, 1, 0]: 0.01366
[0, 1, 1]: 0.01938
[1, 0, 0]: 0.3351
[1, 0, 1]: 0.3622
[1, 1, 0]: 0.04693
[1, 1, 1]: 0.05715
</code></pre></div>
<h2>Empirical probability distribution, via Gibbs</h2>
<div class="highlight"><pre><span></span><code><span class="n">empirical_dist</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
    <span class="n">empirical_dist</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">sample</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">empirical_dist</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="mi">8</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">config</span><span class="p">,</span> <span class="n">likelihood</span> <span class="ow">in</span> <span class="n">distribution</span><span class="p">:</span>
    <span class="n">empirical_probability</span> <span class="o">=</span> <span class="n">empirical_dist</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">config</span><span class="p">)]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">likelihood</span><span class="si">:</span><span class="s1">.4</span><span class="si">}</span><span class="s1"> (true), </span><span class="si">{</span><span class="n">empirical_probability</span><span class="si">:</span><span class="s1">.4</span><span class="si">}</span><span class="s1"> (empirical)'</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[0, 0, 0]: 0.07327 (true), 0.05102 (empirical)
[0, 0, 1]: 0.09227 (true), 0.09184 (empirical)
[0, 1, 0]: 0.01366 (true), 0.0102 (empirical)
[0, 1, 1]: 0.01938 (true), 0.02041 (empirical)
[1, 0, 0]: 0.3351 (true), 0.3673 (empirical)
[1, 0, 1]: 0.3622 (true), 0.398 (empirical)
[1, 1, 0]: 0.04693 (true), 0.03061 (empirical)
[1, 1, 1]: 0.05715 (true), 0.03061 (empirical)
</code></pre></div>
<p>Close, ish enough.</p>
<h2>Scaling up, and hitting the bottleneck</h2>
<p>With data of vary dimensionality <code>n_units</code>, the following plot gives the time in seconds that it takes to train this model for 10 epochs.</p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_15_1.png"/></p>
<p>To reduce computational burden, and/or to fit a Boltzmann machine to data of non-trivial dimensionality (e.g. a 28x28 grey-scale image, which implies a random variable with 28x28=784 dimensions), we need to compute the positive and/or negative phase of our gradient faster than we currently are.</p>
<p>To compute the former more quickly, we could employ mini-batches as in canonical stochastic gradient descent.</p>
<p>In this post, we'll instead focus on ways to speed up the latter. Revisiting its expression, <span class="math">\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)</span>, we readily see that we can create an unbiased estimator for this value by drawing Monte Carlo samples from our model, i.e.</p>
<div class="math">$$
\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j] \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$</div>
<p>So, now we just need a way to draw these samples. Luckily, we have a Gibbs sampler to tap!</p>
<p><strong>Instead of computing the true negative phase, i.e. summing <span class="math">\(x_i  x_j\)</span> over all permissible configurations <span class="math">\(X\)</span> under our model, we can approximate it by evaluating this expression for a few model samples, then taking the mean.</strong></p>
<p>We define this update mechanism here:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">update_parameters_with_gibbs_sampling</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span>
                                          <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">model_samples</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">var_combinations</span><span class="p">,</span> <span class="n">all_configs</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">var_combinations</span><span class="p">:</span>
        <span class="c1"># positive phase</span>
        <span class="n">positive_phase</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># negative phase</span>
        <span class="n">negative_phase</span> <span class="o">=</span> <span class="p">(</span><span class="n">model_samples</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">model_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># update weights</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">biases</span><span class="p">):</span>
        <span class="c1"># positive phase</span>
        <span class="n">positive_phase</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># negative phase</span>
        <span class="n">negative_phase</span> <span class="o">=</span> <span class="n">model_samples</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># update biases</span>
        <span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">biases</span><span class="p">)</span>
</code></pre></div>
<p>Next, we'll define a function that we can parameterize by an optimization algorithm (computing the true negative phase, or approximating it via Gibbs sampling, in the above case) which will train a model for <span class="math">\(n\)</span> epochs and return data requisite for plotting.</p>
<h2>How does training progress for varying data dimensionalities?</h2>
<p>Finally, for data of <code>n_units</code> 3, 4, 5, etc., letâ€™s train models for 100 epochs and plot likelihood curves.</p>
<p>When training with the approximate negative phase, weâ€™ll:</p>
<ul>
<li>Derive model samples from a <strong>1000-sample Gibbs chain. Of course, this is a parameter we can tune, which will affect both model accuracy and training runtime. However, we donâ€™t explore that in this post;</strong> instead, we just pick something reasonable and hold this value constant throughout our experiments.</li>
<li>Train several models for a given <code>n_units</code>; Seaborn will average results for us then plot a single line.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">all_updates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">min_units</span><span class="p">,</span> <span class="n">max_units</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span>

<span class="k">for</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_units</span><span class="p">,</span> <span class="n">max_units</span><span class="p">):</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">n_units</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_units</span><span class="p">)</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="k">if</span> <span class="n">n_units</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">alpha</span> <span class="o">/=</span> <span class="mi">10</span>

    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">updates</span> <span class="o">=</span> <span class="n">train_model_for_n_epochs</span><span class="p">(</span>
            <span class="n">update_parameters_with_gibbs_sampling</span><span class="p">,</span>
            <span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">run_num</span><span class="o">=</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span>
        <span class="p">)</span>
        <span class="n">all_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updates</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">))</span>


    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">updates</span> <span class="o">=</span> <span class="n">train_model_for_n_epochs</span><span class="p">(</span>
            <span class="n">update_parameters_with_true_negative_phase</span><span class="p">,</span>
            <span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">run_num</span><span class="o">=</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span>
        <span class="p">)</span>
        <span class="n">all_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updates</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">))</span>
</code></pre></div>
<h2>Plot</h2>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_23_1.png"/></p>
<p><strong>When we let each algorithm run for 100 epochs, the true negative phase gives a model which assigns higher likelihood to the observed data in all of the above training runs.</strong></p>
<p>Notwithstanding, the central point is that 100 epochs of the true negative phase takes a long time to run.</p>
<p>As such, letâ€™s run each for an equal amount of time, and plot results. Below, we define a function to train models for <span class="math">\(n\)</span> seconds (or 1 epochâ€”whichever comes first).</p>
<div class="highlight"><pre><span></span><code><span class="n">all_updates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">n_seconds</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">min_units</span><span class="p">,</span> <span class="n">max_units</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_units</span><span class="p">,</span> <span class="n">max_units</span><span class="p">):</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">n_units</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_units</span><span class="p">)</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="k">if</span> <span class="n">n_units</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">alpha</span> <span class="o">/=</span> <span class="mi">10</span>

    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">updates</span> <span class="o">=</span> <span class="n">train_model_for_n_seconds</span><span class="p">(</span>
            <span class="n">update_parameters_with_gibbs_sampling</span><span class="p">,</span>
            <span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">n_seconds</span><span class="o">=</span><span class="n">n_seconds</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">run_num</span><span class="o">=</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span>
        <span class="p">)</span>
        <span class="n">all_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updates</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">))</span>


    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">updates</span> <span class="o">=</span> <span class="n">train_model_for_n_seconds</span><span class="p">(</span>
            <span class="n">update_parameters_with_true_negative_phase</span><span class="p">,</span>
            <span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span>
            <span class="n">n_seconds</span><span class="o">=</span><span class="n">n_seconds</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">run_num</span><span class="o">=</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span>
        <span class="p">)</span>
        <span class="n">all_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">updates</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="n">n_units</span><span class="p">))</span>
</code></pre></div>
<h2>How many epochs do we actually get through?</h2>
<p>Before plotting results, letâ€™s examine how many epochs each algorithm completes in its allotted time. In fact, for some values of <code>n_units</code>, we couldnâ€™t even complete a single epoch (when computing the true negative phase) in <span class="math">\(\leq 1\)</span> second.</p>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_28_1.png"/></p>
<p>Finally, we look at performance. With <code>n_units &lt;= 7</code>, we see that 1 second of training with the true negative phase yields a better model. Conversely, <strong>using 7 or more units, the added performance given by using the true negative phase is overshadowed by the amount of time it takes the model to train.</strong></p>
<h2>Plot</h2>
<p><img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_31_1.png"/></p>
<p>Of course, we re-stress that the exact ablation results are conditional (amongst other things) on <strong>the number of Gibbs samples we chose to draw. Changing this will change the results, but not that about which we care the most: the overall trend.</strong></p>
<h2>Summary</h2>
<p>Throughout this post, we've given a thorough introduction to a Boltzmann machine: what it does, how it trains, and some of the computational burdens and considerations inherent.</p>
<p>In the next post, we'll look at cheaper, more inventive algorithms for avoiding the computation of the negative phase, and describe how they're used in common machine learning models and training routines.</p>
<h2>Code</h2>
<p>The <a href="https://github.com/cavaunpeu/boltzmann-machines">repository</a> and <a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-1.ipynb">rendered notebook</a> for this project can be found at their respective links.</p>
<h2>References</h2>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec19.pdf">CSC321 Lecture 19: Boltzmann Machines</a>Â <a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">â†©</a></p>
</li>
<li id="fn:2">
<p><a href="https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/">Derivation: Maximum Likelihood for Boltzmann Machines</a>Â <a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">â†©</a></p>
</li>
<li id="fn:3">
<p><a href="https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf">Boltzmann Machines</a>Â <a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">â†©</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	<hr>
	<h2>Comments</h2>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="willwolf_">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'willwolf';
    var disqus_title = 'A Thorough Introduction to Boltzmann Machines';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</div> </div>
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="row">
			<div class="col-md-11 col-md-offset-1" id="footer-wrapper">
				<div class="col-md-3">
					<div id="social-links">
						<h4>
							Contact
						</h4>
						<li><a href="mailto:williamabrwolf@gmail.com">Email</a></li>
							<li><a href="http://twitter.com/willwolf_">Twitter</a></li>
							<li><a href="http://linkedin.com/in/williamabrwolf">LinkedIn</a></li>
							<li><a href="http://calendly.com/willwolf">Calendly</a></li>
					</div>
				</div>
				<div class="col-md-3">
					<div id="other-links">
						<h4>
							Links
						</h4>
						<ul>
							<li><a href="https://tinyletter.com/willwolf">Newsletter</a></li>
							<li><a href="http://willtravellife.com">Travel Blog</a></li>
							<li><a href="http://github.com/cavaunpeu">Github</a></li>
							<li><a href="https://github.com/cavaunpeu/willwolf.io-source">Source Code</a></li>
						</ul>
					</div>
				</div>
				<div class="col-md-3">
				  <div id="categories">
				    <h4>
				      Categories
				    </h4>
				    <ul>
				      <li><a href="https://willwolf.io/crypto/">crypto</li>
				      <li><a href="https://willwolf.io/geopolitics/">geopolitics</li>
				      <li><a href="https://willwolf.io/life/">life</li>
				      <li><a href="https://willwolf.io/machine-learning/">machine-learning</li>
				    </ul>
				  </div>
				</div>
				<div class="col-md-3">
				  <div id="pages">
				    <h4>
				      Pages
				    </h4>
				    <ul>
				      <li><a href="https://willwolf.io/archive/" title="Archive">Archive</a></li>
				      <li><a href="https://willwolf.io/WillWolfResume.pdf" title="RÃ©sumÃ©">RÃ©sumÃ©</a></li>
				    </ul>
				  </div>
				</div>
			</div>
		</div>
	</div>
</footer><div class="container">
	<div class="row">
		<div class="col-md-12 text-center center-block aw-bottom">
			<p>&copy; Will Wolf 2020</p>
			<p>Powered by Pelican</p>
		</div>
	</div>
</div><!-- JavaScript -->
<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-97412095-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>