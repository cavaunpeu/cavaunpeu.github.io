<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>will wolf - machine-learning</title><link href="https://willwolf.io/" rel="alternate"></link><link href="https://willwolf.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://willwolf.io/</id><updated>2022-01-04T10:00:00-05:00</updated><subtitle>writings on machine learning, crypto, geopolitics, life</subtitle><entry><title>Neural Methods in Simulation-Based Inference</title><link href="https://willwolf.io/2022/01/04/neural-methods-in-sbi/" rel="alternate"></link><published>2022-01-04T10:00:00-05:00</published><updated>2022-01-04T10:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2022-01-04:/2022/01/04/neural-methods-in-sbi/</id><summary type="html">&lt;p&gt;A survey of how neural networks are currently being used in simulation-based inference routines.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Bayesian inference is the task of quantifying a posterior belief over parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; given observed data &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;—where &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; was generated from a model &lt;span class="math"&gt;\(p(\mathbf{x}\mid{\boldsymbol{\theta}})\)&lt;/span&gt;—via Bayes' Theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$
    p(\boldsymbol{\theta}\mid\mathbf{x}) = \frac{p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{x})}
$$&lt;/div&gt;
&lt;p&gt;In numerous applications of scientific interest, e.g. cosmological, climatic or urban-mobility phenomena, the likelihood of the data &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; under the data-generating function &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; is intractable to compute, precluding classical inference approaches. Notwithstanding, &lt;em&gt;simulating&lt;/em&gt; new data &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; from this function is often trivial—for example, by coding the generative process in a few lines of Python—&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# some deterministic logic&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# some stochastic logic&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# whatever you want!&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="n"&gt;simulated_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;—motivating the study of &lt;em&gt;simulation-based&lt;/em&gt; Bayesian &lt;em&gt;inference&lt;/em&gt; methods, termed SBI.&lt;/p&gt;
&lt;p&gt;Furthermore, the evidence &lt;span class="math"&gt;\(p(\mathbf{x}) = \int{p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}d\boldsymbol{\theta}\)&lt;/span&gt; is typically intractable to compute as well. This is because the integral has no closed-form solution; or, were the functional form of the likelihood (which we don't have) and the prior (which we do have) available, expanding these terms yields a summation over an "impractically large" number of terms, e.g. the number of possible cluster assignment configurations in a mixture of Gaussians &lt;a href="#10.1080/01621459.2017.1285773" id="ref-10.1080/01621459.2017.1285773-1"&gt;(Blei et al., 2017)&lt;/a&gt;. For this reason, in SBI, we typically estimate the &lt;em&gt;unnormalized&lt;/em&gt; posterior &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x}) = p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta}) \propto \frac{p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{x})}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recent work has explored the use of neural networks to perform key density estimation tasks, i.e. subroutines, of the SBI routine itself. We refer to this work as Neural SBI. In the following sections, we detail the various classes of these estimation tasks. For a more thorough analysis of their respective motivations, behaviors, and tradeoffs, we refer the reader to the original work.&lt;/p&gt;
&lt;h1&gt;Neural Posterior Estimation&lt;/h1&gt;
&lt;p&gt;In this class of models, we estimate &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt; with a conditional neural density estimator &lt;span class="math"&gt;\(q_{\phi}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt;. Simply, this estimator is a neural network with parameters &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; that accepts &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; as input and produces &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; as output. For example, It is trained on data tuples &lt;span class="math"&gt;\(\{\boldsymbol{\theta}_n, \mathbf{x}_n\}_{1:N}\)&lt;/span&gt; sampled from &lt;span class="math"&gt;\(p(\mathbf{x}, \boldsymbol{\theta}) = p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(p(\boldsymbol{\theta})\)&lt;/span&gt; is a prior we choose, and &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; is our &lt;em&gt;simulator&lt;/em&gt;. For example, we can construct this training set as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, we train our network.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, once trained, we can estimate &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)\)&lt;/span&gt;—our posterior belief over parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; given our &lt;em&gt;observed&lt;/em&gt; (not simulated!) data &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt; as &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x}_o) = q_{\phi}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Learning the wrong estimator&lt;/h2&gt;
&lt;p&gt;Ultimately, our goal is to perform the following computation:&lt;/p&gt;
&lt;div class="math"&gt;$$
q_{\phi}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)
$$&lt;/div&gt;
&lt;p&gt;Such that &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; produces an &lt;em&gt;accurate&lt;/em&gt; estimation of the parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; given observed data &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;, we require that &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; be &lt;em&gt;trained&lt;/em&gt; on tuples &lt;span class="math"&gt;\(\{\boldsymbol{\theta}_n, \mathbf{x}_n\}\)&lt;/span&gt; where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{x}_n \sim p(\mathbf{x}\mid\boldsymbol{\theta}_n)\)&lt;/span&gt; via our simulation step.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mid\mathbf{x}_n - \mathbf{x}_o\mid\)&lt;/span&gt; is small, i.e. our simulated are nearby our observed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Otherwise, &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; will learn to estimate a posterior over parameters given data &lt;em&gt;unlike&lt;/em&gt; our own.&lt;/p&gt;
&lt;h2&gt;Learning a better estimator&lt;/h2&gt;
&lt;p&gt;So, how do we obtain parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}_n\)&lt;/span&gt; that produce &lt;span class="math"&gt;\(\mathbf{x}_n \sim p(\mathbf{x}\mid\boldsymbol{\theta}_n)\)&lt;/span&gt; near &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;? We take those that have high (estimated) posterior density given &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;In this vein, we build our training set as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Stitching this all together, our SBI routine becomes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_ROUNDS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;posterior_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ANY_NUMBER&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Learning the right estimator&lt;/h2&gt;
&lt;p&gt;Unfortunately, we're still left with a problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the first round, we learn &lt;span class="math"&gt;\(q_{\phi, r=0}(\boldsymbol{\theta}\mid\mathbf{x}) \approx p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;, i.e. the &lt;strong&gt;right&lt;/strong&gt; estimator.&lt;/li&gt;
&lt;li&gt;Thereafter, we learn &lt;span class="math"&gt;\(q_{\phi, r}(\boldsymbol{\theta}\mid\mathbf{x}) \approx p(\mathbf{x}\mid\boldsymbol{\theta})q_{\phi, r-1}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt;, i.e. the &lt;strong&gt;wrong&lt;/strong&gt; estimator.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, how do we correct this mistake?&lt;/p&gt;
&lt;p&gt;In &lt;a href="#papamakarios2016" id="ref-papamakarios2016-1"&gt;Papamakarios and Murray (2016)&lt;/a&gt;, the authors adjust the learned posterior &lt;span class="math"&gt;\(q_{\phi, r}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt; by simply dividing it by &lt;span class="math"&gt;\(q_{\phi, r-1}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt; then multiplying it by &lt;span class="math"&gt;\(p(\boldsymbol{\theta})\)&lt;/span&gt;. Furthermore, as they choose &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; to be a &lt;em&gt;Mixture Density Network&lt;/em&gt;—a neural network which outputs the parameters of a mixture of Gaussians—and the prior to be "simple distribution (uniform or Gaussian, as is typically the case in practice)," this adjustment can be done analytically.&lt;/p&gt;
&lt;p&gt;Conversely, &lt;a href="#lueckmann2017" id="ref-lueckmann2017-1"&gt;Lueckmann et al. (2017)&lt;/a&gt; &lt;em&gt;train&lt;/em&gt; &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; on a target &lt;em&gt;reweighted&lt;/em&gt; to similar effect: instead of maximizing the total (log) likelihood &lt;span class="math"&gt;\(\Sigma_{n} \log q_{\phi}(\boldsymbol{\theta}_n\mid\mathbf{x}_n)\)&lt;/span&gt;, they maximize &lt;span class="math"&gt;\(\Sigma_{n} \log w_n q_{\phi}(\boldsymbol{\theta}_n\mid\mathbf{x}_n)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(w_n = \frac{p(\boldsymbol{\theta}_n)}{q_{\phi, r-1}(\boldsymbol{\theta}_n\mid\mathbf{x}_n)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While both approaches carry further nuance and potential pitfalls, they bring us effective methods for using a neural network to directly estimate a faithful posterior in SBI routines.&lt;/p&gt;
&lt;h1&gt;Neural Likelihood Estimation&lt;/h1&gt;
&lt;p&gt;In neural likelihood estimation (NLE), we use a neural network to directly estimate the (intractable) likelihood function of the simulator &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; itself. We denote this estimator &lt;span class="math"&gt;\(q_{\phi}(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt;. Finally, we compute our desired posterior as &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x}_o) \approx q_{\phi}(\mathbf{x}_o\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similar to Neural Posterior Estimation (NPE) approaches, we'd like to learn our estimator on inputs &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; that produce &lt;span class="math"&gt;\(\mathbf{x}_n \sim p(\mathbf{x}\mid\boldsymbol{\theta}_n)\)&lt;/span&gt; near &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;. To do this, we again sample them from regions of high approximate posterior density. In each round &lt;span class="math"&gt;\(r\)&lt;/span&gt;, in NPE, this posterior was &lt;span class="math"&gt;\(q_{\phi, r-1}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)\)&lt;/span&gt;; in NLE, it is &lt;span class="math"&gt;\(q_{\phi, r-1}(\mathbf{x}_o\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;. In both cases, we draw samples from our approximate posterior density, then feed them to the simulator to generate novel data for training our estimator &lt;span class="math"&gt;\(q_{\phi, r}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a more detailed treatment, please refer to original works &lt;a href="#pmlr-v89-papamakarios19a" id="ref-pmlr-v89-papamakarios19a-1"&gt;Papamakarios et al. (2019)&lt;/a&gt; and &lt;a href="#pmlr-v96-lueckmann19a" id="ref-pmlr-v96-lueckmann19a-1"&gt;Lueckmann et al. (2019)&lt;/a&gt; (among others).&lt;/p&gt;
&lt;h1&gt;Neural Likelihood Ratio Estimation&lt;/h1&gt;
&lt;p&gt;In this final class of models, we instead try to directly draw &lt;em&gt;samples&lt;/em&gt; from the true posterior itself. However, since we can't compute &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; nor &lt;span class="math"&gt;\(p(\mathbf{x})\)&lt;/span&gt;, we first need a sampling algorithm that satisifes these constraints. One such class of algorithms is &lt;em&gt;Markov chain Monte Carlo&lt;/em&gt;, termed MCMC.&lt;/p&gt;
&lt;p&gt;In MCMC, we first &lt;em&gt;propose&lt;/em&gt; parameter samples &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; from a proposal distribution. Then, we evaluate their &lt;em&gt;fitness&lt;/em&gt; by asking the question: "does this sample &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; have higher posterior density than the previous sample &lt;span class="math"&gt;\(\boldsymbol{\theta}_j\)&lt;/span&gt; we drew?" Generally, this question is answered through comparison, e.g.&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{
    p(\boldsymbol{\theta}_i\mid\mathbf{x})
} {
    p(\boldsymbol{\theta}_{j}\mid\mathbf{x})
} = \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)p(\boldsymbol{\theta}_i) / p(\mathbf{x})
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)p(\boldsymbol{\theta}_j) / p(\mathbf{x})
}
$$&lt;/div&gt;
&lt;p&gt;Fortunately, the evidence terms &lt;span class="math"&gt;\(p(\mathbf{x})\)&lt;/span&gt; cancel, and the prior densities &lt;span class="math"&gt;\(p(\boldsymbol{\theta})\)&lt;/span&gt; are evaluable. Though we cannot compute the likelihood terms outright, we can estimate their &lt;em&gt;ratio&lt;/em&gt; and proceed with MCMC as per normal. If &lt;span class="math"&gt;\(\frac{p(\boldsymbol{\theta}_i\mid\mathbf{x})}{p(\boldsymbol{\theta}_j\mid\mathbf{x})} \gt 1\)&lt;/span&gt;, we (are likely to) &lt;em&gt;accept&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; as a valid sample from our target posterior.&lt;/p&gt;
&lt;h2&gt;Estimating the likelihood ratio&lt;/h2&gt;
&lt;p&gt;Let us term the likelihood ratio as&lt;/p&gt;
&lt;div class="math"&gt;$$
r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j) = \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)
}
$$&lt;/div&gt;
&lt;p&gt;Ingeniously, &lt;a href="#cranmer2015" id="ref-cranmer2015-1"&gt;Cranmer et al. (2015)&lt;/a&gt; propose to learn a classifier to discriminate samples &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_i)\)&lt;/span&gt; from &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_j)\)&lt;/span&gt;, then use its predictions to estimate &lt;span class="math"&gt;\(r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To do this, they draw training samples &lt;span class="math"&gt;\((\mathbf{x}, y=1) \sim p(\mathbf{x}\mid\boldsymbol{\theta}_i)\)&lt;/span&gt; and &lt;span class="math"&gt;\((\mathbf{x}, y=0) \sim p(\mathbf{x}\mid\boldsymbol{\theta}_j)\)&lt;/span&gt; then train a binary classifer &lt;span class="math"&gt;\(d(y\mid\mathbf{x})\)&lt;/span&gt; on this data. In this vein, a perfect classifier gives:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
d^*(y=1\mid\mathbf{x})
&amp;amp;= \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_i) + p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} \\
d^*(y=0\mid\mathbf{x})
&amp;amp;= \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_i) + p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} \\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Consequently,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
&amp;amp;= \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} \\
&amp;amp;= \frac{
    d^*(y=1\mid\mathbf{x})
} {
    d^*(y=0\mid\mathbf{x})
} \\
&amp;amp;= \frac{
    d^*(y=1\mid\mathbf{x})
} {
    1 - d^*(y=1\mid\mathbf{x})
}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since our classifier won't be perfect, we simply term it &lt;span class="math"&gt;\(d(y\mid\mathbf{x})\)&lt;/span&gt;, where&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\hat{r}(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
&amp;amp;= \frac{d(y=1\mid\mathbf{x})}{1 - d(y=1\mid\mathbf{x})}\\
&amp;amp;\approx r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;With &lt;span class="math"&gt;\(\hat{r}(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)\)&lt;/span&gt; in hand, we can compare the posterior density of proposed samples &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{\theta}_j\)&lt;/span&gt; in our MCMC routine.&lt;/p&gt;
&lt;h2&gt;Generalizing our classifier&lt;/h2&gt;
&lt;p&gt;To use the above classifier in our inference routine, we must &lt;em&gt;retrain&lt;/em&gt; a &lt;em&gt;new&lt;/em&gt; classifier for every &lt;em&gt;unique&lt;/em&gt; set of parameters &lt;span class="math"&gt;\(\{\boldsymbol{\theta}_i, \boldsymbol{\theta}_j\}\)&lt;/span&gt;. Clearly, this is extremely impractical. How can we generalize our classifier such that we only have to train it once?&lt;/p&gt;
&lt;p&gt;In &lt;a href="#cranmer2015" id="ref-cranmer2015-2"&gt;Cranmer et al. (2015)&lt;/a&gt;, the authors learn a &lt;em&gt;single&lt;/em&gt; classifier &lt;span class="math"&gt;\(d(y\mid\mathbf{x}, \boldsymbol{\theta})\)&lt;/span&gt; to discriminate samples &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; from &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; is an &lt;em&gt;arbitrary&lt;/em&gt; parameter value, and &lt;span class="math"&gt;\(\boldsymbol{\theta}_{ref}\)&lt;/span&gt; is a fixed, &lt;em&gt;reference&lt;/em&gt; parameter value. It is trained on data &lt;span class="math"&gt;\((\mathbf{x},  \boldsymbol{\theta}, y=1) \sim p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class="math"&gt;\((\mathbf{x}, \boldsymbol{\theta}_{ref},  y=0) \sim p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;. Once trained, it gives:&lt;/p&gt;
&lt;div class="math"&gt;$$
r(\mathbf{x}\mid\boldsymbol{\theta}, \boldsymbol{\theta}_{ref})
= \frac{
    d^*(y\mid\mathbf{x}, \boldsymbol{\theta})
} {
    1 - d^*(y\mid\mathbf{x}, \boldsymbol{\theta})
}
$$&lt;/div&gt;
&lt;p&gt;Consequently,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
&amp;amp;= \frac{
    r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_{ref})
} {
    r(\mathbf{x}\mid\boldsymbol{\theta}_j, \boldsymbol{\theta}_{ref})
} \\
&amp;amp;= \frac{
    d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_i)
} {
    1 - d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_i)
} * \frac{
    1 - d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_j)
} {
    d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_j)
}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;With a &lt;em&gt;single&lt;/em&gt; model, we can now compare the density of two proposed posterior samples.&lt;/p&gt;
&lt;h2&gt;Improving our generalized classifier&lt;/h2&gt;
&lt;p&gt;Once more, our classifier &lt;span class="math"&gt;\(d(y\mid\mathbf{x}, \boldsymbol{\theta})\)&lt;/span&gt; discriminates samples &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; from &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;. In this vein, in the case that a given &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; was drawn from neither &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; &lt;em&gt;nor&lt;/em&gt; &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;, what should our classifier do? In &lt;a href="#hermans2019" id="ref-hermans2019-1"&gt;Hermans et al. (2019)&lt;/a&gt;, the authors illustrate this problem—&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/neural-sbi/undefined-classifier.png"/&gt;&lt;/p&gt;
&lt;p&gt;—stressing that "poor inference results occur in the absence of support between &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;."&lt;/p&gt;
&lt;p&gt;In solution, they propose to learn a (neural) classifier that instead discriminates between &lt;em&gt;dependent&lt;/em&gt; sample-parameter pairs &lt;span class="math"&gt;\((\mathbf{x}, \boldsymbol{\theta}) \sim p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt; from &lt;em&gt;independent&lt;/em&gt; sample-parameter pairs &lt;span class="math"&gt;\((\mathbf{x}, \boldsymbol{\theta}) \sim p(\mathbf{x})p(\boldsymbol{\theta})\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class="math"&gt;\(p(\mathbf{x})p(\boldsymbol{\theta})\)&lt;/span&gt; occupy the same space, they share a common support. In other words, the likelihood of a given &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; will &lt;em&gt;always&lt;/em&gt; be positive for &lt;em&gt;some&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; in the figure above.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Simulation-based inference is a class of techniques that allows us to perform Bayesian inference where our data-generating model &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; lacks a tractable likelihood function, yet permits simulation of novel data. In the above sections, we detailed several SBI approaches, and ways in which neural networks are currently being used in each.&lt;/p&gt;
&lt;h2&gt;Credit&lt;/h2&gt;
&lt;p&gt;Credit to &lt;a href="https://www.processmaker.com/wp-content/uploads/2021/07/simulation-modeling-process-mining.jpg"&gt;ProcessMaker&lt;/a&gt; for social card image.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr/&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id="10.1080/01621459.2017.1285773"&gt;David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.
&lt;span class="bibtex-protected"&gt;Variational Inference: A Review for Statisticians&lt;/span&gt;.
&lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;, 112(518):859–877, 2017.
&lt;a href="https://arxiv.org/abs/1601.00670"&gt;arXiv:1601.00670&lt;/a&gt;, &lt;a href="https://doi.org/10.1080/01621459.2017.1285773"&gt;doi:10.1080/01621459.2017.1285773&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-10.1080/01621459.2017.1285773-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="cranmer2015"&gt;Kyle Cranmer, Juan Pavez, and Gilles Louppe.
&lt;span class="bibtex-protected"&gt;Approximating Likelihood Ratios with Calibrated Discriminative Classifiers&lt;/span&gt;.
&lt;em&gt;arXiv&lt;/em&gt;, 2015.
&lt;a href="https://arxiv.org/abs/1506.02169"&gt;arXiv:1506.02169&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-cranmer2015-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-cranmer2015-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-cranmer2015-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id="hermans2019"&gt;Joeri Hermans, Volodimir Begy, and Gilles Louppe.
&lt;span class="bibtex-protected"&gt;Likelihood-free MCMC with Amortized Approximate Ratio Estimators&lt;/span&gt;.
&lt;em&gt;arXiv&lt;/em&gt;, 2019.
&lt;a href="https://arxiv.org/abs/1903.04057"&gt;arXiv:1903.04057&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-hermans2019-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="pmlr-v96-lueckmann19a"&gt;Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H. Macke.
Likelihood-free inference with emulator networks.
In Francisco Ruiz, Cheng Zhang, Dawen Liang, and Thang Bui, editors, &lt;em&gt;Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference&lt;/em&gt;, volume 96 of Proceedings of Machine Learning Research, 32–53. 02 Dec 2019. PMLR.
URL: &lt;a href="http://proceedings.mlr.press/v96/lueckmann19a.html"&gt;http://proceedings.mlr.press/v96/lueckmann19a.html&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-pmlr-v96-lueckmann19a-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="lueckmann2017"&gt;Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan Öcal, Marcel Nonnenmacher, and Jakob H Macke.
&lt;span class="bibtex-protected"&gt;Flexible statistical inference for mechanistic models of neural dynamics&lt;/span&gt;.
&lt;em&gt;arXiv&lt;/em&gt;, 2017.
&lt;a href="https://arxiv.org/abs/1711.01861"&gt;arXiv:1711.01861&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-lueckmann2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="papamakarios2016"&gt;George Papamakarios and Iain Murray.
Fast ε-free inference of simulation models with bayesian conditional density estimation.
In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, volume 29. Curran Associates, Inc., 2016.
URL: &lt;a href="https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf"&gt;https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-papamakarios2016-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="pmlr-v89-papamakarios19a"&gt;George Papamakarios, David Sterratt, and Iain Murray.
Sequential neural likelihood: fast likelihood-free inference with autoregressive flows.
In Kamalika Chaudhuri and Masashi Sugiyama, editors, &lt;em&gt;Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, volume 89 of Proceedings of Machine Learning Research, 837–848. PMLR, 16–18 Apr 2019.
URL: &lt;a href="http://proceedings.mlr.press/v89/papamakarios19a.html"&gt;http://proceedings.mlr.press/v89/papamakarios19a.html&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-pmlr-v89-papamakarios19a-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</content><category term="machine-learning"></category></entry><entry><title>Deriving Mean-Field Variational Bayes</title><link href="https://willwolf.io/2018/11/23/mean-field-variational-bayes/" rel="alternate"></link><published>2018-11-23T10:00:00-05:00</published><updated>2018-11-23T10:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-11-23:/2018/11/23/mean-field-variational-bayes/</id><summary type="html">&lt;p&gt;A detailed derivation of Mean-Field Variational Bayes, its connection to Expectation-Maximization, and its implicit motivation for the "black-box variational inference" methods born in recent years.&lt;/p&gt;</summary><content type="html">&lt;p&gt;"Mean-Field Variational Bayes" (MFVB), is similar to &lt;a href="https://willwolf.io/2018/11/11/em-for-lda/"&gt;expectation-maximization&lt;/a&gt; (EM) yet distinct in two key ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We do not minimize &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt;, i.e. perform the E-step, as [in the problems in which we employ mean-field] the posterior distribution &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt; "is too complex to work with,"™ i.e. it has no analytical form.&lt;/li&gt;
&lt;li&gt;Our variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; is a &lt;em&gt;factorized distribution&lt;/em&gt;, i.e.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
q(\mathbf{Z}) = \prod\limits_i^{M} q_i(\mathbf{Z}_i)
$$&lt;/div&gt;
&lt;p&gt;for all latent variables &lt;span class="math"&gt;\(\mathbf{Z}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Briefly, factorized distributions are cheap to compute: if each &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt; is Gaussian, &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; requires optimization of &lt;span class="math"&gt;\(2M\)&lt;/span&gt; parameters (a mean and a variance for each factor); conversely, a non-factorized &lt;span class="math"&gt;\(q(\mathbf{Z}) = \text{Normal}(\mu, \Sigma)\)&lt;/span&gt; would require optimization of &lt;span class="math"&gt;\(M\)&lt;/span&gt; parameters for the mean and &lt;span class="math"&gt;\(\frac{M^2 + M}{2}\)&lt;/span&gt; parameters for the covariance. Following intuition, this gain in computational efficiency comes at the cost of decreased accuracy in approximating the true posterior over latent variables.&lt;/p&gt;
&lt;h2&gt;So, what is it?&lt;/h2&gt;
&lt;p&gt;Mean-field Variational Bayes is an iterative maximization of the ELBO. More precisely, it is an iterative M-step with respect to the variational factors &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the simplest case, we posit a variational factor over every latent variable, &lt;em&gt;as well as every parameter&lt;/em&gt;. In other words, as compared to the log-marginal decomposition in EM, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is absorbed into &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\quad \text{(EM)}
$$&lt;/div&gt;
&lt;p&gt;becomes&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X})} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\quad \text{(MFVB)}
$$&lt;/div&gt;
&lt;p&gt;From there, we simply maximize the ELBO, i.e. &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]\)&lt;/span&gt;, by &lt;em&gt;iteratively maximizing with respect to each variational factor &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt;&lt;/em&gt; in turn.&lt;/p&gt;
&lt;h2&gt;What's this do?&lt;/h2&gt;
&lt;p&gt;Curiously, we note that &lt;span class="math"&gt;\(\log{p(\mathbf{X})}\)&lt;/span&gt; is a &lt;em&gt;fixed quantity&lt;/em&gt; with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;: updating our variational factors &lt;em&gt;will not change&lt;/em&gt; the marginal log-likelihood of our data.&lt;/p&gt;
&lt;p&gt;This said, we note that the ELBO and &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\)&lt;/span&gt; trade off linearly: when one goes up by &lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;, the other goes down by &lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As such, (iteratively) maximizing the ELBO in MFVB is akin to minimizing the divergence between the true posterior over the latent variables given data and our factorized variational approximation thereof.&lt;/p&gt;
&lt;h2&gt;Derivation&lt;/h2&gt;
&lt;p&gt;So, what do these updates look like?&lt;/p&gt;
&lt;p&gt;First, let's break the ELBO into its two main components:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \int{q(\mathbf{Z})\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}}d\mathbf{Z}\\
&amp;amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z} - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;amp;= A + B
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Next, rewrite this expression in a way that isolates a single variational factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, i.e. the factor with respect to which we'd like to maximize the ELBO in a given iteration.&lt;/p&gt;
&lt;h2&gt;Expanding the first term&lt;/h2&gt;
&lt;div class="math"&gt;$$
\begin{align*}
A
&amp;amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}d\mathbf{Z}}\\
&amp;amp;= \int{\prod\limits_{i}q_i(\mathbf{Z}_i)\log{p(\mathbf{X, Z})}d\mathbf{Z}_i}\\
&amp;amp;= \int{q_j(\mathbf{Z}_j)\bigg[\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i\bigg]}d\mathbf{Z}_j\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Following Bishop&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;'s derivation, we've introduced the notation:&lt;/p&gt;
&lt;div class="math"&gt;$$
\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;A few things to note, and in case this looks strange:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Were the left-hand side to read &lt;span class="math"&gt;\(\int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z}\)&lt;/span&gt;, this would look like the perfectly vanilla expectation &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;An expectation maps a function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, e.g. &lt;span class="math"&gt;\(\log{p(\mathbf{X, Z})}\)&lt;/span&gt;, to a single real number. As our expression reads &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; as opposed to &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;, we're conspicuously unable to integrate over the remaining factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;As such, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; gives a function of the value of &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;&lt;/strong&gt; which itself maps to the aforementioned real number.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To further illustrate, let's employ some toy Python code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Suppose `Z = [Z_0, Z_1, Z_2]`, with corresponding (discrete) variational distributions `q_0`, `q_1`, `q_2`&lt;/span&gt;

&lt;span class="n"&gt;q_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;  &lt;span class="c1"&gt;# q_0(1) = .2&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;q_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;q_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Next, suppose we'd like to isolate Z_2&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;, written &lt;code&gt;E_i_neq_j_log_p_X_Z&lt;/code&gt; below, can be computed as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;E_i_neq_j_log_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;Z_i_neq_j_dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;comb&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Z_i_neq_j_dists&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;comb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;comb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Z_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;comb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Continuing with our notes, it was not immediately obvious to me how and why we're able to introduce a second integral sign on line 3 of the derivation above. Notwithstanding, the reason is quite simple; a simple exercise of nested for-loops is illustrative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before beginning, we remind the definition of an integral in code. In its simplest example, &lt;span class="math"&gt;\(\int{ydx}\)&lt;/span&gt; can be written as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lower_lim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper_lim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_ticks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;integral&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# ...where `n_ticks` approaches infinity.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With this in mind, the following confirms the self-evidence of the second integral sign:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# some dummy expression&lt;/span&gt;


&lt;span class="c1"&gt;# Line 2 of `Expanding the first term`&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt;
            &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt;
            &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt;
            &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;TOTAL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;


&lt;span class="c1"&gt;# Line 3 of `Expanding the first term`&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt;
            &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt;
            &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;_total&lt;/span&gt;


&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;TOTAL&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In effect, isolating &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; is akin to the penultimate line &lt;code&gt;total += prob_z_0 * _total&lt;/code&gt;, i.e. multiplying &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; by an intermediate summation &lt;code&gt;_total&lt;/code&gt;.  Therefore, the second integral sign is akin to &lt;code&gt;_total += prob_z_1 * prob_z_2 * ln_p_X_Z(X, Z)&lt;/code&gt;, i.e. the computation of this intermediate summation itself.&lt;/p&gt;
&lt;p&gt;More succinctly, a multi-dimensional integral can be thought of as a nested-for-loop which commutes a global sum. Herein, we are free to compute intermediate sums at will.&lt;/p&gt;
&lt;h2&gt;Expanding the second term&lt;/h2&gt;
&lt;p&gt;Next, let's expand &lt;span class="math"&gt;\(B\)&lt;/span&gt;. We note that this is the entropy of the full variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
B
&amp;amp;= - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\prod\limits_{i}q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)} + \sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q_{i \neq j}(\mathbf{Z}_i)}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] + \text{const}\\
&amp;amp;= - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As we'll be maximizing w.r.t. just &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, we can set all terms that don't include this factor to constants.&lt;/p&gt;
&lt;h2&gt;Putting it back together&lt;/h2&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= A + B\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;One final pseudonym&lt;/h2&gt;
&lt;p&gt;Were we able to replace the expectation in &lt;span class="math"&gt;\(A\)&lt;/span&gt; with the &lt;span class="math"&gt;\(\log\)&lt;/span&gt; of some density &lt;span class="math"&gt;\(D\)&lt;/span&gt;, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
= \int{q_j(\mathbf{Z}_j){ \log{D} }\ d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(A + B\)&lt;/span&gt; could be rewritten as &lt;span class="math"&gt;\(-\text{KL}(q_j(\mathbf{Z}_j)\Vert D)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Acknowledging that &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; is an unnormalized log-likelihood written as a function of &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;, we temporarily rewrite it as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] = \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j})
$$&lt;/div&gt;
&lt;p&gt;As such:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j}) }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \log{\frac{\tilde{p}(\mathbf{X}, \mathbf{Z}_j)}{q_j(\mathbf{Z}_j)}} }d\mathbf{Z}_j} + \text{const}\\
&amp;amp;= - \text{KL}\big(q_j(\mathbf{Z}_j)\Vert \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\big) + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Finally, per this expression, the ELBO reaches its minimum when:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;amp;= \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\\
&amp;amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Or equivalently:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Summing up:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iteratively minimizing the divergence between &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\tilde{p}(\mathbf{X}, \mathbf{Z}_j)\)&lt;/span&gt; for all factors &lt;span class="math"&gt;\(j\)&lt;/span&gt; is our mechanism for maximizing the ELBO&lt;/li&gt;
&lt;li&gt;In turn, maximizing the ELBO is our mechanism for minimizing the KL divergence between the full factorized posterior &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; and the true posterior &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X})\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, as the optimal density &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; relies on those of &lt;span class="math"&gt;\(q_{i \neq j}(\mathbf{Z}_{i})\)&lt;/span&gt;, this optimization algorithm is necessarily &lt;em&gt;iterative&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Normalization constant&lt;/h2&gt;
&lt;p&gt;Nearing the end, we note that &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j) = \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}\)&lt;/span&gt; is not necessarily a normalized density (over &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;). "By inspection," we compute:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;amp;= \frac{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}}{\int{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}d\mathbf{Z}_j}}\\
&amp;amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)} + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;How to actually employ this thing&lt;/h2&gt;
&lt;p&gt;First, plug in values for the right-hand side of:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;Then, attempt to rearrange this expression such that:&lt;/p&gt;
&lt;p&gt;Once exponentiated, giving &lt;span class="math"&gt;\(\exp{\big(\log{q_j(\mathbf{Z}_j)}\big)} = q_j(\mathbf{Z}_j)\)&lt;/span&gt;, we are left with something that, once normalized (by inspection), resembles a known density function (e.g. a Gaussian, a Gamma, etc.).&lt;/p&gt;
&lt;p&gt;NB: This may require significant computation.&lt;/p&gt;
&lt;h1&gt;Approximating a Gaussian&lt;/h1&gt;
&lt;p&gt;Here, we'll approximate a 2D multivariate Gaussian with a factorized mean-field approximation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-3.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-4.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-5.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Summing up&lt;/h1&gt;
&lt;p&gt;Mean-Field Variational Bayes is an iterative optimization algorithm for maximizing a lower-bound of the marginal likelihood of some data &lt;span class="math"&gt;\(\mathbf{X}\)&lt;/span&gt; under a given model with latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;. It accomplishes this task by positing a factorized variational distribution over all latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt; and parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, then computes, &lt;em&gt;analytically&lt;/em&gt;, the algebraic forms and parameters of each factor which maximize this bound.&lt;/p&gt;
&lt;p&gt;In practice, this process can be cumbersome and labor-intensive. As such, in recent years, "black-box variational inference" techniques were born, which &lt;em&gt;fix&lt;/em&gt; the forms of each factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, then optimize its parameters via gradient descent.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Deriving Expectation-Maximization</title><link href="https://willwolf.io/2018/11/11/em-for-lda/" rel="alternate"></link><published>2018-11-11T16:00:00-05:00</published><updated>2018-11-11T16:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-11-11:/2018/11/11/em-for-lda/</id><summary type="html">&lt;p&gt;Deriving the expectation-maximization algorithm, and the beginnings of its application to LDA. Once finished, its intimate connection to variational inference is apparent.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Consider a model with parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; and latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;; the expectation-maximization algorithm (EM) is a mechanism for computing the values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that, under this model, maximize the likelihood of some observed data &lt;span class="math"&gt;\(\mathbf{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The joint probability of our model can be written as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\mathbf{X}, \mathbf{Z}\vert \theta) = p(\mathbf{X}\vert \mathbf{Z}, \theta)p(\mathbf{Z}\vert \theta)
$$&lt;/div&gt;
&lt;p&gt;where, once more, our stated goal is to maximize the marginal likelihood of our data:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \log{\sum_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}
$$&lt;/div&gt;
&lt;p&gt;An example of a latent variable model is the Latent Dirichlet Allocation&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; (LDA) model for uncovering latent topics in documents of text. Once finished deriving the general EM equations, we'll (begin to) apply them to this model.&lt;/p&gt;
&lt;h2&gt;Why not maximum likelihood estimation?&lt;/h2&gt;
&lt;p&gt;As the adage goes, computing the MLE with respect to this marginal is "hard." For one, it requires summing over an (implicitly) humongous number of configurations of latent variables &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Further, as Bishop&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A key observation is that the summation over the latent variables appears inside the logarithm. Even if the joint distribution &lt;span class="math"&gt;\(p(\mathbf{X, Z}\vert\theta)\)&lt;/span&gt; belongs to the exponential family, the marginal distribution &lt;span class="math"&gt;\(p(\mathbf{X}\vert\theta)\)&lt;/span&gt; typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;We'll want something else to maximize instead.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;A lower bound&lt;/h2&gt;
&lt;p&gt;Instead of maximizing the log-marginal &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; (with respect to model parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;), let's maximize a lower-bound with a less-problematic form.&lt;/p&gt;
&lt;p&gt;Perhaps, we'd work with &lt;span class="math"&gt;\(\log{p(\mathbf{X}, \mathbf{Z}\vert \theta)}\)&lt;/span&gt; which, almost tautologically, removes the summation over latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As such, let's derive a lower-bound which features this term. As &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; is often called the log-"evidence," we'll call our expression the "evidence lower-bound," or ELBO.&lt;/p&gt;
&lt;h2&gt;Jensen's inequality&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"&gt;Jensen's inequality&lt;/a&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; generalizes the statement that the line secant to a &lt;strong&gt;concave function&lt;/strong&gt; lies below this function. An example is illustrative:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://alliance.seas.upenn.edu/~cis520/dynamic/2017/wiki/uploads/Lectures/jensen.png"/&gt;&lt;/p&gt;
&lt;p&gt;First, we note that the red line is below the blue for all points for which it is defined.&lt;/p&gt;
&lt;p&gt;Second, working through the example, and assuming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(f(x) = \exp(-(x - 2)^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(v_1 = 1; v_2 = 2.5; \alpha = .3\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{align*}
f(v_1) &amp;amp;\approx .3679\\
f(v_2) &amp;amp;\approx .7788\\
\alpha f(v_1) + (1 - \alpha)f(v_2) &amp;amp;\approx \bf{.6555}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\alpha v_1 + (1 - \alpha)v_2 &amp;amp;= 2.05\\
f(\alpha v_1 + (1 - \alpha)v_2) &amp;amp;\approx \bf{.9975}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;we see that &lt;strong&gt;&lt;span class="math"&gt;\(\alpha f(v_1) + (1 - \alpha)f(v_2) \leq f(\alpha v_1 + (1 - \alpha)v_2)\)&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we arrive at a general form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}_{v}}[f(v)] \leq f(\mathop{\mathbb{E}_{v}}[v])
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(p(v) = \alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Deriving the ELBO&lt;/h2&gt;
&lt;p&gt;In trying to align &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}
= \log{\sum\limits_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}\)&lt;/span&gt; with &lt;span class="math"&gt;\(f(\mathop{\mathbb{E}_{v}}[v])\)&lt;/span&gt;, we see a function &lt;span class="math"&gt;\(f = \log\)&lt;/span&gt; yet no expectation inside. However, given the summation over &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;, introducing some distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; would give us the expectation we desire.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{p(\mathbf{X}\vert\theta)}
&amp;amp;= \log{\sum_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}\\
&amp;amp;= \log{\sum_{\mathbf{Z}}q(\mathbf{Z})\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\\
&amp;amp;= \log{\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; is some distribution over &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt; with parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; (omitted for cleanliness) and known form (e.g. a Gaussian). It is often referred to as a &lt;strong&gt;variational distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;From here, via Jensen's inequality, we can derive the lower-bound:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{p(\mathbf{X}\vert\theta)} = \log{\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}\bigg]}
&amp;amp;\geq \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + R
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Et voilà&lt;/em&gt;, we see that this term contains &lt;span class="math"&gt;\(\log{p(\mathbf{X, Z}\vert\theta)}\)&lt;/span&gt;; the ELBO should now be easier to optimize with respect to our parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;So, what's &lt;span class="math"&gt;\(R\)&lt;/span&gt;?&lt;/h1&gt;
&lt;div class="math"&gt;$$
\begin{align*}
R
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta) - \log{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X}\vert\theta)}\bigg] -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg(\log{p(\mathbf{X}\vert\theta)} - \log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} - \log{p(\mathbf{X}\vert\theta)} + \log{q(\mathbf{Z})}\bigg)\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg( - \log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{q(\mathbf{Z})}\bigg)\\
&amp;amp;=
\sum_{\mathbf{Z}}q(\mathbf{Z})\log{\frac{q(\mathbf{Z})}{p(\mathbf{Z}\vert\mathbf{X}, \theta)}}\\
&amp;amp;= \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Putting it back together:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)
$$&lt;/div&gt;
&lt;h2&gt;The EM algorithm&lt;/h2&gt;
&lt;p&gt;The algorithm can be described by a few simple observations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; is a divergence metric which is strictly non-negative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; does not depend on &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;—if we decrease &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; by changing &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;, the ELBO must increase to compensate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we increase the ELBO by changing &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; will increase as well. In addition, as &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt; now (likely) diverges from &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; in non-zero amount, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; will increase even more.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;The EM algorithm is a repeated alternation between Step 2 (E-step) and Step 3 (M-step).&lt;/strong&gt; After each M-Step, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; is guaranteed to increase (unless it is already at a maximum)&lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;A graphic&lt;sup id="fnref3:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; is further illustrative.&lt;/p&gt;
&lt;h3&gt;Initial decomposition&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/initial_decomp.png"/&gt;&lt;/p&gt;
&lt;p&gt;Here, the ELBO is written as &lt;span class="math"&gt;\(\mathcal{L}(q, \theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;E-step&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/e_step.png"/&gt;&lt;/p&gt;
&lt;p&gt;In other words, holding the parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; constant, minimize &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;. Remember, as &lt;span class="math"&gt;\(q\)&lt;/span&gt; is a distribution with a fixed functional form, this amounts to updating its parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The caption implies that we can always compute &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt;. We will show below that this is not the case for LDA, nor for many interesting models.&lt;/p&gt;
&lt;h3&gt;M-step&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/m_step.png"/&gt;&lt;/p&gt;
&lt;p&gt;In other words, in the M-step, maximize the ELBO with respect to the model parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Expanding the ELBO:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta)}\bigg] + \mathbf{H}[q(\mathbf{Z})]
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;we see that it decomposes into an expectation of the joint distribution over data and latent variables given parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; with respect to the variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;, plus the entropy of &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As our task is to maximize this expression with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we can treat the latter term as a constant.&lt;/p&gt;
&lt;h2&gt;EM for LDA&lt;/h2&gt;
&lt;p&gt;To give an example of the above, we'll examine the classic Latent Dirichlet Allocation&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; paper.&lt;/p&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/lda_formulation.png"/&gt;&lt;/p&gt;
&lt;p&gt;"Given the parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, the joint distribution of a topic mixture &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; topics &lt;span class="math"&gt;\(\mathbf{z}\)&lt;/span&gt;, and a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; words &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; is given by:"&lt;sup id="fnref3:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta, \mathbf{z}, \mathbf{w}\vert \alpha, \beta) = p(\theta\vert \alpha)\prod\limits_{n=1}^{N}p(z_n\vert \theta)p(w_n\vert z_n, \beta)
$$&lt;/div&gt;
&lt;h3&gt;Log-evidence&lt;/h3&gt;
&lt;p&gt;The (problematic) log-evidence of a single document:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{w}\vert \alpha, \beta)} = \log{\int p(\theta\vert \alpha)\prod\limits_{n=1}^{N}\sum\limits_{z_n} p(z_n\vert \theta)p(w_n\vert z_n, \beta)d\theta}
$$&lt;/div&gt;
&lt;p&gt;NB: The parameters of our model are &lt;span class="math"&gt;\(\{\alpha,  \beta\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{\theta, \mathbf{z}\}\)&lt;/span&gt; are our latent variables.&lt;/p&gt;
&lt;h3&gt;ELBO&lt;/h3&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\bigg(\frac{p(\theta\vert \alpha)}{q(\mathbf{Z})}}\prod\limits_{n=1}^{N}p(z_n\vert \theta)p(w_n\vert z_n, \beta)\bigg)\bigg]
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{Z} = \{\theta, \mathbf{z}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;KL term&lt;/h3&gt;
&lt;div class="math"&gt;$$
\text{KL}\big(q(\mathbf{Z})\Vert \frac{p(\theta, \mathbf{z}, \mathbf{w}\vert \alpha, \beta)}{p(\mathbf{w}\vert \alpha, \beta)}\big)
$$&lt;/div&gt;
&lt;p&gt;Peering at the denominator, we see that it includes an integration over all values &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, which we assume is intractable to compute. As such, the "ideal" E-step solution &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\theta, \mathbf{z}\vert \mathbf{w}, \alpha, \beta)\)&lt;/span&gt; will elude us as well.&lt;/p&gt;
&lt;p&gt;In the next post, we'll cover how to minimize this KL term with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; in detail. This effort will begin with the derivation of the mean-field algorithm.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we motivated the expectation-maximization algorithm then derived its general form. We then applied this framework to the LDA model.&lt;/p&gt;
&lt;p&gt;In the next post, we'll expand this logic into mean-field variational Bayes, and eventually, variational inference more broadly.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Wikipedia contributors. "Jensen's inequality." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 29 Oct. 2018. Web. 11 Nov. 2018. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Additional Strategies for Confronting the Partition Function</title><link href="https://willwolf.io/2018/10/29/additional-strategies-partition-function/" rel="alternate"></link><published>2018-10-29T22:00:00-04:00</published><updated>2018-10-29T22:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-10-29:/2018/10/29/additional-strategies-partition-function/</id><summary type="html">&lt;p&gt;Stochastic maximum likelihood, contrastive divergence, negative contrastive estimation and negative sampling for improving or avoiding the computation of the gradient of the log-partition function. (Oof, that's a mouthful.)&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="https://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/"&gt;previous post&lt;/a&gt; we introduced Boltzmann machines and the infeasibility of computing the gradient of its log-partition function &lt;span class="math"&gt;\(\nabla\log{Z}\)&lt;/span&gt;. To this end, we explored one strategy for its approximation: Gibbs sampling. Gibbs sampling is a viable alternative because the expression for our gradient simplifies to an expectation over the model distribution, which can be approximated with Monte Carlo samples.&lt;/p&gt;
&lt;p&gt;In this post, we'll highlight the imperfections of this approximate approach itself, then present more preferable alternatives.&lt;/p&gt;
&lt;h1&gt;Pitfalls of Gibbs sampling&lt;/h1&gt;
&lt;p&gt;To refresh, the two gradients we seek to compute in a reasonable amount of time are:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{Z} = \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\\
\nabla_{b_{i}}\log{Z} = \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$&lt;/div&gt;
&lt;p&gt;Via Gibbs sampling, we approximate each by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Burning in a Markov chain with respect to our model, then selecting &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples from this chain&lt;/li&gt;
&lt;li&gt;Evaluating both functions (&lt;span class="math"&gt;\(x_i  x_j\)&lt;/span&gt;, and &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;) at these samples&lt;/li&gt;
&lt;li&gt;Taking the average of each&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Concretely:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{Z} \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}\\
\nabla_{b_{i}}\log{Z} \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;We perform this sampling process at each gradient step.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The cost of burning in each chain&lt;/h2&gt;
&lt;p&gt;Initializing a Markov chain at a random sample incurs a non-trivial "burn-in" cost. If paying this cost at each gradient step, it begins to add up. How can we do better?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the remainder of the post, we'll explore two new directives for approximating the negative phase more cheaply, and the algorithms they birth.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Directive #1: Cheapen the burn-in process&lt;/h1&gt;
&lt;h2&gt;Stochastic maximum likelihood&lt;/h2&gt;
&lt;p&gt;SML assumes the premise: let's initialize our chain at a point already close to the model's true distribution—reducing or perhaps eliminating the cost of burn-in altogether.  &lt;strong&gt;In this vein, at what sample do we initialize the chain?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In SML, we simply initialize at the terminal value of the previous chain (i.e. the one we manufactured to compute the gradients of the previous mini-batch). &lt;strong&gt;As long as the model has not changed significantly since, i.e. as long as the previous parameter update (gradient step) was not too large, this sample should exist in a region of high probability under the current model.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Implications&lt;/h3&gt;
&lt;p&gt;Per the expression for the full log-likelihood gradient, e.g. &lt;span class="math"&gt;\(\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, the negative phase works to "reduce the probability of the points in which the model strongly, yet wrongly, believes".&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; Since we approximate this term at each parameter update with samples &lt;em&gt;roughly from&lt;/em&gt; the current model's true distribution, &lt;strong&gt;we do not encroach on this foundational task.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Contrastive divergence&lt;/h2&gt;
&lt;p&gt;Alternatively, in the contrastive divergence algorithm, we initialize the chain at each gradient step with a &lt;em&gt;random sample&lt;/em&gt; from the data distribution.&lt;/p&gt;
&lt;h3&gt;Implications&lt;/h3&gt;
&lt;p&gt;With no guarantee that the data distribution resembles the model distribution, we may systematically fail to sample, and thereafter "suppress," points that are incorrectly likely under the latter (as they do not appear in the former!). &lt;strong&gt;This incurs the growth of "spurious modes"&lt;/strong&gt; in our model, aptly named.&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;p&gt;Cheapening the burn-in phase indeed gives us a more efficient training routine. Moving forward, what are some even more aggressive strategies we might explore?&lt;/p&gt;
&lt;h1&gt;Directive #2: Skip the computation of &lt;span class="math"&gt;\(Z\)&lt;/span&gt; altogether&lt;/h1&gt;
&lt;p&gt;Canonically, we write the log-likelihood of our Boltzmann machine as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{\mathcal{L}(x)}
&amp;amp;= \log{\frac{\exp{(H(x))}}{Z}}\\
&amp;amp;= \log{\big(\exp{(H(x))}\big)} - \log{Z}\\
&amp;amp;= H(x) - \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Instead, what if we simply wrote this as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{\mathcal{L}(x)} = H(x) - c
$$&lt;/div&gt;
&lt;p&gt;or, more generally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p_{\text{model}}(x)} = \log{\tilde{p}_{\text{model}}(x; \theta)} - c
$$&lt;/div&gt;
&lt;p&gt;and estimated &lt;span class="math"&gt;\(c\)&lt;/span&gt; as a parameter?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Immediately, we remark that if we optimize this model with maximum likelihood, our algorithm will, trivially, make &lt;span class="math"&gt;\(c\)&lt;/span&gt; arbitrarily negative.&lt;/strong&gt; In other words, the easiest way to increase &lt;span class="math"&gt;\(\log{p_{\text{model}}(x)}\)&lt;/span&gt; is to decrease &lt;span class="math"&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How might we better phrase this problem?&lt;/p&gt;
&lt;h2&gt;Noise contrastive estimation&lt;/h2&gt;
&lt;p&gt;Ingeniously, NCE proposes an alternative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Posit two distributions: the model, and a noise distribution&lt;/li&gt;
&lt;li&gt;Given a data point, predict the distribution (i.e. binary classification) from which this point was generated&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's unpack this a bit.&lt;/p&gt;
&lt;p&gt;Under an (erroneous) MLE formulation, we would optimize the following objective:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x \sim p_{\text{data}}} [\log{p_{\text{model}}}(x)]
$$&lt;/div&gt;
&lt;p&gt;Under NCE, we're going to replace two pieces so as to perform the binary classification task described above (with 1 = "model", and 0 = "noise").&lt;/p&gt;
&lt;p&gt;First, let's swap &lt;span class="math"&gt;\(\log{p_{\text{model}}}(x)\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log{p_{\text{joint}}}(y = 0\vert x)\)&lt;/span&gt;, where:&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(x\vert y) =
\begin{cases}
p_{\text{noise}}(x)\quad y = 0\\
p_{\text{model}}(x)\quad y = 1\\
\end{cases}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(x, y)
= p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(y = 0\vert x)
= \frac{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)}
$$&lt;/div&gt;
&lt;p&gt;Finally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x \sim p_{\text{data}}} [\log{p_{\text{joint}}(y = 0\vert x)}]
$$&lt;/div&gt;
&lt;p&gt;From here, we need to update &lt;span class="math"&gt;\(x \sim p_{\text{data}}\)&lt;/span&gt; to include &lt;span class="math"&gt;\(y\)&lt;/span&gt;. We'll do this in two pedantic steps.&lt;/p&gt;
&lt;p&gt;First, let's write:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x, y=0\ \sim\ p_{\text{noise}}} [\log{p_{\text{joint}}(y\vert x)}]
$$&lt;/div&gt;
&lt;p&gt;This equation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Builds a classifier that discriminates between samples generated from the model distribution and noise distribution &lt;strong&gt;trained only on samples from the latter.&lt;/strong&gt; (Clearly, this will not make for an effective classifier.)&lt;/li&gt;
&lt;li&gt;To train this classifier, we note that the equation asks us to maximize the likelihood of the noise samples under the noise distribution—where the noise distribution itself has no actual parameters we intend to train!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In solution, we trivially expand our expectation to one over both noise samples, and data samples. In doing so, in predicting &lt;span class="math"&gt;\(\log{p_{\text{joint}}(y = 1\vert x)} = 1 - \log{p_{\text{joint}}(y = 0\vert x)}\)&lt;/span&gt;, &lt;strong&gt;we'll be maximizing the likelihood of the data under the model.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x, y\ \sim\ p_{\text{train}}} [\log{p_{\text{joint}}(y \vert x)}]
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\text{train}}(x\vert y) =
\begin{cases}
p_{\text{noise}}(x)\quad y = 0\\
p_{\text{data}}(x)\quad y = 1\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;As a final step, we'll expand our object into something more elegant:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)}\\
&amp;amp;= \frac{1}{1 + \frac{p_{\text{joint}}(y = 1)p_{\text{model}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;em&gt;a priori&lt;/em&gt; that &lt;span class="math"&gt;\(p_{\text{joint}}(x, y)\)&lt;/span&gt; is &lt;span class="math"&gt;\(k\)&lt;/span&gt; times more likely to generate a noise sample, i.e. &lt;span class="math"&gt;\(\frac{p_{\text{joint}}(y = 1)}{p_{\text{joint}}(y = 0)} = \frac{1}{k}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{p_{\text{noise}}(x)\cdot k}}\\
&amp;amp;= \frac{1}{1 + \exp\big(\log{\frac{p_{\text{model}}(x)}{{p_{\text{noise}}(x)\cdot k}}}\big)}\\
&amp;amp;= \sigma\bigg(-\log{\frac{p_{\text{model}}(x)}{{p_{\text{noise}}(x)\cdot k}}}\bigg)\\
&amp;amp;= \sigma\bigg(\log{k} + \log{p_{\text{noise}}(x)} - \log{p_{\text{model}}(x)}\bigg)\\
p_{\text{joint}}(y = 1\vert x)
&amp;amp;= 1 - \sigma\bigg(\log{k} + \log{p_{\text{noise}}(x)} - \log{p_{\text{model}}(x)}\bigg)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Given a joint training distribution over &lt;span class="math"&gt;\((X_{\text{data}}, y=1)\)&lt;/span&gt; and &lt;span class="math"&gt;\((X_{\text{noise}}, y=0)\)&lt;/span&gt;, this is the target we'd like to maximize.&lt;/p&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;For our training data, &lt;strong&gt;we require the ability to sample from our noise distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For our target, &lt;strong&gt;we require the ability to compute the likelihood of some data under our noise distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Therefore, these criteria do place practical restrictions on the types of noise distributions that we're able to consider.&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;We briefly alluded to the fact that our noise distribution is non-parametric. However, there is nothing stopping us from evolving this distribution and giving it trainable parameters, then updating these parameters such that it generates increasingly "optimal" samples.&lt;/p&gt;
&lt;p&gt;Of course, we would have to design what "optimal" means. One interesting approach is called &lt;a href="https://arxiv.org/abs/1805.03642"&gt;Adversarial Contrastive Estimation
&lt;/a&gt;, wherein the authors adapt the noise distribution to generate increasingly "harder negative examples, which forces the main model to learn a better representation of the data."&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;Negative sampling is the same as NCE except:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We consider noise distributions whose likelihood we cannot evaluate&lt;/li&gt;
&lt;li&gt;To accommodate, we simply set &lt;span class="math"&gt;\(p_{\text{noise}}(x) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{p_{\text{noise}}(x)\cdot k}}\\
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{ k}}\\
&amp;amp;= \frac{1}{1 + \exp\big(\log\frac{p_{\text{model}}(x)}{ k}\big)}\\
&amp;amp;=\sigma(-\log\frac{p_{\text{model}}(x)}{ k})\\
&amp;amp;=\sigma(\log{k} - \log{p_{\text{model}}(x)})\\
p_{\text{joint}}(y = 1\vert x)
&amp;amp;= 1 - \sigma(\log{k} - \log{p_{\text{model}}(x)})
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;In code&lt;/h2&gt;
&lt;p&gt;Since I learn best by implementing things, let's play around. Below, we train Boltzmann machines via noise contrastive estimation and negative sampling.&lt;/p&gt;
&lt;h2&gt;Load data&lt;/h2&gt;
&lt;p&gt;For this exercise, we'll fit a Boltzmann machine to the &lt;a href="https://www.kaggle.com/zalando-research/fashionmnist"&gt;Fashion MNIST&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/additional-strategies-partition-function/output_3_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Define model&lt;/h2&gt;
&lt;p&gt;Below, as opposed to in the previous post, I offer a vectorized implementation of the Boltzmann energy function.&lt;/p&gt;
&lt;p&gt;This said, the code is still imperfect: especially re: the line in which I iterate through data points individually to compute the joint likelihood.&lt;/p&gt;
&lt;p&gt;Finally, in &lt;code&gt;Model._H&lt;/code&gt;, I divide by 1000 to get this thing to train. The following is only a toy exercise (like many of my posts); I did not spend much time tuning parameters.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xavier_uniform_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diagonal_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diagonal_mask&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;triu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;            where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;            for which to compute the joint likelihood&lt;/span&gt;

&lt;span class="sd"&gt;        :return: the likelihood, or log-likelihood if `log=True`&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
        &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;

        &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# make copy&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Noise contrastive estimation&lt;/h2&gt;
&lt;p&gt;Train a model using noise contrastive estimation. For our noise distribution, we'll start with a diagonal multivariate Gaussian, from which we can sample, and whose likelihood we can evaluate (as of PyTorch 0.4!).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# define model, noise distribution&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MultivariateNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;covariance_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# define classifier. we add a multiplicative constant to make training more stable.&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# define noise generator&lt;/span&gt;
&lt;span class="n"&gt;noise_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;noiseloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise_sample&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define optimizer, loss&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BCELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

            &lt;span class="c1"&gt;# points from data distribution&lt;/span&gt;
            &lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
            &lt;span class="n"&gt;X_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

            &lt;span class="c1"&gt;# points from noise distribution&lt;/span&gt;
            &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;
            &lt;span class="n"&gt;X_noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

            &lt;span class="c1"&gt;# stack into single input&lt;/span&gt;
            &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_noise&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Batch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; | Loss: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'weights.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'bias.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'c: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'weights.grad.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'bias.grad.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'c.grad: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train model&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.305&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0887&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0794&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0603&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0525&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0503&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0414&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.038&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.034&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0312&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;Next, we'll try negative sampling using some actual images as negative samples&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;noiseset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'data/mnist'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noiseloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noiseset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get some random training images&lt;/span&gt;
&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# show images&lt;/span&gt;
&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torchvision&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/additional-strategies-partition-function/output_12_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Train model&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# define model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define classifier&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# i had to change this learning rate to get this to train&lt;/span&gt;

&lt;span class="c1"&gt;# train&lt;/span&gt;
&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.304&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.027&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0111&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00611&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00505&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00318&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00284&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0029&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0023&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00217&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Sampling&lt;/h1&gt;
&lt;p&gt;Once more, the (ideal) goal of this model is to fit a function &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; to some data, such that we can:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluate its likelihood (wherein it actually tells us that data on which the model was fit is more likely than data on which it was not)&lt;/li&gt;
&lt;li&gt;Draw realistic samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From a Boltzmann machine, our primary strategy for drawing samples is via Gibbs sampling. It's slow, and I do not believe it's meant to work particularly well. Let's draw 5 samples and see how we do.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;

&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;CPU times: user 4min 10s, sys: 4.09 s, total: 4min 14s&lt;/span&gt;
&lt;span class="err"&gt;Wall time: 4min 17s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Takes forever!&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/additional-strategies-partition-function/output_18_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Nothing great. These samples are highly correlated, if perfectly identical, as expected.&lt;/p&gt;
&lt;p&gt;To generate better images, we'll have to let this run for a lot longer and "thin" the chain (taking every &lt;code&gt;every_n&lt;/code&gt; samples, where &lt;code&gt;every_n&lt;/code&gt; is on the order of 1, 10, or 100, roughly).&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, we discussed four additional strategies for both speeding up, as well as outright avoiding, the computation of the gradient of the log-partition function &lt;span class="math"&gt;\(\nabla\log{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While we only presented toy models here, these strategies see successful application in larger undirected graphical models, as well as directed conditional models for &lt;span class="math"&gt;\(p(y\vert x)\)&lt;/span&gt;. One key example of the latter is a language model; though the partition function is a sum over distinct values of &lt;span class="math"&gt;\(y\)&lt;/span&gt; (labels) instead of configurations of &lt;span class="math"&gt;\(x\)&lt;/span&gt; (inputs), it can still be intractable to compute! This is because there are as many distinct values of &lt;span class="math"&gt;\(y\)&lt;/span&gt; as there are tokens in the given language's vocabulary, which is typically on the order of millions.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/boltzmann-machines"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-2.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;@book{Goodfellow-et-al-2016,
title={Deep Learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
publisher={MIT Press},
note={\url{http://www.deeplearningbook.org}},
year={2016}
} &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1805.03642"&gt;Adversarial Contrastive Estimation&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>A Thorough Introduction to Boltzmann Machines</title><link href="https://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/" rel="alternate"></link><published>2018-10-20T14:00:00-04:00</published><updated>2018-10-20T14:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-10-20:/2018/10/20/thorough-introduction-to-boltzmann-machines/</id><summary type="html">&lt;p&gt;A pedantic walk through Boltzmann machines, with focus on the computational thorn-in-side of the partition function.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The principal task of machine learning is to fit a model to some data. In programming terms, this model is an object with two methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;How likely is the query point &lt;span class="math"&gt;\(x\)&lt;/span&gt; under our model? In other words, how likely was it that our model produced &lt;span class="math"&gt;\(x\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Note: The likelihood gives a value proportional to a valid probability, but is not necessarily a valid probability itself.&lt;/p&gt;
&lt;h2&gt;Sample&lt;/h2&gt;
&lt;p&gt;Draw a sample datum &lt;span class="math"&gt;\(x\)&lt;/span&gt; from the model.&lt;/p&gt;
&lt;h2&gt;Notation&lt;/h2&gt;
&lt;p&gt;Canonically, we denote an instance of our &lt;code&gt;Model&lt;/code&gt; in mathematical syntax as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
x \sim p(x)
$$&lt;/div&gt;
&lt;p&gt;Again, this simple notation implies two powerful methods: that we can evaluate the &lt;code&gt;likelihood&lt;/code&gt; of having observed &lt;span class="math"&gt;\(x\)&lt;/span&gt; under our model &lt;span class="math"&gt;\(p\)&lt;/span&gt;, and that we can &lt;code&gt;sample&lt;/code&gt; a new value &lt;span class="math"&gt;\(x\)&lt;/span&gt; from our model &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Often, we work instead with &lt;em&gt;conditional&lt;/em&gt; models, e.g. &lt;span class="math"&gt;\(y \sim p(y\vert x)\)&lt;/span&gt;, in classification and regression tasks. The &lt;code&gt;likelihood&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt; methods apply all the same.&lt;/p&gt;
&lt;h2&gt;Boltzmann machines&lt;/h2&gt;
&lt;p&gt;A Boltzmann machine is one of the simplest mechanisms for modeling &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. It is an undirected graphical model where every dimension &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; of a given observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; influences every other dimension. As such, we might use it to model data which we believe to exhibit this property, e.g. an image (where intuitively, pixel values influence neighboring pixel values). For &lt;span class="math"&gt;\(x \in R^3\)&lt;/span&gt;, our model would look as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/boltzmann-machine.svg"/&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(x \in R^n\)&lt;/span&gt;, a given node &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; would have &lt;span class="math"&gt;\(n - 1\)&lt;/span&gt; outgoing connections in total—one to every other node &lt;span class="math"&gt;\(x_j\ \forall\ j \neq i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, a Boltzmann machine strictly operates on &lt;em&gt;binary&lt;/em&gt; data. This keeps things simple.&lt;/p&gt;
&lt;h2&gt;Computing the likelihood&lt;/h2&gt;
&lt;p&gt;A Boltzmann machines admits the following formula for computing the &lt;code&gt;likelihood&lt;/code&gt; of data points &lt;span class="math"&gt;\(x^{(1)}, ..., x^{(n)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x) = \sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p(x) = \frac{\exp{(H(x))}}{Z}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{i=1}^n p(x^{(i)})
$$&lt;/div&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since our weights can be negative, &lt;span class="math"&gt;\(H(x)\)&lt;/span&gt; can be negative. Since our likelihood is proportional to a valid probability, we'd prefer it to be non-negative.&lt;/li&gt;
&lt;li&gt;To enforce this constraint, we exponentiate &lt;span class="math"&gt;\(H(x)\)&lt;/span&gt; in the second equation.&lt;/li&gt;
&lt;li&gt;To normalize, we divide by the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt;, i.e. the sum of the likelihoods of all possible values of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Computing the partition function by hand&lt;/h2&gt;
&lt;p&gt;In the case of 2-dimensional binary datum &lt;span class="math"&gt;\(x\)&lt;/span&gt;, there are 4 possible "configurations": &lt;span class="math"&gt;\([0, 0], [0, 1], [1, 0], [1, 1]\)&lt;/span&gt;. As such, to compute the likelihood of one of these configurations, e.g.&lt;/p&gt;
&lt;div class="math"&gt;$$
p([1, 0]) = \frac{\exp{(H([1, 0]))}}{\exp{(H([0, 0]))} + \exp{(H([0, 1]))} + \exp{(H([1, 0]))} + \exp{(H([1, 1]))}}
$$&lt;/div&gt;
&lt;p&gt;we see that the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt; is a sum of 4 terms.&lt;/p&gt;
&lt;p&gt;More generally, given &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional &lt;span class="math"&gt;\(x\)&lt;/span&gt;, where each &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; can assume one of &lt;span class="math"&gt;\(v\)&lt;/span&gt; distinct values, computing &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; implies a summation over &lt;span class="math"&gt;\(v^d\)&lt;/span&gt; terms. &lt;strong&gt;With a non-trivially large &lt;span class="math"&gt;\(v\)&lt;/span&gt; or &lt;span class="math"&gt;\(d\)&lt;/span&gt; this becomes intractable to compute.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below, we'll demonstrate how "tractability," i.e. "can we actually compute &lt;span class="math"&gt;\(Z\)&lt;/span&gt; before the end of the universe?" changes with varying &lt;span class="math"&gt;\(d\)&lt;/span&gt; for our Boltzmann machine (of &lt;span class="math"&gt;\(v = 2\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;The likelihood function in code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;        where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;        for which to compute the joint likelihood.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
    &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="c1"&gt;# compute partition function&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;NB: In mathematical Python code, for-loops are bad; we should prefer &lt;code&gt;numpy&lt;/code&gt; instead. Nevertheless, I've used for-loops here because they are easier to read.&lt;/p&gt;
&lt;p&gt;This code block is longer than you might expect because it includes a few supplementary behaviors, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computing the likelihood of one or more points&lt;/li&gt;
&lt;li&gt;Avoiding redundant computation of &lt;code&gt;Z&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Optionally computing the log-likelihood&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Training the model&lt;/h2&gt;
&lt;p&gt;At the outset, the parameters &lt;code&gt;self.weights&lt;/code&gt; and &lt;code&gt;self.biases&lt;/code&gt; of our model are initialized at random. Trivially, such that the values returned by &lt;code&gt;likelihood&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt; are useful, we must first update these parameters by fitting this model to observed data.&lt;/p&gt;
&lt;p&gt;To do so, we will employ the principal of maximum likelihood: compute the parameters that make the observed data maximally likely under the model, via gradient ascent.&lt;/p&gt;
&lt;h2&gt;Gradients&lt;/h2&gt;
&lt;p&gt;Since our model is simple, we can derive exact gradients by hand. We will work with the log-likelihood instead of the true likelihood to avoid issues of computational underflow. Below, we simplify this expression, then compute its various gradients.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(\log{\mathcal{L}}\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{k=1}^n \frac{\exp{(H(x^{(k)})}}{Z}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{\mathcal{L}(x^{(1)}, ..., x^{(n)})}
&amp;amp;= \sum\limits_{k=1}^n \log{\frac{\exp{(H(x^{(k)})}}{Z}}\\
&amp;amp;= \sum\limits_{k=1}^n \log{\big(\exp{(H(x^{(k)})}\big)} - \log{Z}\\
&amp;amp;= \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This gives the total likelihood. Our aim is to maximize the expected likelihood with respect to the data generating distribution.&lt;/p&gt;
&lt;h3&gt;Expected likelihood&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{x \sim p_{\text{data}}}\big[ \mathcal{L}(x) \big]
&amp;amp;= \sum\limits_{k=1}^N p_{\text{data}}(x = x^{(k)}) \mathcal{L(x^{(k)})}\\
&amp;amp;\approx \sum\limits_{k=1}^N \frac{1}{N} \mathcal{L(x^{(k)})}\\
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^N  \mathcal{L(x^{(k)})}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In other words, we wish to maximize the average likelihood of our data under the model. Henceforth, we will refer to this quantity as &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, i.e. &lt;span class="math"&gt;\(\mathcal{L} = \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, deriving the gradient with respect to our weights:&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(\nabla_{w_{i, j}}\log{\mathcal{L}}\)&lt;/span&gt;:&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\nabla_{w_{i, j}} \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)}) - \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;First term:&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)})
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \bigg[ \sum\limits_{i \neq j} w_{i, j} x_i^{(k)} x_j^{(k)} + \sum\limits_i b_i x_i^{(k)} \bigg]\\
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\\
&amp;amp;\approx \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j]
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Second term:&lt;/h3&gt;
&lt;p&gt;NB: &lt;span class="math"&gt;\(\sum\limits_{\mathcal{x}}\)&lt;/span&gt; implies a summation over all &lt;span class="math"&gt;\(v^d\)&lt;/span&gt; possible configurations of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\nabla_{w_{i, j}} \log{Z}
&amp;amp;= \nabla_{w_{i, j}} \log{\sum\limits_{\mathcal{x}}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{\sum\limits_{\mathcal{x}} \exp{(H(x))}} \nabla_{w_{i, j}} \sum\limits_{\mathcal{x}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \nabla_{w_{i, j}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \exp{(H(x))} \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} \frac{\exp{(H(x))}}{Z} \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} p(x) \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} p(x) [x_i  x_j]\\
&amp;amp;= \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Putting it back together&lt;/h3&gt;
&lt;p&gt;Combining these constituent parts, we arrive at the following formula:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
$$&lt;/div&gt;
&lt;p&gt;Finally, following the same logic, we derive the exact gradient with respect to our biases:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{b_i}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$&lt;/div&gt;
&lt;p&gt;The first and second terms of each gradient are called, respectively, &lt;strong&gt;the positive and negative phases.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Computing the positive phase&lt;/h2&gt;
&lt;p&gt;In the following toy example, our data are small: we can compute the positive phase using all of the training data, i.e. &lt;span class="math"&gt;\(\frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\)&lt;/span&gt;. Were our data bigger, we could approximate this expectation with a mini-batch of training data (much like SGD).&lt;/p&gt;
&lt;h2&gt;Computing the negative phase&lt;/h2&gt;
&lt;p&gt;Again, this term asks us to compute then sum the log-likelihood over every possible data configuration in the support of our model, which is &lt;span class="math"&gt;\(O(v^d)\)&lt;/span&gt;. &lt;strong&gt;With non-trivially large &lt;span class="math"&gt;\(v\)&lt;/span&gt; or &lt;span class="math"&gt;\(d\)&lt;/span&gt;, this becomes intractable to compute.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below, we'll begin our toy example by computing the true negative-phase, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, with varying data dimensionalities &lt;span class="math"&gt;\(d\)&lt;/span&gt;. Then, once this computation becomes slow, we'll look to approximate this expectation later on.&lt;/p&gt;
&lt;h2&gt;Parameter updates in code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train model, visualize model distribution&lt;/h2&gt;
&lt;p&gt;Finally, we're ready to train. Using the true negative phase, let's train our model for 100 epochs with &lt;span class="math"&gt;\(d=3\)&lt;/span&gt; then visualize results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset_data_and_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Generate training data, weights, biases, and a list of all data configurations&lt;/span&gt;
&lt;span class="sd"&gt;    in our model's support.&lt;/span&gt;

&lt;span class="sd"&gt;    In addition, generate a list of tuples of the indices of adjacent nodes, which&lt;/span&gt;
&lt;span class="sd"&gt;    we'll use to update parameters without duplication.&lt;/span&gt;

&lt;span class="sd"&gt;    For example, with `n_units=3`, we generate a matrix of weights with shape (3, 3);&lt;/span&gt;
&lt;span class="sd"&gt;    however, there are only 3 distinct weights in this matrix that we'll actually&lt;/span&gt;
&lt;span class="sd"&gt;    want to update: those connecting Node 0 --&amp;gt; Node 1, Node 1 --&amp;gt; Node 2, and&lt;/span&gt;
&lt;span class="sd"&gt;    Node 0 --&amp;gt; Node 2. This function returns a list containing these tuples&lt;/span&gt;
&lt;span class="sd"&gt;    named `var_combinations`.&lt;/span&gt;

&lt;span class="sd"&gt;    :param n_units: the dimensionality of our data `d`&lt;/span&gt;
&lt;span class="sd"&gt;    :param n_obs: the number of observations in our training set&lt;/span&gt;
&lt;span class="sd"&gt;    :param p: a vector of the probabilities of observing a 1 in each index&lt;/span&gt;
&lt;span class="sd"&gt;        of the training data. The length of this vector must equal `n_units`&lt;/span&gt;

&lt;span class="sd"&gt;    :return: weights, biases, var_combinations, all_configs, data&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialize data&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# initialize parameters&lt;/span&gt;
    &lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# a few other pieces we'll need&lt;/span&gt;
    &lt;span class="n"&gt;var_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;all_configs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nd"&gt;@staticmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_inv_logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;            where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;            for which to compute the joint likelihood.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
        &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

        &lt;span class="c1"&gt;# compute partition function&lt;/span&gt;
        &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Can't burn in for more samples than there are in the chain"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_inv_logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# make copy&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reset_data_and_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Epoch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; | Likelihood: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;209.63758306786653&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;162.04280784271083&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;160.49961381649555&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;159.79539070373576&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;159.2853717231018&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.90186293631422&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.6084020645482&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.38094343579155&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.20287017780586&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.06232196551673&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Visualize samples&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_n_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    NB: We add some jitter to the points so as to better visualize density in a given corner of the model.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;111&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;projection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'3d'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 0'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_zlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="n"&gt;plot_n_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; Samples from Model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_7_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The plot roughly matches the data-generating distribution: most points assume values of either &lt;span class="math"&gt;\([1, 0, 1]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([1, 0, 0]\)&lt;/span&gt; (given &lt;span class="math"&gt;\(p=[.8, .1, .5]\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Sampling, via Gibbs&lt;/h2&gt;
&lt;p&gt;The second, final method we need to implement is &lt;code&gt;sample&lt;/code&gt;. In a Boltzmann machine, we typically do this via &lt;a href="http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf"&gt;Gibbs sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To effectuate this sampling scheme, we'll need a model of each data dimension conditional on the other data dimensions. For example, for &lt;span class="math"&gt;\(d=3\)&lt;/span&gt;, we'll need to define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_0\vert x_1, x_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_1\vert x_0, x_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_2\vert x_0, x_1)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that each dimension must assume a 0 or a 1, the above 3 models must necessarily return the probability of observing a 1 (where 1 minus this value gives the probability of observing a 0).&lt;/p&gt;
&lt;p&gt;Let's derive these models using the workhorse axiom of conditional probability, starting with the first:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p(x_0 = 1\vert x_1, x_2)
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_1, x_2)}\\
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{\sum\limits_{x_0 \in [0, 1]} p(x_0, x_1, x_2)}\\
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_0 = 0, x_1, x_2) + p(x_0 = 1, x_1, x_2)}\\
&amp;amp;= \frac{1}{1 + \frac{p(x_0 = 0, x_1, x_2)}{p(x_0 = 1, x_1, x_2)}}\\
&amp;amp;= \frac{1}{1 + \frac{\exp{(H(x_0 = 0, x_1, x_2)))}}{\exp{(H(x_0 = 1, x_1, x_2)))}}}\\
&amp;amp;= \frac{1}{1 + \exp{(H(x_0 = 0, x_1, x_2) - H(x_0 = 1, x_1, x_2))}}\\
&amp;amp;= \frac{1}{1 + \exp{(\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i - (\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i))}}\\
&amp;amp;= \frac{1}{1 + \exp{(-\sum\limits_{j \neq i = 0} w_{i, j} x_j - b_i)}}\\
&amp;amp;= \sigma\bigg(\sum\limits_{j \neq i = 0} w_{i, j} x_j + b_i\bigg)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Pleasantly enough, this model resolves to a simple Binomial GLM, i.e. logistic regression, involving only its neighboring units and the weights that connect them.&lt;/p&gt;
&lt;p&gt;With the requisite conditionals in hand, let's run this chain and compare it with our (trained) model's true probability distribution.&lt;/p&gt;
&lt;h2&gt;True probability distribution&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[0, 0, 0]: 0.07327&lt;/span&gt;
&lt;span class="err"&gt;[0, 0, 1]: 0.09227&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 0]: 0.01366&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 1]: 0.01938&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 0]: 0.3351&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 1]: 0.3622&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 0]: 0.04693&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 1]: 0.05715&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Empirical probability distribution, via Gibbs&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;empirical_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;empirical_probability&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (true), &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;empirical_probability&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (empirical)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[0, 0, 0]: 0.07327 (true), 0.05102 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 0, 1]: 0.09227 (true), 0.09184 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 0]: 0.01366 (true), 0.0102 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 1]: 0.01938 (true), 0.02041 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 0]: 0.3351 (true), 0.3673 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 1]: 0.3622 (true), 0.398 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 0]: 0.04693 (true), 0.03061 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 1]: 0.05715 (true), 0.03061 (empirical)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Close, ish enough.&lt;/p&gt;
&lt;h2&gt;Scaling up, and hitting the bottleneck&lt;/h2&gt;
&lt;p&gt;With data of vary dimensionality &lt;code&gt;n_units&lt;/code&gt;, the following plot gives the time in seconds that it takes to train this model for 10 epochs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_15_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;To reduce computational burden, and/or to fit a Boltzmann machine to data of non-trivial dimensionality (e.g. a 28x28 grey-scale image, which implies a random variable with 28x28=784 dimensions), we need to compute the positive and/or negative phase of our gradient faster than we currently are.&lt;/p&gt;
&lt;p&gt;To compute the former more quickly, we could employ mini-batches as in canonical stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;In this post, we'll instead focus on ways to speed up the latter. Revisiting its expression, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, we readily see that we can create an unbiased estimator for this value by drawing Monte Carlo samples from our model, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j] \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$&lt;/div&gt;
&lt;p&gt;So, now we just need a way to draw these samples. Luckily, we have a Gibbs sampler to tap!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instead of computing the true negative phase, i.e. summing &lt;span class="math"&gt;\(x_i  x_j\)&lt;/span&gt; over all permissible configurations &lt;span class="math"&gt;\(X\)&lt;/span&gt; under our model, we can approximate it by evaluating this expression for a few model samples, then taking the mean.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We define this update mechanism here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                          &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we'll define a function that we can parameterize by an optimization algorithm (computing the true negative phase, or approximating it via Gibbs sampling, in the above case) which will train a model for &lt;span class="math"&gt;\(n\)&lt;/span&gt; epochs and return data requisite for plotting.&lt;/p&gt;
&lt;h2&gt;How does training progress for varying data dimensionalities?&lt;/h2&gt;
&lt;p&gt;Finally, for data of &lt;code&gt;n_units&lt;/code&gt; 3, 4, 5, etc., let’s train models for 100 epochs and plot likelihood curves.&lt;/p&gt;
&lt;p&gt;When training with the approximate negative phase, we’ll:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Derive model samples from a &lt;strong&gt;1000-sample Gibbs chain. Of course, this is a parameter we can tune, which will affect both model accuracy and training runtime. However, we don’t explore that in this post;&lt;/strong&gt; instead, we just pick something reasonable and hold this value constant throughout our experiments.&lt;/li&gt;
&lt;li&gt;Train several models for a given &lt;code&gt;n_units&lt;/code&gt;; Seaborn will average results for us then plot a single line.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_epochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_epochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_23_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When we let each algorithm run for 100 epochs, the true negative phase gives a model which assigns higher likelihood to the observed data in all of the above training runs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Notwithstanding, the central point is that 100 epochs of the true negative phase takes a long time to run.&lt;/p&gt;
&lt;p&gt;As such, let’s run each for an equal amount of time, and plot results. Below, we define a function to train models for &lt;span class="math"&gt;\(n\)&lt;/span&gt; seconds (or 1 epoch—whichever comes first).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;n_seconds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;How many epochs do we actually get through?&lt;/h2&gt;
&lt;p&gt;Before plotting results, let’s examine how many epochs each algorithm completes in its allotted time. In fact, for some values of &lt;code&gt;n_units&lt;/code&gt;, we couldn’t even complete a single epoch (when computing the true negative phase) in &lt;span class="math"&gt;\(\leq 1\)&lt;/span&gt; second.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_28_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Finally, we look at performance. With &lt;code&gt;n_units &amp;lt;= 7&lt;/code&gt;, we see that 1 second of training with the true negative phase yields a better model. Conversely, &lt;strong&gt;using 7 or more units, the added performance given by using the true negative phase is overshadowed by the amount of time it takes the model to train.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_31_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Of course, we re-stress that the exact ablation results are conditional (amongst other things) on &lt;strong&gt;the number of Gibbs samples we chose to draw. Changing this will change the results, but not that about which we care the most: the overall trend.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Throughout this post, we've given a thorough introduction to a Boltzmann machine: what it does, how it trains, and some of the computational burdens and considerations inherent.&lt;/p&gt;
&lt;p&gt;In the next post, we'll look at cheaper, more inventive algorithms for avoiding the computation of the negative phase, and describe how they're used in common machine learning models and training routines.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/boltzmann-machines"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-1.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec19.pdf"&gt;CSC321 Lecture 19: Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/"&gt;Derivation: Maximum Likelihood for Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf"&gt;Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>From Gaussian Algebra to Gaussian Processes, Part 2</title><link href="https://willwolf.io/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/" rel="alternate"></link><published>2018-06-12T08:00:00-04:00</published><updated>2018-06-12T08:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-06-12:/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/</id><summary type="html">&lt;p&gt;Introducing the RBF kernel, and motivating its ubiquitous use in Gaussian processes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, we covered the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions (i.e. function evaluations)&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian (just pluck out the pieces of interest)&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset gives another Gaussian (a simple algebraic formula)&lt;/li&gt;
&lt;li&gt;Posterior over functions (the linear map of the posterior over weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X_{*})^T\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Covariances (the second thing we need in order to specify a multivariate Gaussian)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;If any of the above is still not clear, please look no further, and re-visit the &lt;a href="https://willwolf.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/"&gt;previous post&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Conversely, we did not directly cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we'll explain these two.&lt;/p&gt;
&lt;h2&gt;The more features we use, the more expressive our model&lt;/h2&gt;
&lt;p&gt;We concluded the previous post by plotting posteriors over function evaluations given various &lt;code&gt;phi_func&lt;/code&gt;s, i.e. a function that creates "features" &lt;span class="math"&gt;\(\phi(X)\)&lt;/span&gt; given an input &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 inputs&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 corresponding outputs, which we'll use later on&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# makes D=2 features for each input&lt;/span&gt;


&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One common such set of features are those given by "radial basis functions", a.k.a. the "squared exponential" function, defined as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, the choice of which features to use is ultimately arbitrary, i.e. a choice left to the modeler.&lt;/p&gt;
&lt;p&gt;Throughout the exercise, we saw that the larger the dimensionality &lt;span class="math"&gt;\(d\)&lt;/span&gt; of our feature function &lt;code&gt;phi_func&lt;/code&gt;, the more expressive, i.e. less endemically prone to overfitting, our model became.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, how far can we take this?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Computing features is expensive&lt;/h2&gt;
&lt;p&gt;Ideally, we'd compute as many features as possible for each input element, i.e. employ &lt;code&gt;phi_func(x, D=some_huge_number)&lt;/code&gt;. Unfortunately, the cost of doing so adds up, and ultimately becomes intractable past meaningfully-large values of &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perhaps there's a better way?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How are these things used?&lt;/h2&gt;
&lt;p&gt;Let's bring back our GP equations, and prepare ourselves to &lt;em&gt;squint&lt;/em&gt;! In the previous post, we outlined the following modeling process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define prior distribution over weights and function evaluations, &lt;span class="math"&gt;\(P(w, y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Marginalizing &lt;span class="math"&gt;\(P(w, y)\)&lt;/span&gt; over &lt;span class="math"&gt;\(y\)&lt;/span&gt;, i.e. &lt;span class="math"&gt;\(\int P(w, y)dy\)&lt;/span&gt;, and given some observed function evaluations &lt;span class="math"&gt;\(y\)&lt;/span&gt;, compute the posterior distribution over weights, &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;After linear-mapping &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; onto some new, transformed test input &lt;span class="math"&gt;\(\phi(X_*)^T\)&lt;/span&gt;, compute the posterior distribution over function evaluations, &lt;span class="math"&gt;\(P(y_*\ \vert\ y) = P(\phi(X_{*})^Tw\ \vert\ y)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, let's unpack #2 and #3.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First, the mathematical equation:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(w\vert y)
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}\Sigma_y^{-1}(y - \mu_y), \Sigma_w - \Sigma_{wy}\Sigma_y^{-1}\Sigma_{wy}^T)\\
    \\
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}(y - \mu_w^T \phi(X)), \Sigma_w - \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}\Sigma_{wy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Next, this equation in code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define initial parameters&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# dimensionality of `phi_func`&lt;/span&gt;

&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# often a vector of zeros, though it doesn't have to be&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# often the identity matrix, though it doesn't have to be&lt;/span&gt;

&lt;span class="c1"&gt;# Featurize `X_train`&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Params of prior distribution over function evals&lt;/span&gt;
&lt;span class="n"&gt;mu_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt;
     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cov_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;

&lt;span class="c1"&gt;# Params of posterior distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;
&lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;
           &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(P(y_*\ \vert\ y) = P(\phi(X_{*})^Tw\ \vert\ y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(X_*\)&lt;/span&gt; is a set of test points, e.g. &lt;code&gt;np.linspace(-10, 10, 200)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition, let's call &lt;span class="math"&gt;\(X_* \rightarrow\)&lt;/span&gt; &lt;code&gt;X_test&lt;/code&gt; and &lt;span class="math"&gt;\(y_* \rightarrow\)&lt;/span&gt; &lt;code&gt;y_test&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mathematical equations in code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Featurize `X_test`&lt;/span&gt;
&lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# The following two equations were defined above&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;
&lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;

&lt;span class="c1"&gt;# The mean of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

&lt;span class="c1"&gt;# The covariance of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Never alone&lt;/h2&gt;
&lt;p&gt;Squinting at the equations for &lt;code&gt;mu_y_test_post&lt;/code&gt; and &lt;code&gt;cov_y_test_post&lt;/code&gt;, we see that &lt;code&gt;phi_x&lt;/code&gt; and &lt;code&gt;phi_x_test&lt;/code&gt; appear &lt;strong&gt;only in the presence of another &lt;code&gt;phi_x&lt;/code&gt;, or &lt;code&gt;phi_x_test&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These four distinct such terms are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In mathematical notation, they are (respectively):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Simplifying further&lt;/h2&gt;
&lt;p&gt;These are nothing more than &lt;em&gt;scaled&lt;/em&gt; (via the &lt;span class="math"&gt;\(\Sigma_w\)&lt;/span&gt; term) dot products in some expanded feature space &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Until now, we've explicitly chosen what this &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; function is.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If the scaling matrix &lt;span class="math"&gt;\(\Sigma_w\)&lt;/span&gt; is &lt;a href="https://en.wikipedia.org/wiki/Positive-definite_matrix"&gt;positive definite&lt;/a&gt;, we can state the following, using &lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X)\)&lt;/span&gt;, i.e. &lt;code&gt;phi_x.T @ cov_w @ phi_x&lt;/code&gt;, as an example:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\Sigma_w = (\sqrt{\Sigma_w})^2
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\phi(X)^T \Sigma_w \phi(X)
    &amp;amp;= \big(\sqrt{\Sigma_w}\phi(X)\big)^T\big(\sqrt{\Sigma_w}\phi(X)\big)\\
    &amp;amp;= \varphi(X)^T\varphi(X)\\
    &amp;amp;= \varphi(X) \cdot \varphi(X)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As such, our four distinct scaled-dot-product terms can be rewritten as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*) = \varphi(X_*) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X) = \varphi(X_*) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X) = \varphi(X) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*) = \varphi(X) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;In other words, these terms can be equivalently written as dot-products in some space &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NB: we have &lt;strong&gt;not&lt;/strong&gt; explicitly chosen what this &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; function is.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Kernels&lt;/h2&gt;
&lt;p&gt;A "kernel" is a function which gives the similarity between individual elements in two sets, i.e. a Gram matrix.&lt;/p&gt;
&lt;p&gt;For instance, imagine we have two sets of countries, &lt;span class="math"&gt;\(\{\text{France}, \text{Germany}, \text{Iceland}\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{\text{Morocco}, \text{Denmark}\}\)&lt;/span&gt;, and that similarity is given by an integer value in &lt;span class="math"&gt;\([1, 5]\)&lt;/span&gt;, where 1 is the least similar, and 5 is the most. Applying a kernel to these sets might give a Gram matrix such as:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped=""&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;France&lt;/th&gt;
&lt;th&gt;Germany&lt;/th&gt;
&lt;th&gt;Iceland&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;Morocco&lt;/th&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Denmark&lt;/th&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;When you hear the term "kernel" in the context of machine learning, think "similarity between things in lists."&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NB: A "list" could be a list of vectors, i.e. a matrix. A vector, or a matrix, are the canonical inputs to a kernel.&lt;/p&gt;
&lt;h2&gt;Mercer's Theorem&lt;/h2&gt;
&lt;p&gt;Mercer's Theorem has as a key result that any kernel function can be expressed as a dot product, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
K(X, X') = \varphi(X) \cdot \varphi (X')
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; is some function that creates &lt;span class="math"&gt;\(d\)&lt;/span&gt; features out of &lt;span class="math"&gt;\(X\)&lt;/span&gt; (in the same vein as &lt;code&gt;phi_func&lt;/code&gt; from above).&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;To illustrate, I'll borrow an example from &lt;a href="https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is"&gt;CrossValidated&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;"For example, consider a simple polynomial kernel &lt;span class="math"&gt;\(K(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2\)&lt;/span&gt; with &lt;span class="math"&gt;\(\mathbf x, \mathbf y \in \mathbb R^2\)&lt;/span&gt;. This doesn't seem to correspond to any mapping function &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;, it's just a function that returns a real number. Assuming that &lt;span class="math"&gt;\(\mathbf x = (x_1, x_2)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf y = (y_1, y_2)\)&lt;/span&gt;, let's expand this expression:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
K(\mathbf x, \mathbf y)
    &amp;amp;= (1 + \mathbf x^T \mathbf y)^2\\
    &amp;amp;= (1 + x_1 \, y_1  + x_2 \, y_2)^2\\
    &amp;amp;= 1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 + 2 x_2 y_2 + 2 x_1 x_2 y_1 y_2
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Note that this is nothing else but a dot product between two vectors &lt;span class="math"&gt;\((1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)\)&lt;/span&gt; and &lt;span class="math"&gt;\((1, y_1^2, y_2^2, \sqrt{2} y_1, \sqrt{2} y_2, \sqrt{2} y_1 y_2)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\varphi(\mathbf x) = \varphi(x_1, x_2) = (1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)\)&lt;/span&gt;. So the kernel &lt;span class="math"&gt;\(K(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2 = \varphi(\mathbf x) \cdot \varphi(\mathbf y)\)&lt;/span&gt; computes a dot product in 6-dimensional space without explicitly visiting this space."&lt;/p&gt;
&lt;h3&gt;What this means&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/kernels-for-gaussian-processes.svg"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with inputs &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Our goal is to compute the similarity between then, &lt;span class="math"&gt;\(\text{Sim}(X, Y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Bottom path&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Lifting these inputs into some feature space, then computing their dot-product in that space, i.e. &lt;span class="math"&gt;\(\varphi(X) \cdot \varphi (Y)\)&lt;/span&gt; (where &lt;span class="math"&gt;\(F = \varphi\)&lt;/span&gt;, since I couldn't figure out how to draw a &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; in &lt;a href="http://draw.io"&gt;draw.io&lt;/a&gt;), is one strategy for computing this similarity.&lt;/li&gt;
&lt;li&gt;Unfortunately, this robustness comes at a cost: &lt;strong&gt;the computation is extremely expensive.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Top Path&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A valid kernel computes similarity between inputs. The function it employs might be extremely simple, e.g. &lt;span class="math"&gt;\((X - Y)^{2}\)&lt;/span&gt;; &lt;strong&gt;the computation is extremely cheap.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Mercer!&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mercer's Theorem tells us that every valid kernel, i.e. the top path, is &lt;em&gt;implicitly traversing the bottom path.&lt;/em&gt; &lt;strong&gt;In other words, kernels allow us to directly compute the result of an extremely expensive computation, extremely cheaply.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How does this help?&lt;/h2&gt;
&lt;p&gt;Once more, the Gaussian process equations are littered with the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*) = \varphi(X_*) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X) = \varphi(X_*) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X) = \varphi(X) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*) = \varphi(X) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, we previously established that the more we increase the dimensionality &lt;span class="math"&gt;\(d\)&lt;/span&gt; of our given feature function, the more flexible our model becomes.&lt;/p&gt;
&lt;p&gt;Finally, past any meaningfully large value of &lt;span class="math"&gt;\(d\)&lt;/span&gt;, and irrespective of what &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; actually is, &lt;strong&gt;this computation becomes intractably expensive.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Kernels!&lt;/h3&gt;
&lt;p&gt;You know where this is going.&lt;/p&gt;
&lt;p&gt;Given Mercer's theorem, we can state the following equalities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X_*) \cdot \varphi(X_*) = K(X_*, X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X_*) \cdot \varphi(X) = K(X_*, X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X) \cdot \varphi(X) = K(X, X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X) \cdot \varphi(X_*) = K(X, X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Which kernels to choose?&lt;/h2&gt;
&lt;p&gt;At the outset, we stated that our primary goal was to increase &lt;span class="math"&gt;\(d\)&lt;/span&gt;. As such, &lt;strong&gt;let's pick the kernel whose implicit &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; has the largest dimensionality possible.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we saw that the kernel &lt;span class="math"&gt;\(k(\mathbf x, \mathbf y)\)&lt;/span&gt; was implicitly computing a &lt;span class="math"&gt;\(d=6\)&lt;/span&gt;-dimensional dot-product. Which kernels compute a &lt;span class="math"&gt;\(d=100\)&lt;/span&gt;-dimensional dot-product? &lt;span class="math"&gt;\(d=1000\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How about &lt;span class="math"&gt;\(d=\infty\)&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Radial basis function, a.k.a. the "squared-exponential"&lt;/h2&gt;
&lt;p&gt;This kernel is implicitly computing a &lt;span class="math"&gt;\(d=\infty\)&lt;/span&gt;-dimensional dot-product. That's it. &lt;strong&gt;That's why it's so ubiquitous in Gaussian processes.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Rewriting our equations&lt;/h2&gt;
&lt;p&gt;With all of the above in mind, let's rewrite the equations for the parameters of our posterior distribution over function evaluations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The mean of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

               &lt;span class="c1"&gt;# Now, substituting in our kernels&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

&lt;span class="c1"&gt;# The covariance of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;

                &lt;span class="c1"&gt;# Now, substituting in our kernels&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Defining the kernel in code&lt;/h2&gt;
&lt;p&gt;Mathematically, the RBF kernel is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
K(X, Y) = \exp(-\frac{1}{2}\vert X - Y \vert ^2)
$$&lt;/div&gt;
&lt;p&gt;To conclude, let's define a Python function for the parameters of our posterior over function evaluations, using this RBF kernel as &lt;code&gt;k&lt;/code&gt;, then plot the resulting distribution.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 inputs&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 corresponding outputs, which we'll use below&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# vector of test inputs&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rbf_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (len(x), 1)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (1, len(y))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (len(x), len(y))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rbf_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# The following quantity is used in both `mu_y_test_post` and `cov_y_test_post`;&lt;/span&gt;
&lt;span class="c1"&gt;# we extract it into a separate variable for readability&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;            
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Visualizing results&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_17_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;And for good measure, with some samples from the posterior:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_19_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;p&gt;In this post, we've unpacked the notion of a kernel, and its ubiquitous use in Gaussian Processes.&lt;/p&gt;
&lt;p&gt;In addition, we've introduced the RBF kernel, i.e. "squared exponential" kernel, and motivated its widespread application in these models.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/gaussian-processes"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/gaussian-processes/blob/master/gaussian-processes-part-2.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://gaussianprocess.org/gpml/?"&gt;Gaussian Processes for Machine Learning&lt;/a&gt;. Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Mercers-Theorem"&gt;What is an intuitive explanation of Mercer's Theorem?&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>From Gaussian Algebra to Gaussian Processes, Part 1</title><link href="https://willwolf.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/" rel="alternate"></link><published>2018-03-31T19:00:00-04:00</published><updated>2018-03-31T19:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-03-31:/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/</id><summary type="html">&lt;p&gt;A thorough, straightforward, un-intimidating introduction to Gaussian processes in NumPy.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Most &lt;strong&gt;introductory tutorials&lt;/strong&gt; on Gaussian processes start with a nose-punch of statements, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions.&lt;/li&gt;
&lt;li&gt;A Gaussian process is non-parametric, i.e. it has an infinite number of parameters (duh?).&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian.&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset of its elements gives another Gaussian.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They continue with terms, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Posterior over functions&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;li&gt;Covariances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alas, this is often confusing to the GP beginner; the following is the introductory tutorial on GPs that I wish I'd had myself.&lt;/p&gt;
&lt;p&gt;By the end of this tutorial, you should understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What a Gaussian process is and how to build one in NumPy.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The motivations behind their functional form&lt;/strong&gt;, i.e. how the GP comes to be.&lt;/li&gt;
&lt;li&gt;The statements and terms above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;
&lt;h2&gt;Playing with Gaussians&lt;/h2&gt;
&lt;p&gt;Before moving within 500 nautical miles of the Gaussian process, we're going to start with something far easier: vanilla Gaussians themselves. This will help us to build intuition. &lt;strong&gt;We'll arrive at the GP before you realize.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Gaussian distribution, a.k.a. the Normal distribution, can be thought of as a Python object which:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is instantiated with characteristic parameters &lt;code&gt;mu&lt;/code&gt; (the mean) and &lt;code&gt;var&lt;/code&gt; (the variance).&lt;/li&gt;
&lt;li&gt;Has a single public method, &lt;code&gt;density&lt;/code&gt;, which accepts a &lt;code&gt;float&lt;/code&gt; value &lt;code&gt;x&lt;/code&gt;, and returns a &lt;code&gt;float&lt;/code&gt; proportional to the probability of this &lt;code&gt;Gaussian&lt;/code&gt; having produced &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# the standard deviation is the square-root of the variance&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, how do we make those cool bell-shaped plots? A 2D plot is just a list of tuples— each with an &lt;code&gt;x&lt;/code&gt;, and a corresponding &lt;code&gt;y&lt;/code&gt;—shown visually.&lt;/p&gt;
&lt;p&gt;As such, we lay out our &lt;code&gt;x&lt;/code&gt;-axis, then compute the corresponding &lt;code&gt;y&lt;/code&gt;—the &lt;code&gt;density&lt;/code&gt;—for each. We'll choose an arbitrary &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;variance&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'`Gaussian(mu=.123, var=.456)` Density'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_8_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;If we increase the variance &lt;code&gt;var&lt;/code&gt;, what happens?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;bigger_number&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;3.45&lt;/span&gt;

&lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bigger_number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'`Gaussian(mu=.123, var=bigger_number)` Density'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_10_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The density gets fatter. This should be familiar to you.&lt;/p&gt;
&lt;p&gt;Similarly, we can draw &lt;em&gt;samples&lt;/em&gt; from a &lt;code&gt;Gaussian&lt;/code&gt; distribution, e.g. from the initial &lt;code&gt;Gaussian(mu=.123, var=.456)&lt;/code&gt; above. Its corresponding density plot (also above) governs this procedure, where &lt;code&gt;(x, y)&lt;/code&gt; tuples give the (unnormalized) probability &lt;code&gt;y&lt;/code&gt; that a given sample will take the value &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;x&lt;/code&gt;-values with large corresponding &lt;code&gt;y&lt;/code&gt;-values are more likely to be sampled. Here, values near .123 are most likely to be sampled.&lt;/p&gt;
&lt;p&gt;Let's add a method to our class, draw 500 samples, then plot their histogram.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rvs&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# Add method to class&lt;/span&gt;
&lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;

&lt;span class="c1"&gt;# Instantiate new Gaussian&lt;/span&gt;
&lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Draw samples&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# Plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Histogram of 500 samples from `Gaussian(mu=.123, var=.456)`'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_13_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;This looks similar to the true &lt;code&gt;Gaussian(mu=.123, var=.456)&lt;/code&gt; density we plotted above. The more random samples we draw (then plot), the closer this histogram will approximate the true density.&lt;/p&gt;
&lt;p&gt;Now, we'll start to move a bit faster.&lt;/p&gt;
&lt;h2&gt;2D Gaussians&lt;/h2&gt;
&lt;p&gt;We just drew samples from a 1-dimensional Gaussian, i.e. the &lt;code&gt;sample&lt;/code&gt; itself was a single float. The parameter &lt;code&gt;mu&lt;/code&gt; dictated the most-likely value for the &lt;code&gt;sample&lt;/code&gt; to assume, and the variance &lt;code&gt;var&lt;/code&gt; dictated how much these sample-values vary (hence the name variance).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5743030051553177&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="mf"&gt;0.06160509014194515&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="mf"&gt;1.050830033400354&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In 2D, each sample will be a list of two numbers. &lt;code&gt;mu&lt;/code&gt; will dictate the most-likely pair of values for the &lt;code&gt;sample&lt;/code&gt; to assume, and a second parameter (yet unnamed) will dictate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How much the values for the first element of the pair vary&lt;/li&gt;
&lt;li&gt;How much the values for the second element of the pair vary&lt;/li&gt;
&lt;li&gt;How much the first and second elements vary with each other, e.g. if the first element is larger than expected (i.e. larger than its corresponding mean), to what extent does the second element "follow suit" (and assume a value larger than expected as well)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second parameter is the &lt;strong&gt;covariance matrix&lt;/strong&gt;, &lt;code&gt;cov&lt;/code&gt;. The elements on the diagonal give Items 1 and 2. The elements off the diagonal give Item 3. The covariance matrix is always square, and the values along its diagonal are always non-negative.&lt;/p&gt;
&lt;p&gt;Given a 2D &lt;code&gt;mu&lt;/code&gt; and 2x2 &lt;code&gt;cov&lt;/code&gt;, we can draw samples from the 2D Gaussian. Here, we'll use NumPy. Inline, we comment on the expected shape of the samples.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_draws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_draws&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;The purple dots should center around `(x, y) = (0, 0)`.&lt;/span&gt;

&lt;span class="sd"&gt;`np.diag([1, 1])` gives the covariance matrix `[[1, 0], [0, 1]]`:&lt;/span&gt;

&lt;span class="sd"&gt;`x`-values have a variance of `var=1`;&lt;/span&gt;
&lt;span class="sd"&gt;`y`-values have `var=1`;&lt;/span&gt;
&lt;span class="sd"&gt;these values do not covary with one another.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'purple'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;The blue dots should center around `(x, y) = (1, 3)`. Same story with the covariance.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'orange'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Here, the values along the diagonal of the covariance matrix are much larger:&lt;/span&gt;

&lt;span class="sd"&gt;the cloud of green points should be much more disperse.&lt;/span&gt;

&lt;span class="sd"&gt;There is no off-diagonal covariance.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;The covariance matrix has off-diagonal values of -2.&lt;/span&gt;

&lt;span class="sd"&gt;This means that if `x` trends above its mean, `y` will tend to vary *twice as much, below its mean.*&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Covariances of 4.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;

&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'blue'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Draws from 2D-Gaussians with Varying (mu, cov)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_17_1.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Gaussians are closed under linear maps&lt;/h2&gt;
&lt;p&gt;Each cloud of Gaussian dots tells us the following:&lt;/p&gt;
&lt;div class="math"&gt;$$
(x, y) \sim \text{Normal}(\mu, \Sigma)
$$&lt;/div&gt;
&lt;p&gt;In other words, the draws &lt;span class="math"&gt;\((x, y)\)&lt;/span&gt; are distributed normally with 2D-mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and 2x2 covariance &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's assume &lt;span class="math"&gt;\((x, y)\)&lt;/span&gt; is a vector named &lt;span class="math"&gt;\(w\)&lt;/span&gt;. Giving subscripts to the parameters of our Gaussian, we can rewrite the above as:&lt;/p&gt;
&lt;div class="math"&gt;$$
w \sim \text{Normal}(\mu_w, \Sigma_w)
$$&lt;/div&gt;
&lt;p&gt;Next, imagine we have some matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt; of size 200x2. If &lt;span class="math"&gt;\(w\)&lt;/span&gt; is distributed as above, how is &lt;span class="math"&gt;\(Aw\)&lt;/span&gt; distributed? Gaussian algebra tells us the following:&lt;/p&gt;
&lt;div class="math"&gt;$$
Aw \sim \text{Normal}(A\mu_w,\ A^T\Sigma_w A)
$$&lt;/div&gt;
&lt;p&gt;In other words, &lt;span class="math"&gt;\(Aw\)&lt;/span&gt;, the "linear map" of &lt;span class="math"&gt;\(w\)&lt;/span&gt; onto &lt;span class="math"&gt;\(A\)&lt;/span&gt;, is (incidentally) Gaussian-distributed as well.&lt;/p&gt;
&lt;p&gt;Let's plot some draws from this distribution. Let's assume each row of &lt;span class="math"&gt;\(A\)&lt;/span&gt; (of which there are 200, each containing 2 elements) is computed via the (arbitrary) function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, how do we get &lt;span class="math"&gt;\(A\)&lt;/span&gt;? Well, we could simply make such a matrix ourselves — &lt;code&gt;np.random.randn(200, 2)&lt;/code&gt; for instance. Separately, imagine we start with a 200D vector &lt;span class="math"&gt;\(X\)&lt;/span&gt; of arbitrary floats, use the above function to make 2 "features" for each, then take the transpose. This gives us our 200x2 matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, and still with the goal of obtaining samples &lt;span class="math"&gt;\(Aw\)&lt;/span&gt;, we'll multiply this matrix by our 2D mean-vector of weights, &lt;span class="math"&gt;\(\mu_w\)&lt;/span&gt;. You can think of the latter as passing a batch of data through a linear model (where our data have features &lt;span class="math"&gt;\(x = [x_1, x_2]\)&lt;/span&gt;, and our parameters are &lt;span class="math"&gt;\(\mu_w = [w_1, w_2]\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Finally, we'll take draws from this &lt;span class="math"&gt;\(\text{Normal}(A\mu_w,\ A\Sigma_w A^T)\)&lt;/span&gt;. This will give us tuples of the form &lt;code&gt;(x, Aw)&lt;/code&gt;. For simplicity, we'll hereafter refer to this tuple as &lt;code&gt;(x, y)&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the original &lt;code&gt;x&lt;/code&gt;-value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the value obtained after:&lt;ul&gt;
&lt;li&gt;Making features out of &lt;span class="math"&gt;\(X\)&lt;/span&gt; and taking the transpose, giving &lt;span class="math"&gt;\(A\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Taking the linear combination of &lt;span class="math"&gt;\(A\)&lt;/span&gt; with the mean-vector of weights&lt;/li&gt;
&lt;li&gt;Taking a draw from the multivariate-Gaussian we just defined, then plucking out the sample-element corresponding to &lt;code&gt;x&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Each draw from our Gaussian will yield 200 &lt;code&gt;y&lt;/code&gt;-values, each corresponding to its original &lt;code&gt;x&lt;/code&gt;. In other words, it will yield 200 &lt;code&gt;(x, y)&lt;/code&gt; tuples — which we can plot.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To make it clear that &lt;span class="math"&gt;\(A\)&lt;/span&gt; was computed as a function of &lt;span class="math"&gt;\(X\)&lt;/span&gt;, let's rename it to &lt;span class="math"&gt;\(A = \phi(X)^T\)&lt;/span&gt;, and rewrite our distribution as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(X)^Tw \sim \text{Normal}(\phi(X)^T\mu_w,\ \phi(X)^T\Sigma_w \phi(X))
$$&lt;/div&gt;
&lt;p&gt;In addition, let's set &lt;span class="math"&gt;\(\mu_w =\)&lt;/span&gt; &lt;code&gt;np.array([0, 0])&lt;/code&gt; and &lt;span class="math"&gt;\(\Sigma_w =\)&lt;/span&gt; &lt;code&gt;np.diag([1, 2])&lt;/code&gt;. Finally, we'll take draws, then plot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# x-values&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Make features, as before&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (2, len(x))&lt;/span&gt;

&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Params of distribution over linear map (lm)&lt;/span&gt;
&lt;span class="n"&gt;mu_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt;
&lt;span class="n"&gt;cov_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Random Draws from a Distribution over Linear Maps'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Plot draws. `lm` is a vector of 200 `y` values, each corresponding to the original `x`-values&lt;/span&gt;
    &lt;span class="n"&gt;lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_lm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# lm.shape: (200,)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_20_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This distribution over linear maps gives a distribution over functions&lt;/strong&gt;, where the "mean function" is &lt;span class="math"&gt;\(\phi(X)^T\mu_w\)&lt;/span&gt; (which reads directly from the &lt;code&gt;mu_lm&lt;/code&gt; variable above).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notwithstanding, I find this phrasing to be confusing&lt;/strong&gt;; to me, a "distribution over functions" sounds like some opaque object that spits out algebraic symbols via logic miles above my cognitive ceiling. As such, I instead think of this in more intuitive terms as a &lt;strong&gt;distribution over function evaluations&lt;/strong&gt;, where a single function evaluation is a list of &lt;code&gt;(x, y)&lt;/code&gt; tuples and nothing more.&lt;/p&gt;
&lt;p&gt;For example, given a vector &lt;code&gt;x = np.array([1, 2, 3])&lt;/code&gt; and a function &lt;code&gt;lambda x: x**2&lt;/code&gt;, an evaluation of this function gives &lt;code&gt;y = np.array([1, 4, 9])&lt;/code&gt;. We now have tuples &lt;code&gt;[(1, 1), (2, 4), (3, 9)]&lt;/code&gt; from which we can create a line plot. This gives one "function evaluation."&lt;/p&gt;
&lt;p&gt;Above, we sampled 17 function evaluations, then plotted the 200 resulting &lt;code&gt;(x, y)&lt;/code&gt; tuples (as our input was a 200D vector &lt;span class="math"&gt;\(X\)&lt;/span&gt;) for each. The evaluations are similar because of the given mean function &lt;code&gt;mu_lm&lt;/code&gt;; they are different because of the given covariance matrix &lt;code&gt;cov_lm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's try some different "features" for our &lt;code&gt;x&lt;/code&gt;-values then plot the same thing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Make different, though still arbitrary, features&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_23_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;"The features we choose give a 'language' with which we can express a relationship between &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; Some features are more expressive than others; some restrict us entirely from expressing certain relationships.&lt;/p&gt;
&lt;p&gt;For further illustration, let's employ step functions as features and see what happens.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Make features, as before&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_25_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Gaussians are closed under conditioning and marginalization&lt;/h2&gt;
&lt;p&gt;Let's revisit the 2D Gaussians plotted above. They took the form&lt;/p&gt;
&lt;div class="math"&gt;$$
(x, y) \sim \mathcal{N}(\mu, \Sigma)
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt; denotes the Normal, i.e. Gaussian distribution.&lt;/p&gt;
&lt;p&gt;Said differently:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(x, y) = \mathcal{N}(\mu, \Sigma)
$$&lt;/div&gt;
&lt;p&gt;And now a bit more rigorously:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(x, y) = \mathcal{N}\bigg([\mu_x, \mu_y],
    \begin{bmatrix}
    \Sigma_x &amp;amp; \Sigma_{xy}\\
    \Sigma_{xy}^T &amp;amp; \Sigma_y\\
    \end{bmatrix}\bigg)
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;NB: In this case, all 4 "Sigmas" in the 2x2 covariance matrix are floats. If our covariance were bigger, say 31x31, then these 4 Sigmas would be &lt;strong&gt;matrices&lt;/strong&gt; (with an aggregate size totaling 31x31).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What if we wanted to know the distribution over &lt;span class="math"&gt;\(y\)&lt;/span&gt; conditional on &lt;span class="math"&gt;\(x\)&lt;/span&gt; taking on a certain value, e.g. &lt;span class="math"&gt;\(P(y\vert x &amp;gt; 1)\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y\)&lt;/span&gt; is a single element, so the resulting conditional will be a univariate distribution. To gain intuition, let's do this in a very crude manner:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;345&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# some random sample size&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;y_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Histogram of y-values, when x &amp;gt; 1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_28_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Cool! Looks kind of Gaussian as well.&lt;/p&gt;
&lt;p&gt;Instead, what if we wanted to know the functional form of the real density, &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;, instead of this empirical distribution of its samples? One of the axioms of conditional probability tells us that:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x, y)}{P(x)} = \frac{P(x, y)}{\int P(x, y)dy}
$$&lt;/div&gt;
&lt;p&gt;The right-most denominator can be written as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\int P(x, y)dy
    &amp;amp;= \int \mathcal{N}\bigg([\mu_x, \mu_y],
        \begin{bmatrix}
        \Sigma_x &amp;amp; \Sigma_{xy}\\
        \Sigma_{xy}^T &amp;amp; \Sigma_y\\
        \end{bmatrix}\bigg)
   dy\\
   \\
   &amp;amp;= \mathcal{N}(\mu_x, \Sigma_x)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Marginalizing a &amp;gt; 1D Gaussian over one of its elements yields another Gaussian&lt;/strong&gt;: you just "pluck out" the elements you'd like to examine. &lt;strong&gt;In other words, Gaussians are closed under marginalization.&lt;/strong&gt; "It's almost too easy to warrant a formula."&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As an example, imagine we had the following Gaussian &lt;span class="math"&gt;\(P(a, b, c)\)&lt;/span&gt;, and wanted to compute the marginal over the first 2 elements, i.e. &lt;span class="math"&gt;\(P(a, b) = \int P(a, b, c)dc\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# P(a, b, c)&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;66&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;77&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# P(a, b)&lt;/span&gt;
&lt;span class="n"&gt;mu_marginal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cov_marginal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# That's it.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we compute the conditional Gaussian of interest—a result well-documented by mathematicians long ago:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert x)
    &amp;amp;= \frac{
            \mathcal{N}\bigg(
                [\mu_x, \mu_y],
                \begin{bmatrix}
                \Sigma_x &amp;amp; \Sigma_{xy}\\
                \Sigma_{xy}^T &amp;amp; \Sigma_y\\
                \end{bmatrix}
            \bigg)
            }
            {\mathcal{N}(\mu_x, \Sigma_x)}\\
   \\
   &amp;amp;= \mathcal{N}(\mu_y + \Sigma_{xy}\Sigma_x^{-1}(x - \mu_x), \Sigma_y - \Sigma_{xy}\Sigma_x^{-1}\Sigma_{xy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt; can be a matrix. From there, just plug stuff in.&lt;/p&gt;
&lt;p&gt;Conditioning a &amp;gt; 1D Gaussian on one (or more) of its elements yields another Gaussian. &lt;strong&gt;In other words, Gaussians are closed under conditioning.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Inferring the weights&lt;/h2&gt;
&lt;p&gt;We previously posited a distribution over some vector of weights, &lt;span class="math"&gt;\(w \sim \text{Normal}(\mu_w, \Sigma_w)\)&lt;/span&gt;. In addition, we posited a distribution over the linear map of these weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X)^T\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \phi(X)^Tw \sim \text{Normal}(\phi(X)^T\mu_w,\ \phi(X)^T\Sigma_w \phi(X))
$$&lt;/div&gt;
&lt;p&gt;Given some ground-truth samples from this distribution &lt;span class="math"&gt;\(y = \phi(X)^Tw\)&lt;/span&gt;, i.e. ground-truth "function evaluations," we'd like to infer the weights &lt;span class="math"&gt;\(w\)&lt;/span&gt; most consistent with &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In machine learning, we equivalently say that given a model and some observed data &lt;code&gt;(X_train, y_train)&lt;/code&gt;, we'd like to compute/train/optimize the weights of said model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Most precisely, our goal is to infer &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; (where &lt;span class="math"&gt;\(y\)&lt;/span&gt; are our observed function evaluations). To do this, we simply posit a joint distribution over both quantities:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(w, y) =
    \mathcal{N}\bigg(
        [\mu_w, \phi(X)^T\mu_w],
        \begin{bmatrix}
        \Sigma_w &amp;amp; \Sigma_{wy}\\
        \Sigma_{wy}^T &amp;amp; \phi(X)^T\Sigma_w \phi(X)\\
        \end{bmatrix}
    \bigg)
$$&lt;/div&gt;
&lt;p&gt;Then compute the conditional via the formula above:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(w\vert y)
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}\Sigma_y^{-1}(y - \mu_y), \Sigma_w - \Sigma_{wy}\Sigma_y^{-1}\Sigma_{wy}^T)\\
    \\
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}(y - \mu_w^T \phi(X)), \Sigma_w - \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}\Sigma_{wy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This formula gives the posterior distribution over our weights &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; given the model and observed data tuples &lt;code&gt;(x, y)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Until now, we've assumed a 2D &lt;span class="math"&gt;\(w\)&lt;/span&gt;, and therefore a &lt;span class="math"&gt;\(\phi(X)\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt; as well. Moving forward, we'll work with weights and features in a higher-dimensional space, &lt;span class="math"&gt;\(\mathbb{R}^{20}\)&lt;/span&gt;; this will give us a more expressive language with which to capture the true relationship between some quantity &lt;span class="math"&gt;\(x\)&lt;/span&gt; and its corresponding &lt;span class="math"&gt;\(y\)&lt;/span&gt;. &lt;span class="math"&gt;\(\mathbb{R}^{20}\)&lt;/span&gt; is an arbitrary choice; it could have been &lt;span class="math"&gt;\(\mathbb{R}^{17}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\mathbb{R}^{31}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\mathbb{R}^{500}\)&lt;/span&gt; as well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The true function that maps `x` to `y`. This is what we are trying to recover with our mathematical model.&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;true_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;


&lt;span class="c1"&gt;# x-values&lt;/span&gt;
&lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="c1"&gt;# y-train&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;true_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="c1"&gt;# A function to make some arbitrary features&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape(D, len(x))&lt;/span&gt;


&lt;span class="c1"&gt;# A function that computes the parameters of the linear map distribution&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_linear_map_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;mu_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
    &lt;span class="n"&gt;cov_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu_lm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_lm&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_weights_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    NB: "Computing a posterior," and given that that posterior is Gaussian, implies nothing more than&lt;/span&gt;
&lt;span class="sd"&gt;    computing the mean-vector and covariance matrix of this Gaussian.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="c1"&gt;# Featurize x_train&lt;/span&gt;
    &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Params of prior distribution over function evals&lt;/span&gt;
    &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_linear_map_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Params of posterior distribution over weights&lt;/span&gt;
    &lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt;


&lt;span class="c1"&gt;# Compute weights posterior&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_weights_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As with our prior over our weights, we can equivalently draw samples from the posterior over our weights, then plot. These samples will be 20D vectors; we reduce them to 2D for ease of visualization.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Draw samples&lt;/span&gt;
&lt;span class="n"&gt;samples_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;samples_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Reduce to 2D for ease of visualization&lt;/span&gt;
&lt;span class="n"&gt;first_dim_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;second_dim_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples_prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;first_dim_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;second_dim_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples_post&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_34_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Samples from the prior are plotted in orange; samples from the posterior are plotted in blue. As this is a stochastic dimensionality-reduction algorithm, the results will be slightly different each time.&lt;/p&gt;
&lt;p&gt;At best, we can see that the posterior has slightly larger values in its covariance matrix, evidenced by the fact that the blue cloud is more disperse than the orange, and has probably maintained a similar mean. The magnitude of change (read: a small one) is expected, as we've only conditioned on 6 ground-truth tuples.&lt;/p&gt;
&lt;h1&gt;Predicting on new data&lt;/h1&gt;
&lt;p&gt;Previously, we sampled function evaluations by centering a multivariate Gaussian on &lt;span class="math"&gt;\(\phi(X)^T\mu_{w}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\mu_w\)&lt;/span&gt; was the mean of the prior distribution over weights. We'd now like to do the same thing, but use our posterior over weights instead. How does this work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Well, Gaussians are closed under linear maps.&lt;/strong&gt; So, we just follow the formula we had used above.&lt;/p&gt;
&lt;p&gt;This time, instead of input vector &lt;span class="math"&gt;\(X\)&lt;/span&gt;, we'll use a new input vector called &lt;span class="math"&gt;\(X_{*}\)&lt;/span&gt;, i.e. the "new data" on which we'd like to predict.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(X_{*})^Tw \sim \text{Normal}(\phi(X_{*})^T\mu_{w, \text{post}},\ \phi(X_{*})^T \Sigma_{w, \text{post}}\phi(X_{*}))
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;This gives us a posterior distribution over function evaluations.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In machine learning parlance, this is akin to: given some test data &lt;code&gt;X_test&lt;/code&gt;, and a model whose weights were trained/optimized with respect to/conditioned on some observed ground-truth tuples &lt;code&gt;(X_train, y_train)&lt;/code&gt;, we'd like to generate predictions &lt;code&gt;y_test&lt;/code&gt;, i.e. samples from the posterior over function evaluations.&lt;/p&gt;
&lt;p&gt;The function to compute this posterior, i.e. compute the mean-vector and covariance matrix of this Gaussian, will appear both short and familiar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_weights_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_linear_map_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To plot, we typically just plot the error bars, i.e. the space within &lt;code&gt;(mu_y_post - var_y_post, mu_y_post + var_y_post)&lt;/code&gt; for each &lt;code&gt;x&lt;/code&gt;, as well as the ground-truth tuples as big red dots. &lt;strong&gt;This gives nothing more than a picture of the mean-vector and covariance of our posterior.&lt;/strong&gt; Optionally, we can plot samples from this posterior as well, as we did with our prior.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Posterior Distribution over Function Evaluations'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Extract the variances, i.e. the diagonal, of our covariance matrix&lt;/span&gt;
    &lt;span class="n"&gt;var_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Plot the error bars.&lt;/span&gt;
    &lt;span class="c1"&gt;# To do this, we fill the space between `(mu_y_post - var_y_post, mu_y_post + var_y_post)` for each `x`&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill_between&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_y_post&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;var_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_y_post&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;var_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'#23AEDB'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Scatter-plot our original 6 `(x, y)` tuples&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'ro'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Optionally plot actual samples (function evaluations) from this posterior&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_40_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The posterior distribution is nothing more than a distribution over function evaluations (from which we've sampled 25 function evaluations above) &lt;em&gt;most consistent with our model and observed data tuples.&lt;/em&gt; As such, and to give further intuition, a crude way of computing this distribution might be continuously &lt;em&gt;drawing samples from our prior over function evaluations, and keeping only the ones that pass through, i.e. are "most consistent with," all of the red points above.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, we stated before that &lt;strong&gt;the features we choose (i.e. our &lt;code&gt;phi_func&lt;/code&gt;) give a "language" with which we can express the relationship between &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt; Here, we've chosen a language with 20 words. What if we chose a different 20?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="c1"&gt;# Still arbitrary, i.e. a modeling choice!&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_42_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Not great. As a brief aside, how do we read the plot above? It's simply a function, a transformation, a lookup: given an &lt;span class="math"&gt;\(x\)&lt;/span&gt;, it tells us the corresponding expected value &lt;span class="math"&gt;\(y\)&lt;/span&gt;, and the variance around this estimate.&lt;/p&gt;
&lt;p&gt;For instance, right around &lt;span class="math"&gt;\(x = -3\)&lt;/span&gt;, we can see that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is somewhere in &lt;span class="math"&gt;\([-1, 1]\)&lt;/span&gt;; given that we've only conditioned on 6 "training points," we're still quite unsure as to what the true answer is. To this effect, a GP (and other fully-Bayesian models) allows us to quantify this uncertainty judiciously.&lt;/p&gt;
&lt;p&gt;Now, let's try some more features and examine the model we're able to build.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Still arbitrary, i.e. a modeling choice!&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_44_1.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_45_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;That last one might look familiar. Therein, the features we chose (still arbitrarily, really) are called "radial basis functions" (among other names).&lt;/p&gt;
&lt;p&gt;We've loosely examined what happens when we change the language through which we articulate our model. Next, what if we changed the size of its vocabulary?&lt;/p&gt;
&lt;p&gt;First, let's backtrack, and try 8 of these radial basis functions instead of 20.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_47_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Very different! Holy overfit. What about 250?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_49_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;It appears that the more features we use, the more expressive, and/or less endemically prone to overfitting, our model becomes.&lt;/p&gt;
&lt;p&gt;So, how far do we take this? &lt;code&gt;D = 1000&lt;/code&gt;? &lt;code&gt;D = 50000&lt;/code&gt;? How high can we go? &lt;strong&gt;We'll pick up here in the next post.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this tutorial, we've arrived at the mechanical notion of a Gaussian process via simple Gaussian algebra.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thus far, we've elucidated the following ideas:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions (i.e. function evaluations) ✅&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian (just pluck out the pieces of interest) ✅&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset gives another Gaussian (a simple algebraic formula) ✅&lt;/li&gt;
&lt;li&gt;Posterior over functions (the linear map of the posterior over weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X_{*})^T\)&lt;/span&gt;) ✅&lt;/li&gt;
&lt;li&gt;Covariances (the second thing we need in order to specify a multivariate Gaussian) ✅&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conversely, we did not yet cover (directly):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;li&gt;"A Gaussian process is non-parametric, i.e. it has an infinite number of parameters"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These will be the subject of the following post. Thanks for reading.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/gaussian-processes"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/gaussian-processes/blob/master/gaussian-processes-part-1.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=50Vgw11qn0o"&gt;Gaussian Processes 1 - Philipp Hennig - MLSS 2013 Tübingen
&lt;/a&gt; (from which this post takes heavy inspiration) &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="http://gaussianprocess.org/gpml/?"&gt;Gaussian Processes for Machine Learning&lt;/a&gt;. Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"&gt;Fitting Gaussian Process Models in Python&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;a href="http://sashagusev.github.io/2016-01/GP.html"&gt;Gaussian process regression
&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Neurally Embedded Emojis</title><link href="https://willwolf.io/2017/06/19/neurally-embedded-emojis/" rel="alternate"></link><published>2017-06-19T13:00:00-04:00</published><updated>2017-06-19T13:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-06-19:/2017/06/19/neurally-embedded-emojis/</id><summary type="html">&lt;p&gt;Convolutional variational autoencoders for emoji generation and Siamese text-question-emoji-answer models. Keras, bidirectional LSTMs and snarky tweets &lt;a href="https://twitter.com/united"&gt;@united&lt;/a&gt; within.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As I move through my 20's I'm consistently delighted by the subtle ways in which I've changed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Will at 22: Reggaeton is a miserable, criminal assault to my ears.&lt;/p&gt;
&lt;p&gt;Will at 28: &lt;a href="https://www.youtube.com/watch?v=72UO0v5ESUo"&gt;Despacito (Remix)&lt;/a&gt; for breakfast, lunch, dinner.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Will at 22: Western Europe is boring. No — I've seen a lot of it! Everything is too clean, too nice, too perfect for my taste.&lt;/p&gt;
&lt;p&gt;Will at 28, in Barcelona, after 9 months in &lt;a href="https://willwolf.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;Casablanca&lt;/a&gt;: Wait a second: &lt;em&gt;I get it now&lt;/em&gt;. What &lt;em&gt;is&lt;/em&gt; this summertime paradise of crosswalks, vehicle civility and apple-green parks and where has it been all my life?&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Will at 22: Emojis are weird.&lt;/p&gt;
&lt;p&gt;Will at 28: 🚀 🤘 💃🏿 🚴🏻 🙃.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Emojis are an increasingly-pervasive sub-lingua-franca of the internet. They capture meaning in a rich, concise manner — alternative to the 13 seconds of mobile thumb-fumbling required to capture the same meaning with text. Furthermore, they bring two levels of semantic information: their context within raw text and the pixels of the emoji itself.&lt;/p&gt;
&lt;h2&gt;Question-answer models&lt;/h2&gt;
&lt;p&gt;The original aim of this post was to explore Siamese question-answer models of the type typically applied to the &lt;a href="https://github.com/shuzi/insuranceQA"&gt;InsuranceQA Corpus&lt;/a&gt; as introduced in "Applying Deep Learning To Answer Selection: A Study And An Open Task" (&lt;a href="https://arxiv.org/pdf/1508.01585v2.pdf"&gt;Feng, Xiang, Glass, Wang, &amp;amp; Zhou, 2015&lt;/a&gt;). We'll call them SQAM for clarity. The basic architecture looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="qa model architecture" class="img-responsive" src="https://willwolf.io/figures/qa_model_architecture.png"/&gt;&lt;/p&gt;
&lt;p&gt;By layer and in general terms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An input — typically a sequence of token ids — for both question (Q) and answer (A).&lt;/li&gt;
&lt;li&gt;An embedding layer.&lt;/li&gt;
&lt;li&gt;Convolutional layer(s), or any layers that extract features from the matrix of embeddings. (A matrix, because the respective inputs are sequences of token ids; each id is embedded into its own vector.)&lt;/li&gt;
&lt;li&gt;A max-pooling layer.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;tanh&lt;/code&gt; non-linearity.&lt;/li&gt;
&lt;li&gt;The cosine of the angle between the resulting, respective embeddings.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;As canonical recommendation&lt;/h3&gt;
&lt;p&gt;Question answering can be viewed as canonical recommendation: embed entities into Euclidean space in a meaningful way, then compute dot products between these entities and sort the list. In this vein, the above network is (thus far) quite similar to classic matrix factorization yet with the following subtle tweaks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Instead of factorizing our matrix via &lt;a href="https://en.wikipedia.org/wiki/Singular_value_decomposition"&gt;SVD&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Ordinary_least_squares"&gt;OLS&lt;/a&gt; we build a neural network that accepts &lt;code&gt;(question, answer)&lt;/code&gt;, i.e. &lt;code&gt;(user, item)&lt;/code&gt;, pairs and outputs their similarity. The second-to-last layer gives the respective embeddings. We train this network in a supervised fashion, optimizing its parameters via stochastic gradient descent.&lt;/li&gt;
&lt;li&gt;Instead of jumping directly from input-index (or sequence thereof) to embedding, we first compute convolutional features.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In contrast, the network above boasts one key difference: both question and answer, i.e. user and item, are transformed via a single set of parameters — an initial embedding layer, then convolutional layers — en route to their final embedding.&lt;/p&gt;
&lt;p&gt;Furthermore, and not unique to SQAMs, our network inputs can be &lt;em&gt;any&lt;/em&gt; two sequences of (tokenized, max-padded, etc.) text: we are not restricted to only those observed in the training set.&lt;/p&gt;
&lt;h2&gt;Question-emoji models&lt;/h2&gt;
&lt;p&gt;Given my accelerating proclivity for the internet's new alphabet, I decided to build text-question-&lt;em&gt;emoji&lt;/em&gt;-answer models instead. In fact, this setup gives an additional avenue for prediction: if we make a model of the answers (emojis) themselves, we can now predict on, i.e. compute similarity with, each of&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Emojis we saw in the training set.&lt;/li&gt;
&lt;li&gt;New emojis, i.e. either not in the training set or new (like, released months from now) altogether.&lt;/li&gt;
&lt;li&gt;Novel emojis &lt;em&gt;generated&lt;/em&gt; from the model of our data. In this way, we could conceivably answer a question with: "we suggest this new emoji we've algorithmically created ourselves that no one's ever seen before."&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;
&lt;h2&gt;Convolutional variational autoencoders&lt;/h2&gt;
&lt;p&gt;Variational autoencoders are comprised of two models: an encoder and a decoder. The encoder embeds our 872 &lt;a href="https://github.com/twitter/twemoji"&gt;emojis&lt;/a&gt; of size &lt;span class="math"&gt;\((36, 36, 4)\)&lt;/span&gt; into a low-dimensional latent code, &lt;span class="math"&gt;\(z_e \in \mathbb{R}^{16}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(z_e\)&lt;/span&gt; is a sample from an emoji-specific Gaussian. The decoder takes as input &lt;span class="math"&gt;\(z_e\)&lt;/span&gt; and produces a reconstruction of the original emoji. As each individual &lt;span class="math"&gt;\(z_e\)&lt;/span&gt; is normally distributed, &lt;span class="math"&gt;\(z\)&lt;/span&gt; should be distributed normally as well. We can verify this with a quick simulation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;z_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="aggregate gaussian" class="img-responsive" src="https://willwolf.io/figures/aggregate_gaussian.png"/&gt;&lt;/p&gt;
&lt;p&gt;Training a variational autoencoder to learn low-dimensional emoji embeddings serves two principal ends:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can feed these low-dimensional embeddings as input to our SQAM.&lt;/li&gt;
&lt;li&gt;We can generate novel emojis with which to answer questions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the embeddings in #1 are multivariate Gaussian, we can perform #2 by passing Gaussian samples into our decoder. We can do this by sampling evenly-spaced percentiles from the inverse CDF of the aggregate embedding distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;percentiles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;percentiles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ppf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;generated_emoji&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;NB: &lt;code&gt;norm.ppf&lt;/code&gt; does &lt;em&gt;not&lt;/em&gt; accept a &lt;code&gt;size&lt;/code&gt; parameter; I believe sampling from the inverse CDF of a &lt;em&gt;multivariate&lt;/em&gt; Gaussian is non-trivial in Python.&lt;/p&gt;
&lt;p&gt;Similarly, we could simply iterate over &lt;code&gt;(mu, sd)&lt;/code&gt; pairs outright:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rvs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;generated_emoji&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The ability to generate new emojis via samples from a well-studied distribution, the Gaussian, is a key reason for choosing a variational autoencoder.&lt;/p&gt;
&lt;p&gt;Finally, as we are working with images, I employ convolutional intermediary layers.&lt;/p&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EMOJIS_DIR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'data/emojis'&lt;/span&gt;
&lt;span class="n"&gt;N_CHANNELS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_CHANNELS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;emojis_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;slug&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;listdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJIS_DIR&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJIS_DIR&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;emoji&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;emoji&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emoji&lt;/span&gt;

&lt;span class="n"&gt;emojis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Split data into train, validation sets&lt;/h3&gt;
&lt;p&gt;Additionally, scale pixel values to &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;255.&lt;/span&gt;
&lt;span class="n"&gt;X_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;255.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    X_train:  (685, 36, 36, 4)&lt;/span&gt;
&lt;span class="err"&gt;    X_val:    (182, 36, 36, 4)&lt;/span&gt;
&lt;span class="err"&gt;    y_train:  (685, 36, 36, 4)&lt;/span&gt;
&lt;span class="err"&gt;    y_val:    (182, 36, 36, 4)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before we begin, let's examine some emojis.&lt;/p&gt;
&lt;p&gt;&lt;img alt="emojis" class="img-responsive" src="https://willwolf.io/images/emojis.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Model emojis&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;
&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Variational layer&lt;/h3&gt;
&lt;p&gt;This is taken from a previous post of mine, &lt;a href="https://willwolf.io/2017/05/08/transfer-learning-flight-delay-prediction/"&gt;Transfer Learning for Flight Delay Prediction via Variational Autoencoders&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;KerasLayer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''A custom "variational" Keras layer that completes the&lt;/span&gt;
&lt;span class="sd"&gt;        variational autoencoder.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            embedding_dim : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_mean_weights'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_mean_bias'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_log_var_weights'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_log_var_bias'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt;
        &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;binary_crossentropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_output_shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Autoencoder&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# encoder&lt;/span&gt;
&lt;span class="n"&gt;original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'original'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;variational_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;variational_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;variational_params&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'encoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# decoder&lt;/span&gt;
&lt;span class="n"&gt;encoded&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;

&lt;span class="n"&gt;upsample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;encoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;reshape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;upsample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;deconv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;encoded&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deconv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;deconv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deconv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;deconv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;deconv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;reconstructed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_CHANNELS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;encoded&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reconstructed&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'decoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# end-to-end&lt;/span&gt;
&lt;span class="n"&gt;encoder_decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The full model &lt;code&gt;encoder_decoder&lt;/code&gt; is composed of separate models &lt;code&gt;encoder&lt;/code&gt; and &lt;code&gt;decoder&lt;/code&gt;. Training the former will implicitly train the latter two; they are available for our use thereafter.&lt;/p&gt;
&lt;p&gt;The above architecture takes inspiration from &lt;a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py"&gt;Keras&lt;/a&gt;, &lt;a href="https://github.com/blei-lab/edward/blob/master/examples/vae_convolutional.py"&gt;Edward&lt;/a&gt; and the GDGS (gradient descent by grad student) method by as discussed by &lt;a href="https://www.reddit.com/r/MachineLearning/comments/6hso7g/d_how_do_people_come_up_with_all_these_crazy_deep/"&gt;Brudaks on Reddit&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A popular method for designing deep learning architectures is GDGS (gradient descent by grad student).
This is an iterative approach, where you start with a straightforward baseline architecture (or possibly an earlier SOTA), measure its effectiveness; apply various modifications (e.g. add a highway connection here or there), see what works and what does not (i.e. where the gradient is pointing) and iterate further on from there in that direction until you reach a (local?) optimum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm not a grad student but I think it still plays.&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;encoder_decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;003&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;encoder_decoder_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder_decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Generate emojis&lt;/h2&gt;
&lt;p&gt;As promised we'll generate emojis. Again, latent codes are distributed as a (16-dimensional) Gaussian; to generate, we'll simply take samples thereof and feed them to our &lt;code&gt;decoder&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While scanning a 16-dimensional hypercube, i.e. taking (evenly-spaced, usually) samples from our latent space, is a few lines of Numpy, visualizing a 16-dimensional grid is impractical. In solution, we'll work on a 2-dimensional grid while treating subsets of our latent space as homogenous.&lt;/p&gt;
&lt;p&gt;For example, if our 2-D sample were &lt;code&gt;(0, 1)&lt;/code&gt;, we could posit 16-D samples as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;A. `(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1)`&lt;/span&gt;
&lt;span class="err"&gt;B. `(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1)`&lt;/span&gt;
&lt;span class="err"&gt;C. `(0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1)`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, if another sample were &lt;code&gt;(2, 3.5)&lt;/code&gt;, we could posit 16-D samples as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;A. `(2, 2, 2, 2, 2, 2, 2, 2, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5)`&lt;/span&gt;
&lt;span class="err"&gt;B. `(2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5)`&lt;/span&gt;
&lt;span class="err"&gt;C. `(2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5)`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is no math here: I'm just creating 16-element lists in different ways. We'll then plot "A-lists," "B-lists," etc. separately.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compose_code_A&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compose_code_B&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compose_code_C&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="n"&gt;ticks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ticks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_generated_emojis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compose_code_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="generated emojis A" class="img-responsive" src="https://willwolf.io/figures/generated_emojis_A.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_generated_emojis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compose_code_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="generated emojis B" class="img-responsive" src="https://willwolf.io/figures/generated_emojis_B.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_generated_emojis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compose_code_C&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="generated emojis C" class="img-responsive" src="https://willwolf.io/figures/generated_emojis_C.png"/&gt;&lt;/p&gt;
&lt;p&gt;As our emojis live in a continuous latent space we can observe the smoothness of the transition from one to the next.&lt;/p&gt;
&lt;p&gt;The generated emojis have the makings of maybe some devils, maybe some bubbles, maybe some hearts, maybe some fish. I doubt they'll be featured on your cell phone's keyboard anytime soon.&lt;/p&gt;
&lt;h2&gt;Text-question, emoji-answer&lt;/h2&gt;
&lt;p&gt;I spent a while looking for an adequate dataset to no avail. (Most Twitter datasets are not open-source, I requested my own tweets days ago and continue to wait, etc.) As such, I'm working with the &lt;a href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment"&gt;Twitter US Airline Sentiment&lt;/a&gt; dataset: tweets are labeled as &lt;code&gt;positive&lt;/code&gt;, &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;negative&lt;/code&gt; which I've mapped to 🎉, 😈 and 😡.&lt;/p&gt;
&lt;h3&gt;Contrastive loss&lt;/h3&gt;
&lt;p&gt;We've thus far discussed the SQAM. Our final model will make use of two SQAM's in parallel, as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Receive &lt;code&gt;(question, correct_answer, incorrect_answer)&lt;/code&gt; triplets as input.&lt;/li&gt;
&lt;li&gt;Compute the cosine similarity between &lt;code&gt;question&lt;/code&gt;, &lt;code&gt;correct_answer&lt;/code&gt; via &lt;code&gt;SQAM_1&lt;/code&gt; — &lt;code&gt;correct_cos_sim&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Compute the cosine similarity between &lt;code&gt;question&lt;/code&gt;, &lt;code&gt;incorrect_answer&lt;/code&gt; via &lt;code&gt;SQAM_2&lt;/code&gt; — &lt;code&gt;incorrect_cos_sim&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model is trained to minimize the following: &lt;code&gt;max(0, margin - correct_cos_sim + incorrect_cos_sim)&lt;/code&gt;, a variant of the &lt;a href="https://en.wikipedia.org/wiki/Hinge_loss"&gt;hinge loss&lt;/a&gt;. This ensures that &lt;code&gt;(question, correct_answer)&lt;/code&gt; pairs have a higher cosine similarity than &lt;code&gt;(question, incorrect_answer)&lt;/code&gt; pairs, mediated by &lt;code&gt;margin&lt;/code&gt;. Note that this function is differentiable: it is simply a &lt;a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"&gt;ReLU&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Architecture&lt;/h3&gt;
&lt;p&gt;A single SQAM receives two inputs: a &lt;code&gt;question&lt;/code&gt; — a max-padded sequence of token ids — and an &lt;code&gt;answer&lt;/code&gt; — an emoji's 16-D latent code.&lt;/p&gt;
&lt;p&gt;To process the &lt;code&gt;question&lt;/code&gt; we employ the following steps, i.e. network layers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;pre-trained-with-Glove&lt;/a&gt; 100-D embedding for each token id. This gives a matrix of size &lt;code&gt;(MAX_QUESTION_LEN, GLOVE_EMBEDDING_DIM)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pass the result through a bidirectional LSTM — (apparently) key to current &lt;a href="https://explosion.ai/blog/deep-learning-formula-nlp"&gt;state&lt;/a&gt;-of-the-&lt;a href="https://www.youtube.com/watch?v=nFCxTtBqF5U"&gt;art&lt;/a&gt; results in a variety of NLP tasks. This can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize two matrices of size &lt;code&gt;(MAX_QUESTION_LEN, LSTM_HIDDEN_STATE_SIZE)&lt;/code&gt;: &lt;code&gt;forward_matrix&lt;/code&gt; and &lt;code&gt;backward_matrix&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Pass the sequence of token ids through an LSTM and return all hidden states. The first hidden state is a function of, i.e. is computed using, the first token id's embedding; place it in the first row of &lt;code&gt;forward_matrix&lt;/code&gt;. The second hidden state is a function of the first and second token-id embeddings; place it in the second row of &lt;code&gt;forward_matrix&lt;/code&gt;. The third hidden state is a function of the first and second and third token-id embeddings, and so forth.&lt;/li&gt;
&lt;li&gt;Do the same thing but pass the sequence to the LSTM in reverse order. Place the first hidden state in the &lt;em&gt;last&lt;/em&gt; row of &lt;code&gt;backward_matrix&lt;/code&gt;, the second hidden state in the &lt;em&gt;second-to-last&lt;/em&gt; row of &lt;code&gt;backward_matrix&lt;/code&gt;, etc.&lt;/li&gt;
&lt;li&gt;Concatenate &lt;code&gt;forward_matrix&lt;/code&gt; and &lt;code&gt;backward_matrix&lt;/code&gt; into a single matrix of size &lt;code&gt;(MAX_QUESTION_LEN, 2 * LSTM_HIDDEN_STATE_SIZE)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://keras.io/layers/pooling/#maxpooling1d"&gt;Max-pool&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Flatten.&lt;/li&gt;
&lt;li&gt;Dense layer with ReLU activations, down to 10 dimensions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To process the &lt;code&gt;answer&lt;/code&gt; we employ the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Dense layer with ReLU activations.&lt;/li&gt;
&lt;li&gt;Dense layer with ReLU activations, down to 10 dimensions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now of equal size, we further process our &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; with a &lt;em&gt;single&lt;/em&gt; set of dense layers — the key difference between a SQAM and (the neural-network formulation of) other canonical &lt;code&gt;(user, item)&lt;/code&gt; recommendation algorithms. The last of these layers employs &lt;code&gt;tanh&lt;/code&gt; activations as suggested in Feng et al. (2015).&lt;/p&gt;
&lt;p&gt;Finally, we compute the cosine similarity between the resulting embeddings.&lt;/p&gt;
&lt;h2&gt;Prepare questions, answers&lt;/h2&gt;
&lt;h3&gt;Import tweets&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tweets_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data/tweets.csv'&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="s1"&gt;'text'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'airline_sentiment'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tweets_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;index&lt;/th&gt;
&lt;th&gt;text&lt;/th&gt;
&lt;th&gt;airline_sentiment&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;2076&lt;/td&gt;
&lt;td&gt;@united that's not an apology. Say it.&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;7534&lt;/td&gt;
&lt;td&gt;@JetBlue letting me down in San Fran. No Media...&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;14441&lt;/td&gt;
&lt;td&gt;@AmericanAir where do I look for cabin crew va...&lt;/td&gt;
&lt;td&gt;neutral&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;13130&lt;/td&gt;
&lt;td&gt;@AmericanAir just sad that even after spending...&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;3764&lt;/td&gt;
&lt;td&gt;@united What's up with the reduction in E+ on ...&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3&gt;Embed answers into 16-D latent space&lt;/h3&gt;
&lt;p&gt;Additionally, scale the latent codes; these will be fed to our network as input.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# embed&lt;/span&gt;
&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'1f389.png'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'1f608.png'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'1f621.png'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# scale&lt;/span&gt;
&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build vectors of correct, incorrect answers&lt;/span&gt;
&lt;span class="n"&gt;embedding_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'positive'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'neutral'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'negative'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;sentiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;embedding_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tweets_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'airline_sentiment'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;embedding_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;incorrect_sentiment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiments&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;embedding_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;incorrect_sentiment&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;questions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweets_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'text'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;correct_answers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We've now built (only) one &lt;code&gt;(question, correct_answer, incorrect_answer)&lt;/code&gt; training triplet for each ground-truth &lt;code&gt;(question, correct_answer)&lt;/code&gt;. In practice, we should likely have many more, i.e. &lt;code&gt;(question, correct_answer, incorrect_answer_1), (question, correct_answer, incorrect_answer_2), ..., (question, correct_answer, incorrect_answer_n)&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Construct sequences of token ids&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;VOCAB_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2000&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;questions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;question_seqs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;questions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;question_seqs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Split data into train, validation sets&lt;/h3&gt;
&lt;p&gt;NB: We don't actually have &lt;code&gt;y&lt;/code&gt; values: we pass &lt;code&gt;(question, correct_answer, incorrect_answer)&lt;/code&gt; triplets to our network and try to minimize &lt;code&gt;max(0, margin - correct_cos_sim + incorrect_cos_sim)&lt;/code&gt;. Notwithstanding, Keras requires that we pass both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; (as Numpy arrays); we pass the latter as a vector of 0's.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;

&lt;span class="n"&gt;questions_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;correct_answers_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answers_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;questions_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;correct_answers_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answers_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;y_train_dummy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;questions_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y_val_dummy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;questions_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    questions_train:         (4079, 20)&lt;/span&gt;
&lt;span class="err"&gt;    correct_answers_train:   (4079, 16)&lt;/span&gt;
&lt;span class="err"&gt;    incorrect_answers_train: (4079, 16)&lt;/span&gt;
&lt;span class="err"&gt;    questions_val:           (921, 20)&lt;/span&gt;
&lt;span class="err"&gt;    correct_answers_val:     (921, 16)&lt;/span&gt;
&lt;span class="err"&gt;    incorrect_answers_val:   (921, 16)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Build embedding layer from Glove vectors&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;GLOVE_EMBEDDING_DIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="c1"&gt;# 1. Load Glove embeddings&lt;/span&gt;

&lt;span class="c1"&gt;# 2. Build embeddings matrix&lt;/span&gt;

&lt;span class="c1"&gt;# 3. Build Keras embedding layer&lt;/span&gt;
&lt;span class="n"&gt;embedding_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GLOVE_EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Build Siamese question-answer model (SQAM)&lt;/h3&gt;
&lt;p&gt;GDGS architecture, ✌️.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;LSTM_HIDDEN_STATE_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

&lt;span class="c1"&gt;# question&lt;/span&gt;
&lt;span class="n"&gt;question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int32'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;question_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;biLSTM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bidirectional&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LSTM_HIDDEN_STATE_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_sequences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;question_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;max_pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MaxPool1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;biLSTM&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# answer&lt;/span&gt;
&lt;span class="n"&gt;answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# combine&lt;/span&gt;
&lt;span class="n"&gt;shared_dense_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;shared_dense_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;shared_dense_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# compute cosine sim, a normalized dot product&lt;/span&gt;
&lt;span class="n"&gt;cosine_sim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;qa_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cosine_sim&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'qa_model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="qa model" class="img-responsive" src="https://willwolf.io/figures/qa_model.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Build contrastive model&lt;/h3&gt;
&lt;p&gt;Two Siamese networks, trained jointly so as to minimize the hinge loss of their respective outputs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# contrastive model&lt;/span&gt;
&lt;span class="n"&gt;correct_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;correct_cos_sim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qa_model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answer&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;incorrect_cos_sim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qa_model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answer&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hinge_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cos_sims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;margin&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;correct&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos_sims&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;margin&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;incorrect&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;contrastive_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Lambda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hinge_loss&lt;/span&gt;&lt;span class="p"&gt;)([&lt;/span&gt;&lt;span class="n"&gt;correct_cos_sim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_cos_sim&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;contrastive_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;contrastive_loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'contrastive_model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Build prediction model&lt;/h3&gt;
&lt;p&gt;This is what we'll use to compute the cosine similarity of novel &lt;code&gt;(question, answer)&lt;/code&gt; pairs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;prediction_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;qa_model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'prediction_model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Fit contrastive model&lt;/h3&gt;
&lt;p&gt;Fitting &lt;code&gt;contrastive_model&lt;/code&gt; will implicitly fit &lt;code&gt;prediction_model&lt;/code&gt; as well, so long as the latter has been compiled.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# compile&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clipnorm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;contrastive_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;prediction_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# fit&lt;/span&gt;
&lt;span class="n"&gt;contrastive_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;questions_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answers_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers_train&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_train_dummy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;questions_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answers_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers_val&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_val_dummy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Train on 4089 samples, validate on 911 samples&lt;/span&gt;
&lt;span class="err"&gt;Epoch 1/3&lt;/span&gt;
&lt;span class="err"&gt;4089/4089 [==============================] - 18s - loss: 0.1069 - val_loss: 0.0929&lt;/span&gt;
&lt;span class="err"&gt;Epoch 2/3&lt;/span&gt;
&lt;span class="err"&gt;4089/4089 [==============================] - 14s - loss: 0.0796 - val_loss: 0.0822&lt;/span&gt;
&lt;span class="err"&gt;Epoch 3/3&lt;/span&gt;
&lt;span class="err"&gt;4089/4089 [==============================] - 14s - loss: 0.0675 - val_loss: 0.0828&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Predict on new tweets&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous"&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;"@usairways I've called for 3 days and can't get thru. is there some secret method i can use that doesn't result in you hanging up on me?"&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;"@AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!"&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;new_questions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"@usairways I've called for 3 days and can't get thru. is there some secret method i can use that doesn't result in you hanging up on me?"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"@AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!"&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;new_questions_seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;new_questions_seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_sentiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeats&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prediction_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #1&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predictions:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    🎉 (Positive): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😈 (Neutral) : &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😡 (Negative): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;negative_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;Predictions:&lt;/span&gt;
&lt;span class="c"&gt;    🎉 (Positive): 0.51141&lt;/span&gt;
&lt;span class="c"&gt;    😈 (Neutral) : 0.56273&lt;/span&gt;
&lt;span class="c"&gt;    😡 (Negative): 0.9728&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #2&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predictions:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    🎉 (Positive): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😈 (Neutral) : &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😡 (Negative): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;negative_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;Predictions:&lt;/span&gt;
&lt;span class="c"&gt;    🎉 (Positive): 0.41422&lt;/span&gt;
&lt;span class="c"&gt;    😈 (Neutral) : 0.61587&lt;/span&gt;
&lt;span class="c"&gt;    😡 (Negative): 0.99161&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #3&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predictions:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    🎉 (Positive): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😈 (Neutral) : &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😡 (Negative): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;negative_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;Predictions:&lt;/span&gt;
&lt;span class="c"&gt;    🎉 (Positive): 0.87107&lt;/span&gt;
&lt;span class="c"&gt;    😈 (Neutral) : 0.46741&lt;/span&gt;
&lt;span class="c"&gt;    😡 (Negative): 0.73435&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Additionally, we can predict on the full set of emojis&lt;/h3&gt;
&lt;p&gt;Some emoji embeddings contain &lt;code&gt;np.inf&lt;/code&gt; values, unfortunately. We could likely mitigate this by further tweaking the hyperparameters of our autoencoder.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inf_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isinf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;inf_mask&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;% of values are `np.inf`.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;all_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;inf_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;4.15% of values are `np.inf`.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_sentiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeats&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prediction_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #1&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;preds_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;top_5_matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_top_5_argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds_1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;display_top_5_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_5_matches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="predicted tweets 1" class="img-responsive" src="https://willwolf.io/images/predicted_tweets_1.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Tweet #2&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;preds_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;top_5_matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_top_5_argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;display_top_5_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_5_matches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="predicted tweets 2" class="img-responsive" src="https://willwolf.io/images/predicted_tweets_2.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Tweet #3&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;preds_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;top_5_matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_top_5_argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds_3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;display_top_5_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_5_matches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="predicted tweets 3" class="img-responsive" src="https://willwolf.io/images/predicted_tweets_3.png"/&gt;&lt;/p&gt;
&lt;p&gt;Not particularly useful. These emojis have 0 notion of sentiment, though: the model is simply predicting on their (pixel-based) latent codes.&lt;/p&gt;
&lt;h2&gt;Future work&lt;/h2&gt;
&lt;p&gt;In this work, we trained a convolutional variational autoencoder to model the distribution of emojis. Next, we trained a Siamese question-answer model to answer text questions with emoji answers. Finally, we were able to use the latter to predict on novel emojis from the former.&lt;/p&gt;
&lt;p&gt;Moving forward, I see a few logical steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use emoji embeddings that are conscious of sentiment — likely trained via a different network altogether. This way, we could make more meaningful (sentiment-based) predictions on novel emojis.&lt;/li&gt;
&lt;li&gt;Predict on emojis generated from the autoencoder.&lt;/li&gt;
&lt;li&gt;Add 1-D convolutions to the text side of the SQAM.&lt;/li&gt;
&lt;li&gt;Add an &lt;a href="https://www.quora.com/What-is-attention-in-the-context-of-deep-learning"&gt;"attention"&lt;/a&gt; mechanism — the one component missing from the &lt;a href="https://explosion.ai/blog/deep-learning-formula-nlp"&gt;"embed, encode, attend, predict"&lt;/a&gt; dynamic quartet of modern NLP.&lt;/li&gt;
&lt;li&gt;Improve the stability of our autoencoder so as to not produce embeddings containing &lt;code&gt;np.inf&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sincere thanks for reading, and emojis 🤘.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/neurally-embedded-emojis"&gt;repository&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/neurally-embedded-emojis/blob/master/neurally-embedded-emojis.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ben.bolte.cc/blog/2016/language.html"&gt;Deep Language Modeling for Question Answering using Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1508.01585v2.pdf"&gt;Applying Deep Learning To Answer Selection: A Study And An Open Task&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1511.04108.pdf"&gt;LSTM Based Deep Learning Models For Non-Factoid Answer Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://explosion.ai/blog/deep-learning-formula-nlp"&gt;Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py"&gt;Keras Examples - Convolutional Variational Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html"&gt;Introducing Variational Autoencoders (in Prose and Code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html"&gt;Under the Hood of the Variational Autoencoder (in Prose and Code)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Random Effects Neural Networks in Edward and Keras</title><link href="https://willwolf.io/2017/06/15/random-effects-neural-networks/" rel="alternate"></link><published>2017-06-15T18:00:00-04:00</published><updated>2017-06-15T18:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-06-15:/2017/06/15/random-effects-neural-networks/</id><summary type="html">&lt;p&gt;Coupling nimble probabilistic models with neural architectures in Edward and Keras: "what worked and what didn't," a conceptual overview of random effects, and directions for further exploration.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Bayesian probabilistic models provide a nimble and expressive framework for modeling "small-world" data. In contrast, deep learning offers a more rigid yet much more powerful framework for modeling data of massive size. &lt;a href="http://edwardlib.org/"&gt;Edward&lt;/a&gt; is a probabilistic programming library that bridges this gap: "black-box" variational inference enables us to fit extremely flexible Bayesian models to large-scale data. Furthermore, these models themselves may take advantage of classic deep-learning architectures of arbitrary complexity.&lt;/p&gt;
&lt;p&gt;Edward uses &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; for symbolic gradients and data flow graphs. As such, it interfaces cleanly with other libraries that do the same, namely &lt;a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html"&gt;TF-Slim&lt;/a&gt;, &lt;a href="https://github.com/google/prettytensor"&gt;PrettyTensor&lt;/a&gt; and &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt;. Personally, I've been working often with the latter, and am consistently delighted by the ease with which it allows me to specify complex neural architectures.&lt;/p&gt;
&lt;p&gt;The aim of this post is to lay a practical foundation for Bayesian modeling in Edward, then explore how, and how easily, we can extend these models in the direction of classical deep learning via Keras. It will give both a conceptual overview of the models below, as well as notes on the practical considerations of their implementation —  what worked and what didn't. Finally, this post will conclude with concrete ways in which to extend these models further, of which there are many.&lt;/p&gt;
&lt;p&gt;If you're just getting started with Edward or Keras, I recommend first perusing the &lt;a href="http://edwardlib.org/tutorials"&gt;Edward tutorials&lt;/a&gt; and &lt;a href="https://keras.io/"&gt;Keras documentation&lt;/a&gt; respectively.&lt;/p&gt;
&lt;p&gt;To "pull us down the path," we build three models in additive fashion: a Bayesian linear regression model, a Bayesian linear regression model with random effects, and a neural network with random effects. We fit them on the &lt;a href="https://www.kaggle.com/c/zillow-prize-1"&gt;Zillow Prize&lt;/a&gt; dataset, which asks us to predict &lt;code&gt;logerror&lt;/code&gt; (in house-price estimate, i.e. the "Zestimate") given metadata for a list of homes. These models are intended to be demonstrative, not performant: they will not win you the prize in their current form.&lt;/p&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;h3&gt;Build training DataFrame&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transactions_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;properties_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'left'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left_on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'id_parcel'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right_on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'id_parcel'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Drop columns containing too many nulls&lt;/h3&gt;
&lt;p&gt;Bayesian probabilistic models allow us to flexibly model &lt;em&gt;missing&lt;/em&gt; data itself. To this end, we conceive of a given predictor as a vector of both:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Observed values.&lt;/li&gt;
&lt;li&gt;Parameters in place of missing values, which will form a posterior distribution for what this value might have been.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In a (partially-specified, for brevity) linear model, this might look as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
y_i \sim \mathcal{N}(\mu_i, \sigma)\\
\mu_i = \alpha + \beta_N N_i\\
N_i \sim \mathcal{N}(\nu, \sigma_N)\\
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; is our sometimes-missing predictor. When &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; is observed, &lt;span class="math"&gt;\(\mathcal{N}(\nu, \sigma_N)\)&lt;/span&gt; serves as a likelihood: given this data-point, we tweak retrodictive distributions on the parameters &lt;span class="math"&gt;\((\nu, \sigma_N)\)&lt;/span&gt; by which it was produced. Conversely, when &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; is missing it serves as a prior: after learning distributions of &lt;span class="math"&gt;\((\nu, \sigma_N)\)&lt;/span&gt; we can generate a likely value of &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; itself. Finally, inference will give us (presumably-wide) distributions on the model's belief in what was the true value of each missing &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; conditional on the data observed.&lt;/p&gt;
&lt;p&gt;I tried this in Edward, albeit briefly, to no avail. Dustin Tran gives an &lt;a href="https://discourse.edwardlib.org/t/how-to-handle-missing-values-in-gaussian-matrix-factorization/95/2"&gt;example&lt;/a&gt; of how one might accomplish this task in the case of Gaussian Matrix Factorization. In my case, I wasn't able to apply a 2-D missing-data-mask placeholder to a 2-D data placeholder via &lt;a href="https://www.tensorflow.org/api_docs/python/tf/gather"&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt; nor &lt;a href="https://www.tensorflow.org/api_docs/python/tf/gather_nd"&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;. With more effort, I'm sure I could make this work. Help appreciated.&lt;/p&gt;
&lt;p&gt;For now, we'll first drop columns containing too many null values, then, after choosing a few of the predictors most correlated with the target, drop the remaining rows containing nulls.&lt;/p&gt;
&lt;h3&gt;Select three fixed-effect predictors&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;fixed_effect_predictors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;'area_live_finished'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'num_bathroom'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'build_year'&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Select one random-effect predictor&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;zip_codes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'region_zip'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'category'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Split data into train, validation sets&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    X_train:  (36986, 3)&lt;/span&gt;
&lt;span class="err"&gt;    X_val:    (36986, 3)&lt;/span&gt;
&lt;span class="err"&gt;    y_train:  (36986,)&lt;/span&gt;
&lt;span class="err"&gt;    y_val:    (36986,)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Bayesian linear regression&lt;/h1&gt;
&lt;p&gt;Using three fixed-effect predictors we'll fit a model of the following form:&lt;/p&gt;
&lt;div class="math"&gt;$$
y_i \sim \mathcal{N}(\mu_i, 1)\\
\mu_i = \alpha + \beta x_i\\
\alpha \sim \mathcal{N}(0, 1)\\
\beta \sim \mathcal{N}(0, 1)\\
$$&lt;/div&gt;
&lt;p&gt;Having normalized our data to have mean 0 and unit-variance, we place our priors on a similar scale.&lt;/p&gt;
&lt;p&gt;To infer posterior distributions of the model's parameters conditional on the data observed we employ variational inference — one of three inference classes supported in Edward. This approach posits posterior inference as posterior &lt;em&gt;approximation&lt;/em&gt; via &lt;em&gt;optimization&lt;/em&gt;, where optimization is done via stochastic, gradient-based methods. This is what enables us to scale complex probabilistic functional forms to large-scale data.&lt;/p&gt;
&lt;p&gt;For an introduction to variational inference and Edward's API thereof, please reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/inference"&gt;Edward: Inference of Probabilistic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/variational-inference"&gt;Edward: Variational Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/klqp"&gt;Edward: KL(q||p) Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/api/inference"&gt;Edward: API and Documentation - Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, I provide an introduction to the basic math behind variational inference and the &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;ELBO&lt;/a&gt; in the previous post on this blog: &lt;a href="https://willwolf.io/2017/07/06/further-exploring-common-probabilistic-models/"&gt;Further Exploring Common Probabilistic Models&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;p&gt;For the approximate q-distributions, we apply the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/nn/softplus"&gt;softplus function&lt;/a&gt; — &lt;code&gt;log(exp(z) + 1)&lt;/code&gt; — to the scale parameter values at the suggestion of the Edward docs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

&lt;span class="c1"&gt;# fixed-effects placeholders&lt;/span&gt;
&lt;span class="n"&gt;fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# fixed-effects parameters&lt;/span&gt;
&lt;span class="n"&gt;β_fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# approximate fixed-effects distributions&lt;/span&gt;
&lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softplus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;qα&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softplus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Infer parameters&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INIT_OP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KLqp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;250/250 [100%] ██████████████████████████████ Elapsed: 4s | Loss: 35405.105&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Criticize model&lt;/h3&gt;
&lt;h4&gt;Visualize data fit given parameter priors&lt;/h4&gt;
&lt;p&gt;&lt;img alt="data fit given parameter priors" class="img-responsive" src="https://willwolf.io/figures/data_fit_given_parameter_priors.png"/&gt;&lt;/p&gt;
&lt;h4&gt;Visualize data fit given parameter posteriors&lt;/h4&gt;
&lt;p&gt;&lt;img alt="data fit given parameter posteriors" class="img-responsive" src="https://willwolf.io/figures/data_fit_given_parameter_posteriors.png"/&gt;&lt;/p&gt;
&lt;p&gt;It appears as if our model fits the data along the first two dimensions. This said, we could improve this fit considerably. This will become apparent when we compute the MAE on our validation set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;param_posteriors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
  &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;X_val_feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;y_posterior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_posteriors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Mean validation `logerror`: {y_val.mean()}'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;compute_mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_posterior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val_feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Mean&lt;/span&gt; &lt;span class="k"&gt;validation&lt;/span&gt; &lt;span class="n n-Quoted"&gt;`logerror`&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.012986094738549725&lt;/span&gt;
&lt;span class="n"&gt;Mean&lt;/span&gt; &lt;span class="n"&gt;absolute&lt;/span&gt; &lt;span class="k"&gt;error&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="k"&gt;validation&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.089943&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Inspect residuals&lt;/h4&gt;
&lt;p&gt;&lt;img alt="bayesian linear regression residuals" class="img-responsive" src="https://willwolf.io/figures/bayesian_linear_regression_residuals.png"/&gt;&lt;/p&gt;
&lt;p&gt;"The residuals appear normally distributed with mean 0: this is a good sanity check for the model."&lt;sup&gt;1&lt;/sup&gt; However, with respect to the magnitude of the mean of the validation &lt;code&gt;logerror&lt;/code&gt;, our validation score is terrible. This is likely due to the fact that three predictors are not nearly sufficient for capturing the variation in the response. (Additionally, because the response itself is an &lt;em&gt;error&lt;/em&gt;, it should be fundamentally harder to capture than the thing actually being predicted — the house price. This is because Zillow's team has already built models to capture this signal, then effectively threw the remaining "uncaptured" signal into this competition, i.e. "figure out how to get right the little that we got wrong.")&lt;/p&gt;
&lt;h4&gt;Inspect parameter posteriors&lt;/h4&gt;
&lt;p&gt;&lt;img alt="bayesian linear regression posteriors" class="img-responsive" src="https://willwolf.io/figures/bayesian_linear_regression_posteriors.png"/&gt;&lt;/p&gt;
&lt;p&gt;In keeping with the definition of multivariate linear regression itself, the above parameter posteriors tell us: "conditional on the assumption that the log-error and fixed effects can be related by a straight line, what is the predictive value of one variable once I already know the values of all other variables?"&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;Bayesian linear regression with random effects&lt;/h2&gt;
&lt;p&gt;Random effects models — also known as hierarchical models — allow us to ascribe distinct behaviors to different "clusters" of observations, i.e. groups that may each act in a materially unique way. Furthermore, these models allow us to infer these tendencies in a &lt;em&gt;collaborative&lt;/em&gt; fashion: while each cluster is assumed to behave differently, it can learn its parameters by heeding to the behavior of the population at large. In this example, we assume that houses in different zipcodes — holding all other predictors constant — should be priced in different ways.&lt;/p&gt;
&lt;p&gt;For clarity, let's consider the two surrounding extremes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate a single set of parameters for the population, i.e. the vanilla, &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"&gt;scikit-learn linear regression&lt;/a&gt;, Bayesian or not. This confers no distinct behaviors to houses in different zipcodes.&lt;/li&gt;
&lt;li&gt;Estimate a set of parameters for each individual zipcode, i.e. split the data into its cluster groups and estimate a single model for each. This confers maximally distinct behaviors to houses in different zip codes: the behavior of one cluster knows nothing about that of the others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Random-effects models "walk the line" between these two approaches — between maximally &lt;em&gt;underfitting&lt;/em&gt; and maximally &lt;em&gt;overfitting&lt;/em&gt; the behavior of each cluster. To this effect, its parameter estimates exhibit the canonical "shrinkage" phenomenon: the estimate for a given parameter is balanced between the within-cluster expectation and the global expectation. Smaller clusters exhibit larger shrinkage; larger clusters, i.e. those for which we've observed more data, are more bullheaded (in typical Bayesian fashion). A later plot illustrates this point.&lt;/p&gt;
&lt;p&gt;We specify our random-effects functional form as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With respect to the previous model, we've simply added &lt;code&gt;α_random_effects&lt;/code&gt; to the mean of our response. As such, this is a &lt;em&gt;varying-intercepts&lt;/em&gt; model: the intercept term will be different for each cluster. To this end, we learn the &lt;em&gt;global&lt;/em&gt; intercept &lt;code&gt;α&lt;/code&gt; as well as the &lt;em&gt;offsets&lt;/em&gt; from this intercept &lt;code&gt;α_random_effects&lt;/code&gt; — a random variable with as many dimensions as there are zipcodes. In keeping with the notion of "offset," we ascribe it a prior of &lt;code&gt;(0, σ_zc)&lt;/code&gt;. This approach allows us to flexibly extend the model to include more random effects, e.g. city, architecture style, etc. With only one, however, we could have equivalently included the global intercept &lt;em&gt;inside&lt;/em&gt; of our prior, i.e. &lt;code&gt;α_random_effects ~ Normal(α, σ_zc)&lt;/code&gt;, with priors on both &lt;code&gt;α&lt;/code&gt; and &lt;code&gt;σ_zc&lt;/code&gt; as per usual. This way, our random effect would no longer be a zip-code-specific &lt;em&gt;offset&lt;/em&gt; from the global intercept, but a vector of zip-code-specific intercepts outright.&lt;/p&gt;
&lt;p&gt;Finally, as Richard McElreath notes, "we can think of the &lt;code&gt;σ_zc&lt;/code&gt; parameter for each cluster as a crude measure of that cluster's "relevance" in explaining variation in the response variable."&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zip_codes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# random-effect placeholder&lt;/span&gt;
&lt;span class="n"&gt;zip_codes_ph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# random-effect parameter&lt;/span&gt;
&lt;span class="n"&gt;σ_zip_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([]))))&lt;/span&gt;
&lt;span class="n"&gt;α_zip_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;σ_zip_code&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gather&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;α_zip_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zip_codes_ph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# approximate random-effect distribution&lt;/span&gt;
&lt;span class="n"&gt;qα_zip_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softplus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Infer parameters&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α_zip_code&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα_zip_code&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INIT_OP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KLqp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zip_codes_ph&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;zip_codes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;250/250 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 34898.613&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Criticize model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean absolute error on validation data: 0.084635&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Inspect residuals&lt;/h4&gt;
&lt;p&gt;&lt;img alt="bayesian linear regression with random effects residuals" class="img-responsive" src="https://willwolf.io/figures/bayesian_linear_regression_with_random_effects_residuals.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Plot shrinkage&lt;/h3&gt;
&lt;p&gt;To illustrate shrinkage we'll pare our model down to intercepts only (removing the fixed effects entirely). We'll first fit a random-effects model on the full dataset then compute the cluster-specific-intercept posterior means. Next, we'll fit a separate model to each individual cluster and compute the intercept posterior mean of each. The plot below shows how estimates from the former can be viewed as "estimates from the latter — shrunk towards the global-intercept posterior mean."&lt;/p&gt;
&lt;p&gt;Finally, &lt;span style="color: #377eb8"&gt;blue&lt;/span&gt;, &lt;span style="color: #4daf4a"&gt;green&lt;/span&gt; and &lt;span style="color: #ff7f00"&gt;orange&lt;/span&gt; points represent small, medium and large clusters respectively. As mentioned before, the larger the cluster size, i.e. the more data points we've observed belonging to a given cluster, the &lt;em&gt;less&lt;/em&gt; prone it is to shrinkage towards the mean.&lt;/p&gt;
&lt;p&gt;&lt;img alt="shrinkage plot" class="img-responsive" src="https://willwolf.io/figures/shrinkage_plot.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Neural network with random effects&lt;/h2&gt;
&lt;p&gt;Neural networks are powerful function approximators. Keras is a library that lets us flexibly define complex neural architectures. Thus far, we've been approximating the relationship between our fixed effects and response variable with a simple dot product; can we leverage Keras to make this relationship more expressive? Is it painless? Finally, how does it integrate with Edward's existing APIs and constructs? Can we couple nimble generative models with deep neural networks?&lt;/p&gt;
&lt;p&gt;While my experimentation was brief, all answers point delightfully towards "yes" for two simple reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edward and Keras both run on TensorFlow.&lt;/li&gt;
&lt;li&gt;"Black-box" variational inference makes everything scale.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This said, we must be nonetheless explicit about what's "Bayesian" and what's not, i.e. for which parameters do we infer full (approximate) posterior distributions, and for which do we infer point estimates of the posterior distribution.&lt;/p&gt;
&lt;p&gt;Below, we drop a &lt;code&gt;neural_network&lt;/code&gt; in place of our dot product. Our latent variables remain &lt;code&gt;β_fixed_effects&lt;/code&gt;, &lt;code&gt;α&lt;/code&gt; and &lt;code&gt;α_zip_code&lt;/code&gt;: while we will infer their full (approximate) posterior distributions as before, we'll only compute &lt;em&gt;point estimates&lt;/em&gt; for the parameters of the neural network as in the typical case. Conversely, to the best of my knowledge, to infer full distributions for the latter, we'll need to specify our network manually in raw TensorFlow, i.e. ditch Keras entirely. We then treat our weights and biases as standard latent variables and infer their approximate posteriors via variational inference.  Edward's documentation contains a straightforward &lt;a href="http://edwardlib.org/tutorials/bayesian-neural-network"&gt;tutorial&lt;/a&gt; to this end.&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;neural_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'output'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;neural_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α_zip_code&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα_zip_code&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INIT_OP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KLqp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zip_codes_ph&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;zip_codes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RMSPropOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;1000/1000 [100%] ██████████████████████████████ Elapsed: 18s | Loss: 34446.191&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Criticize model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean absolute error on validation data: 0.081484&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Inspect residuals&lt;/h4&gt;
&lt;p&gt;&lt;img alt="neural network with random effects residuals" class="img-responsive" src="https://willwolf.io/figures/neural_network_with_random_effects_residuals.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Future work&lt;/h1&gt;
&lt;p&gt;We've now laid a stable, if trivially simple foundation for building models with Edward and Keras. From here, I see two distinct paths to building more expressive probabilistic models using these tools:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build probabilistic models in Edward, and abstract deep-network-like subgraphs into Keras layers. This allows us to flexibly define complex neural architectures, e.g. a &lt;a href="https://keras.io/getting-started/functional-api-guide/#video-question-answering-model"&gt;video question answering model&lt;/a&gt;, with a nominal amount of code, yet restricts us from, or at least makes it awkward to, infer full posterior distributions for the subgraph parameters.&lt;/li&gt;
&lt;li&gt;Build probabilistic models in Edward, and specify deep-network-like subgraphs with raw TensorFlow — ditching Keras entirely. Defining deep-network-like subgraphs becomes more cumbersome, while inferring full posterior distributions for the subgraph parameters becomes more natural and consistent with the flow of Edward code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This work has shown a few basic variants of (generalized) Bayesian linear regression models. From here, there's tons more to explore — varying-slopes models, Gaussian process regression, mixture models and probabilistic matrix factorizations to name a random few.&lt;/p&gt;
&lt;p&gt;Edward and Keras have proven a flexible, expressive and powerful duo for performing inference in deep probabilistic models. The models we built were simple; the only direction to go, and to go rather painlessly, is more.&lt;/p&gt;
&lt;p&gt;Many thanks for reading.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/random-effects-neural-networks"&gt;repository&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/random-effects-neural-networks/blob/master/zillow.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/linear-mixed-effects-models"&gt;Edward - Linear Mixed Effects Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;McElreath, Richard. "Chapter 5." Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton, FL: CRC, Taylor &amp;amp; Francis Group, 2016. N. pag. Print.&lt;/li&gt;
&lt;li&gt;McElreath, Richard. "Chapter 12." Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton, FL: CRC, Taylor &amp;amp; Francis Group, 2016. N. pag. Print.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/"&gt;The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"&gt;Keras as a simplified interface to TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html"&gt;Mixture Density Networks with Edward, Keras and TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sl8r000.github.io/ab_testing_statistics/use_a_hierarchical_model/"&gt;Use a Hierarchical Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;McElreath, Richard. "Chapter 14." Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton, FL: CRC, Taylor &amp;amp; Francis Group, 2016. N. pag. Print.&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Further Exploring Common Probabilistic Models</title><link href="https://willwolf.io/2017/07/06/further-exploring-common-probabilistic-models/" rel="alternate"></link><published>2017-06-06T10:00:00-04:00</published><updated>2017-06-06T10:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-06-06:/2017/07/06/further-exploring-common-probabilistic-models/</id><summary type="html">&lt;p&gt;Exploring generative vs. discriminative models, and sampling and variational methods for approximate inference through the lens of Bayes' theorem.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="https://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;previous post&lt;/a&gt; on this blog sought to expose the statistical underpinnings of several machine learning models you know and love. Therein, we made the analogy of a swimming pool: you start on the surface — you know what these models do and how to use them for fun and profit — dive to the bottom — you deconstruct these models into their elementary assumptions and intentions — then finally, work your way back to the surface — reconstructing their functional forms, optimization exigencies and loss functions one step at a time.&lt;/p&gt;
&lt;p&gt;In this post, we're going to stay on the surface: instead of deconstructing common models, we're going to further explore the relationships between them — swimming to different corners of the pool itself. Keeping us afloat will be Bayes' theorem — a balanced, dependable yet at times fragile pool ring, so to speak — which we'll take with us wherever we go.&lt;/p&gt;
&lt;p&gt;&lt;img alt="pool ring" class="img-responsive" src="https://c1.staticflickr.com/7/6151/6137348439_199f5119be_b.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;While there are many potential themes of probabilistic models we might explore, we'll herein focus on two: &lt;strong&gt;generative vs. discriminative models&lt;/strong&gt;, and &lt;strong&gt;"fully Bayesian" vs. "lowly point estimate" learning&lt;/strong&gt;. We will stick to the supervised setting as well.&lt;/p&gt;
&lt;p&gt;Finally, our pool ring is not a godhead — we are not nautical missionaries brandishing a divine statistical truth, demanding that each model we encounter implement this truth in a rigid, bottom-up fashion. Instead, we'll explore the unique goals, formulations and shortcomings of each, and fall back on Bayes' theorem to bridge the gaps between. Without it, we'd quickly start sinking.&lt;/p&gt;
&lt;h1&gt;Discriminative vs. generative models&lt;/h1&gt;
&lt;p&gt;The goal of a supervised model is to compute the distribution over outcomes &lt;span class="math"&gt;\(y\)&lt;/span&gt; given an input &lt;span class="math"&gt;\(x\)&lt;/span&gt;, written &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;. If &lt;span class="math"&gt;\(y\)&lt;/span&gt; is discrete, this distribution is a probability mass function, e.g. a multinomial or binomial distribution. If continuous, it is a probability density function, e.g. a Gaussian distribution.&lt;/p&gt;
&lt;h2&gt;Discriminative models&lt;/h2&gt;
&lt;p&gt;In discriminative models, we immediately direct our focus to this output distribution. Taking an example from the &lt;a href="https://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;previous post&lt;/a&gt;, let's assume a softmax regression which receives some data &lt;span class="math"&gt;\(x\)&lt;/span&gt; and predicts a multi-class label &lt;code&gt;red or green or blue&lt;/code&gt;. The model's output distribution is therefore multinomial; a multinomial distribution requires as a parameter a vector &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; of respective outcome probabilities, e.g. &lt;code&gt;{red: .27, green: .11, blue: .62}&lt;/code&gt;. We can compute these individual probabilities via the softmax function, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\pi_k = \frac{e^{\eta_k}}{\sum\limits_{k=1}^K e^{\eta_k}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta_k = \theta_k^Tx\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a matrix of weights which we must infer, and &lt;span class="math"&gt;\(x\)&lt;/span&gt; is our input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;p&gt;Typically, we perform inference by taking the &lt;em&gt;maximum likelihood estimate&lt;/em&gt;: "which parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; most likely gave rise to the observed data pairs &lt;span class="math"&gt;\(D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)}))\)&lt;/span&gt; via the relationships described above?" We compute this estimate by maximizing the log-likelihood function with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, or equivalently minimizing the negative log-likelihood in identical fashion — the latter better known as a "loss function" in machine learning parlance.&lt;/p&gt;
&lt;p&gt;Unfortunately, the maximum likelihood estimate includes no information about the plausibility of the chosen parameter value itself. As such, we often place a &lt;em&gt;prior&lt;/em&gt; on our parameter and take the &lt;a href="https://en.wikipedia.org/wiki/Arg_max"&gt;"argmax"&lt;/a&gt; over their product. This gives the &lt;em&gt;maximum a posteriori&lt;/em&gt; estimate, or MAP.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\theta_{MAP}
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(y^{(i)}\vert x^{(i)}; \theta)P(\theta)\\
&amp;amp;= \underset{\theta}{\arg\max}\ \sum\limits_{i=1}^{m} \log{P(y^{(i)}\vert x^{(i)}; \theta)} + \log{P(\theta)}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\log{P(\theta)}\)&lt;/span&gt; term can be easily rearranged into what is better known as a &lt;em&gt;regularization term&lt;/em&gt; in machine learning, where the type of prior distribution we place on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; gives the type of regularization term.&lt;/p&gt;
&lt;p&gt;The argmax finds the point(s) &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; at which the given function attains its maximum value. As such, the typical discriminative model — softmax regression, logistic regression, linear regression, etc. — returns a single, lowly point estimate for the parameter in question.&lt;/p&gt;
&lt;h3&gt;How do we compute this value?&lt;/h3&gt;
&lt;p&gt;In the trivial case where &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is 1-dimensional, we can take the derivative of the function in question with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, set it equal to 0, then solve for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. (Additionally, in order to verify that we have indeed obtained a maximum, we should compute a second derivative and assert that its value is negative.)&lt;/p&gt;
&lt;p&gt;In the more realistic case where &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a high-dimensional vector or matrix, we can compute the argmax by way of an optimization routine like stochastic gradient ascent or, as is more common, the argmin by way of stochastic gradient descent.&lt;/p&gt;
&lt;h3&gt;What if we're uncertain about our parameter estimates?&lt;/h3&gt;
&lt;p&gt;Consider the following three scenarios — taken from Daphne Koller's &lt;a href="https://www.coursera.org/learn/probabilistic-graphical-models-3-learning/home/welcome"&gt;Learning in Probabilistic Graphical Models&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Two teams play 10 times, and the first wins 7 of the 10 matches.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt; &lt;em&gt;Infer that the probability of the first team winning is 0.7.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Seems reasonable, right?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A coin is tossed 10 times, and comes out &lt;code&gt;heads&lt;/code&gt; on 7 of the 10 tosses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt; &lt;em&gt;Infer that the probability of observing &lt;code&gt;heads&lt;/code&gt; is 0.7.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Changing only the analogy, this now seems wholly unreasonable — right?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A coin is tossed 10000 times, and comes out &lt;code&gt;heads&lt;/code&gt; on 7000 of the 10000 tosses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt; &lt;em&gt;Infer that the probability of observing &lt;code&gt;heads&lt;/code&gt; is 0.7.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, increasing the observed counts, the previous scenario now seems plausible.&lt;/p&gt;
&lt;p&gt;I find this a terrific succession of examples with which to convey the notion of &lt;em&gt;uncertainty&lt;/em&gt; — that the more data we have, the less uncertain we are about what's really going on. This notion is at the heart of Bayesian statistics and is extremely intuitive to us as humans. Unfortunately, when we compute "lowly point estimates," i.e. the argmin of the loss function with respect to our parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we are discarding this uncertainty entirely. Should our model be fit with &lt;span class="math"&gt;\(n\)&lt;/span&gt; observations where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is not a large number, our estimate would amount to that of Example #2: &lt;em&gt;a coin is tossed &lt;span class="math"&gt;\(n\)&lt;/span&gt; times, and comes out &lt;code&gt;heads&lt;/code&gt; on &lt;code&gt;int(.7n)&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; tosses — infer that the probability of observing &lt;code&gt;heads&lt;/code&gt; is squarely, unflinchingly, &lt;code&gt;0.7&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;What does including uncertainty look like?&lt;/h3&gt;
&lt;p&gt;It looks like a &lt;em&gt;distribution&lt;/em&gt; — a range of possible values for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Further, these values are of varying plausibility as dictated by the data we've observed. In Example #2, while we'd still say that &lt;span class="math"&gt;\(\Pr(\text{heads}) = .7\)&lt;/span&gt; is the parameter value &lt;em&gt;most likely&lt;/em&gt; to have generated our data, we'd additionally maintain that other values in &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt; are plausible, albeit less so, as well. Again, this logic should be simple to grasp: it comes easy to us as humans.&lt;/p&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;With the parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; in hand prediction is simple: just plug back into our original function &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;. With a point estimate for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we compute but a single value for &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Generative models&lt;/h2&gt;
&lt;p&gt;In generative models, we instead compute &lt;em&gt;component parts&lt;/em&gt; of the desired output distribution &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; instead of directly computing &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; itself. To examine these parts, we'll turn to Bayes' theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x\vert y)P(y)}{P(x)}
$$&lt;/div&gt;
&lt;p&gt;The numerator posits a generative mechanism for the observed data pairs &lt;span class="math"&gt;\(D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)}))\)&lt;/span&gt; in idiomatic terms; it states that each pair was generated by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Selecting a label &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt; from &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt;. If our model is predicting &lt;code&gt;red or green or blue&lt;/code&gt;, &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt; is likely a multinomial distribution.&lt;ul&gt;
&lt;li&gt;If our observed label counts are &lt;code&gt;{'red': 20, 'green': 50, 'blue': 30}&lt;/code&gt;, we would retrodictively believe this multinomial distribution to have had a parameter vector near &lt;span class="math"&gt;\(\pi = [.2, .5, .3]\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given a label &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt;, select a value &lt;span class="math"&gt;\(x^{(i)}\)&lt;/span&gt; from &lt;span class="math"&gt;\(P(x\vert y)\)&lt;/span&gt;. Trivially, this means that we are positing &lt;em&gt;three distinct distributions&lt;/em&gt; of this form: &lt;span class="math"&gt;\(P(x\vert y=\text{red}), P(x\vert y=\text{green}), P(x\vert y=\text{blue})\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;For example, if &lt;span class="math"&gt;\(y^{(i)} = \text{red}\)&lt;/span&gt;, draw &lt;span class="math"&gt;\(x^{(i)}\)&lt;/span&gt; from &lt;span class="math"&gt;\(P(x\vert y=\text{red})\)&lt;/span&gt;, and so forth.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;p&gt;The inference task is to compute &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt; and each distinct &lt;span class="math"&gt;\(P(x\vert y_k)\)&lt;/span&gt;. In a classification setting, the former is likely a multinomial distribution. The latter might be a multinomial distribution or a set of binomial distributions in the case of discrete-feature data, or a set of Gaussian distributions in the case of continuous-feature data. In fact, these distributions can be whatever you'd like, dictated by the idiosyncrasies of the problem at hand.&lt;/p&gt;
&lt;p&gt;Finally, we can compute these distributions as per normal: via a maximum likelihood estimate, a MAP estimate, etc.&lt;/p&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;To compute &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; we return to Bayes' theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x\vert y)P(y)}{P(x)}
$$&lt;/div&gt;
&lt;p&gt;We have the numerator &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt; and three distinct conditional distributions &lt;span class="math"&gt;\(P(x\vert y=\text{red}), P(x\vert y=\text{green})\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x\vert y=\text{blue})\)&lt;/span&gt; in hand. What about the denominator?&lt;/p&gt;
&lt;h3&gt;Conditional probability and marginalization&lt;/h3&gt;
&lt;p&gt;The axiom of conditional probability allows us to write &lt;span class="math"&gt;\(P(B\vert A)P(A) = P(B, A)\)&lt;/span&gt;, i.e. the &lt;em&gt;joint probability&lt;/em&gt; of &lt;span class="math"&gt;\(B\)&lt;/span&gt; and &lt;span class="math"&gt;\(A\)&lt;/span&gt;. This is a simple algebraic manipulation. As such, we can rewrite Bayes' theorem in its more compact form.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x, y)}{P(x)}
$$&lt;/div&gt;
&lt;p&gt;Another manipulation of probability distributions is the &lt;em&gt;marginalization&lt;/em&gt; operator, which allows us to write:&lt;/p&gt;
&lt;div class="math"&gt;$$
\int P(x, y)dy = P(x)
$$&lt;/div&gt;
&lt;p&gt;As such, we can &lt;em&gt;marginalize &lt;span class="math"&gt;\(y\)&lt;/span&gt; out of the numerator&lt;/em&gt; so as to obtain the denominator we require. This denominator is often called the "evidence."&lt;/p&gt;
&lt;h3&gt;Marginalization example&lt;/h3&gt;
&lt;p&gt;Marginalization took me a while to understand. Imagine we have the following joint probability distribution out of which we'd like to marginalize &lt;span class="math"&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.03\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.14\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.09\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.34\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.23\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.17\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The result of this marginalization is &lt;span class="math"&gt;\(P(B)\)&lt;/span&gt;, i.e. "what is the probability of observing each of the distinct values of &lt;span class="math"&gt;\(B\)&lt;/span&gt;?" In this example there are two — &lt;span class="math"&gt;\(b^7\)&lt;/span&gt; and &lt;span class="math"&gt;\(b^8\)&lt;/span&gt;. To marginalize over &lt;span class="math"&gt;\(A\)&lt;/span&gt;, we simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Delete the &lt;span class="math"&gt;\(A\)&lt;/span&gt; column.&lt;/li&gt;
&lt;li&gt;"Collapse" the remaining columns — in this case, &lt;span class="math"&gt;\(B\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step 1 gives:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.03\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.14\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.09\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.34\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.23\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.17\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Step 2 gives:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.03 + .09 = .12\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.14 + .34 + .23 + .17 = .88\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;The denominator&lt;/h3&gt;
&lt;p&gt;In the context of our generative model with a given input &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the result of this marginalization is a &lt;em&gt;scalar&lt;/em&gt; — not a distribution. To see why, let's construct the joint distribution — the numerator — then marginalize:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(x, y)\)&lt;/span&gt;:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(y\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(P(y, X)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\text{red}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{red}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\text{green}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{green}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\text{blue}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{blue}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\int P(x, y)dy = P(x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(P(y, X)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{red}, x) + P(y = \text{green}, x) + P(y = \text{blue}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The resulting probability distribution is over a single value: it is a scalar. This scalar &lt;em&gt;normalizes&lt;/em&gt; the respective numerator terms such that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(y = \text{red}, x)}{P(x)} +
\frac{P(y = \text{green}, x)}{P(x)} +
\frac{P(y = \text{blue}, x)}{P(x)}
= 1
$$&lt;/div&gt;
&lt;p&gt;This gives &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;: a valid probability distribution over the class labels &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Partition function&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; often takes another name and even another variable: &lt;span class="math"&gt;\(Z\)&lt;/span&gt;, the &lt;em&gt;partition function&lt;/em&gt;. The stated purpose of this function is to normalize the numerator such that the above summation-to-1 holds. This normalization is necessary because the numerators typically will not sum to 1 themselves, which follows logically from the fact that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\sum\limits_{k = 1}^K P(y = k) = 1
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(x\vert y = k) \neq 1
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\((1)\)&lt;/span&gt; is always true, the "&lt;span class="math"&gt;\(\neq\)&lt;/span&gt;" in &lt;span class="math"&gt;\((2)\)&lt;/span&gt; would need to become an "&lt;span class="math"&gt;\(=\)&lt;/span&gt;" such that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum\limits_{k = 1}^K P(y = k)P(x\vert y = k) = 1
$$&lt;/div&gt;
&lt;p&gt;Unfortunately, &lt;span class="math"&gt;\(P(x\vert y = k) = 1\)&lt;/span&gt; is rarely if ever the case.&lt;/p&gt;
&lt;p&gt;As you'll now note, the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-specific partition function gives a result equivalent to that of the marginalized-over-&lt;span class="math"&gt;\(y\)&lt;/span&gt; joint distribution: a scalar value &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; with which to normalize the numerator. However, crucially, please keep in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The partition function is a specific component of a probabilistic model. It always yields a scalar&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Marginalization is a &lt;strong&gt;much more general&lt;/strong&gt; operation performed on a probability distribution, which yields a scalar only when the remaining variable(s) are homogeneous, i.e. each remaining column contains a single distinct value.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;In the majority of cases, marginalization will simply yield a reduced probability distribution over many value configurations, similar to the &lt;span class="math"&gt;\(P(B)\)&lt;/span&gt; example above.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;In practice, this is superfluous&lt;/h3&gt;
&lt;p&gt;If we neglect to compute &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;, i.e. if we don't normalize our joint distributions &lt;span class="math"&gt;\(P(x, y = k)\)&lt;/span&gt;, we'll be left with an invalid probability distribution &lt;span class="math"&gt;\(\tilde{P}(y\vert x)\)&lt;/span&gt; whose values do not sum to 1. This distribution might look like &lt;code&gt;P(y|x) = {'red': .00047, 'green': .0011, 'blue': .0000853}&lt;/code&gt;. &lt;em&gt;If our goal is to simply compute the most likely label, taking the argmax of this unnormalized distribution works just fine.&lt;/em&gt; This follows trivially from our Bayesian pool ring:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{y}{\arg\max}\ \frac{P(x, y)}{P(x)} = \underset{y}{\arg\max}\ P(x, y)
$$&lt;/div&gt;
&lt;h1&gt;"Fully Bayesian learning"&lt;/h1&gt;
&lt;p&gt;We previously lamented the shortcomings of "lowly point estimates" and sang the praises of inferring the full distribution instead. Unfortunately, this is often a computationally-hard thing to do.&lt;/p&gt;
&lt;p&gt;To see why, let's revisit Bayes' theorem. Assume we are estimating the parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; of a softmax regression model and have placed a prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. In concrete terms, this estimate can be written as &lt;span class="math"&gt;\(P(\theta\vert D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)})))\)&lt;/span&gt;: the distribution over our belief in the true value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given the data we've observed. Bayes' theorem allows us to expand this quantity into:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\theta\vert D) = \frac{P(D\vert\theta)P(\theta)}{P(D)}
$$&lt;/div&gt;
&lt;p&gt;Previously, we computed a "lowly point estimate" for this distribution — the MAP — as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\theta_{MAP}
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(y^{(i)}\vert x^{(i)}; \theta)P(\theta)\\
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(\theta\vert (y^{(i)}, x^{(i)}))\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;While &lt;span class="math"&gt;\(P(y^{(i)}\vert x^{(i)}; \theta)P(\theta) \neq P(\theta\vert (y^{(i)}, x^{(i)}))\)&lt;/span&gt;, the argmaxes of the respective products &lt;em&gt;are&lt;/em&gt; equal. For this reason, we were able to compute a point estimate for &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;, i.e. a "summarization" of &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; in a single value, without ever computing the denominator &lt;span class="math"&gt;\(P(D)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(As a brief aside, please note that we could summarize &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; with &lt;em&gt;any&lt;/em&gt; single value from this distribution. We often select the maximum likelihood estimate — the single value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that most likely gave rise to our data, or the MAP — the single value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that both most likely gave rise to our data and most plausibly occurred itself.)&lt;/p&gt;
&lt;p&gt;To compute &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; — trivially, a full distribution as the term suggests — we will need to compute &lt;span class="math"&gt;\(P(D)\)&lt;/span&gt; after all. As before, this can be accomplished via marginalization:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(\theta\vert D)
&amp;amp;= \frac{P(D\vert\theta)P(\theta)}{P(D)}\\
&amp;amp;= \frac{P(D, \theta)}{P(D)}\\
&amp;amp;= \frac{P(D, \theta)}{\int P(D, \theta)d\theta}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; takes continuous values, we can no longer employ the "delete and collapse" method of marginalization in discrete distributions. Furthermore, in all but trivial cases, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a high-dimensional vector or matrix, leaving us to compute a "high-dimensional integral that lacks an analytic (closed-form) solution — the central computational challenge in inference."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As such, computing the full distribution &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; is &lt;em&gt;approximating&lt;/em&gt; the full distribution &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;. To this end, we'll introduce two new families of algorithms.&lt;/p&gt;
&lt;h2&gt;Markov chain monte carlo&lt;/h2&gt;
&lt;p&gt;In small to medium-sized models, we often take an alternative ideological approach to approximating &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;: instead of computing a distribution, i.e. the canonical parameters of a gory algebraic expression which control its shape — we produce &lt;em&gt;samples&lt;/em&gt; from this distribution. Roughly speaking, the aggregate of these samples then gives, retrodictively, the distribution itself. The general family of these methods is known as Markov chain monte carlo, or MCMC.&lt;/p&gt;
&lt;p&gt;In simple terms, MCMC inference for a given parameter &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; to some value &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Compute the prior probability of &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt; and the probability of having observed our data under &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt; — &lt;span class="math"&gt;\(P(\phi_{\text{current}})\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(D\vert \phi_{\text{current}})\)&lt;/span&gt;, respectively. Their product gives &lt;span class="math"&gt;\(P(D, \phi_{\text{current}})\)&lt;/span&gt; — the joint probability of having observed the proposed parameter value and our observed data given this value.&lt;/li&gt;
&lt;li&gt;Add &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt; to a big green plastic bucket of "accepted values."&lt;/li&gt;
&lt;li&gt;Propose moving to a new, nearby value &lt;span class="math"&gt;\(\phi_{\text{proposal}}\)&lt;/span&gt;. This value is drawn from an entirely separate &lt;em&gt;sampling distribution&lt;/em&gt; which bears no influence on our prior &lt;span class="math"&gt;\(P(\phi)\)&lt;/span&gt; nor likelihood function &lt;span class="math"&gt;\(P(D\vert \phi)\)&lt;/span&gt;. Repeat Step 2 using &lt;span class="math"&gt;\(\phi_{\text{proposal}}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Walk the following tree:&lt;ul&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(P(D, \phi_{\text{proposal}}) \gt P(D, \phi_{\text{current}})\)&lt;/span&gt;:&lt;ul&gt;
&lt;li&gt;Set &lt;span class="math"&gt;\(\phi_{\text{current}} = \phi_{\text{proposal}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Move to Step 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Else:&lt;ul&gt;
&lt;li&gt;With some small probability:&lt;ul&gt;
&lt;li&gt;Set &lt;span class="math"&gt;\(\phi_{\text{current}} = \phi_{\text{proposal}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Move to Step 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Else:&lt;ul&gt;
&lt;li&gt;Move to Step 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After collecting a few thousand samples — and discarding the first few hundred, in which we drunkenly amble towards the region of high joint probability (a quantity &lt;em&gt;proportional&lt;/em&gt; to the posterior probability) — we now have a bucket of samples from our desired posterior distribution. Nota bene: we never had to touch the high-dimensional integral &lt;span class="math"&gt;\(\int P(D, \theta)d\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Variational inference&lt;/h2&gt;
&lt;p&gt;In large-scale models, MCMC methods are often too slow. Conversely, variational inference provides a framework for casting the problem of posterior approximation as one of &lt;em&gt;optimization&lt;/em&gt; — far faster than a sampling-based approach. This yields an &lt;em&gt;analytical&lt;/em&gt; approximation to &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;. The following explanation of variational inference is taken largely from a previous post of mine: &lt;a href="https://willwolf.io/2017/05/08/transfer-learning-flight-delay-prediction/"&gt;Transfer Learning for Flight Delay Prediction&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our approximating distribution we'll choose one that is simple, parametric and familiar: the normal (Gaussian) distribution, parameterized by some set of parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}(\theta\vert D)$$&lt;/div&gt;
&lt;p&gt;Our goal is to force this distribution to closely resemble the original; the &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL divergence&lt;/a&gt; quantifies their difference:&lt;/p&gt;
&lt;div class="math"&gt;$$KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D)) = \int{q_{\lambda}(\theta\vert D)\log\frac{q_{\lambda}(\theta\vert D)}{P(\theta\vert D)}d\theta}$$&lt;/div&gt;
&lt;p&gt;To this end, we compute its argmin with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}^{*}(\theta\vert D) = \underset{\lambda}{\arg\min}\ \text{KL}(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D))$$&lt;/div&gt;
&lt;p&gt;Expanding the divergence, we obtain:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D))
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\log\frac{q_{\lambda}(\theta\vert D)}{P(\theta\vert D)}d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\log\frac{q_{\lambda}(\theta\vert D)P(D)}{P(\theta, D)}d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D) -\log{P(\theta, D)} + \log{P(D)}}\bigg)d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta + \log{P(D)}\int{q_{\lambda}(\theta\vert D)d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta + \log{P(D)} \cdot 1
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since only the integral depends on &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, minimizing the entire expression with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; amounts to minimizing this term. Incidentally, the opposite (negative) of this term is called the &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;ELBO&lt;/a&gt;, or the "evidence lower bound."&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO(\lambda) = -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta
$$&lt;/div&gt;
&lt;p&gt;To see why, let's plug the ELBO into the equation above and solve for &lt;span class="math"&gt;\(\log{P(D)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\log{P(D)} = ELBO(\lambda) + KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D))$$&lt;/div&gt;
&lt;p&gt;In English: "the log of the evidence is at least the lower bound of the evidence plus the divergence from our (variational) approximation of the posterior &lt;span class="math"&gt;\(q_{\lambda}(\theta\vert D)\)&lt;/span&gt; to our true posterior &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;."&lt;/p&gt;
&lt;p&gt;As such, minimizing this divergence is equivalent to &lt;em&gt;maximizing&lt;/em&gt; the ELBO, as:&lt;/p&gt;
&lt;div class="math"&gt;$$
KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D)) = -ELBO(\lambda) + \log{P(D)}
$$&lt;/div&gt;
&lt;h3&gt;Optimization&lt;/h3&gt;
&lt;p&gt;Let's restate the equation for the ELBO and rearrange further:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
ELBO(\lambda)
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta\\
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(D\vert \theta)} - \log{P(\theta)}}\bigg)d\theta\\
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} - \log{P(\theta)}}\bigg)d\theta + \log{P(D\vert \theta)}\int{q_{\lambda}(\theta\vert D)d\theta}\\
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\log{\frac{q_{\lambda}(\theta\vert D)}{P(\theta)}}d\theta} + \log{P(D\vert \theta)} \cdot 1\\
&amp;amp;= \log{P(D\vert \theta)} -KL(q_{\lambda}(\theta\vert D)\Vert P(\theta))\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Again, our goal is to maximize this expression or minimize its opposite:&lt;/p&gt;
&lt;div class="math"&gt;$$
-\log{P(D\vert \theta)} + KL(q_{\lambda}(\theta\vert D)\Vert P(\theta))
$$&lt;/div&gt;
&lt;p&gt;One step further, we obtain:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;= -\log{P(D\vert \theta)} + q_{\lambda}(\theta\vert D)\log{q_{\lambda}(\theta\vert D)} - q_{\lambda}(\theta\vert D)\log{P(\theta)}\\
&amp;amp;= \mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[-\log{P(D\vert \theta)} +\log{q_{\lambda}(\theta\vert D)} - \log{P(\theta)}]\\
&amp;amp;= \mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[-\big(\log{P(D,  \theta)} -\log{q_{\lambda}(\theta\vert D)}\big)]\\
&amp;amp;= -\mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[\log{P(D,  \theta)}] + \mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[\log{q_{\lambda}(\theta\vert D)}]\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In machine learning parlance: "minimize the negative log joint probability of our data and parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; — a MAP estimate — plus the entropy of our variational approximation." As a &lt;em&gt;higher&lt;/em&gt; entropy is desirable — an approximation which distributes its mass in a &lt;em&gt;conservative&lt;/em&gt; fashion — this minimization is a balancing act between the two terms.&lt;/p&gt;
&lt;p&gt;For a more in-depth discussion of both entropy and KL-divergence please see &lt;a href="https://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;Minimizing the Negative Log-Likelihood, in English&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Posterior predictive distribution&lt;/h1&gt;
&lt;p&gt;With our estimate for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; as a full distribution, we can now make a new prediction as a full distribution as well.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert x, D)
&amp;amp;= \int P(y\vert x, D, \theta)P(\theta\vert x, D)d\theta\\
&amp;amp;= \int P(y\vert x, \theta)P(\theta\vert D)d\theta\\
\end{align*}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The right term under the integral is the posterior distribution of our parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given the "training" data, &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;. Since it does not depend on a new input &lt;span class="math"&gt;\(x\)&lt;/span&gt; we have removed &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The left term under the integral is our likelihood function: given an &lt;span class="math"&gt;\(x\)&lt;/span&gt; and a &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, it produces a &lt;span class="math"&gt;\(y\)&lt;/span&gt;. While this function does depend on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; — whose values are pulled from our posterior &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; — it does not depend on &lt;span class="math"&gt;\(D\)&lt;/span&gt; itself. As such, we have removed &lt;span class="math"&gt;\(D\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Integrating over &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; yields a distribution over &lt;span class="math"&gt;\(y\)&lt;/span&gt;: we've now captured not just the uncertainty in &lt;em&gt;inference&lt;/em&gt;, but also the corresponding uncertain in our &lt;em&gt;predictions&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;What do these distributions actually do for me?&lt;/h1&gt;
&lt;p&gt;Said differently, "why is it important to quantify uncertainty?"&lt;/p&gt;
&lt;p&gt;I think we, as humans, are exceptionally qualified to answer this question: we need to look no further than ourselves, our choices, our environment.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The cross-walk says "go." Do I:&lt;ul&gt;
&lt;li&gt;Close my eyes, lie down for a 15-second nap in the middle of the road, then walk backwards the rest of the way?&lt;/li&gt;
&lt;li&gt;Quickly look both ways then walk leisurely across the road, keeping an eye out for cyclists at the same time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A company emails to say "we'd like to discuss the possibility of a full-time role." Do I:&lt;ul&gt;
&lt;li&gt;Respond saying "Great! Let's chat further" while continuing to speak with other companies.&lt;/li&gt;
&lt;li&gt;Respond saying "Great! Let's chat further" and promptly sever all contact with other companies.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An extremely reliable lifelong friend calls to say they've found me a beautiful studio in Manhattan for $600/month, and would need to confirm in the next 24 hours if I'd like to take it. Do I:&lt;ul&gt;
&lt;li&gt;Take it.&lt;/li&gt;
&lt;li&gt;Call three friends to ask if they think that this makes sense.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An extremely sketchy real estate broker calls to say they've found me a beautiful studio in Manhattan for $600/month, and would need to confirm in the next 24 hours if I'd like to take it. Do I:&lt;ul&gt;
&lt;li&gt;Take it.&lt;/li&gt;
&lt;li&gt;Call three friends to ask if they think that this makes sense.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The notion is the same in probabilistic modeling. Furthermore, we often build models with "not big data," and therefore have a substantially non-zero amount of uncertainty in our parameter estimates and subsequent predictions.&lt;/p&gt;
&lt;p&gt;Finally, with distributional estimates in hand, we can begin to make more robust, measured and logical decisions. We can do this because, while point estimates give a quick summary of the dynamics of our system, distributions tell the full, thorough story: where the peaks are, their width and height, their distance from one another, etc. For an excellent exploration of what we can do with posterior distributions, check out Rasmus Bååth's &lt;a href="http://www.sumsar.net/blog/2015/01/probable-points-and-credible-intervals-part-two/"&gt;Probable Points and Credible Intervals, Part 2: Decision Theory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Many thanks for reading, and to our pool ring Bayes'.&lt;/p&gt;
&lt;p&gt;&lt;img alt="girls having drinks on pool rings" class="img-responsive" src="https://ak8.picdn.net/shutterstock/videos/19157749/thumb/2.jpg"/&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://edwardlib.org/tutorials/inference"&gt;Edward — Inference of Probabilistic Models&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Minimizing the Negative Log-Likelihood, in English</title><link href="https://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/" rel="alternate"></link><published>2017-05-18T12:24:00-04:00</published><updated>2017-05-18T12:24:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-05-18:/2017/05/18/minimizing_the_negative_log_likelihood_in_english/</id><summary type="html">&lt;p&gt;Statistical underpinnings of the machine learning models we know and love. A walk through random variables, entropy, exponential family distributions, generalized linear models, maximum likelihood estimation, cross entropy, KL-divergence, maximum a posteriori estimation and going "fully Bayesian."&lt;/p&gt;</summary><content type="html">&lt;p&gt;Roughly speaking, my machine learning journey began on &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt;. "There's data, a model (i.e. estimator) and a loss function to optimize," I learned. "Regression models predict continuous-valued real numbers; classification models predict 'red,' 'green,' 'blue.' Typically, the former employs the mean squared error or mean absolute error; the latter, the cross-entropy loss. Stochastic gradient descent updates the model's parameters to drive these losses down." Furthermore, to fit these models, just &lt;code&gt;import sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A dexterity with the above is often sufficient for—at least from a technical stance—both employment and impact as a data scientist. In industry, commonplace prediction and inference problems—binary churn, credit scoring, product recommendation and A/B testing, for example—are easily matched with an off-the-shelf algorithm plus proficient data scientist for a measurable boost to the company's bottom line. In a vacuum I think this is fine: the winning driver does not &lt;em&gt;need&lt;/em&gt; to know how to build the car. Surely, I've been this person before.&lt;/p&gt;
&lt;p&gt;Once fluid with "scikit-learn fit and predict," I turned to statistics. I was always aware that the two were related, yet figured them ultimately parallel sub-fields of my job. With the former, I build classification models; with the latter, I infer signup counts with the Poisson distribution and MCMC—right?&lt;/p&gt;
&lt;p&gt;Before long, I dove deeper into machine learning—reading textbooks, papers and source code and writing this blog. Therein, I began to come across &lt;em&gt;terms I didn't understand used to describe the things that I did.&lt;/em&gt; "I understand what the categorical cross-entropy loss is, what it does and how it's defined," for example: &lt;em&gt;&lt;strong&gt;"why are you calling it the negative log-likelihood?"&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Marginally wiser, I now know two truths about the above:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Techniques we anoint as "machine learning"—classification and regression models, notably—have their underpinnings almost entirely in statistics. For this reason, terminology often flows between the two.&lt;/li&gt;
&lt;li&gt;None of this stuff is new.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The goal of this post is to take three models we know, love, and know how to use and explain what's really going on underneath the hood. I will assume the reader is familiar with concepts in both machine learning and statistics, and comes in search of a deeper understanding of the connections therein. There will be math—but only as much as necessary. Most of the derivations can be skipped without consequence.&lt;/p&gt;
&lt;p&gt;When deploying a predictive model in a production setting, it is generally in our best interest to &lt;code&gt;import sklearn&lt;/code&gt;, i.e. use a model that someone else has built. This is something we already know how to do. As such, this post will start and end here: your head is currently above water; we're going to dive into the pool, touch the bottom, then work our way back to the surface. Lemmas will be written in &lt;em&gt;&lt;strong&gt;bold&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bottom of pool" class="img-responsive" src="https://previews.123rf.com/images/cookelma/cookelma1603/cookelma160300003/53105871-man-sitting-on-the-bottom-of-the-swimming-pool-under-water.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;First, let's meet our three protagonists. We'll define them in &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; for the illustrative purpose of a unified and idiomatic API.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/"&gt;Linear regression&lt;/a&gt; with mean squared error&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/"&gt;Logistic regression&lt;/a&gt; with binary cross-entropy loss&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/"&gt;Softmax regression&lt;/a&gt; with categorical cross-entropy loss&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we'll select four components key to each: its response variable, functional form, loss function and loss function plus regularization term. For each model, we'll describe the statistical underpinnings of each component—the steps on the ladder towards the surface of the pool.&lt;/p&gt;
&lt;p&gt;Before diving in, we'll need to define a few important concepts.&lt;/p&gt;
&lt;h2&gt;Random variable&lt;/h2&gt;
&lt;p&gt;I define a random variable as "a thing that can take on a bunch of different values."&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"The tenure of despotic rulers in Central Africa" is a random variable. It could take on values of 25.73 years, 14.12 years, 8.99 years, ad infinitum; it could not take on values of 1.12 million years, nor -5 years.&lt;/li&gt;
&lt;li&gt;"The height of the next person to leave the supermarket" is a random variable.&lt;/li&gt;
&lt;li&gt;"The color of shirt I wear on Mondays" is a random variable. (Incidentally, this one only has ~3 distinct values.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Probability distribution&lt;/h2&gt;
&lt;p&gt;A probability distribution is a lookup table for the likelihood of observing each unique value of a random variable. Assuming a given variable can take on values in &lt;span class="math"&gt;\(\{\text{rain, snow, sleet, hail}\}\)&lt;/span&gt;, the following is a valid probability distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Trivially, these values must sum to 1.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;probability mass function&lt;/em&gt; is a probability distribution for a discrete-valued random variable.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;probability density function&lt;/em&gt; &lt;em&gt;&lt;strong&gt;gives&lt;/strong&gt;&lt;/em&gt; a probability distribution for a continuous-valued random variable.&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Gives&lt;/em&gt;, because this function itself is not a lookup table. Given a random variable that takes on values in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;, we do not and cannot define &lt;span class="math"&gt;\(\Pr(X = 0.01)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Pr(X = 0.001)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Pr(X = 0.0001)\)&lt;/span&gt;, etc.&lt;/li&gt;
&lt;li&gt;Instead, we define a function that tells us the probability of observing a value within a certain &lt;em&gt;range&lt;/em&gt;, i.e. &lt;span class="math"&gt;\(\Pr(0.01 &amp;lt; X &amp;lt; .4)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;This is the probability density function, where &lt;span class="math"&gt;\(\Pr(0 \leq X \leq 1) = 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Entropy&lt;/h2&gt;
&lt;p&gt;Entropy quantifies the number of ways we can reach a given outcome. Imagine 8 friends are splitting into 2 taxis en route to a Broadway show. Consider the following two scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Four friends climb into each taxi.&lt;/em&gt; We could accomplish this with the following assignments:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# fill the first, then the second&lt;/span&gt;
&lt;span class="n"&gt;assignment_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# alternate assignments&lt;/span&gt;
&lt;span class="n"&gt;assignment_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# alternate assignments in batches of two&lt;/span&gt;
&lt;span class="n"&gt;assignment_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# etc.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;All friends climb into the first taxi.&lt;/em&gt; There is only one possible assignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;assignment_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(In this case, the Broadway show is probably in &lt;a href="http://willtravellife.com/2013/04/how-does-a-west-african-bush-taxi-work/"&gt;West Africa&lt;/a&gt; or a similar part of the world.)&lt;/p&gt;
&lt;p&gt;Since there are more ways to reach the first outcome than there are the second, the first outcome has a higher entropy.&lt;/p&gt;
&lt;h3&gt;More explicitly&lt;/h3&gt;
&lt;p&gt;We compute entropy for probability distributions. This computation is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p) = -\sum\limits_{i=1}^{n} p_i \log{p_i}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are &lt;span class="math"&gt;\(n\)&lt;/span&gt; distinct events.&lt;/li&gt;
&lt;li&gt;Each event &lt;span class="math"&gt;\(i\)&lt;/span&gt; has probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entropy is the &lt;em&gt;weighted-average log probability&lt;/em&gt; over possible events—this much reads directly from the equation—which measures the &lt;em&gt;uncertainty inherent in their probability distribution.&lt;/em&gt; The higher the entropy, the less certain we are about the value we're going to get.&lt;/p&gt;
&lt;p&gt;Let's calculate the entropy of our distribution above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob_dist&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prob_dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mf"&gt;1.1055291211185652&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For comparison, let's assume two more distributions and calculate their respective entropies.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;p_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mf"&gt;0.8304250977453105&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mf"&gt;0.2460287703075343&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the first distribution, we are least certain as to what tomorrow's weather will bring. As such, this has the highest entropy. In the third distribution, we are almost certain it's going to hail. As such, this has the lowest entropy.&lt;/p&gt;
&lt;p&gt;Finally, it is a probability distribution that dictates the different taxi assignments just above. A distribution for a random variable that has many possible outcomes has a higher entropy than a distribution that gives only one.&lt;/p&gt;
&lt;p&gt;Now, let's dive into the pool. We'll start at the bottom and work our way back to the top.&lt;/p&gt;
&lt;h1&gt;Response variable&lt;/h1&gt;
&lt;p&gt;Roughly speaking, each model looks as follows. It is a diamond that receives an input and produces an output.&lt;/p&gt;
&lt;p&gt;&lt;img alt="simple input/output model" class="img-responsive" src="https://willwolf.io/images/simple_input_output_model.png"/&gt;&lt;/p&gt;
&lt;p&gt;The models differ in the type of response variable they predict, i.e. the &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression predicts a continuous-valued real number. Let's call it &lt;code&gt;temperature&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Logistic regression predicts a binary label. Let's call it &lt;code&gt;cat or dog&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Softmax regression predicts a multi-class label. Let's call it &lt;code&gt;red or green or blue&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each model, the response variable can take on a bunch of different values. In other words, they are &lt;em&gt;random variables.&lt;/em&gt; What probability distribution is associated with each?&lt;/p&gt;
&lt;p&gt;Unfortunately, we don't know. All we do know, in fact, is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;temperature&lt;/code&gt; has an underlying true mean &lt;span class="math"&gt;\(\mu \in (-\infty, \infty)\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2 \in (0, \infty)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cat or dog&lt;/code&gt; takes on the value &lt;code&gt;cat&lt;/code&gt; or &lt;code&gt;dog&lt;/code&gt;. The likelihood of observing each outcome does not change over time, in the same way that &lt;span class="math"&gt;\(\Pr(\text{heads})\)&lt;/span&gt; for a fair coin is always &lt;span class="math"&gt;\(0.5\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;red or green or blue&lt;/code&gt; takes on the value &lt;code&gt;red&lt;/code&gt; or &lt;code&gt;green&lt;/code&gt; or &lt;code&gt;blue&lt;/code&gt;. The likelihood of observing each outcome does not change over time, in the same way that the probability of rolling a given number on a fair die is always &lt;span class="math"&gt;\(\frac{1}{6}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For clarity, each one of these assumptions is utterly banal. Nonetheless, &lt;em&gt;can we use them nonetheless to select probability distributions for our random variables?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Maximum entropy distributions&lt;/h2&gt;
&lt;p&gt;Consider another continuous-valued random variable: "Uber's yearly profit." Like &lt;code&gt;temperature&lt;/code&gt;, it also has an underlying true mean &lt;span class="math"&gt;\(\mu \in (-\infty, \infty)\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2 \in (0, \infty)\)&lt;/span&gt;. Trivially, the respective means and variances will be different. Assume we observe 10 (fictional) values of each that look as follows:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;uber&lt;/th&gt;
&lt;th&gt;temperature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-100&lt;/td&gt;
&lt;td&gt;-50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-80&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-20&lt;/td&gt;
&lt;td&gt;56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-10&lt;/td&gt;
&lt;td&gt;63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;-43&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plotting, we get:&lt;/p&gt;
&lt;p&gt;&lt;img alt="temperature random variable" class="img-responsive" src="https://willwolf.io/figures/temperature_random_variable.png"/&gt;
&lt;img alt="uber random variable" class="img-responsive" src="https://willwolf.io/figures/uber_random_variable.png"/&gt;&lt;/p&gt;
&lt;p&gt;We are not given the true underlying probability distribution associated with each random variable—not its general "shape," nor the parameters that control this shape. We will &lt;em&gt;never&lt;/em&gt; be given these things, in fact: the point of statistics is to infer what they are.&lt;/p&gt;
&lt;p&gt;To make an initial choice we keep two things in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;We'd like to be conservative&lt;/em&gt;. We've only seen ten values of "Uber's yearly profit;" we don't want to discount the fact that the next twenty could fall into &lt;span class="math"&gt;\([-60, -50]\)&lt;/span&gt; just because they haven't yet been observed.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;We need to choose the same probability distribution "shape" for both random variables, as we've made identical assumptions for each&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As such, we'd like the most conservative distribution that obeys the "utterly banal" constraints stated above. This is the &lt;a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution"&gt;&lt;em&gt;maximum entropy distribution&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;temperature&lt;/code&gt;, the maximum entropy distribution is the &lt;a href="https://en.wikipedia.org/wiki/Normal_distribution"&gt;Gaussian distribution&lt;/a&gt;. Its probability density function is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\bigg(-\frac{(y - \mu)^2}{2\sigma^2}\bigg)}
$$&lt;/div&gt;
&lt;p&gt;For &lt;code&gt;cat or dog&lt;/code&gt;, it is the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution"&gt;binomial distribution&lt;/a&gt;. Its probability mass function  (for a single observation) is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;(I've written the probability of the positive event as &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;, e.g. &lt;span class="math"&gt;\(\phi = .5\)&lt;/span&gt; for a fair coin.)&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;red or green or blue&lt;/code&gt;, it is the &lt;a href="https://en.wikipedia.org/wiki/Multinomial_distribution"&gt;multinomial distribution&lt;/a&gt;. Its probability mass function (for a single observation) is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
\phi_{\text{red}} &amp;amp; \text{outcome = red}\\
\phi_{\text{green}} &amp;amp; \text{outcome = green}\\
1 - \phi_{\text{red}} - \phi_{\text{green}} &amp;amp; \text{outcome = blue}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;While it may seem like we've "waved our hands" over the connection between the stated equality constraints for the response variable of each model and the respective distributions we've selected, it is &lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange multipliers&lt;/a&gt; that succinctly and algebraically bridge this gap. This &lt;a href="https://www.dsprelated.com/freebooks/sasp/Maximum_Entropy_Property_Gaussian.html"&gt;post&lt;/a&gt; gives a terrific example of this derivation. I've chosen to omit it as I did not feel it would contribute to the clarity nor direction of this post.&lt;/p&gt;
&lt;p&gt;Finally, while we do assume that a Gaussian dictates the true distribution of values of both "Uber's yearly profit" and &lt;code&gt;temperature&lt;/code&gt;, it is, trivially, a different Gaussian for each. This is because each random variable has its own true underlying mean and variance. These values make the respective Gaussians taller or wider—shifted left or shifted right.&lt;/p&gt;
&lt;h1&gt;Functional form&lt;/h1&gt;
&lt;p&gt;Our three protagonists generate predictions via distinct functions: the &lt;a href="https://en.wikipedia.org/wiki/Identity_function"&gt;identity function&lt;/a&gt; (i.e. a no-op), the &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid function&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax function&lt;/a&gt;, respectively. The Keras output layers make this clear:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this section, I'd like to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Show how each of the Gaussian, binomial and multinomial distributions can be reduced to the same functional form.&lt;/li&gt;
&lt;li&gt;Show how this common functional form allows us to naturally derive the output functions for our three protagonist models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Exponential family distributions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In probability and statistics, an &lt;a href="https://en.wikipedia.org/wiki/Exponential_family"&gt;"exponential family"&lt;/a&gt; is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, on account of some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Wikipedia&lt;/p&gt;
&lt;p&gt;I don't relish quoting this paragraph—and especially one so deliriously ambiguous. This said, the reality is that exponential functions provide, at a minimum, a unifying framework for deriving the canonical activation and loss functions we've come to know and love. To move forward, we simply have to cede that the "mathematical conveniences, on account of some useful algebraic properties, etc." that motivate this "certain form" are not totally heinous nor misguided.&lt;/p&gt;
&lt;p&gt;A distribution belongs to the exponential family if it can be written in the following form:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y; \eta) = b(y)\exp(\eta^T T(y) - a(\eta))
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta\)&lt;/span&gt; is the &lt;em&gt;canonical parameter&lt;/em&gt; of the distribution. (We will hereby work with the single-canonical-parameter exponential family form.)&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y)\)&lt;/span&gt; is the &lt;em&gt;sufficient statistic&lt;/em&gt;. It is often the case that &lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; is the &lt;em&gt;log partition function&lt;/em&gt;, which normalizes the distribution. (A more in-depth discussion of this normalizing constant can be found in a previous post of mine: &lt;a href="https://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/"&gt;Deriving the Softmax from First Principles&lt;/a&gt;.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;"A fixed choice of &lt;span class="math"&gt;\(T\)&lt;/span&gt;, &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; defines a family (or set) of distributions that is parameterized by &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;; as we vary &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;, we then get different distributions within this family."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; This simply means that a coin with &lt;span class="math"&gt;\(\Pr(\text{heads}) = .6\)&lt;/span&gt; gives a different distribution over outcomes than one with &lt;span class="math"&gt;\(\Pr(\text{heads}) = .7\)&lt;/span&gt;. Easy.&lt;/p&gt;
&lt;h3&gt;Gaussian distribution&lt;/h3&gt;
&lt;p&gt;Since we're working with the single-parameter form, we'll assume that &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; is known and equals &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert \mu, \sigma^2)
&amp;amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\bigg(-\frac{(y - \mu)^2}{2\sigma^2}\bigg)}\\
&amp;amp;= \frac{1}{\sqrt{2\pi}}\exp{\bigg(-\frac{(y - \mu)^2}{2}\bigg)}\\
&amp;amp;= \frac{1}{\sqrt{2\pi}}\exp{\bigg(-\frac{1}{2}(y^2 - 2\mu y + \mu^2)\bigg)}\\
&amp;amp;= \frac{1}{\sqrt{2\pi}}\exp{\bigg(-\frac{1}{2}y^2\bigg)} \cdot \exp{\bigg(\mu y - \frac{1}{2}\mu^2\bigg)}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta = \mu\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta) = \frac{1}{2}\mu^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(b(y) = \frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}y^2)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we'll express &lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; itself:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
a(\eta)
&amp;amp;= \frac{1}{2}\mu^2\\
&amp;amp;= \frac{1}{2}\eta^2
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Binomial distribution&lt;/h3&gt;
&lt;p&gt;We previously defined the binomial distribution (for a single observation) in a crude, piecewise form. We'll now define it in a more compact form which will make it easier to show that it is a member of the exponential family. Again, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gives the probability of observing the true class, i.e. &lt;span class="math"&gt;\(\Pr(\text{cat}) = .7 \implies \phi = .3\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert \phi)
&amp;amp;= \phi^y(1-\phi)^{1-y}\\
&amp;amp;= \exp\bigg(\log\bigg(\phi^y(1-\phi)^{1-y}\bigg)\bigg)\\
&amp;amp;= \exp\bigg(y\log{\phi} + \log(1-\phi) - y\log(1-\phi)\bigg)\\
&amp;amp;= \exp\bigg(\log\bigg(\frac{\phi}{1-\phi}\bigg)y + \log(1-\phi)\bigg) \\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta = \log\bigg(\frac{\phi}{1-\phi}\bigg)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta) = -\log(1-\phi)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(b(y) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we'll express &lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;, i.e. the parameter that this distribution accepts:&lt;/p&gt;
&lt;div class="math"&gt;$$
\eta = \log\bigg(\frac{\phi}{1-\phi}\bigg) \implies \phi = \frac{1}{1 + e^{-\eta}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
a(\eta)
&amp;amp;= -\log(1-\phi)\\
&amp;amp;= -\log\bigg(1-\frac{1}{1 + e^{-\eta}}\bigg)\\
&amp;amp;= -\log\bigg(\frac{1}{1 + e^{\eta}}\bigg)\\
&amp;amp;= \log(1 + e^{\eta})\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;You will recognize our expression for &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;—the probability of observing the true class—as the sigmoid function.&lt;/p&gt;
&lt;h3&gt;Multinomial distribution&lt;/h3&gt;
&lt;p&gt;Like the binomial distribution, we'll first rewrite the multinomial (for a single observation) in a more compact form. &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; gives a vector of class probabilities for the &lt;span class="math"&gt;\(K\)&lt;/span&gt; classes; &lt;span class="math"&gt;\(k\)&lt;/span&gt; denotes one of these classes.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \pi) = \prod\limits_{k=1}^{K}\pi_k^{y_k}
$$&lt;/div&gt;
&lt;p&gt;This is almost pedantic: it says that &lt;span class="math"&gt;\(\Pr(y=k)\)&lt;/span&gt; equals the probability of observing class &lt;span class="math"&gt;\(k\)&lt;/span&gt;. For example, given&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;we would compute:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\Pr(y = \text{snow} = [0, 1, 0, 0])
&amp;amp;= (.14^0 * .37^1 * .03^0 * .46^0)\\
&amp;amp;= .37\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Expanding into the exponential family form gives:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert \pi)
&amp;amp;= \prod\limits_{k=1}^{K}\pi_k^{y_k}\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K}y_k\log{\pi_k}\bigg)\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K-1}y_k\log{\pi_k} + \bigg(1 - \sum\limits_{k=1}^{K-1}y_k\bigg)\log\bigg(1 - \sum\limits_{k=1}^{K-1}\pi_k\bigg)\bigg)\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K-1}y_k\log{\pi_k} - \bigg(\sum\limits_{k=1}^{K-1}y_k\bigg) \log(\pi_K) + \log(\pi_K)), \quad \text{where}\ \pi_K = 1 - \sum\limits_{k=1}^{K-1}\pi_k\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K-1}\log\bigg(\frac{\pi_k}{\pi_K}\bigg) y_k + \log(\pi_K)\bigg)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta_k = \log\bigg(\frac{\pi_k}{\pi_K}\bigg)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta) = -\log(\pi_K)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(b(y) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we'll express &lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;, i.e. the parameter that this distribution accepts:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\eta_k
  &amp;amp;= \log\bigg(\frac{\pi_k}{\pi_K}\bigg) \implies\\
\frac{\pi_k}{\pi_K}
  &amp;amp;= e^{\eta_k} \implies\\
\sum\limits_{k=1}^K \frac{\pi_k}{\pi_K}
  &amp;amp;= \sum\limits_{k=1}^K e^{\eta_k} \implies\\
\frac{1}{\pi_K}\sum\limits_{k=1}^K \pi_k
  &amp;amp;= \sum\limits_{k=1}^K e^{\eta_k} \implies\\
\frac{1}{\pi_K} \cdot 1
  &amp;amp;= \sum\limits_{k=1}^K e^{\eta_k} \implies\\
\pi_K
  &amp;amp;= \frac{1}{\sum\limits_{k=1}^K e^{\eta_k}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Plugging back into the second line we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\pi_k}{\frac{1}{\sum\limits_{k=1}^K e^{\eta_k}}}
  &amp;amp;= e^{\eta_k}\ \implies\\
\pi_k
  &amp;amp;= \frac{e^{\eta_k}}{\sum\limits_{k=1}^K e^{\eta_k}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This you will recognize as the softmax function. (For a probabilistically-motivated derivation, please see a previous &lt;a href="https://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/"&gt;post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Finally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\pi_k}{\pi_K}
  &amp;amp;= e^{\eta_k} \implies\\
\frac{\pi_K}{\pi_K}
  &amp;amp;= e^{\eta_K} \implies\\
\eta_K &amp;amp;= 0\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
a(\eta)
&amp;amp;= -\log(\pi_K)\\
&amp;amp;= \log(\pi_K^{-1})\\
&amp;amp;= \log\Bigg(\frac{\sum\limits_{k=1}^K e^{\eta_k}}{e^{\eta_K}}\Bigg)\\
&amp;amp;= \log\Bigg(\sum\limits_{k=1}^K e^{\eta_k}\Bigg)\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;Generalized linear models&lt;/h2&gt;
&lt;p&gt;Each protagonist model outputs a response variable that is distributed according to some (exponential family) distribution. However, the &lt;em&gt;canonical parameter&lt;/em&gt; of this distribution, i.e. the thing we pass in, will &lt;em&gt;vary per observation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Consider the logistic regression model that's predicting &lt;code&gt;cat or dog&lt;/code&gt;. If we input a picture of a cat, we'll output "cat" according to the stated distribution.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;If we input a picture of a dog, we'll output "dog" according the same distribution.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Trivially, &lt;em&gt;the &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; value must be different in each case.&lt;/em&gt; In the former, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; should be small, such that we output "cat" with probability &lt;span class="math"&gt;\(1 - \phi \approx 1\)&lt;/span&gt;. In the latter, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; should be large, such that we output "dog" with probability &lt;span class="math"&gt;\(\phi \approx 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, what dictates the following?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; in the case of linear regression, in which &lt;span class="math"&gt;\(y_i \sim \mathcal{N}(\mu_i, \sigma^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; in the case of logistic regression, in which &lt;span class="math"&gt;\(y_i \sim \text{Binomial}(\phi_i, 1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\pi_i\)&lt;/span&gt; in the case of softmax regression, in which &lt;span class="math"&gt;\(y_i \sim \text{Multinomial}(\pi_i, 1)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, I've introduced the subscript &lt;span class="math"&gt;\(i\)&lt;/span&gt;. This makes explicit the &lt;code&gt;cat or dog&lt;/code&gt; dynamic from above: each input to a given model will result in its &lt;em&gt;own&lt;/em&gt; canonical parameter being passed to the distribution on the response variable. (That logistic regression better make &lt;span class="math"&gt;\(\phi_i \approx 0\)&lt;/span&gt; when looking at a picture of a cat!)&lt;/p&gt;
&lt;p&gt;Finally, how do we go from a 10-feature input &lt;span class="math"&gt;\(x\)&lt;/span&gt; to this canonical parameter? We take a linear combination:&lt;/p&gt;
&lt;div class="math"&gt;$$
\eta = \theta^Tx
$$&lt;/div&gt;
&lt;h3&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\eta = \theta^Tx = \mu_i\)&lt;/span&gt;. This is what we need for the normal distribution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; The identity function (i.e a no-op) gives us the mean of the response variable. This mean is required by the normal distribution, which dictates the outcomes of the continuous-valued target &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Logistic regression&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\eta = \theta^Tx = \log\bigg(\frac{\phi_i}{1-\phi_i}\bigg)\)&lt;/span&gt;. To solve for &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;, we solve for &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As you'll remember we did this above: &lt;span class="math"&gt;\(\phi_i = \frac{1}{1 + e^{-\eta}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; The sigmoid function gives us the probability that the response variable takes on the positive class. This probability is required by the binomial distribution, which dictates the outcomes of the binary target &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Softmax regression&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\eta = \theta^Tx = \log\bigg(\frac{\pi_k}{\pi_K}\bigg)\)&lt;/span&gt;. To solve for &lt;span class="math"&gt;\(\pi_i\)&lt;/span&gt;, i.e. the full vector of probabilities for observation &lt;span class="math"&gt;\(i\)&lt;/span&gt;, we solve for each individual probability &lt;span class="math"&gt;\(\pi_{k, i}\)&lt;/span&gt; then put them in a list.&lt;/p&gt;
&lt;p&gt;We did this above as well: &lt;span class="math"&gt;\(\pi_{k, i} = \frac{e^{\eta_k}}{\sum\limits_{k=1}^K e^{\eta_k}}\)&lt;/span&gt;. This is the softmax function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; The softmax function gives us the probability that the response variable takes on each of the possible classes. This probability mass function is required by the multinomial distribution, which dictates the outcomes of the multi-class target &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, why a linear model, i.e. why &lt;span class="math"&gt;\(\eta = \theta^Tx\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Andrew Ng calls it a "design choice."&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; I've motivated this formulation a bit in the &lt;a href="https://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/"&gt;softmax post&lt;/a&gt;. mathematicalmonk&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; would probably have a more principled explanation than us both. For now, we'll make do with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A linear combination is perhaps the simplest way to consider the impact of each feature on the canonical parameter.&lt;/li&gt;
&lt;li&gt;A linear combination commands that either &lt;span class="math"&gt;\(x\)&lt;/span&gt;, or a &lt;em&gt;function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/em&gt;, vary linearly with &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;. As such, we could write our model as &lt;span class="math"&gt;\(\eta = \theta^T\Phi(x)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; applies some complex transformation to our features. This makes the "simplicity" of the linear combination less simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Loss function&lt;/h1&gt;
&lt;p&gt;We've now discussed how each response variable is generated, and how we compute the parameters for those distributions on a per-observation basis. Now, how do we quantify how good these parameters are?&lt;/p&gt;
&lt;p&gt;To get us started, let's go back to predicting &lt;code&gt;cat or dog&lt;/code&gt;. If we input a picture of a cat, we should compute &lt;span class="math"&gt;\(\phi \approx 0\)&lt;/span&gt; given our binomial distribution.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;A perfect computation gives &lt;span class="math"&gt;\(\phi = 0\)&lt;/span&gt;. The loss function quantifies how close we got.&lt;/p&gt;
&lt;h2&gt;Maximum likelihood estimation&lt;/h2&gt;
&lt;p&gt;Each of our three random variables receives a parameter—&lt;span class="math"&gt;\(\mu, \phi\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; respectively. We then pass in a &lt;span class="math"&gt;\(y\)&lt;/span&gt;: for discrete-valued random variables, the associated probability mass function tells us the probability of observing this value; for continuous-valued random variables, the associated probability density function tells us the density of the probability space around this value (a number proportional to the probability).&lt;/p&gt;
&lt;p&gt;If we instead &lt;em&gt;fix&lt;/em&gt; &lt;span class="math"&gt;\(y\)&lt;/span&gt; and pass in varying &lt;em&gt;parameter values&lt;/em&gt;, this same function becomes a &lt;em&gt;likelihood function&lt;/em&gt;. It will tell us the likelihood of a given parameter having produced the now-fixed &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If this is not clear, consider the following example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Moroccan walks into a bar. He's wearing a football jersey that's missing a sleeve. He has a black eye, and blood on his jeans. How did he most likely spend his day?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;At home, reading a book.&lt;/li&gt;
&lt;li&gt;Training for a bicycle race.&lt;/li&gt;
&lt;li&gt;At the soccer game drinking beers with his friends—all of whom are MMA fighters that despise the other team.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;We'd like to pick the parameter that most likely gave rise to our data. This is the &lt;em&gt;maximum likelihood estimate&lt;/em&gt;. Mathematically, we define it as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{\text{parameter}}{\arg\max}\ P(y\vert \text{parameter})
$$&lt;/div&gt;
&lt;p&gt;As we've now seen (ad nauseum), &lt;span class="math"&gt;\(y\)&lt;/span&gt; depends on the parameter its generating random variable receives. Additionally, this parameter—&lt;span class="math"&gt;\(\mu, \phi\)&lt;/span&gt; or &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;—is defined in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;. Further, &lt;span class="math"&gt;\(\eta = \theta^T x\)&lt;/span&gt;. As such, &lt;span class="math"&gt;\(y\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; and the observed data &lt;span class="math"&gt;\(x\)&lt;/span&gt;. This is perhaps &lt;em&gt;the&lt;/em&gt; elementary truism of machine learning—you've known this since Day 1.&lt;/p&gt;
&lt;p&gt;Since our observed data are fixed, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is the only thing that we can vary. Let's rewrite our argmax in these terms:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\max}\ P(y\vert x; \theta)
$$&lt;/div&gt;
&lt;p&gt;Finally, this expression gives the argmax over a single data point, i.e. training observation, &lt;span class="math"&gt;\((x^{(i)}, y^{(i)})\)&lt;/span&gt;. To give the likelihood over all observations (assuming they are independent of one another, i.e. the outcome of the first observation does not impact that of the third), we take the product.&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\max} \prod\limits_{i=1}^{m}P(y^{(i)}\vert x^{(i)}; \theta)
$$&lt;/div&gt;
&lt;p&gt;The product of numbers in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt; gets very small, very quickly. Let's maximize the log-likelihood instead so we can work with sums.&lt;/p&gt;
&lt;h3&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;Maximize the log-likelihood of the Gaussian distribution. Remember, &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; assemble to give &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\theta^Tx = \mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{P(y\vert x; \theta)}
&amp;amp;= \log{\prod\limits_{i=1}^{m}P(y^{(i)}\vert x^{(i)}; \theta)}\\
&amp;amp;= \sum\limits_{i=1}^{m}\log{P(y^{(i)}\vert x^{(i)}; \theta)}\\
&amp;amp;= \sum\limits_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\bigg)}}\\
&amp;amp;= \sum\limits_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}} + \sum\limits_{i=1}^{m}\log\Bigg(\exp{\bigg(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\bigg)}\Bigg)\\
&amp;amp;= m\log{\frac{1}{\sqrt{2\pi}\sigma}} - \frac{1}{2\sigma^2}\sum\limits_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2\\
&amp;amp;= C_1 - C_2\sum\limits_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Maximizing the log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to maximizing the negative mean squared error between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction thereof.&lt;/p&gt;
&lt;p&gt;Notwithstanding, most optimization routines &lt;em&gt;minimize&lt;/em&gt;. So, for practical purposes, we go the other way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the mean squared error between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction thereof.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Logistic regression&lt;/h3&gt;
&lt;p&gt;Same thing.&lt;/p&gt;
&lt;p&gt;Negative log-likelihood:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
-\log{P(y\vert x; \theta)}
&amp;amp;= -\log{\prod\limits_{i = 1}^m(\phi^{(i)})^{y^{(i)}}(1 - \phi^{(i)})^{1 - y^{(i)}}}\\
&amp;amp;= -\sum\limits_{i = 1}^m\log{\bigg((\phi^{(i)})^{y^{(i)}}(1 - \phi^{(i)})^{1 - y^{(i)}}\bigg)}\\
&amp;amp;= -\sum\limits_{i = 1}^my^{(i)}\log{(\phi^{(i)})} + (1 - y^{(i)})\log{(1 - \phi^{(i)})}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the binary cross-entropy (i.e. binary log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability thereof.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Multinomial distribution&lt;/h3&gt;
&lt;p&gt;Negative log-likelihood:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
-\log{P(y\vert x; \theta)}
&amp;amp;= -\log\prod\limits_{i=1}^{m}\prod\limits_{k=1}^{K}\pi_k^{y_k}\\
&amp;amp;= -\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_k\log\pi_k\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the categorical cross-entropy (i.e. multi-class log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability distribution thereof.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Maximum a posteriori estimation&lt;/h1&gt;
&lt;p&gt;When estimating &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; via the MLE, we put no constraints on the permissible values thereof. More explicitly, we allow &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to be &lt;em&gt;equally likely to assume any real number&lt;/em&gt;—be it &lt;span class="math"&gt;\(0\)&lt;/span&gt;, or &lt;span class="math"&gt;\(10\)&lt;/span&gt;, or &lt;span class="math"&gt;\(-20\)&lt;/span&gt;, or &lt;span class="math"&gt;\(2.37 \times 10^{36}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In practice, this assumption is both unrealistic and impractical: typically, we do wish to constrain &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; (our weights) to a non-infinite range of values. We do this by putting a &lt;em&gt;prior&lt;/em&gt; on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Whereas the MLE computes &lt;span class="math"&gt;\(\underset{\theta}{\arg\max}\ P(y\vert x; \theta)\)&lt;/span&gt;, the maximum a posteriori estimate, or MAP, computes &lt;span class="math"&gt;\(\underset{\theta}{\arg\max}\ P(y\vert x; \theta)P(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As before, we start by taking the log. Our joint likelihood with prior now reads:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\theta_{MAP}
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(y^{(i)}\vert x^{(i)}; \theta)P(\theta)\\
&amp;amp;= \underset{\theta}{\arg\max}\ \sum\limits_{i=1}^{m} \log{P(y^{(i)}\vert x^{(i)}; \theta)} + \log{P(\theta)}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;We dealt with the left term in the previous section. Now, we'll simply tack on the log-prior to the respective log-likelihoods.&lt;/p&gt;
&lt;p&gt;As every element of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a continuous-valued real number, let's assign it a Gaussian distribution with mean 0 and variance &lt;span class="math"&gt;\(V\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta \sim \mathcal{N}(0, V)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{P(\theta\vert 0, V)}
&amp;amp;= \log\Bigg(\frac{1}{\sqrt{2\pi}V}\exp{\bigg(-\frac{(\theta - 0)^2}{2V^2}\bigg)}\Bigg)\\
&amp;amp;= \log{C_1} -\frac{\theta^2}{2V^2}\\
&amp;amp;= \log{C_1} - C_2\theta^2\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our goal is to maximize this term plus the log-likelihood—or equivalently, minimize their opposite—with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. For a final step, let's discard the parts that don't include &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{C_1} - C_2\theta^2
&amp;amp;\propto - C_2\theta^2\\
&amp;amp;\propto C\Vert \theta\Vert_{2}^{2}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This is L2 regularization. Furthermore, placing different prior distributions on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; yields different regularization terms; notably, a &lt;a href="https://en.wikipedia.org/wiki/Laplace_distribution"&gt;Laplace prior&lt;/a&gt; gives the L1.&lt;/p&gt;
&lt;h2&gt;Linear regression&lt;/h2&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\min} \sum\limits_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 + C\Vert \theta\Vert_{2}^{2}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given a Gaussian prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the mean squared error between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction thereof, plus the sum of the squares of the elements of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\min}
-\sum\limits_{i = 1}^my^{(i)}\log{(\phi^{(i)})} + (1 - y^{(i)})\log{(1 - \phi^{(i)})} + C\Vert \theta\Vert_{2}^{2}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given a Gaussian prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the binary cross-entropy (i.e. binary log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability thereof, plus the sum of the squares of the elements of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;div class="math"&gt;$$
-\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_k\log\pi_k + C\Vert \theta\Vert_{2}^{2}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given a Gaussian prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the categorical cross-entropy (i.e. multi-class log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability distribution thereof, plus the sum of the squares of the elements of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, in machine learning, we say that regularizing our weights ensures that "no weight becomes too large," i.e. too "influential" in predicting &lt;span class="math"&gt;\(y\)&lt;/span&gt;. In statistical terms, we can equivalently say that this term &lt;em&gt;restricts the permissible values of these weights to a given interval.&lt;/em&gt; (Furthermore, this interval is dictated by the scaling constant &lt;span class="math"&gt;\(C\)&lt;/span&gt;, which intrinsically parameterizes the prior distribution itself.* In L2 regularization, this scaling constant gives the variance of the Gaussian.)&lt;/p&gt;
&lt;h1&gt;Going fully Bayesian&lt;/h1&gt;
&lt;p&gt;The key goal of a predictive model is to compute the following distribution:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x, D) = \int P(y\vert x, D, \theta)P(\theta\vert x, D)d\theta
$$&lt;/div&gt;
&lt;p&gt;By term, this reads:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(y\vert x, D)\)&lt;/span&gt;: given historical data &lt;span class="math"&gt;\(D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)}))\)&lt;/span&gt;, i.e. some training data, and a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, compute the distribution of the possible values of the response &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;In machine learning, we typically select a &lt;em&gt;single value&lt;/em&gt; from this distribution, i.e. point estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(y\vert x, D, \theta)\)&lt;/span&gt;: given historical data &lt;span class="math"&gt;\(D\)&lt;/span&gt;, a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;em&gt;any plausible value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/em&gt;, i.e. perhaps not the optimal value, compute &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;This is given by the functional form of the model in question, i.e. &lt;span class="math"&gt;\(y = \theta^Tx\)&lt;/span&gt; in the case of linear regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(\theta\vert x, D)\)&lt;/span&gt;: given historical data &lt;span class="math"&gt;\(D\)&lt;/span&gt; and a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, compute the distribution of the values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that plausibly gave rise to our data.&lt;ul&gt;
&lt;li&gt;The &lt;span class="math"&gt;\(x\)&lt;/span&gt; plays no part; it's simply there such that the expression under the integral factors correctly.&lt;/li&gt;
&lt;li&gt;In machine learning, we typically select the MLE or MAP estimate of that distribution, i.e. a single value, or point estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a perfect world, we'd do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the &lt;em&gt;full distribution&lt;/em&gt; over &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;With each value in this distribution and a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, compute &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;NB: &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is an object which contains all of our weights. In 10-feature linear regression, it will have 10 elements. In a neural network, it could have millions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We now have a &lt;em&gt;full distribution&lt;/em&gt; over the possible values of the response &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Instead of a point estimate for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, and a point estimate for &lt;span class="math"&gt;\(y\)&lt;/span&gt; given a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; (which makes use of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;), we have distributions for each&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, in complex systems with a non-trivial functional form and number of weights, this computation becomes intractably large. As such, in fully Bayesian modeling, we approximate these distributions. In classic machine learning, we assign them a single value (point estimate). It's a bit lazy, really.&lt;/p&gt;
&lt;p&gt;&lt;img alt="@betanalpha bayesian tweet" class="img-responsive" src="https://willwolf.io/images/going_fully_bayesian.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;I hope this post serves as useful context for the machine learning models we know and love. A deeper understanding of these algorithms offers humility—the knowledge that none of these concepts are particularly new—as well as a vision for how to extend these algorithms in the direction of robustness and increased expressivity.&lt;/p&gt;
&lt;p&gt;Thanks so much for reading this far. Now, climb out of the pool, grab a towel and &lt;code&gt;import sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="drink and towel" class="img-responsive" src="https://www.washingtonian.com/wp-content/uploads/2015/05/Pool520-994x664.jpg"/&gt;&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;p&gt;I recently gave a talk on this topic at &lt;a href="https://www.facebook.com/groups/265793323822652"&gt;Facebook Developer Circle: Casablanca&lt;/a&gt;. Voilà the:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/WilliamWolfDataScien/youve-been-doing-statistics-all-along"&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/aboullaite.mohammed/videos/1959648697600819/"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://cs229.stanford.edu/materials.html"&gt;CS229 Machine Learning Course Materials, Lecture Notes 1&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA"&gt;mathematical monk - Machine Learning&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Transfer Learning for Flight Delay Prediction via Variational Autoencoders</title><link href="https://willwolf.io/2017/05/08/transfer-learning-flight-delay-prediction/" rel="alternate"></link><published>2017-05-08T12:45:00-04:00</published><updated>2017-05-08T12:45:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-05-08:/2017/05/08/transfer-learning-flight-delay-prediction/</id><summary type="html">&lt;p&gt;Autoencoding airports via variational autoencoders to improve flight delay prediction. Additionally, a principled look at variational inference itself and its connections to machine learning.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this work, we explore improving a vanilla regression model with knowledge learned elsewhere. As a motivating example, consider the task of predicting the number of checkins a given user will make at a given location. Our training data consist of checkins from 4 users across 4 locations in the week of May 1st, 2017 and looks as follows:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;user_id&lt;/th&gt;
&lt;th&gt;location&lt;/th&gt;
&lt;th&gt;checkins&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We'd like to predict how many checkins user 3 will make at location &lt;code&gt;b&lt;/code&gt; in the coming week. How well will our model do?&lt;/p&gt;
&lt;p&gt;While each &lt;code&gt;user_id&lt;/code&gt; might represent some unique behavior - e.g. user &lt;code&gt;3&lt;/code&gt; sleeps late yet likes going out for dinner - and each location might represent its basic characteristics - e.g. location &lt;code&gt;b&lt;/code&gt; is an open-late sushi bar - this is currently unbeknownst to our model. To this end, gathering this metadata and joining it to our training set is a clear option. If quality, thorough, explicit metadata are available, affordable and practical to acquire, this is likely the path to pursue. If not, we'll need to explore a more creative approach. How far can we get with &lt;em&gt;implicit&lt;/em&gt; metadata learned from an external task?&lt;/p&gt;
&lt;h2&gt;Transfer learning&lt;/h2&gt;
&lt;p&gt;Transfer learning allows us to use knowledge acquired in one task to improve performance in another. Suppose, for example, that we've been tasked with translating Portuguese to English and are given a basic phrasebook from which to learn. After a week, we take a lengthy test. A friend of ours - a fluent Spanish speaker who knows nothing of Portuguese - is tasked the same. Who gets a better score?&lt;/p&gt;
&lt;h2&gt;Predicting flight delays&lt;/h2&gt;
&lt;p&gt;The goal of this work is to predict flight delays - a basic regression task. The data comprise 6,872,294 flights from 2008 via the &lt;a href="https://www.transportation.gov/"&gt;United States Department of Transportation's&lt;/a&gt; &lt;a href="https://www.bts.gov/"&gt;Bureau of Transportation Statistics&lt;/a&gt;. I downloaded them from &lt;a href="http://stat-computing.org/dataexpo/2009/the-data.html"&gt;stat-computing.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Each row consists of, among other things: &lt;code&gt;DayOfWeek&lt;/code&gt;, &lt;code&gt;DayofMonth&lt;/code&gt;, &lt;code&gt;Month&lt;/code&gt;, &lt;code&gt;ScheduledDepTimestamp&lt;/code&gt; (munged from &lt;code&gt;CRSDepTime&lt;/code&gt;), &lt;code&gt;Origin&lt;/code&gt;, &lt;code&gt;Dest&lt;/code&gt; and &lt;code&gt;UniqueCarrier&lt;/code&gt; (airline), and well as &lt;code&gt;CarrierDelay&lt;/code&gt;, &lt;code&gt;WeatherDelay&lt;/code&gt;, &lt;code&gt;NASDelay&lt;/code&gt;, &lt;code&gt;SecurityDelay&lt;/code&gt;, &lt;code&gt;LateAircraftDelay&lt;/code&gt; - all in minutes - which we will sum to create &lt;code&gt;total_delay&lt;/code&gt;. We'll consider a random sample of 50,000 flights to make things easier. (For a more in-depth exploration of these data, please see this project's &lt;a href="http://github.com/cavaunpeu/transfer-learning-for-flight-prediction/explore.R"&gt;repository&lt;/a&gt;.)&lt;/p&gt;
&lt;h2&gt;Routes, airports&lt;/h2&gt;
&lt;p&gt;While we can expect &lt;code&gt;DayOfWeek&lt;/code&gt;, &lt;code&gt;DayofMonth&lt;/code&gt; and &lt;code&gt;Month&lt;/code&gt; to give some seasonal delay trends - delays are likely higher on Sundays or Christmas, for example - the &lt;code&gt;Origin&lt;/code&gt; and &lt;code&gt;Dest&lt;/code&gt; columns might suffer from the same pathology as &lt;code&gt;user_id&lt;/code&gt; and &lt;code&gt;location&lt;/code&gt; above: a rich behavioral indicator represented in a crude, "isolated" way. (A token in a bag-of-words model, as opposed to its respective word2vec representation, gives a clear analogy.) How can we infuse this behavioral knowledge into our original task?&lt;/p&gt;
&lt;h2&gt;An auxiliary task&lt;/h2&gt;
&lt;p&gt;In 2015, I read a particularly-memorable blog post entitled &lt;a href="http://allentran.github.io/graph2vec"&gt;Towards Anything2Vec&lt;/a&gt; by Allen Tran. Therein, Allen states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Like pretty much everyone, I'm obsessed with word embeddings word2vec or GloVe. Although most of machine learning in general is based on turning things into vectors, it got me thinking that we should probably be learning more fundamental representations for objects, rather than hand tuning features. Here is my attempt at turning random things into vectors, starting with graphs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this post, Allen seeks to embed nodes - U.S. patents, incidentally - in a directed graph into vector space by predicting the inverse of the path-length to nodes nearby. To me, this (thus-far) epitomizes the "data describe the individual better than they describe themself:" while we could ask the nodes to self-classify into patents on "computing," "pharma," "materials," etc., the connections between these nodes - formal citations, incidentally - will capture their "true" subject matters (and similarities therein) better than the authors ever could. Formal language, necessarily, generalizes.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://openflights.org/data.html"&gt;OpenFlights&lt;/a&gt; contains data for over "10,000 airports, train stations and ferry terminals spanning the globe" and the routes between. My goal is to train a neural network that, given an origin airport and its latitude and longitude, predicts the destination airport, latitude and longitude. This network will thereby "encode" each airport into a vector of arbitrary size containing rich information about, presumably, the diversity and geography of the destinations it services: its "place" in the global air network. Surely, a global hub like Heathrow - a fact presumably known to our neural network, yet unknown to our initial dataset with one-hot airport indices - has longer delays on Christmas than than a two-plane airstrip in Alaska.&lt;/p&gt;
&lt;p&gt;Crucially, we note that while our original (down-sampled) dataset contains delays amongst 298 unique airports, our auxiliary &lt;code&gt;routes&lt;/code&gt; dataset comprises flights amongst 3186 unique airports. Notwithstanding, information about &lt;em&gt;all&lt;/em&gt; airports in the latter is &lt;em&gt;distilled&lt;/em&gt; into vector representations then injected into the former; even though we might not know about delays to/from Casablanca Mohammed V Airport (CMN), latent information about this airport will still be &lt;em&gt;intrinsically considered&lt;/em&gt; when predicting delays between other airports to/from which CMN flies.&lt;/p&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;p&gt;Our flight-delay design matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; will include the following columns: &lt;code&gt;DayOfWeek&lt;/code&gt;, &lt;code&gt;DayofMonth&lt;/code&gt;, &lt;code&gt;Month&lt;/code&gt;, &lt;code&gt;ScheduledDepTimestamp&lt;/code&gt;, &lt;code&gt;Origin&lt;/code&gt;, &lt;code&gt;Dest&lt;/code&gt; and &lt;code&gt;UniqueCarrier&lt;/code&gt;. All columns will be one-hotted for simplicity. (Alternatively, I explored mapping each column to its respective &lt;code&gt;value_counts()&lt;/code&gt;, i.e. &lt;code&gt;X.loc[:, col] = X[col].map(col_val_counts)&lt;/code&gt;, which led to less agreeable convergence.)&lt;/p&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dataset sizes:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Train:      &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Test:       &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (30000, 657)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (10000, 657)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (10000, 657)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Flight-delay models&lt;/h2&gt;
&lt;p&gt;Let's build two baseline models with the data we have. Both models have a single ReLU output and are trained to minimize the mean squared error of the predicted delay via stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;ReLU was chosen as an output activation because delays are both bounded below at 0 and bi-modal. I considered three separate strategies for predicting this distribution.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Train a network with two outputs: &lt;code&gt;total_delay&lt;/code&gt; and &lt;code&gt;total_delay == 0&lt;/code&gt; (Boolean). Optimize this network with a composite loss function: mean squared error and binary cross-entropy, respectively.&lt;/li&gt;
&lt;li&gt;Train a "poor-man's" hierarchical model: a logistic regression to predict &lt;code&gt;total_delay == 0&lt;/code&gt; and a standard regression to predict &lt;code&gt;total_delay&lt;/code&gt;. Then, compute the final prediction as a thresholded ternary, e.g. &lt;code&gt;y_pred = np.where(y_pred_lr &amp;gt; threshhold, 0, y_pred_reg)&lt;/code&gt;. Train the regression model with both all observations, and just those where &lt;code&gt;total_delay &amp;gt; 0&lt;/code&gt;, and see which works best.&lt;/li&gt;
&lt;li&gt;Train a single network with a ReLU activation. This gives a reasonably elegant way to clip our outputs below at 0, and mean-squared-error still tries to place our observations into the correct mode (of the bimodal output distribution; this said, mean-squared-error may try to "play it safe" and predict between the modes).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I chose Option #3 because it performed best in brief experimentation and was the simplest to both fit and explain.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metaclass&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ABCMeta&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            input_dim : The number of columns in our design matrix.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'float32'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;144&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'regression_output'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DeeperRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            input_dim : The number of columns in our design matrix.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;            dropout_p : The percentage of units to drop in the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dropout layer.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout_p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'float32'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'input'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;144&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;144&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'regression_output'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Simple regression&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;


&lt;span class="n"&gt;simple_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="simple regression initial" class="img-responsive" src="https://willwolf.io/figures/transfer_learning_simple_regression_initial.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Deeper regression&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeeperRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="deeper regression initial" class="img-responsive" src="https://willwolf.io/figures/transfer_learning_deeper_regression_initial.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Test set predictions&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_pred_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y_pred_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;mse_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_simple&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mse_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_deeper&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, simple regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_simple&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, deeper regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_deeper&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean squared error, simple regression: 2.331459019628268&lt;/span&gt;
&lt;span class="err"&gt;Mean squared error, deeper regression: 2.3186310632259204&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Learning airport embeddings&lt;/h2&gt;
&lt;p&gt;We propose two networks through which to learn airport embeddings: a dot product siamese network, and a &lt;a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"&gt;variational autoencoder&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Dot product siamese network&lt;/h2&gt;
&lt;p&gt;This network takes as input origin and destination IDs, latitudes and longitudes. It gives as output a binary value indicating whether or not a flight-route between these airports exists. The &lt;code&gt;airports&lt;/code&gt; DataFrame gives the geographic metadata. The &lt;code&gt;routes&lt;/code&gt; DataFrame gives &lt;em&gt;positive&lt;/em&gt; training examples for our network. To build negative samples, we employ, delightfully, "negative sampling."&lt;/p&gt;
&lt;h3&gt;Negative sampling&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;routes&lt;/code&gt; gives exlusively &lt;code&gt;(origin, dest, exists = 1)&lt;/code&gt; triplets. To create triplets where &lt;code&gt;exists = 0&lt;/code&gt;, we simply build them ourself: &lt;code&gt;(origin, fake_dest, exists = 0)&lt;/code&gt;. It's that simple.&lt;/p&gt;
&lt;p&gt;Inspired by &lt;a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"&gt;word2vec's approach&lt;/a&gt; to an almost identical problem, I pick &lt;code&gt;fake_dest&lt;/code&gt;'s based on the frequency with which they occur in the dataset - more frequent samples being more likely to be selected - via:&lt;/p&gt;
&lt;div class="math"&gt;$$P(a_i) = \frac{  {f(a_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(a_j)}^{3/4} \right) }$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is an airport. To choose a &lt;code&gt;fake_dest&lt;/code&gt; for a given &lt;code&gt;origin&lt;/code&gt;, we first remove all of the real &lt;code&gt;dest&lt;/code&gt;'s, re-normalize &lt;span class="math"&gt;\(P(a)\)&lt;/span&gt;, then take a multinomial draw.&lt;/p&gt;
&lt;p&gt;For a more complete yet equally approachable explanation, please see &lt;a href="https://arxiv.org/pdf/1402.3722.pdf"&gt;Goldberg and Levy&lt;/a&gt;. For an &lt;em&gt;extremely thorough&lt;/em&gt; review of related methods, see Sebastian Ruder's &lt;a href="http://sebastianruder.com/word-embeddings-softmax/"&gt;On word embeddings - Part 2: Approximating the Softmax&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Variational autoencoder&lt;/h2&gt;
&lt;h3&gt;Discriminative models&lt;/h3&gt;
&lt;p&gt;The previous network is a &lt;em&gt;discriminative&lt;/em&gt; model: given two inputs &lt;code&gt;origin&lt;/code&gt; and &lt;code&gt;dest&lt;/code&gt;, it outputs the conditional probability that &lt;code&gt;exists = 1&lt;/code&gt;. While discriminative models are effective in distinguishing &lt;em&gt;between&lt;/em&gt; output classes, they don't offer an idea of what data look like within each class itself. To see why, let's restate Bayes' rule for a given input &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y\vert x) = \frac{P(x\vert Y)P(Y)}{P(x)} = \frac{P(x, Y)}{P(x)}$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Discriminative classifiers jump directly to estimating &lt;span class="math"&gt;\(P(Y\vert x)\)&lt;/span&gt; without modeling its component parts &lt;span class="math"&gt;\(P(x, Y)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Instead, as the intermediate step, they simply compute an &lt;em&gt;unnormalized&lt;/em&gt; joint distribution &lt;span class="math"&gt;\(\tilde{P}(x, Y)\)&lt;/span&gt; and a normalizing "partition function." The following then gives the model's predictions for the same reason that &lt;span class="math"&gt;\(\frac{.2}{1} = \frac{3}{15}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y\vert x) = \frac{P(x, Y)}{P(x)} = \frac{\tilde{P}(x, Y)}{\text{partition function}}$$&lt;/div&gt;
&lt;p&gt;This is explained much more thoroughly in a previous blog post: &lt;a href="https://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/"&gt;Deriving the Softmax from First Principles&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Generative models&lt;/h3&gt;
&lt;p&gt;Conversely, a variational autoencoder is a &lt;em&gt;generative&lt;/em&gt; model: instead of jumping &lt;em&gt;directly&lt;/em&gt; to the conditional probability of all possible outputs given a specific input, they first compute the true component parts: the joint probability distribution over data and inputs alike, &lt;span class="math"&gt;\(P(X, Y)\)&lt;/span&gt;, and the distribution over our data, &lt;span class="math"&gt;\(P(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The joint probability can be rewritten as &lt;span class="math"&gt;\(P(X, Y) = P(Y)P(X\vert Y)\)&lt;/span&gt;: as such, generative models tell us the distribution over classes in our dataset, as well as the distribution of inputs within each class. Suppose we are trying to predict t-shirt colors with a 3-feature input; generative models would tell us: "30% of your t-shirts are green - typically produced by inputs near &lt;code&gt;x = [1, 2, 3]&lt;/code&gt;; 40% are red - typically produced by inputs near &lt;code&gt;x = [10, 20, 30]&lt;/code&gt;; 30% are blue - typically produced by inputs near &lt;code&gt;x = [100, 200, 300]&lt;/code&gt;. This is in contrast to a discriminative model which would simply compute: given an input &lt;span class="math"&gt;\(x\)&lt;/span&gt;, your output probabilities are: &lt;span class="math"&gt;\(\{\text{red}: .2, \text{green}: .3, \text{blue}: .5\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;To generate new data with a generative model, we draw from &lt;span class="math"&gt;\(P(Y)\)&lt;/span&gt;, then &lt;span class="math"&gt;\(P(X\vert Y)\)&lt;/span&gt;. To make predictions, we solicit &lt;span class="math"&gt;\(P(Y), P(x\vert Y)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; and employ Bayes' rule outright.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Manifold assumption&lt;/h3&gt;
&lt;p&gt;The goal of both autoencoders is to discover underlying "structure" in our data: while each airport can be one-hot encoded into a 3186-dimensional vector, we wish to learn a, or even the, reduced space in which our data both live and vary. This concept is well understood through the "manifold assumption," explained succinctly in this &lt;a href="https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning"&gt;CrossValidated thread&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Imagine that you have a bunch of seeds fastened on a glass plate, which is resting horizontally on a table. Because of the way we typically think about space, it would be safe to say that these seeds live in a two-dimensional space, more or less, because each seed can be identified by the two numbers that give that seed's coordinates on the surface of the glass.&lt;/p&gt;
&lt;p&gt;Now imagine that you take the plate and tilt it diagonally upwards, so that the surface of the glass is no longer horizontal with respect to the ground. Now, if you wanted to locate one of the seeds, you have a couple of options. If you decide to ignore the glass, then each seed would appear to be floating in the three-dimensional space above the table, and so you'd need to describe each seed's location using three numbers, one for each spatial direction. But just by tilting the glass, you haven't changed the fact that the seeds still live on a two-dimensional surface. So you could describe how the surface of the glass lies in three-dimensional space, and then you could describe the locations of the seeds on the glass using your original two dimensions.&lt;/p&gt;
&lt;p&gt;In this thought experiment, the glass surface is akin to a low-dimensional manifold that exists in a higher-dimensional space : no matter how you rotate the plate in three dimensions, the seeds still live along the surface of a two-dimensional plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, the full spectrum of that which characterizes an airport can be described by just a few numbers. Varying one of these numbers - making it larger or smaller - would result in an airport of slightly different "character;" if one dimension were to represent "global travel hub"-ness, a value of &lt;span class="math"&gt;\(-1000\)&lt;/span&gt; along this dimension might give us that hangar in Alaska.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;In the context of autoencoders (and dimensionality reduction algorithms), "learning 'structure' in our data" means nothing more than finding that ceramic plate amidst a galaxy of stars&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Graphical models&lt;/h3&gt;
&lt;p&gt;Variational autoencoders do not have the same notion of an "output" - namely, "does a route between two airports exist?" - as our dot product siamese network. To detail this model, we'll start near first principles with probabilistic graphical models with our notion of the ceramic plate in mind:&lt;/p&gt;
&lt;p&gt;&lt;img alt="VAE pgm" class="img-responsive" src="https://willwolf.io/images/vae_pgm.png"/&gt;&lt;/p&gt;
&lt;p&gt;Coordinates on the plate detail airport character; choosing coordinates - say, &lt;code&gt;[global_hub_ness = 500, is_in_asia = 500]&lt;/code&gt; - allows us to &lt;em&gt;generate&lt;/em&gt; an airport. In this case, it might be Seoul. In variational autoencoders, ceramic-plate coordinates are called the "latent vector," denoted &lt;span class="math"&gt;\(z\)&lt;/span&gt;. The joint probability of our graphical model is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$P(z)P(x\vert z) = P(z, x)$$&lt;/div&gt;
&lt;p&gt;Our goal is to infer the priors that likely generated these data via Bayes' rule:&lt;/p&gt;
&lt;div class="math"&gt;$$P(z\vert x) = \frac{P(z)P(x\vert z)}{P(x)}$$&lt;/div&gt;
&lt;p&gt;The denominator is called the &lt;strong&gt;evidence&lt;/strong&gt;; we obtain it by marginalizing the joint distribution over the latent variables:&lt;/p&gt;
&lt;div class="math"&gt;$$P(x) = \int P(x\vert z)P(z)dz$$&lt;/div&gt;
&lt;p&gt;Unfortunately, this asks us to consider &lt;em&gt;all possible configurations&lt;/em&gt; of the latent vector &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Should &lt;span class="math"&gt;\(z\)&lt;/span&gt; exist on the vertices of a cube in &lt;span class="math"&gt;\(\mathbb{R}^3\)&lt;/span&gt;, this would not be very difficult; should &lt;span class="math"&gt;\(z\)&lt;/span&gt; be a continuous-valued vector in &lt;span class="math"&gt;\(\mathbb{R}^{10}\)&lt;/span&gt;, this becomes a whole lot harder. Computing &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; is problematic.&lt;/p&gt;
&lt;h3&gt;Variational inference&lt;/h3&gt;
&lt;p&gt;In fact, we could attempt to use MCMC to compute &lt;span class="math"&gt;\(P(z\vert x)\)&lt;/span&gt;; however, this is slow to converge. Instead, let's compute an &lt;em&gt;approximation&lt;/em&gt; to this distribution then try to make it closely resemble the (intractable) original. In this vein, we introduce &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;variational inference&lt;/a&gt;, which "allows us to re-write statistical inference problems (i.e. infer the value of a random variable given the value of another random variable) as optimization problems (i.e. find the parameter values that minimize some objective function)."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Let's choose our approximating distribution as simple, parametric and one we know well: the Normal (Gaussian) distribution. Were we able to compute &lt;span class="math"&gt;\(P(z\vert x) = \frac{P(x, z)}{P(x)}\)&lt;/span&gt;, it is &lt;em&gt;intrinsic&lt;/em&gt; that &lt;span class="math"&gt;\(z\)&lt;/span&gt; is contingent on &lt;span class="math"&gt;\(x\)&lt;/span&gt;; when building our own distribution to approximate &lt;span class="math"&gt;\(P(z\vert x)\)&lt;/span&gt;, we need to be &lt;em&gt;explicit&lt;/em&gt; about this contingency: different values for &lt;span class="math"&gt;\(x\)&lt;/span&gt; should be assumed to have been generated by different values of &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Let's write our approximation as follows, where &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; parameterizes the Gaussian for a given &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}(z\vert x)$$&lt;/div&gt;
&lt;p&gt;Finally, as stated previously, we want to make this approximation closely resemble the original; the &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL divergence&lt;/a&gt; quantifies their difference:&lt;/p&gt;
&lt;div class="math"&gt;$$KL(q_{\lambda}(z\vert x)\Vert P(z\vert x)) = \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)}{P(z\vert x)}dz}$$&lt;/div&gt;
&lt;p&gt;Our goal is to obtain the argmin with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}^{*}(z\vert x) = \underset{\lambda}{\arg\min}\ \text{KL}(q_{\lambda}(z\vert x)\Vert P(z\vert x))$$&lt;/div&gt;
&lt;p&gt;Expanding the divergence, we obtain:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
KL(q_{\lambda}(z\vert x)\Vert P(z\vert x))
&amp;amp;= \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)}{P(z\vert x)}dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)P(x)}{P(z, x)}dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x) -\log{P(z, x)} + \log{P(x)}}\bigg)dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz + \log{P(x)}\int{q_{\lambda}(z\vert x)dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz + \log{P(x)} \cdot 1
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As such, since only the left term depends on &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, minimizing the entire expression with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; amounts to minimizing this term. Incidentally, the opposite (negative) of this term is called the &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;ELBO&lt;/a&gt;, or the "evidence lower bound." To see why, let's plug the ELBO into the equation above and solve for &lt;span class="math"&gt;\(\log{P(x)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\log{P(x)} = ELBO(\lambda) + KL(q_{\lambda}(z\vert x)\Vert P(z\vert x))$$&lt;/div&gt;
&lt;p&gt;In English: "the log of the evidence is at least the lower bound of the evidence plus the divergence between our true posterior &lt;span class="math"&gt;\(P(z\vert x)\)&lt;/span&gt; and our (variational) approximation to this posterior &lt;span class="math"&gt;\(q_{\lambda}(z\vert x)\)&lt;/span&gt;."&lt;/p&gt;
&lt;p&gt;Since the left term above is the opposite of the ELBO, minimizing this term is equivalent to &lt;em&gt;maximizing&lt;/em&gt; the ELBO.&lt;/p&gt;
&lt;p&gt;Let's restate the equation and rearrange further:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
ELBO(\lambda)
&amp;amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz\\
&amp;amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(x\vert z)} - \log{P(z)}}\bigg)dz\\
&amp;amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} - \log{P(z)}}\bigg)dz + \log{P(x\vert z)}\int{q_{\lambda}(z\vert x)dz}\\
&amp;amp;= -\int{q_{\lambda}(z\vert x)\log{\frac{q_{\lambda}(z\vert x)}{P(z)}}dz} + \log{P(x\vert z)} \cdot 1\\
&amp;amp;= \log{P(x\vert z)} -KL(q_{\lambda}(z\vert x)\Vert P(z))
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our goal is to maximize this expression, or minimize the opposite:&lt;/p&gt;
&lt;div class="math"&gt;$$-\log{P(x\vert z)} + KL(q_{\lambda}(z\vert x)\Vert P(z))$$&lt;/div&gt;
&lt;p&gt;In machine learning parlance: "minimize the negative log likelihood of our data (generated via &lt;span class="math"&gt;\(z\)&lt;/span&gt;) plus the divergence between the true distribution of &lt;span class="math"&gt;\(z\)&lt;/span&gt; (the ceramic plate) and our approximation thereof."&lt;/p&gt;
&lt;p&gt;See what we did?&lt;/p&gt;
&lt;h3&gt;Finally, back to neural nets&lt;/h3&gt;
&lt;p&gt;The variational autoencoder consists of an encoder network and a decoder network.&lt;/p&gt;
&lt;h4&gt;Encoder&lt;/h4&gt;
&lt;p&gt;The encoder network takes as input &lt;span class="math"&gt;\(x\)&lt;/span&gt; (an airport) and produces as output &lt;span class="math"&gt;\(z\)&lt;/span&gt; (the latent "code" of that airport, i.e. its location on the ceramic plate). As an intermediate step, it produces multivariate Gaussian parameters &lt;span class="math"&gt;\((\mu_{x_i}, \sigma_{x_i})\)&lt;/span&gt; for each airport. These parameters are then plugged into a Gaussian &lt;span class="math"&gt;\(q\)&lt;/span&gt;, from which we &lt;em&gt;sample&lt;/em&gt; a value &lt;span class="math"&gt;\(z\)&lt;/span&gt;. The encoder is parameterized by a weight matrix &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;Decoder&lt;/h4&gt;
&lt;p&gt;The decoder network takes as input &lt;span class="math"&gt;\(z\)&lt;/span&gt; and produces &lt;span class="math"&gt;\(P(x\vert z)\)&lt;/span&gt;: a reconstruction of the airport vector (hence, autoencoder). It is parameterized by a weight matrix &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;Loss function&lt;/h4&gt;
&lt;p&gt;The network's loss function is the sum of the mean squared reconstruction error of the original input &lt;span class="math"&gt;\(x\)&lt;/span&gt; and the KL divergence between the true distribution of &lt;span class="math"&gt;\(z\)&lt;/span&gt; and its approximation &lt;span class="math"&gt;\(q\)&lt;/span&gt;. Given the reparameterization trick (next section) and another healthy scoop of algebra, we write this in Python code as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;`z_mean` gives the mean of the Gaussian that generates `z`&lt;/span&gt;
&lt;span class="sd"&gt;`z_log_var` gives the log-variance of the Gaussian that generates `z`&lt;/span&gt;
&lt;span class="sd"&gt;`z` is generated via:&lt;/span&gt;

&lt;span class="sd"&gt;  z = z_mean + K.exp(z_log_var / 2) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + K.exp( log(z_std)**2 / 2 ) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + K.exp( (2 * log(z_std) / 2 ) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + K.exp( log(z_std) ) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + z_std * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;kl_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;kl_loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Reparameterization trick&lt;/h3&gt;
&lt;p&gt;When back-propagating the network's loss to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; , we need to go through &lt;span class="math"&gt;\(z\)&lt;/span&gt; — &lt;em&gt;a sample&lt;/em&gt; taken from &lt;span class="math"&gt;\(q_{\theta}(z\vert x)\)&lt;/span&gt;. Trivially, this sample is a scalar; intuitively, its derivative should be non-zero. In solution, we'd like the sample to depend not on the &lt;em&gt;stochasticity&lt;/em&gt; of the random variable, but on the random variable's &lt;em&gt;parameters&lt;/em&gt;. To this end, we employ the &lt;a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important"&gt;"reparametrization trick"&lt;/a&gt;, such that the sample depends on these parameters &lt;em&gt;deterministically&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As a quick example, this trick allows us to write &lt;span class="math"&gt;\(\mathcal{N}(\mu, \sigma)\)&lt;/span&gt; as &lt;span class="math"&gt;\(z = \mu + \sigma \odot \epsilon\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\epsilon \sim \mathcal{N}(0, 1)\)&lt;/span&gt;. Drawing samples this way allows us to propagate error backwards through our network.&lt;/p&gt;
&lt;h2&gt;Auxiliary data&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# build X_routes, y_routes&lt;/span&gt;
&lt;span class="n"&gt;geo_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_latitude'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'origin_longitude'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dest_latitude'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dest_longitude'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;routes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;geo_cols&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;routes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'exists'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;geo_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;geo_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# split training, test data&lt;/span&gt;
&lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;val_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;val_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dataset sizes:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Train:      &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Test:       &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (87630, 6)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (21907, 6)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (21907, 6)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Dot product embedding model&lt;/h2&gt;
&lt;p&gt;To start, let's train our model with a single latent dimension then visualize the results on the world map.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unique_airports&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DotProductEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            embedding_size : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# inputs&lt;/span&gt;
        &lt;span class="n"&gt;origin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;origin_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin_geo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest_geo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# embeddings&lt;/span&gt;
        &lt;span class="n"&gt;origin_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin_embedding'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# dot product&lt;/span&gt;
        &lt;span class="n"&gt;dot_product&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dot_product&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dot_product&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dot_product&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;dot_product&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# dense layers&lt;/span&gt;
        &lt;span class="n"&gt;tanh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dot_product&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tanh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# output&lt;/span&gt;
        &lt;span class="n"&gt;exists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dp_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DotProductEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dp_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;SVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_to_dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dp_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dot'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'svg'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="dot product embedding model" class="img-responsive" src="https://willwolf.io/figures/dot_product_embedding_model.svg"/&gt;&lt;/p&gt;
&lt;h2&gt;Fit&lt;/h2&gt;
&lt;p&gt;&lt;img alt="dot product embedding model fit" class="img-responsive" src="https://willwolf.io/figures/dot_product_embedding_model_fit.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Visualize embeddings&lt;/h2&gt;
&lt;p&gt;To visualize results, we'll:
1. Compose a list of unique origin airports.
2. Extract the learned (1-dimensional) embedding for each.
3. Scale the results to &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.
4. Use the scaled embedding as a percentile-index into a color gradient. Here, we've chosen the colors of the rainbow: low values are blue/purple, and high values are orange/red.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_embeddings_on_world_map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unique_origins&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{static}&lt;/span&gt;&lt;span class="s1"&gt;/figures/dp_model_map.html'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="800" src="https://willwolf-public.s3.amazonaws.com/transfer-learning-flight-delays/dp_model_map.html" width="946"&gt;&lt;/iframe&gt;
&lt;h2&gt;Variational autoencoder&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;KerasLayer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''A custom "variational" Keras layer that completes the&lt;/span&gt;
&lt;span class="sd"&gt;        variational autoencoder.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            output_dim : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt;
        &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_output_shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VariationalAutoEncoderEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            embedding_size : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dense_layer_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# encoder&lt;/span&gt;
        &lt;span class="n"&gt;origin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;origin_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin_geo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;variational_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;variational_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'encoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# decoder&lt;/span&gt;
        &lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest_geo'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'decoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# end-to-end&lt;/span&gt;
        &lt;span class="n"&gt;encoder_decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;encoder_decoder&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vae_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VariationalAutoEncoderEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;003&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;vae_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vae_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'mean_squared_logarithmic_error'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                  &lt;span class="n"&gt;loss_weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;SVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_to_dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vae_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dot'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'svg'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="vae embedding model" class="img-responsive" src="https://willwolf.io/figures/vae_embedding_model.svg"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# build VAE training, test sets&lt;/span&gt;
&lt;span class="n"&gt;one_hot_airports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X_train_r_origin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_val_r_origin&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_test_r_origin&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_train_r_dest&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_val_r_dest&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;     &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_test_r_dest&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dataset sizes:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Train:      &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train_r_origin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val_r_origin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Test:       &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_r_origin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (87630, 3186)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (21907, 3186)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (21907, 3186)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Fit&lt;/h2&gt;
&lt;p&gt;&lt;img alt="vae product embedding model fit" class="img-responsive" src="https://willwolf.io/figures/vae_product_embedding_model_fit.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Visualize&lt;/h2&gt;
&lt;p&gt;&lt;iframe allowfullscreen="" frameborder="0" height="800" src="https://willwolf-public.s3.amazonaws.com/transfer-learning-flight-delays/vae_model_map.html" width="946"&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;h2&gt;Finally, transfer the learning&lt;/h2&gt;
&lt;p&gt;Retrain both models with 20 latent dimensions, then join the embedding back to our original dataset.&lt;/p&gt;
&lt;h2&gt;Extract embeddings, construct joint dataset&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (30000, 151)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (10000, 151)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (10000, 151)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train original models&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;simple_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0005&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="simple regression augmented" class="img-responsive" src="https://willwolf.io/figures/transfer_learning_simple_regression_augmented.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeeperRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="deeper regression augmented" class="img-responsive" src="https://willwolf.io/figures/transfer_learning_deeper_regression_augmented.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_pred_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y_pred_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;mse_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_simple&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mse_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_deeper&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, simple regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_simple&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, deeper regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_deeper&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean squared error, simple regression: 2.3176028493805263&lt;/span&gt;
&lt;span class="err"&gt;Mean squared error, deeper regression: 2.291221474968889&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In fitting these models to both the original and "augmented" datasets, I spent time tuning their parameters — regularization strengths, amount of dropout, number of epochs, learning rates, etc. Additionally, the respective datasets are of different dimensionality. For these reasons, comparison between the two sets of models is clearly not "apples to apples."&lt;/p&gt;
&lt;p&gt;Notwithstanding, the airport embeddings do seem to provide a nice lift over our original one-hot encodings. Of course, their use is not limited to predicting flight delays: they can be used in any task concerned with airports. Additionally, these embeddings give insight into the nature of the airports themselves: those nearby in vector space can be considered as "similar" by some latent metric. To figure out what these metrics mean, though - it's back to the map.&lt;/p&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://allentran.github.io/graph2vec"&gt;Towards Anything2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ben.bolte.cc/blog/2017/unsupervised-calcium-modeling.html"&gt;Deep Learning for Calcium Imaging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1403.6652.pdf"&gt;DeepWalk: Online Learning of Social Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/"&gt;Variational Autoencoder: Intuition and Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html"&gt;Introducing Variational Autoencoders (in Prose and Code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dohmatob.github.io/research/2016/10/22/VAE.html"&gt;Variational auto-encoder for "Frey faces" using keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/transfer-learning/"&gt;Transfer Learning - Machine Learning's Next Frontier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"&gt;Tutorial - What is a variational autoencoder?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.evjang.com/2016/08/variational-bayes.html"&gt;A Beginner's Guide to Variational Methods: Mean-Field Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/"&gt;Variational Autoencoder: Intuition and Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning"&gt;CrossValidated - What is the manifold assumption in semi-supervised learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;David Blei - Variational Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/variational-inference"&gt;Edward - Variational Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf"&gt;On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/flight-delays"&gt;repository&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/flight-delays/blob/master/notebooks/flight-prediction.ipynb?flush_cache=true"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://blog.evjang.com/2016/08/variational-bayes.html"&gt;A Beginner's Guide to Variational Methods: Mean-Field Approximation&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Deriving the Softmax from First Principles</title><link href="https://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/" rel="alternate"></link><published>2017-04-19T17:26:00-04:00</published><updated>2017-04-19T17:26:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-04-19:/2017/04/19/deriving-the-softmax-from-first-principles/</id><summary type="html">&lt;p&gt;Deriving the softmax from first conditional probabilistic principles, and how this framework extends naturally to define the softmax regression, conditional random fields, naive Bayes and hidden Markov models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The original goal of this post was to explore the relationship between the softmax and sigmoid functions. In truth, this relationship had always seemed just out of reach: "One has an exponent in the numerator! One has a summation! One has a 1 in the denominator!" And of course, the two have different names.&lt;/p&gt;
&lt;p&gt;Once derived, I quickly realized how this relationship backed out into a more general modeling framework motivated by the conditional probability axiom itself. As such, this post first explores how the sigmoid is but a special case of the softmax, and the underpinnings of each in Gibbs distributions, factor products and probabilistic graphical models. Next, we go on to show how this framework extends naturally to define canonical model classes such as the softmax regression, conditional random fields, naive Bayes and hidden Markov models.&lt;/p&gt;
&lt;h2&gt;Our goal&lt;/h2&gt;
&lt;p&gt;This is a predictive model. It is a diamond that receives an input and produces an output.&lt;/p&gt;
&lt;p&gt;&lt;img alt="simple input/output model" class="img-responsive" src="https://willwolf.io/images/simple_input_output_model.png"/&gt;&lt;/p&gt;
&lt;p&gt;The input is a vector &lt;span class="math"&gt;\(\mathbf{x} = [x_0, x_1, x_2, x_3]\)&lt;/span&gt;. There are 3 possible outputs: &lt;span class="math"&gt;\(a, b, c\)&lt;/span&gt;. The goal of our model is to predict the probability of producing each output conditional on the input, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})$$&lt;/div&gt;
&lt;p&gt;Of course, a probability is but a real number that lies on the closed interval &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;How does the input affect the output?&lt;/h2&gt;
&lt;p&gt;Our input is a list of 4 numbers; each one affects &lt;em&gt;each possible output&lt;/em&gt; to a &lt;em&gt;different extent&lt;/em&gt;. We'll call this effect a "weight." 4 inputs times 3 outputs equals 12 distinct weights. They might look like this:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(b\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(c\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.1&lt;/td&gt;
&lt;td&gt;.4&lt;/td&gt;
&lt;td&gt;.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.2&lt;/td&gt;
&lt;td&gt;.3&lt;/td&gt;
&lt;td&gt;.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.3&lt;/td&gt;
&lt;td&gt;.2&lt;/td&gt;
&lt;td&gt;.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.4&lt;/td&gt;
&lt;td&gt;.1&lt;/td&gt;
&lt;td&gt;.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Producing an output&lt;/h2&gt;
&lt;p&gt;Given an input &lt;span class="math"&gt;\(x = [x_0, x_1, x_2, x_3]\)&lt;/span&gt;, our model will use the above weights to produce a number for each output &lt;span class="math"&gt;\(a, b, c\)&lt;/span&gt;. The effect of each input element will be additive. The reason why will be explained later on.&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{a} = \sum\limits_{i=0}^{3}w_{i, a}x_i\\
\tilde{b} = \sum\limits_{i=0}^{3}w_{i, b}x_i\\
\tilde{c} = \sum\limits_{i=0}^{3}w_{i, c}x_i\\
$$&lt;/div&gt;
&lt;p&gt;These sums will dictate what output our model produces. The biggest number wins. For example, given&lt;/p&gt;
&lt;div class="math"&gt;$$
\{\tilde{a}: 5, \tilde{b}: 7, \tilde{c}: 9\}
$$&lt;/div&gt;
&lt;p&gt;our model will have the best chance of producing a &lt;span class="math"&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Converting to probabilities&lt;/h2&gt;
&lt;p&gt;We said before that our goal is to obtain the following:&lt;/p&gt;
&lt;div class="math"&gt;$$P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; is &lt;strong&gt;bold&lt;/strong&gt; so as to represent &lt;em&gt;any&lt;/em&gt; input value. Given that we now have a &lt;em&gt;specific&lt;/em&gt; input value, namely &lt;span class="math"&gt;\(x\)&lt;/span&gt;, we can state our goal more precisely:&lt;/p&gt;
&lt;div class="math"&gt;$$P(a\vert x), P(b\vert x), P(c\vert x)$$&lt;/div&gt;
&lt;p&gt;Thus far, we just have &lt;span class="math"&gt;\(\{\tilde{a}: 5, \tilde{b}: 7, \tilde{c}: 9\}\)&lt;/span&gt;. To convert each value to a probability, i.e. an un-special number in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;, we just divide by the sum.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert x) = \frac{5}{5+7+9} = \frac{5}{21}\\
P(b\vert x) = \frac{7}{5+7+9} = \frac{7}{21}\\
P(c\vert x) = \frac{9}{5+7+9} = \frac{9}{21}\\
$$&lt;/div&gt;
&lt;p&gt;Finally, to be a valid probability distribution, all numbers must sum to 1.&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{5}{21} + \frac{7}{21} + \frac{9}{21} = 1 \checkmark
$$&lt;/div&gt;
&lt;h2&gt;What if our values are negative?&lt;/h2&gt;
&lt;p&gt;If one of our initial unnormalized probabilities were negative, i.e. &lt;span class="math"&gt;\(\{\tilde{a}: -5, \tilde{b}: 7, \tilde{c}: 9\}\)&lt;/span&gt;, this all breaks down.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert x) = \frac{-5}{-5+7+9} = \frac{-5}{11}\\
P(b\vert x) = \frac{7}{-5+7+9} = \frac{7}{11}\\
P(c\vert x) = \frac{9}{-5+7+9} = \frac{9}{11}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\frac{-5}{11}\)&lt;/span&gt; is not a valid probability as it does not fall in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To ensure that all unnormalized probabilities are positive, we must first pass them through a function that takes as input a real number and produces as output a strictly positive real number. This is simply an exponent; let's choose &lt;a href="https://en.wikipedia.org/wiki/E_(mathematical_constant)"&gt;Euler's number (&lt;span class="math"&gt;\(e\)&lt;/span&gt;)&lt;/a&gt; for now. The rationale for this choice will be explained later on (though do note that any positive exponent would serve our stated purpose).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\tilde{a} &amp;amp;= -5 \rightarrow e^{-5}\\
\tilde{b} &amp;amp;= 7 \rightarrow e^{7}\\
\tilde{c} &amp;amp;= 9 \rightarrow e^{9}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our &lt;em&gt;normalized&lt;/em&gt; probabilities, i.e. valid probabilities, now look as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert x) = \frac{e^{-5}}{e^{-5}+e^7+e^9}\\
P(b\vert x) = \frac{e^{7}}{e^{-5}+e^7+e^9}\\
P(c\vert x) = \frac{e^{9}}{e^{-5}+e^7+e^9}
$$&lt;/div&gt;
&lt;p&gt;More generally,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}\quad \text{for}\ y = a, b, c
$$&lt;/div&gt;
&lt;p&gt;This is the softmax function.&lt;/p&gt;
&lt;h2&gt;Relationship to the sigmoid&lt;/h2&gt;
&lt;p&gt;Whereas the softmax outputs a valid probability distribution over &lt;span class="math"&gt;\(n \gt 2\)&lt;/span&gt; distinct outputs, the sigmoid does the same for &lt;span class="math"&gt;\(n = 2\)&lt;/span&gt;. As such, the sigmoid is simply a special case of the softmax. By this definition, and assuming our model only produces two possible outputs &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt;, we can write the sigmoid for a given input &lt;span class="math"&gt;\(x\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mathbf{x}) = \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}\quad \text{for}\ y = p, q
$$&lt;/div&gt;
&lt;p&gt;Similar so far. However, notice that we only need to compute probabilities for &lt;span class="math"&gt;\(p\)&lt;/span&gt;, as &lt;span class="math"&gt;\(P(y = q\vert \mathbf{x}) = 1 - P(y = p\vert \mathbf{x})\)&lt;/span&gt;. On this note, let's re-expand the expression for &lt;span class="math"&gt;\(P(y = p\vert \mathbf{x})\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = p\vert \mathbf{x}) = \frac{e^{\tilde{p}}}{e^{\tilde{p}} + e^{\tilde{q}}}
$$&lt;/div&gt;
&lt;p&gt;Then, dividing both the numerator and denominator by &lt;span class="math"&gt;\(e^{\tilde{p}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y = p\vert \mathbf{x})
&amp;amp;= \frac{e^{\tilde{p}}}{e^{\tilde{p}} + e^{\tilde{q}}}\\
&amp;amp;= \frac{ \frac{e^{\tilde{p}}}{e^{\tilde{p}}} }{\frac{e^{\tilde{p}}}{e^{\tilde{p}}} + \frac{e^{\tilde{q}}}{e^{\tilde{p}}}}\\
&amp;amp;= \frac{1}{1 + e^{\tilde{q} - \tilde{p}}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Finally, we can plug this back into our original complement:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{1}{1 + e^{\tilde{q} - \tilde{p}}} = 1 - \frac{1}{1 + e^{\tilde{p} - \tilde{q}}}
$$&lt;/div&gt;
&lt;p&gt;Our equation is &lt;a href="https://en.wikipedia.org/wiki/Underdetermined_system"&gt;&lt;em&gt;underdetermined&lt;/em&gt;&lt;/a&gt; as there are more unknowns (two) than equations (one). As such, our system will have an infinite number of solutions &lt;span class="math"&gt;\((\tilde{p},\tilde{q})\)&lt;/span&gt;. For this reason, we can simply fix one of these values outright. Let's set &lt;span class="math"&gt;\(\tilde{q} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = p\vert \mathbf{x}) = \frac{1}{1 + e^{- \tilde{p}}}
$$&lt;/div&gt;
&lt;p&gt;This is the sigmoid function. Lastly,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = q\vert \mathbf{x}) = 1 - P(y = p\vert \mathbf{x})
$$&lt;/div&gt;
&lt;h2&gt;Why is the unnormalized probability a summation?&lt;/h2&gt;
&lt;p&gt;We all take for granted the semantics of the canonical linear combination &lt;span class="math"&gt;\(\sum\limits_{i}w_ix_i\)&lt;/span&gt;. But why do we sum in the first place?&lt;/p&gt;
&lt;p&gt;To answer this question, we'll first restate our goal: to predict the probability of producing each output &lt;em&gt;conditional&lt;/em&gt; on the input, i.e. &lt;span class="math"&gt;\(P(Y = y\vert \mathbf{x})\)&lt;/span&gt;. Next, we'll revisit the &lt;a href="https://en.wikipedia.org/wiki/Conditional_probability"&gt;definition of conditional probability&lt;/a&gt; itself:&lt;/p&gt;
&lt;div class="math"&gt;$$P(B\vert A) = \frac{P(A, B)}{P(A)}$$&lt;/div&gt;
&lt;p&gt;Personally, I find this a bit difficult to explain. Let's rearrange to obtain something more intuitive.&lt;/p&gt;
&lt;div class="math"&gt;$$P(A, B) = P(A)P(B\vert A)$$&lt;/div&gt;
&lt;p&gt;This reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The probability of observing (given values of) &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(B\)&lt;/span&gt; concurrently, ie. the joint probability of &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(B\)&lt;/span&gt;, is equal to the probability of observing &lt;span class="math"&gt;\(A\)&lt;/span&gt; times the probability of observing &lt;span class="math"&gt;\(B\)&lt;/span&gt; given that &lt;span class="math"&gt;\(A\)&lt;/span&gt; has occurred.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, assume that the probability of birthing a girl is &lt;span class="math"&gt;\(.55\)&lt;/span&gt;, and the probability of a girl liking math is &lt;span class="math"&gt;\(.88\)&lt;/span&gt;. Therefore,&lt;/p&gt;
&lt;div class="math"&gt;$$P(\text{sex} = girl, \text{likes} = math) = .55 * .88 = .484$$&lt;/div&gt;
&lt;p&gt;Now, let's rewrite our original model output in terms of the definition above.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}
= \frac{e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}{\sum\limits_{y} e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;Remember, we exponentiated each unnormalized probability &lt;span class="math"&gt;\(\tilde{y}\)&lt;/span&gt; so as to convert it to a strictly positive number. Technically, this number should be called &lt;span class="math"&gt;\(\tilde{P}(y, \mathbf{x})\)&lt;/span&gt; as it may be &lt;span class="math"&gt;\(\gt 1\)&lt;/span&gt; and therefore not yet a valid a probability. As such, we need to introduce one more term to our equality chain, given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;This is the arithmetic equivalent of &lt;span class="math"&gt;\(\frac{.2}{1} = \frac{3}{15}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the left term:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The numerator is a valid joint probability distribution.&lt;/li&gt;
&lt;li&gt;The denominator, "the probability of observing any value of &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;", is 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the right term:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The numerator is a strictly positive unnormalized probability distribution.&lt;/li&gt;
&lt;li&gt;The denominator is some constant that ensures that&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\frac{\tilde{P}(a, \mathbf{x})}{\text{normalizer}} + \frac{\tilde{P}(b, \mathbf{x})}{\text{normalizer}} + \frac{\tilde{P}(c, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;sums to 1. In fact, this "normalizer" is called a &lt;strong&gt;partition function&lt;/strong&gt;; we'll come back to this below.&lt;/p&gt;
&lt;p&gt;With this in mind, let's break down the numerator of our softmax equation a bit further.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\begin{split}
e^{\tilde{y}}
&amp;amp;= e^{\big(\sum\limits_{i}w_ix_i\big)}\\
&amp;amp;= e^{(w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3)}\\
&amp;amp;= e^{(w_0x_0)} e^{(w_1x_1)} e^{(w_2x_2)} e^{(w_3x_3)}\\
&amp;amp;= \tilde{P}(a, \mathbf{x})
\end{split}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Lemma: Given that our output function&lt;sup&gt;1&lt;/sup&gt; performs exponentiation &lt;em&gt;so as to obtain a valid conditional probability distribution over possible model outputs&lt;/em&gt;, it follows that our input to this function&lt;sup&gt;2&lt;/sup&gt; should be a summation of weighted model input elements&lt;sup&gt;3&lt;/sup&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The softmax function.&lt;/li&gt;
&lt;li&gt;One of &lt;span class="math"&gt;\(\tilde{a}, \tilde{b}, \tilde{c}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Model input elements are &lt;span class="math"&gt;\([x_0, x_1, x_2, x_3]\)&lt;/span&gt;. Weighted model input elements are &lt;span class="math"&gt;\(w_0x_0, w_1x_1, w_2x_2, w_3x_3\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unfortunately, this only holds if we buy the fact that &lt;span class="math"&gt;\(\tilde{P}(a, \mathbf{x}) = \prod\limits_i e^{(w_ix_i)}\)&lt;/span&gt; in the first place. Introducing the &lt;a href="https://www.youtube.com/watch?v=kFcjl3A9QuA&amp;amp;t=314s"&gt;Gibbs distribution&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Gibbs distribution&lt;/h2&gt;
&lt;p&gt;A Gibbs distribution gives the unnormalized joint probability distribution over a set of outcomes, analogous to the &lt;span class="math"&gt;\(e^{\tilde{a}}, e^{\tilde{b}}, e^{\tilde{c}}\)&lt;/span&gt; computed above, as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
= \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)\\
\Phi = \{\phi_1(\mathbf{D_1}), ..., \phi_k(\mathbf{D_k})\}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; defines a set of &lt;strong&gt;factors.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Factors&lt;/h3&gt;
&lt;p&gt;A factor is a function that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Takes a list of random variables as input. This list is known as the &lt;strong&gt;scope&lt;/strong&gt; of the factor.&lt;/li&gt;
&lt;li&gt;Returns a value for every unique combination of values that the random variables can take, i.e. for every entry in the cross-product space of its scope.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, a factor with scope &lt;span class="math"&gt;\(\{\mathbf{A, B}\}\)&lt;/span&gt; might look like:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(20\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(25\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(15\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Probabilistic graphical models&lt;/h3&gt;
&lt;p&gt;Inferring behavior from complex systems amounts (typically) to computing the joint probability distribution over its possible outcomes. For example, imagine we have a business problem in which:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The day-of-week (&lt;span class="math"&gt;\(\mathbf{A}\)&lt;/span&gt;) and the marketing channel (&lt;span class="math"&gt;\(\mathbf{B}\)&lt;/span&gt;) impact the probability of customer signup (&lt;span class="math"&gt;\(\mathbf{C}\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Customer signup impacts our annual recurring revenue (&lt;span class="math"&gt;\(\mathbf{D}\)&lt;/span&gt;) and end-of-year hiring projections (&lt;span class="math"&gt;\(\mathbf{E}\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Our ARR and hiring projections impact how much cake we will order for the holiday party (&lt;span class="math"&gt;\(\mathbf{F}\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We might draw our system as such:&lt;/p&gt;
&lt;p&gt;&lt;img alt="simple probabilistic graphical model" class="img-responsive" src="http://i3.buimg.com/afca455be01523af.png"/&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to compute &lt;span class="math"&gt;\(P(A, B, C, D, E, F)\)&lt;/span&gt;. In most cases, we'll only have data on small subsets of our system; for example, a controlled experiment we once ran to investigate the relationships between &lt;span class="math"&gt;\(A, B, C\)&lt;/span&gt;, or a survey asking employees how much cake they like eating at Christmas. It is rare, if wholly unreasonable, to ever have access to the full joint probability distribution for a moderately complex system.&lt;/p&gt;
&lt;p&gt;To compute this distribution we break it into pieces. Each piece is a &lt;strong&gt;factor&lt;/strong&gt; which details the behavior of some subset of the system. (As one example, a factor might give the number of times you've observed &lt;span class="math"&gt;\((\mathbf{A} &amp;gt; \text{3pm}, \mathbf{B} = \text{Facebook}, \mathbf{C} &amp;gt; \text{50 signups})\)&lt;/span&gt; in a given day.) To this effect, we say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A desired, &lt;em&gt;unnormalized&lt;/em&gt; probability distribution &lt;span class="math"&gt;\(\tilde{P}\)&lt;/span&gt; &lt;em&gt;factorizes over&lt;/em&gt; a graph &lt;span class="math"&gt;\(G\)&lt;/span&gt; if there exists a set of a factors &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; such that
&lt;div class="math"&gt;$$
\tilde{P} = \tilde{P}_{\Phi} = \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)
$$&lt;/div&gt;
where &lt;span class="math"&gt;\(\Phi = \{\phi_1(\mathbf{D_1}), ..., \phi_k(\mathbf{D_k})\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(G\)&lt;/span&gt; is the &lt;em&gt;induced graph&lt;/em&gt; for &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first half of this lemma does nothing more than restate the definition of an unnormalized Gibbs distribution. Expanding on the second half, we note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The graph induced by a set of factors is a pretty picture in which we draw a circle around each variable in the factor domain superset and draw lines between those that appear concurrently in a given factor domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With two factors &lt;span class="math"&gt;\(\phi(\mathbf{A, B}), \phi(\mathbf{B, C})\)&lt;/span&gt; the "factor domain superset" is &lt;span class="math"&gt;\(\{\mathbf{A, B, C}\}\)&lt;/span&gt;. The induced graph would have three circles with lines connecting &lt;span class="math"&gt;\(\mathbf{A}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{B}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{C}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, it follows that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given a business problem with variables &lt;span class="math"&gt;\(\mathbf{A, B, C, D, E, F}\)&lt;/span&gt; — we can draw a picture of it.&lt;/li&gt;
&lt;li&gt;We can build factors that describe the behavior of subsets of this problem. Realistically, these will only be small subsets.&lt;/li&gt;
&lt;li&gt;If the graph induced by our factors looks like the one we drew, we can represent our system as a factor product.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unfortunately, the resulting factor product &lt;span class="math"&gt;\(\tilde{P}_{\Phi}\)&lt;/span&gt; is still unnormalized just like &lt;span class="math"&gt;\(e^{\tilde{a}}, e^{\tilde{b}}, e^{\tilde{c}}\)&lt;/span&gt; in our original model.&lt;/p&gt;
&lt;h2&gt;Partition function&lt;/h2&gt;
&lt;p&gt;The partition function was the denominator, i.e. "normalizer", in our softmax function. It is used to turn an unnormalized probability distribution into a normalized (i.e. valid) probability distribution. A true Gibbs distribution is given as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
= \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)\\
P_{\Phi}(\mathbf{X_1, ..., X_n})
= \frac{1}{\mathbf{Z}_{\Phi}}\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{Z}_{\Phi}\)&lt;/span&gt; is the partition function.&lt;/p&gt;
&lt;p&gt;To compute this function, we simply add up all the values in the unnormalized table. Given &lt;span class="math"&gt;\(\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})\)&lt;/span&gt; as:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(20\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(25\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(15\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathbf{Z}_{\Phi}
&amp;amp;= 20 + 25 + 15 + 4\\
&amp;amp;= 64
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our valid probability distribution then becomes:&lt;/p&gt;
&lt;table class="table table-hover table-striped"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{20}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{25}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{15}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{4}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is our denominator in the softmax function.&lt;/p&gt;
&lt;p&gt;*I have not given an example of the actual arithmetic of a factor product (of multiple factors). It's trivial. Google.&lt;/p&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;p&gt;Once more, the goal of our model is to predict the probability of producing each output conditional on the given input, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})
$$&lt;/div&gt;
&lt;p&gt;In machine learning training data we're given the building block of a &lt;em&gt;joint probability distribution&lt;/em&gt;, e.g. a ledger of observed co-occurences of inputs and outputs. We surmise that each input element affects each possible output to a different extent, i.e. we multiply it by a weight. Next, we exponentiate each product &lt;span class="math"&gt;\(w_ix_i\)&lt;/span&gt;, i.e. factor, then multiply the results (alternatively, we could exponentiate the linear combination of the factors, i.e. features in machine learning parlance): this gives us an unnormalized joint probability distribution over all (input and output) variables.&lt;/p&gt;
&lt;p&gt;What we'd like is a valid probability distribution over possible outputs &lt;em&gt;conditional&lt;/em&gt; on the input, i.e. &lt;span class="math"&gt;\(P(y\vert \mathbf{x})\)&lt;/span&gt;. Furthermore, our output is a single, "1-of-k" variable in &lt;span class="math"&gt;\(\{a, b, c\}\)&lt;/span&gt; (as opposed to a sequence of variables). This is the definition, almost verbatim, of softmax regression.&lt;/p&gt;
&lt;p&gt;*Softmax regression is also known as multinomial regression, or multi-class logistic regression. Binary logistic regression is a special case of softmax regression in the same way that the sigmoid is a special case of the softmax.&lt;/p&gt;
&lt;p&gt;To compute our conditional probability distribution, we'll revisit Equation (1):&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}
= \frac{e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}{\sum\limits_{y} e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;In other words, the probability of producing each output conditional on the input is equivalent to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The softmax function.&lt;/li&gt;
&lt;li&gt;An exponentiated factor product of input elements normalized by a partition function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It almost speaks for itself.&lt;/p&gt;
&lt;h3&gt;Our partition function depends on &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In order to compute a distribution over &lt;span class="math"&gt;\(y\)&lt;/span&gt; &lt;em&gt;conditional&lt;/em&gt; on &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;, our partition function becomes &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dependent. In other words, for a given input &lt;span class="math"&gt;\(x = [x_0, x_1, x_2, x_3]\)&lt;/span&gt;, our model computes the conditional probabilities &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;. While this may seem like a trivial if pedantic restatement of what the softmax function does, it is important to note that our model is effectively computing a &lt;em&gt;family&lt;/em&gt; of conditional distributions — one for each unique input &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Conditional random field&lt;/h2&gt;
&lt;p&gt;Framing our model in this way allows us to extend naturally into other classes of problems. Imagine we are trying to assign a label to each individual word in a given conversation, where possible labels include: &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;offering an olive branch&lt;/code&gt;, and &lt;code&gt;them is fighting words&lt;/code&gt;. Our problem now differs from our original model in one key way, and another possibly-key way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our outcome is now a &lt;em&gt;sequence of labels&lt;/em&gt;. It is no longer "1-of-k." A possible sequence of labels in the conversation "hey there jerk shit roar" might be: &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;them is fighting words&lt;/code&gt;, &lt;code&gt;them is fighting words&lt;/code&gt;, &lt;code&gt;them is fighting words&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;There &lt;em&gt;might be&lt;/em&gt; relationships between the words that would influence the final output label sequence. For example, for each individual word, was the person both cocking their fists while enunciating that word, the one previous &lt;em&gt;and&lt;/em&gt; the one previous? In other words, we build factors (i.e. features) that speak to the spatial relationships between our input elements. We do this because we think these relationships might influence the final output (when we say our model "assumes dependencies between features," this is what we mean).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The conditional random field output function is a softmax just the same. In other words, if we build a softmax regression for our conversation-classification task where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our output is a sequence of labels&lt;/li&gt;
&lt;li&gt;Our features are a bunch of (spatially-inspired) interaction features, a la &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we've essentially just built a conditional random field.&lt;/p&gt;
&lt;p&gt;Of course, modeling the full distribution of outputs conditional on the input, where our output is again a sequence of labels, incurs combinatorial explosion really quick (for example, a 5-word speech would already have &lt;span class="math"&gt;\(3^5 = 243\)&lt;/span&gt; possible outputs). For this we use some dynamic-programming-magic to ensure that we compute &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; in a reasonable amount of time. I won't cover this topic here.&lt;/p&gt;
&lt;h2&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;Naive Bayes is identical to softmax regression with one key difference: instead of modeling the conditional distribution &lt;span class="math"&gt;\(P(y\vert \mathbf{x})\)&lt;/span&gt; we model the joint distribution &lt;span class="math"&gt;\(P(y, \mathbf{x})\)&lt;/span&gt;, given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y, \mathbf{x}) = P(y)\prod\limits_{i=1}^{K}P(x_i\vert y)
$$&lt;/div&gt;
&lt;p&gt;In effect, this model gives a (normalized) Gibbs distribution outright where the factors are &lt;em&gt;already valid probabilities&lt;/em&gt; expressing the relationship between each input element and the output.&lt;/p&gt;
&lt;h3&gt;The distribution of our data&lt;/h3&gt;
&lt;p&gt;Crucially, neither Naive Bayes nor softmax regression make any assumptions about the distribution of the data, &lt;span class="math"&gt;\(P(\mathbf{x})\)&lt;/span&gt;. (Were this not the case, we'd have to state information like "I think the probability of observing the &lt;em&gt;input&lt;/em&gt; &lt;span class="math"&gt;\(x = [x_0 = .12, x_1 = .34, x_2 = .56, x_3 = .78]\)&lt;/span&gt; is &lt;span class="math"&gt;\(.00047\)&lt;/span&gt;," which would imply in the most trivial sense of the word that we are making &lt;em&gt;assumptions&lt;/em&gt; about the distribution of our data.)&lt;/p&gt;
&lt;p&gt;In softmax regression, our model looks as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;While the second term is equal to the third, we never actually have to compute its denominator in order to obtain the first.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In Naive Bayes, we simply assume that the probability of observing each input element &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; depends on &lt;span class="math"&gt;\(y\)&lt;/span&gt; and nothing else, evidenced by its functional form. As such, &lt;span class="math"&gt;\(P(\mathbf{x})\)&lt;/span&gt; is not required.&lt;/p&gt;
&lt;h2&gt;Hidden Markov models and beyond&lt;/h2&gt;
&lt;p&gt;Finally, hidden Markov models are to naive Bayes what conditional random fields are to softmax regression: the former in each pair builds upon the latter by modeling a &lt;em&gt;sequence&lt;/em&gt; of labels. This graphic&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; gives a bit more insight into these relationships:&lt;/p&gt;
&lt;p&gt;&lt;img alt="generative vs. discriminative models" class="img-responsive" src="https://willwolf.io/images/generative_discriminative_models_flowchart.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Where does &lt;span class="math"&gt;\(e\)&lt;/span&gt; come from?&lt;/h2&gt;
&lt;p&gt;Equation (2) states that the numerator of the softmax, i.e. the exponentiated linear combination of input elements, is equivalent to the unnormalized joint probability of our inputs and outputs as given by the Gibbs distribution factor product.&lt;/p&gt;
&lt;p&gt;However, this only holds if one of the following two are true:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our factors are of the form &lt;span class="math"&gt;\(e^{z}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Our factors take any arbitrary form, and we "anticipate" that this form will be exponentiated within the softmax function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Remember, the point of this exponentiation was to put our weighted input elements "on the arithmetic path to becoming valid probabilities," i.e. to make them strictly positive. This said, there is nothing (to my knowledge) that mandates that a factor produce a strictly positive number. So which came first — the chicken or the egg (the exponent or the softmax)?&lt;/p&gt;
&lt;p&gt;In truth, I'm not actually sure, but I do believe we can safely treat the softmax numerator and an unnormalized Gibbs distribution as equivalent and simply settle on: &lt;em&gt;call it what you will, we need an exponent in there somewhere to put this thing in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This exercise has made the relationships between canonical machine learning models, activation functions and the basic axiom of conditional probability a whole lot clearer. For more information, please reference the resources below — especially Daphne Koller's material on &lt;a href="https://www.coursera.org/learn/probabilistic-graphical-models"&gt;probabilistic graphical models&lt;/a&gt;. Thanks so much for reading.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=PGBlyKtfB74&amp;amp;index=19&amp;amp;list=P6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH"&gt;Conditional random fields - linear chain CRF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/"&gt;Introduction to Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1011.4088v1.pdf"&gt;An Introduction to Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=kFcjl3A9QuA&amp;amp;t=559s"&gt;General Gibbs Distribution - Professor Daphne Koller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=2BXoj778YU8&amp;amp;t=636s"&gt;Conditional Random Fields - Professor Daphne Koller&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1011.4088v1.pdf"&gt;An Introduction to Conditional Random Fields&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Approximating Implicit Matrix Factorization with Shallow Neural Networks</title><link href="https://willwolf.io/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/" rel="alternate"></link><published>2017-04-07T10:07:00-04:00</published><updated>2017-04-07T10:07:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-04-07:/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/</id><summary type="html">&lt;p&gt;In this post, we look to beat the performance of &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Implicit Matrix Factorization&lt;/a&gt; on a recommendation task using 5 different neural network architectures.&lt;/p&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;pre { line-height: 125%; }
td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }
span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }
td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
 .highlight pre  .hll { background-color: #ffffcc }
 .highlight pre  { background: #f8f8f8; }
 .highlight pre  .c { color: #408080; font-style: italic } /* Comment */
 .highlight pre  .err { border: 1px solid #FF0000 } /* Error */
 .highlight pre  .k { color: #008000; font-weight: bold } /* Keyword */
 .highlight pre  .o { color: #666666 } /* Operator */
 .highlight pre  .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
 .highlight pre  .cm { color: #408080; font-style: italic } /* Comment.Multiline */
 .highlight pre  .cp { color: #BC7A00 } /* Comment.Preproc */
 .highlight pre  .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
 .highlight pre  .c1 { color: #408080; font-style: italic } /* Comment.Single */
 .highlight pre  .cs { color: #408080; font-style: italic } /* Comment.Special */
 .highlight pre  .gd { color: #A00000 } /* Generic.Deleted */
 .highlight pre  .ge { font-style: italic } /* Generic.Emph */
 .highlight pre  .gr { color: #FF0000 } /* Generic.Error */
 .highlight pre  .gh { color: #000080; font-weight: bold } /* Generic.Heading */
 .highlight pre  .gi { color: #00A000 } /* Generic.Inserted */
 .highlight pre  .go { color: #888888 } /* Generic.Output */
 .highlight pre  .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
 .highlight pre  .gs { font-weight: bold } /* Generic.Strong */
 .highlight pre  .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
 .highlight pre  .gt { color: #0044DD } /* Generic.Traceback */
 .highlight pre  .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
 .highlight pre  .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
 .highlight pre  .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
 .highlight pre  .kp { color: #008000 } /* Keyword.Pseudo */
 .highlight pre  .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
 .highlight pre  .kt { color: #B00040 } /* Keyword.Type */
 .highlight pre  .m { color: #666666 } /* Literal.Number */
 .highlight pre  .s { color: #BA2121 } /* Literal.String */
 .highlight pre  .na { color: #7D9029 } /* Name.Attribute */
 .highlight pre  .nb { color: #008000 } /* Name.Builtin */
 .highlight pre  .nc { color: #0000FF; font-weight: bold } /* Name.Class */
 .highlight pre  .no { color: #880000 } /* Name.Constant */
 .highlight pre  .nd { color: #AA22FF } /* Name.Decorator */
 .highlight pre  .ni { color: #999999; font-weight: bold } /* Name.Entity */
 .highlight pre  .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
 .highlight pre  .nf { color: #0000FF } /* Name.Function */
 .highlight pre  .nl { color: #A0A000 } /* Name.Label */
 .highlight pre  .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
 .highlight pre  .nt { color: #008000; font-weight: bold } /* Name.Tag */
 .highlight pre  .nv { color: #19177C } /* Name.Variable */
 .highlight pre  .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
 .highlight pre  .w { color: #bbbbbb } /* Text.Whitespace */
 .highlight pre  .mb { color: #666666 } /* Literal.Number.Bin */
 .highlight pre  .mf { color: #666666 } /* Literal.Number.Float */
 .highlight pre  .mh { color: #666666 } /* Literal.Number.Hex */
 .highlight pre  .mi { color: #666666 } /* Literal.Number.Integer */
 .highlight pre  .mo { color: #666666 } /* Literal.Number.Oct */
 .highlight pre  .sa { color: #BA2121 } /* Literal.String.Affix */
 .highlight pre  .sb { color: #BA2121 } /* Literal.String.Backtick */
 .highlight pre  .sc { color: #BA2121 } /* Literal.String.Char */
 .highlight pre  .dl { color: #BA2121 } /* Literal.String.Delimiter */
 .highlight pre  .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
 .highlight pre  .s2 { color: #BA2121 } /* Literal.String.Double */
 .highlight pre  .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
 .highlight pre  .sh { color: #BA2121 } /* Literal.String.Heredoc */
 .highlight pre  .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
 .highlight pre  .sx { color: #008000 } /* Literal.String.Other */
 .highlight pre  .sr { color: #BB6688 } /* Literal.String.Regex */
 .highlight pre  .s1 { color: #BA2121 } /* Literal.String.Single */
 .highlight pre  .ss { color: #19177C } /* Literal.String.Symbol */
 .highlight pre  .bp { color: #008000 } /* Name.Builtin.Pseudo */
 .highlight pre  .fm { color: #0000FF } /* Name.Function.Magic */
 .highlight pre  .vc { color: #19177C } /* Name.Variable.Class */
 .highlight pre  .vg { color: #19177C } /* Name.Variable.Global */
 .highlight pre  .vi { color: #19177C } /* Name.Variable.Instance */
 .highlight pre  .vm { color: #19177C } /* Name.Variable.Magic */
 .highlight pre  .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Abstract"&gt;Abstract&lt;a class="anchor-link" href="#Abstract"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Implicit-Matrix-Factorization"&gt;Implicit Matrix Factorization&lt;a class="anchor-link" href="#Implicit-Matrix-Factorization"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The Implicit Matrix Factorization (IMF) algorithm as presented by &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Hu, Koren and Volinksy&lt;/a&gt; is a widely popular, effective method for recommending items to users. This approach was born from necessity: while explicit feedback as to our users' tastes - a questionnaire completed at user signup, for example - makes building a recommendation engine straightforward, we often have nothing more than &lt;em&gt;implicit feedback&lt;/em&gt; data - view count, click count, time spent on page, for example - which serve instead as a proxy for these preferences. Crucially, the latter feedback is asymmetric: while a high view count might indicate positive preference for a given item, a low view count cannot be said to do the opposite. Perhaps, the user simply didn't know the item was there.&lt;/p&gt;
&lt;p&gt;IMF begins with a ratings matrix $R$, where $R_{u, i}$ gives the implicit feedback value observed for user $u$ and item $i$. Next, it constructs two other matrices defined as follows:&lt;/p&gt;
$$
p_{u, i} =
\begin{cases}
1 &amp;amp; r_{u, i} \gt 0\\
0 &amp;amp; r_{u, i} = 0
\end{cases}\\
c_{u, i} = 1 + \alpha r_{u, i}
$$&lt;p&gt;$P$ gives a binary matrix indicating our belief in the existence of each user's preference for each item. $C$ gives our &lt;em&gt;confidence&lt;/em&gt; in the existence of each user's preference for each item where, trivially, larger values of $r_{u, i}$ give us higher confidence that user $u$ indeed likes item $i$.&lt;/p&gt;
&lt;p&gt;Next, IMF outlines its goal: let's embed each user and item into $\mathbb{R}^f$ such that their dot product approximates the former's true preference for the latter. Finally, and naming user vectors $x_u \in \mathbb{R}^f$ and item vectors $y_i \in \mathbb{R}^f$, we compute the argmin of the following objective:&lt;/p&gt;
$$
\underset{x_{*}, y_{*}}{\arg\min}\sum\limits_{u, i}c_{u, i}\big(p_{u, i} - x_u^Ty_i\big)^2 + \lambda\bigg(\sum\limits_u\|x_u\|^2 + \sum\limits_i\|y_u\|^2\bigg)
$$&lt;p&gt;Once sufficiently minimized, we can compute expected preferences $\hat{p}_{u, i} = x_u^Ty_i$ for unobserved $\text{(user, item)}$ pairs; recommendation then becomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For a given user $u$, compute predicted preferences $\hat{p}_{u, i} = x_u^Ty_i$ for all items $i$.&lt;/li&gt;
&lt;li&gt;Sort the list in descending order.&lt;/li&gt;
&lt;li&gt;Returning the top $n$ items.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="Shallow-neural-networks"&gt;Shallow neural networks&lt;a class="anchor-link" href="#Shallow-neural-networks"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;IMF effectively gives a function $f: u, i \rightarrow \hat{p}_{u, i}$. As before, our goal is to minimize the function above, which we can now rewrite as:&lt;/p&gt;
$$
\underset{x_{*}, y_{*}}{\arg\min}\sum\limits_{u, i}c_{u, i}\big(p_{u, i} - f(u, i)\big)^2 + \lambda\bigg(\sum\limits_u\|x_u\|^2 + \sum\limits_i\|y_u\|^2\bigg)
$$&lt;p&gt;To approximate this function, I turn to our favorite &lt;a href="http://neuralnetworksanddeeplearning.com/chap4.html"&gt;universal function approximator&lt;/a&gt;: neural networks optimized with stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;This work is built around a toy web application I authored long ago: &lt;a href="http://dotify.herokuapp.com/"&gt;dotify&lt;/a&gt;. At present, dotify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pulls data nightly from &lt;a href="https://spotifycharts.com/regional"&gt;Spotify Charts&lt;/a&gt;. These data contain the number of streams for that day's top 200 songs for each of 55 countries.&lt;/li&gt;
&lt;li&gt;Computes an implicit matrix factorization nightly, giving vectors for both countries and songs.&lt;/li&gt;
&lt;li&gt;Allows the user to input a "country-arithmetic" expression, i.e. "I want music like &lt;code&gt;Colombia x Turkey - Germany&lt;/code&gt;." It then performs this arithmetic with the chosen vectors and recommends songs to the composite.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this work, I first fit and cross-validate an implicit matrix factorization model, establishing the three requisite parameters: $f$, the dimensionality of the latent vectors; $\alpha$, the scalar multiple used in computing $C$; $\lambda$ the regularization strength used in on our loss function.&lt;/p&gt;
&lt;h3 id="Network-architectures"&gt;Network architectures&lt;a class="anchor-link" href="#Network-architectures"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Next, I explore five different shallow neural network architectures in attempt to improve upon the observed results. These architectures are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A trivially "Siamese" network which first embeds each country and song index into $\mathbb{R}^f$ in parallel then computes a dot-product of the embeddings. This is roughly equivalent to what is being done by IMF.&lt;/li&gt;
&lt;li&gt;Same as previous, but with a bias embedding for each set, in $\mathbb{R}$, added to the dot-product.&lt;/li&gt;
&lt;li&gt;Same as &lt;a href="#network_1"&gt;#1&lt;/a&gt;, but concatenate the vectors instead. Then, stack 3 fully-connected layers with ReLU activations, batch normalization after each, and dropout after the first. Finally, add a 1-unit dense layer on the end, and add bias embeddings to the result. (NB: I wanted to add the bias embeddings to the respective $\mathbb{R}^f$ embeddings at the outset, but couldn't figure out how to do this in Keras.)&lt;/li&gt;
&lt;li&gt;Same as &lt;a href="#network_2"&gt;#2&lt;/a&gt;, except feed in the song title text as well. This text is first tokenized, then padded to a maximum sequence length, then embedded into a fixed-length vector by an LSTM, then reduced to a single value by a dense layer with a ReLU activation. Finally, this scalar is concatenated to the scalar output that &lt;a href="#network_2"&gt;#2&lt;/a&gt; would produce, and the result is fed into a final dense layer with a linear activation - i.e. a linear combination of the two.&lt;/li&gt;
&lt;li&gt;Same as &lt;a href="#network_4"&gt;#4&lt;/a&gt;, except feed in the song artist index as well. This index is first embedded into a vector, then reduced to a scalar by a dense layer with a ReLU activation. Finally, this scalar is concatenated with the two scalars produced in the second-to-last layer of &lt;a href="#network_4"&gt;#4&lt;/a&gt;, then fed into a final dense layer with a linear activation. Like the previous, this is a linear combination of the three inputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="Results"&gt;Results&lt;a class="anchor-link" href="#Results"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;While the various network architectures are intruiging for both the ease with which we can incorporate multiple, disparate inputs into our function, and the ability to then train this function end-to-end with respect to our main minimization objective, we find no reason to prefer them over the time-worn Implicit Matrix Factorization algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Data-preparation"&gt;Data preparation&lt;a class="anchor-link" href="#Data-preparation"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To construct our ratings matrix, I take the sum of total streams for each $\text{(country, song)}$ pair. The data are limited to a given number of "top songs," defined as a song that appeared on Spotify Charts &lt;em&gt;on a given date&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Because the values exist on wildly different orders of magnitude, I scale the results as follows:&lt;/p&gt;
$$\tilde{r}_{u, i} = \log{\bigg(\frac{1 + r_{u, i}}{\epsilon}\bigg)}$$&lt;p&gt;To start, let's build a ratings matrix for a small sample of the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [143]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RatingsMatrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_top_songs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[143]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;5552&lt;/th&gt;
&lt;th&gt;5553&lt;/th&gt;
&lt;th&gt;5557&lt;/th&gt;
&lt;th&gt;5558&lt;/th&gt;
&lt;th&gt;5560&lt;/th&gt;
&lt;th&gt;5562&lt;/th&gt;
&lt;th&gt;5565&lt;/th&gt;
&lt;th&gt;5582&lt;/th&gt;
&lt;th&gt;5583&lt;/th&gt;
&lt;th&gt;5585&lt;/th&gt;
&lt;th&gt;...&lt;/th&gt;
&lt;th&gt;33062&lt;/th&gt;
&lt;th&gt;33064&lt;/th&gt;
&lt;th&gt;33065&lt;/th&gt;
&lt;th&gt;33066&lt;/th&gt;
&lt;th&gt;33067&lt;/th&gt;
&lt;th&gt;33068&lt;/th&gt;
&lt;th&gt;33069&lt;/th&gt;
&lt;th&gt;33070&lt;/th&gt;
&lt;th&gt;33071&lt;/th&gt;
&lt;th&gt;33072&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;country_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;3.643437&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;2.546472&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;3.597833&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;3.175760&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;1.138153&lt;/td&gt;
&lt;td&gt;0.961264&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 2110 columns&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Next, let's get an idea of the sparsity of our data and how many songs each country has streamed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [144]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sparsity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;non_zero_entries_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sparsity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Our ratings matrix has &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;% non-zero entries.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;non_zero_entries_percent&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Our ratings matrix has 7.3999999999999995% non-zero entries.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [145]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Our ratings matrix contains &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; countries and &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; unique songs.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Our ratings matrix contains 55 countries and 2110 unique songs.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [146]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Distribution of # of Unique Songs Streamed per Country'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Unique Songs Streamed'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'# of Countries'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlwAAAGDCAYAAAD+nM7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Construct-training,-validation-sets"&gt;Construct training, validation sets&lt;a class="anchor-link" href="#Construct-training,-validation-sets"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In reviewing literature on recommender engine evaluation, it seems common to create training and validation sets as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Filter your ratings matrix for users (countries) that meet some criterion, i.e. they've streamed above a certain threshold of songs.&lt;/li&gt;
&lt;li&gt;Select a random $x\%$ of their items (songs).&lt;/li&gt;
&lt;li&gt;"Move" these items into a validation matrix; set them to 0 in the training matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To construct this split, I first compute a sensible threshold for Step #1, then "move" a random 20% of songs from the training matrix to the validation matrix.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [147]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The 15th percentile of songs rated by country is &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;The 15th percentile of songs rated by country is 54.5.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's choose 50 songs as our cutoff, and move 20% of the songs in qualifying rows to a validation set.&lt;/p&gt;
&lt;p&gt;NB: The actual ratings matrix is located at &lt;code&gt;RatingsMatrix.R_ui&lt;/code&gt;. This reflects an API choice I made when first starting this project.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [12]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;split_ratings_matrix_into_training_and_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fraction_to_drop&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;training_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;
    &lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deepcopy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        
        &lt;span class="n"&gt;rated_songs_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;rated_songs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;rated_songs_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;n_songs_to_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rated_songs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;fraction_to_drop&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;songs_to_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rated_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        
        &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [152]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;more_than_50_ratings_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;split_ratings_matrix_into_training_and_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_50_ratings_mask&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Evaluation"&gt;Evaluation&lt;a class="anchor-link" href="#Evaluation"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Evaluating recommender systems is an inexact science because there is no "right" answer. In production, this evaluation is often done via the A/B test of a proxy metric important to the business - revenue, for example. In training, the process is less clear. To this end, the authors of the IMF paper offer the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Evaluation of implicit-feedback recommender requires appropriate measures. In the traditional setting where a user is specifying a numeric score, there are clear metrics such as mean squared error to measure success in prediction. However with implicit models we have to take into account availability of the item, competition for the item with other items, and repeat feedback. For example, if we gather data on television viewing, it is unclear how to evaluate a show that has been watched more than once, or how to compare two shows that are on at the same time, and hence cannot both be watched by the user.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Additionally, they state:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It is important to realize that we do not have a reliable feedback regarding which programs are unloved, as not watching a program can stem from multiple different reasons. In addition, we are currently unable to track user reactions to our recommendations. Thus, precision based metrics are not very appropriate, as they require knowing which programs are undesired to a user. However, watching a program is an indication of liking it, making recall-oriented measures applicable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In solution, they propose evaluating the "expected percentile ranking" defined as follows:&lt;/p&gt;
$$
\overline{\text{rank}} = \frac{\sum_{u, i}\tilde{r}_{u, i}^t\text{rank}_{u, i}}{\sum_{u, i}\tilde{r}_{u, i}^t}
$$&lt;p&gt;Here, $\text{rank}_{u, i}$ gives the percentile-ranking of the predicted preference, i.e. if $\hat{p}_{u = 17, i = 34}$ is the largest of all predicted preferences, then $\text{rank}_{u = 17, i = 34} = 0\%$. Similarly, the smallest of the predicted preferences, i.e. the last on the list, equals $100\%$.&lt;/p&gt;
&lt;p&gt;The following class accepts a training matrix, validation matrix and a matrix of predicted preferences. It then exposes the mean expected percentile ranking for both training and validation sets as properties on the instance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ExpectedPercentileRankingsEvaluator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predicted_preferences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate_train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate_validation&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_evaluate_train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_evaluate_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;expected_percentile_rankings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;preferences&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
                &lt;span class="s1"&gt;'predicted_preference'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="s1"&gt;'rank'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="s1"&gt;'percentile_rank'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;ground_truth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ground_truth&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'percentile_rank'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;ground_truth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;denominator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ground_truth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;expected_percentile_rankings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;numerator&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;denominator&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;expected_percentile_rankings&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mean_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mean_expected_percentile_rankings_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_validation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Grid-search"&gt;Grid search&lt;a class="anchor-link" href="#Grid-search"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Next, we perform a basic grid search to find reasonable values for $\alpha$ and $\lambda$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;grid_search_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Result'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'alpha lmbda'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;alpha_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;lmbda_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;alpha_values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;lmbda_values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;implicit_mf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ImplicitMF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;predicted_preferences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;country_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;song_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        
        &lt;span class="n"&gt;evaluator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ExpectedPercentileRankingsEvaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_50_ratings_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preferences&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_validation&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's visualize the results for clarity. I plot the opposite of the validation score such that the parameters corresponding to the &lt;em&gt;darkest&lt;/em&gt; square are most favorable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [159]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;grid_search_items&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
&lt;span class="n"&gt;grid_search_items&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;grid_search_items&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;validation_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;validation_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Reds'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;alpha_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha_values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lbmda_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda_values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lbmda_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\alpha$ = &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\lambda$ = &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; 
            &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'center'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'center'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'w'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Grid Search Results'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\lambda$ Values'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\alpha$ Values'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAyQAAAJSCAYAAAAyBgFsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8jNf+B/DPzGQy2SaLJLIv9sRaDZHYqq1Lq/jpomhR
NMRSlFK7NmppLKW0rlL7RVuqtJdyVYtaQkltSSxF9n2RfTLJTH5/jA6RmSQTyTwT/bxfr3m9OPuM
40xOvs95HlF5eXk5iIiIiIiIBCAWegBERERERPTPxQ0JEREREREJhhsSIiIiIiISDDckREREREQk
GG5IiIiIiIhIMNyQEBERERGRYLghISKTpVarsWfPHgwZMgSdO3dG+/btMWDAAGzYsAElJSVV1t2/
fz9at26tNz81NRWtWrXC+fPnqywzd+5cdO/eHW3btkWvXr2wcOFCZGRk1Po91YXq3psuI0aMQKtW
rSq82rVrhxdffBGrV6+GSqWqp9HqHsu8efMAAOXl5Thw4ACysrKM1j8REZkWM6EHQESkS1lZGUJD
QxEdHY1JkyYhODgYMpkMf/75J9asWYOIiAhs3boVIpFIZ/1+/fqhZ8+ete6/pKQEw4cPR4sWLfDl
l1/CyckJcXFxWLlyJUaMGIEff/wR5ubmtW5fCP3798fs2bO1f8/Pz8fRo0exZs0aWFtbY9y4cUYf
U2RkJGbNmoXjx48bvW8iIjIN3JAQkUnasmULzp8/j/3796Nly5badE9PT3To0AEvv/wyTp48iV69
eumsb2FhAQsLi1r3f+bMGSQkJODAgQOwsbEBAHh4eODzzz9H79698fvvv+PFF1+sdftCsLCwgLOz
s/bvzs7OmDBhAiIiIvDzzz8LsiHhs3mJiIiXbBGRySkvL8euXbswaNCgCpuRv3l7e+Pw4cN47rnn
AGguYerbty8+/vhjBAQE4MMPP6x0WVNSUhLGjRuHjh074oUXXsDvv/9e5RgkEgkA4OTJkxXSvby8
cPjwYQQFBWnTfvnlFwwcOBDt2rXDSy+9hM2bN0OtVmvzz58/j+HDh6Njx45o27Yt/u///g+nTp3S
5r/wwgsIDw9H3759ERQUhKioKJSWlmL16tV47rnn8Mwzz2Do0KG4fPlyhbF89913eOGFF9C+fXsM
Hz4c9+7dq+6j1cnc3Fz7fgEgNzcXc+bMQZcuXRAYGIixY8fi7t272vy7d+9izJgxePbZZxEQEICJ
EyciMTERAJCYmIhWrVrh4sWL2vK60v5Of/vttwEAL774ItatWweVSoXw8HD06NEDbdu2xYABA/Dz
zz/X6n0REVHDwA0JEZmcxMREpKamVvih/3E+Pj4VLteKjY1FQUEBDhw4gNDQ0AplS0tLERISguLi
YuzZswdLly7Fxo0bqxxDcHAw2rRpg+nTp6Nfv35YvHgxjh49ivz8fDRr1gzW1tYANBuWGTNmYOTI
kTh06BBmzpyJHTt2YP369QCAlJQUjB07FgEBAfjxxx+xb98+uLm5YdasWVAqldr+9uzZg08++QRf
ffUV/P39sXjxYnz//fdYsGABDh48CH9/f4SEhCA7OxsAoFKp8OOPP2LdunXYs2cPsrKy8NFHHxn0
OSuVShw4cABnzpzBwIEDAWjO7YwbNw7p6en4+uuvsXv3bri7u+Ott95CTk4OAGDGjBlwd3fHDz/8
gF27diEnJwdz5841qG8AcHNz035Oe/fuxZgxY7B7924cO3YM69atw5EjR/DSSy/hgw8+QEJCgsHt
ExFRw8BLtojI5GRmZgIAHBwcKqQPHDiwwg+mAwYMwKJFi7R/nzhxIry8vAAAV65c0aafPXsW9+7d
w+bNm+Hu7g4AmD9/fpWXKJmbm2PXrl3Yvn07Dh8+jJ07d2Lnzp2QyWQYO3YsJk+eDADYsGEDhg0b
hjfeeAOAJnpTWFiIBQsWYOLEiSgtLcXUqVMxZswY7QZq1KhReOedd5CVlQU3NzcAmihJYGAgAKCg
oADff/89Fi1ahN69ewMA5s2bBwsLC9y/f187xsWLF8PX1xcAMGTIEKxdu7bKz/XAgQM4fPiw9u8K
hQI+Pj6YM2cOhg8fDgCIiIjAtWvXcOHCBe2lamFhYYiIiMB3332H0NBQxMXFoVu3bvDw8ICZmRlW
rFih/TczhEQigZ2dHQCgUaNGsLa2RlxcHCwtLeHh4QFnZ2dMnDgR7du3h729vcHtExFRw8ANCRGZ
nL9/+MzNza2QvmHDBpSWlgJApQiDSCSCp6enzvZu374NBwcH7WYEADp06FDtOCwtLTF+/HiMHz8e
WVlZOHfuHPbu3YsvvvgCjo6OeOuttxATE4Nr167hm2++0dZTq9VQKBRISkqCt7c3Bg0ahO3bt+Pm
zZuIi4tDTEwMAFS4s9XfGykAuHfvHkpLS9G+fXttmpmZGWbNmgUAuHz5MkQiEXx8fLT5tra21d55
rHfv3pg+fTrUajUuXryI8PBw9O7dGyNGjNCWiY6OhkqlQo8ePSrULSkpwZ07dwAAU6dORXh4OHbv
3o2goCD06tULr7zySrWfZ0289dZbOHbsGHr27Im2bduiR48eGDBgAORyeZ20T0REpocbEiIyOd7e
3nBycsLFixfRr18/bfqjG4rHD6yLxWK9d70SiUSVDk9LpdIqx/Ddd9+hvLwcQ4YMAQA4Ojqif//+
eOWVVzBs2DCcPHkSb731FqRSKUJCQjBgwIBKbbi4uODWrVt4++230aFDBwQHB6Nfv34oKyvD+PHj
K5SVyWQ1Htvf7/fxO4xVd0DcxsZGu4lp0qQJ5HI5pk6dCltbW220SCqVwt7eHt99912l+lZWVgCA
kSNHol+/fvjtt99w9uxZLFu2DFu2bMHBgwd19mvILYWbNm2KX375BefOncOZM2dw6NAhfPXVV/j6
668RHBxc43aIiKjh4BkSIjI5EokEb7/9Nvbv36/9rfyjlEql9ixFTfj7+yMnJwexsbHatOvXr1dZ
586dO1i3bh2KiooqpItEIsjlcjg6OgIAmjdvjtjYWPj4+Ghft27dwurVqwEA3377Ldzc3PD111/j
3XffRY8ePZCWlgZA/wbC29sbZmZmFcaoVqvRt29fHDp0qMbvuzovvfQS+vfvj7Vr1+LmzZsAgBYt
WmgvC/v7/Xh6emLNmjX4448/kJOTg08++QRlZWUYPHgwVq9ejW3btuHu3bu4ceOGdjNVWFio7efR
z/1xj2+qdu3ahf/973/o2bMn5syZg59//hlNmjTB0aNH6+x9ExGRaeGGhIhM0rhx4xAcHIxhw4Zh
69atuH37NhISEvDTTz/h9ddfx927dxEQEFCjtrp06YI2bdpg5syZuHbtGiIjI7F48eIq64wePRrl
5eUYOXIkTpw4gaSkJFy5cgWrVq3CxYsXMXr0aADAhAkTcOjQIWzcuBGxsbE4ceIEFi5cCAsLC5ib
m8PV1RVJSUk4c+YMkpKScPDgQe1m5dFLzh5lZWWFt956C6tXr8bJkycRGxuLRYsWITc3F126dDHg
U6zevHnzYG1tjYULF0KtViM4OBjPPPMM3n//fVy8eBH37t3D/Pnz8euvv6Jly5aws7PDqVOnsHDh
Qty4cQNxcXHYv38/bG1t0aRJEzRu3BgeHh7aTcrFixexZs0avc+L+fvmADExMcjPz9dueH777Tck
JSXh+PHjSExMrNEldkRE1DDxki0iMklmZmZYv349Dh48iP3792PDhg0oKiqCu7s7unfvjnXr1mkP
dFdHIpFg06ZNCAsLw8iRI2FjY4P333+/yjtDubq6as+LhIWFISMjA1ZWVujUqRP27NmDFi1aAAB6
9uyJ5cuXY+PGjVi7di0aNWqEQYMGYdq0aQA0lzfduXMH06ZNg0qlQrNmzRAWFoY5c+bg2rVraNas
mc7+Z86cCYlEgrlz56KwsBDt2rXD5s2b4eTkZNgHWY1GjRphzpw5mDVrFv7zn/9g5MiR+PLLLxEe
Ho6JEydCqVTC398fmzdvRvPmzQEAX331FT799FOMGDECSqVSO7a/z3ksX74cS5cuxcCBA7WH5vXd
QKB58+bo27cvpk2bhmHDhmHWrFlQKBQICwtDZmYm3NzcMHnyZLz66qt1+r6JiMh0iMr5VCoiIiIi
IhIIL9kiIiIiIiLBcENCRERERESC4YaEiIiIiIgEww0JEREREREJhhsSIiIiIiISjNFu+3vc2cNY
XREZ5GJBsdBDINLL30pWfSEiAdwu1v0cHSJT8EFRltBDqJHxIluj9bWhPM9ofRmKERIiIiIiIhIM
NyRERERERCQYPqmdiIiIiEgAjAxo8HMgIiIiIiLBMEJCRERERCQAsUgk9BBMAiMkREREREQkGEZI
iIiIiIgEwMiABj8HIiIiIiISDCMkREREREQCEPMICQBGSIiIiIiISECMkBARERERCYCRAQ1+DkRE
REREJBhGSIiIiIiIBMDnkGgwQkJERERERILhhoSIiIiIiATDS7aIiIiIiATAyIAGPwciIiIiIhIM
IyRERERERALggxE1GCEhIiIiIiLBMEJCRERERCQARgY0+DkQEREREZFgGCEhIiIiIhKAiA9GBMAI
CRERERERCYgREiIiIiIiATAyoMHPgYiIiIiIBMMICRERERGRAPgcEg1GSIiIiIiISDCMkBARERER
CYCRAQ1+DkREREREJBhGSIiIiIiIBCDmc0gAMEJCREREREQC4oaEiIiIiIgEw0u2iIiIiIgEwMiA
Bj8HIiIiIiISDCMkREREREQC4IMRNRghISIiIiIiwTBCQkREREQkAEYGNPg5EBERERGRYBghISIi
IiISgBg8RAIwQkJERERERAJihISIiIiISAC8y5YGIyRERERERCQYRkiIiIiIiATAyIAGPwciIiIi
IhIMIyRERERERALgGRINRkiIiIiIiEgwjJAQEREREQmAzyHRYISEiIiIiIgEww0JEREREREJhpds
EREREREJgIfaNRghISIiIiIiwXBD0sC0WvEp/FavqJwhFqPZ/Nnofj0Sz8XeQrstG2Hu7FTzfF1q
U4f+sfqsXYWX1n9eKV0kFqPnogWYdDca0zLiMWj3Nlg1dq5xvi61qUP/bO1XhqPDmpWVM8Ri+C+Y
iz5Rl9Ev7i902roJssfWzirzdalNHfrH6r12JfqsX1MpXSQWo3vYAoTejcLk9DgM2LW10tpZVb4u
talD9UtsxJcpM/Xx0SOazpoBz1EjdOd9+AHchgxG9KSpiBz4GmRubmi3dVON82vTJtHfui+Yg45j
R+vOmz8bbYcPxaGQidj9r/6Qe7jj1T3ba5xfmzaJHtVq9kz4jh6pO2/WDHgNHYw/J03BmQGvwsLd
DZ22ba5xfm3aJPpb1wWz0SFE99oZPH8W2gwfiiMhk/BtnwGw8XDHwN3bapxfmzaJhMINyRMys7eH
38pw9LhxDT1vXUezBXMBAAH//QGeehYZQ1n4eOPZH/bCY9RIFCckVsoXSaXwGvcu7iwJR/bJ35F/
9Tquj5sA+y6BsOvcqdp8XWpTh0yLhYM9+q77DJMTbmNK0h08t/gjAMDbxw/j2Qlj66QPO18fDD1y
EB3HjkZufEKlfLFUioBJoTi1cDFifz2BtMtXcXDEu/DsGgSPoMBq83WpTR0yPVJ7e7RftRx9b0Xh
pb+i4f/RPABAt0MH0WTsmDrpw8rHG10P7IPv6HdQpGftbBoagpjFy5Bx4hRyr17DpZDxcAwKhMOD
tbOqfF1qU4dMi4WDPXqvW4WJ8bcwMfEv9PhEs3YO/eUQOo6vu7Vz8M8H0CFkNPL0rJ3PTgzF6Y8W
I+7XE0i/fBWHRobAo2sQ3Lt0rjZfl9rUofonFhnvZcp4qP0JmNnaotPhg1AVFiJm6nRY+/uh+bzZ
UCQkwtLXB8k7d1eqY+HliW6R5/W2edzZo1KafedOUCQl43roRLTduL5SvrxtG5jJ5cg5c1abpkhI
RHFcPOyDAlFeVlZlfu4fFw1uU1cdMh0yO1sM/+0IlAWF+Hn8ZDi19sdzixYgNy4e9k18cWXLjkp1
bL29MOHmFb1thls2qpTmERSI/MQk/PTOWAzc8XWlfJcO7SCzlSP+1GltWl58Au7HxsGzWxDUZWVV
5idFXDC4TV11yLSY2dqi+5GfoCosxOUp02Dr7wf/+XNQHJ8Aa18fxO3YVamOpZcn/nX5D71t/ujo
VimtUWBnFCcn49K4CQjYtKFSvl27NpDK5cg8/XCdK05IRGFcPByDu6BcVVZlfo6OdbC6NnXVIdMh
s7PFsF+PQFlYiKMTpsCxtT96hM1HXnw87Jr64upW3Wvn2BuX9ba5ysqxUpp7UCDyE5NxaNQ49N9e
+cqDxg/WuYTH1rnc2Dh4dAuGWqWqMj/5fOX/K9W1qasO/bMpFArMnDkTWVlZsLa2Rnh4OBo1qviz
QHh4OCIjI1FWVoYhQ4bgzTffRHZ2NmbMmAGFQoHGjRtj2bJlsLS0rLIvbkiegO+0KTBv7IxzgYNQ
mp2DzKPH4B06Fs3mz0bsqs+hLimpVEeRlIzf2zxjUD+p+/Yjdd9+vfkyd80XcUlKaoX0krQ0yDzc
q82vTZtk2oJnfQBrFxfsejEAxVnZ+OvQEXSePAHPLVqIs5+uhErH3MxPTMIXvn4G9RP9zV5Ef7NX
b778wVwpSE6pkF6Qkgq5p0e1+bVpk0xfy+lTYdHYGcc7/R+U2dlIO/I/NB0/Dv4L5uLWytU6187i
pGQc9W9vUD+Je79H4t7v9eZbumvmkuLxdS41FZYe7tXm16ZNMm1dPpwOK5fG+KZdJxRnZePOoSMI
eG88uoctQEQVa+e/m/gb1E/MN3sRU8XaaVPNOlddfm3aJGGY8oMR9+zZg5YtW2Ly5Mk4dOgQ1q9f
j/nz52vzIyIiEB8fj2+//RZKpRKvvPIK+vbti/Xr16N///547bXXsHHjRnz77bcYNWpUlX1xQ/IE
3IYMRvLO3SjNztGmleXnwUxui8TtlX+LAgBQq6FMz6jTcUgsLVGuUqG8rKxiVyVKiGWyavNr0yaZ
trZvD8WVrTtQnJWtTSvJy4PM1haXN23VWadcrUZhWnqdjsPM0hJqlQrqx+aRqqQEZjKLavNr0yaZ
Pq+hbyJuxy4osx/Oz7K8PJjZ2iJ2m/61s8RIa6dKWfXa+Xd+bdok09bm7aG4pmftvPL1Np11ytVq
FNXx2inVu84pYWYhqza/Nm0SPe7SpUsICQkBAPTs2RPr11e8Sqdjx47w93+4GVepVDAzM8OlS5cQ
GhqqrffZZ59xQ1JfrJo3g7mzE7JPnqqQLhJLkLBpM9RFxTrryTzcEXTmhN52T/q2NHgsKoUCIokE
IokE5SqVNl0sM4e6qKja/Nq0SaarUcsWsG7sjNjjJyqkiyQSXPzyK5Tq+feTe3kgJPKc3nZXO3sb
PJYyhQJiHfNIIpOhtKio2vzatEmmzaZFc8icnZBxouLaCYkEdzd+DZWetdPSwwPPnz2pt93DPs0N
Hou+dU5ibg5VUXG1+bVpk0xXo5YtYNXYGXG/nqiQLpZIELl+I8r0rZ2eHhgVeVZnHgCsa+xj8FjK
ivWtc+YoLSyqNr82bZIwTOVsx969e7F9e8Wbwzg6OkIulwMArK2tkZ+fXyFfJpNBJpOhtLQUs2fP
xpAhQ2BtbY2CgoIq6+nCDUktWfpofjhTJCRp0+w6d4Kljzfyr0XpradMTcOF5/vU6VhKkpIBAOYu
LihJTtamy1xckJGSWm1+bdok02Xvq/nye/SgpEdQIOx9fZB+9ZreegXJqdja5bk6HUt+oub/h42b
q/bP2r8nJ1ebX5s2ybRZeWvWzqLEh4fMHTp3grWPN/KuXddbT5GaipO9etfpWIqTNPNH5uICxaPr
nKsrFClHq82vTZtkumx9NXMzL/7h3HTv0hl2vj7IqGrtTEnFzqBedTqW/AfzyMbVBflJD+eRjZsr
7iSnVJtfmzbpn23w4MEYPHhwhbT33nsPhYWFAIDCwkLY2tpWqpebm4spU6YgMDBQGxWxsbFBYWEh
LCws9NZ7HO+yVUt//3bBzMFem9b8wV1iRCL9291ylQrF92L1vmojPyoaZfn5cOgapE2z8PKEpY83
7p87X21+bdok06V+MDctGjlo03ot+Vjzh2rm5v279/S+aiP96nWU5OXDq3tXbZqttxfsfX2QePpc
tfm1aZNM299rp7n9w7Wz9ccLNH+oZn4W3ovV+6qNvOvRKM3Ph1O3YG2apZcnrH28kXUuotr82rRJ
puvvuWnxyPd6T4HWzowH65xnj27aNFtvL9g9WOeqy69NmyQMkRFfhnr22Wdx8qQmMn3q1CkEBARU
yFcoFBg1ahRef/11TJo0qcb1dJF8/PHHH9dijAa7t+IzY3RjNKW5efAKGQ0bv1YoSUuD77QpsGrS
BGW5uZDa2yH/6jWU5eXVeb9uQ99E6f37yDx67GGiSgUzWzl8Jk9EwY2bMJPboPXa1Si+dw+xq9dW
n/+ARC6HmdwG6uLiGtd5GiQry6ov1IAocnMRMHEcnFr7oSAlDV1nfQCHZk2huH8fFg4OSP3zCkpy
635uthsxDIqc+/jr0BFtWrlKBZmdLbpMn4LM6BuQ2crxysYvkXPnLs6Fr6o2/2/mtnKYy+UoKyqu
cZ2nhbP06Qpkl+bmoenYMZD7+UGRmoaW06fCpmkTlN6/D3N7B9y/crVe1k6vYUNQev8+0o78T5tW
rlJBaitH88mTkBdzE1K5HM98sQaFd2Nx+7PPq83/m5lcDjO5HKri4hrXeRpkl6mqL9SAlOTmouOE
sXBq7YfC1FR0mTVdu3ZaOtgjrZ7WzjbDNWvnncfXTltbdJ4+GZnRMTC3laPvV1/g/p27OL/8s2rz
/1Zp7axBnadF13mzhB5Cjfy5TMfDrutJxzkfGlTez88P3377LXbs2IGYmBgsWLAA1tbWWL58OeRy
OY4dO4ZffvkF6enp+OGHH/DDDz+gc+fOCA4Oxvr167Fnzx7cv38fM2fOhFQqrbKvp+ubzojKcnIQ
PXkami+Yiw47t6Ig5gYuvzUS9sFBaBW+BPlR0Yj/4t9GG8/dpcshMpOizfp1EEvNkPXrCdycNbfG
+QDQcukiOHQNxtmAoBrXIdOjyM7B4bGT8Nzij/D6vl3IuB6Nfa8OgVf3rvjXmuVIv3YdFz5bZ7Tx
nPp4CcRSKfpv2QCJVIq7x47j2Psza5wPAL1XLoN3z+7Y4PdMjeuQaSrNycGfk6bC/6P56LJrG/Ki
b+D80OFw7BqEduHLkBcVhb/WVb69eX25sSQcYjMpnt3wBcRSM6Qf/w3XPpxb43wAaLvsEzh164pf
OgbWuA6ZHkV2Do6MnYSeiz/CoL27kBkVjf2vDYVn9654cXU4Mq5F4Y/Vxls7T4ctgVhqhn6bN0As
lSL22HEcn/ZhjfMB4PkVy+DVsxu+9u9Y4zpkXKZyhkQXS0tLrF1b+ZfQH36omTPt27fXe1h982bD
HgYrKi8vLzd4hLWg6/kaRKbgYgEPmpLp8rfi3W/INN0uVgo9BCK9PijKEnoINbLNztlofY3Krds7
FdYlRkiIiIiIiARgys8hMSYeaiciIiIiIsFwQ0JERERERILhJVtERERERAIw5UPtxsQICRERERER
CYYREiIiIiIiATAyoMHPwUicXuqDF9ISEHT6N1g28RV6OEQAgOb9X8aHhZl4N/Ic7Js2EXo49A/m
+nJfDMhIwvNnT8K6ia/Qw6F/sGb9X8b0ggyMunSW6yKRkXBDYiR5lyJxfdxEWDZrCvfhb9VPJ2Ix
ms2fje7XI/Fc7C2027IR5s5ONa7easWn8FttvCeGkvCSL1zEjyND0KhFM3QYM7Le++uzdhVeWl/9
06pFYjF6LlqASXejMS0jHoN2b4NVY+Pdq52ML+fiJVwKGQ/rZk3hPfLteu+v/cpwdFizsmKiWAz/
BXPRJ+oy+sX9hU5bN0FW3Rpamzpk0lIuXMR/R4bAoUUztBtd/+ti77Ur0Wf9mgppIrEY3cMWIPRu
FCanx2HArq2V1sCalHmS8mQcIiO+TBk3JEaizMhE+sGfkHv+Amza+NdLH00//ABuQwYjetJURA58
DTI3N7TbuqlmdWfNgOeoEfUyLjJdRekZuPH9ASSejUDjtm3qta/uC+ag49jRNSs7fzbaDh+KQyET
sftf/SH3cMere7bX6/hIWCUZmUg++BOyIy7AtnXreu2r1eyZ8NXxg2arWTPgNXQw/pw0BWcGvAoL
dzd02lb104ZrU4dMW1F6Bm7tP4iksxFwble/c7HrgtnoEFJ5XQyePwtthg/FkZBJ+LbPANh4uGPg
7m0Gl3mS8kTGxA2JkRXduQubVq3qvF2RVAqvce/izpJwZJ/8HflXr+P6uAmw7xIIu86d9Naz8PHG
sz/shceokShOSKzzcVHDkH37Dpxa+9VL23a+Phh65CA6jh2N3PiEasuLpVIETArFqYWLEfvrCaRd
voqDI96FZ9cgeAQF1ssYyXQU3LkLuV/dr5EAYOXjja4H9sF39Dsoemy9E0mlaBoagpjFy5Bx4hRy
r17DpZDxcAwKhIOeNbQ2dajhyPnrDhz9629dHPzzAXQIGY28x9ZFsVSKZyeG4vRHixH36wmkX76K
QyND4NE1CO5dOte4jKFtkjDEIpHRXqaMh9qNSOrYCI0HvAKpgwMkNjZQFRToLGfh5Ylukef1tnPc
2aNSmrxtG5jJ5cg5c1abpkhIRHFcPOyDApH7x0Wdbdl37gRFUjKuh05E243rDXxH9DSwdHJEq1cH
wrKRA8zlcijz83WWs/X2woSbV/S2E27ZSGe6R1Ag8hOT8NM7YzFwx9fVjselQzvIbOWIP3Vam5YX
n4D7sXHw7BaEpIgL1bZBDZO5oyPcB74CcwcHmMltUJave4209PLEvy7/obedHx3ddKY3CuyM4uRk
XBo3AQGbNlTIs2vXBlK5HJmnH66hxQmJKIyLh2NwF+ToWENrU4caBksnR7QYVLN1ceyNy3rbWWXl
qDPdPSgQ+YnJODRqHPpvr3glQ+MHa2DCY2tgbmwcPLoFI/n8HzUqY2ibRELihsSIWoR9BIg1QSlr
v1bIu3imFsEqAAAgAElEQVRJZzlFUjJ+b/OMQW3L3DVfwCUpqRXSS9LSIPNw11svdd9+pO7bb1Bf
9HR54dNPIHowL51a++n9YspPTMIXvob/tjD6m72I/mZvjcvLH8zXguSUCukFKamQe1bejNPTo80n
H2nnorxVK+ToWSOLk5Jx1L+9we0n7v0eiXu/15ln6a6Zd4rH19DUVFjqWUNrU4cahueWPVwXHf1b
IeWC7s1lfmIS/t3E8MuwY77Zixg966JNDdbAmpQxtE0ShmnHLYyHl2wZiUO3rnB941VETZgMALDx
a6m/sFoNZXqG3pcuEktLlKtUKC8rq9hUiRJimazO3gc9Xbx7dkfroYPx0+hxAACnKi5PKFerUZiW
rvdVV8wsLaFWqaB+bC6rSkpgJrOos37ItDh27wrPN17DpdBJAFD1ZVtqNUrSM/S+akPfGqpS6l9D
a1OHTJ9Xz+7wH/oGDo8JBYAqL2ctV6tRlJau91UbUr1roBJmFrIalzG0TSIhMUJiBCKpFK1WLEPS
jl3IOnYciuQUWPvpX+BkHu4IOnNCb/5J38qbGZVCAZFEApFEgnKVSpsulplDXVT0ROOnp5NYKkWf
tStxZfN23D1yDPlJyXCu4oYLci8PhESe05u/2tm7TsZVplBArGMuS2QylHIuP5VEUinarwxH3I7/
IP3YcRQnJ0NexebY0sMDz589qTf/sE9zg8egbw2VmJtDVVRcZ3XItImlUvT+fCWubtmBew/WRcfW
VayLnh4YFXlWb/66xj4Gj6GsWN8aaI7SwqIalzG0TRIGIyQa3JAYge+USTCT2+CvT5YCAApv3IB1
K/0REmVqGi4838egPkqSkgEA5i4uKElO1qbLXFyQ8djlBEQAEDTjfZjbynFiQRgAICMqpsoISUFy
KrZ2ea7ex5WfmAQAsHFz1f5Z+/dH5jY9PVpMfQ9SuQ2iw5YAAPJjbsK2iiiyIjUVJ3v1rtMxFCdp
5prMxQWKR9dQV1coUo7WWR0ybYEzpsLcVo7fH6yLmdHVrIspqdgZ1KtOx5D/YF7ZuLogP+nhvLJx
c8WdB5dc1aSMoW0SCYkbknpm2cQXPlPfQ/R770P14FBcQfQNuA5+TW+dcpUKxfdiDeonPyoaZfn5
cOgapD0TYuHlCUsfb9w/p/+APP0z2TdtgqCZ7+NQyEQo8zTzMiMqGm2GDtZbp1ylwv279+p9bOlX
r6MkLx9e3btqz57YenvB3tcHiaf1R2ioYbJu4osW709G5MQpKHuwRuZFx8Bz8Ot665SrVCg0cI2s
Tt71aJTm58OpW7D2nImllyesfbyRdS6izuqQ6bJv2gSBM97HkUfWxczr0fAfZtx1MePBGujZo5v2
nImttxfsHlkDa1LG0DZJGIyQaHBDUs9ahS9Fzu9nkP7jf7VpBTduQubiAjN7e5Tdv18n/ZQrlUjc
uh3NwxZAmZ2N0sxMtApfhpwzZ5F3KRIAIJHLITaXojQru076pIarz+crEffbKdzcf1CblhkVAxs3
V1g42EORUzfzsibMbeWQmJujODMLgOb6+z83bsbzyxahOCsbRRkZ6PP5SsSfOo1kPQdLqeFqt+JT
ZJw6jZRH1sj8Gzdg4eoCqb09SutojayOWqlE7JZtaB22ECVZ2VBmZqLdimXIPH0WORcjteXM5HKI
zc2hzMqqcR1qGF5cswLxJ07h1g8/atMyo2/AxtW466JKqcSVjVvw3LIwFGdloSgjEy+uWYGEU6eR
8uDObTUp8+jaWpPyRELihqQeubz6f7AL7ITz3Z+vkF4YEwMAsPFvVafRi7tLl0NkJkWb9esglpoh
69cTuDlrrja/5dJFcOgajLMBQXXWJzU8/m++Bo+gztgc0LVCekZUNADAqbU/Es8Y7zdmvVcug3fP
7tjg9/DOcqc+XgKxVIr+WzZAIpXi7rHjOPb+TKONiYzD47VBaNS5E37r1qtCel70DQCA3N8P2UaM
NNxYEg6xmRTPbvgCYqkZ0o//hmsfzq1Qpu2yT+DUrSt+6RhY4zpk+vwGvwb3oM7YFtCtQnrmg3XR
sbU/koy4Lp4OWwKx1Az9Nm+AWCpF7LHjOD7tQ4PKPL9iGbx6dsPX/h1r3CYZn8jEnw9iLKLy8vJy
Y3Sk69kZRKbgYgEPn5Lp8rfiHXDINN0uVgo9BCK9PijKEnoINfJ9I1ej9fV6tumeKeZtf4mIiIiI
SDC8ZIuIiIiISAC8YEuDERIiIiIiIhIMIyRERERERAJgZECDnwMREREREQmGERIiIiIiIgHwrr8a
jJAQEREREZFgGCEhIiIiIhKAiPfZAsAICRERERERCYgREiIiIiIiATA+osEICRERERERCYYREiIi
IiIiATBCosEICRERERERCYYREiIiIiIiAYgZIgHACAkREREREQmIERIiIiIiIgHwOSQajJAQERER
EZFgGCEhIiIiIhIA4yMajJAQEREREZFguCEhIiIiIiLB8JItIiIiIiIBiHjNFgBGSIiIiIiISECM
kBARERERCYABEg1GSIiIiIiISDCMkBARERERCUDMGAkARkiIiIiIiEhAjJAQEREREQmA8RENRkiI
iIiIiEgwjJAQEREREQmAzyHRYISEiIiIiIgEwwgJEREREZEAGCDRYISEiIiIiIgEwwgJEREREZEA
RIyRAGCEhIiIiIiIBMQICRERERGRAMQMkABghISIiIiIiATEDQkREREREQmGl2wREREREQmAV2xp
MEJCRERERESCYYSEiIiIiEgAjJBoMEJCRERERESCYYSEiIiIiEgAfDCiBiMkREREREQkGEZIiIiI
iIgEIGKABAAjJEREREREJCBGSIiIiIiIBMDIgAY/ByIiIiIiEgwjJEREREREAuAREg1GSIiIiIiI
SDCMkBARERERCUDE22wBYISEiIiIiIgExAgJEREREZEATDk+olAoMHPmTGRlZcHa2hrh4eFo1KhR
hTLh4eGIjIxEWVkZhgwZgjfffBPJycmYO3cuVCoVysvLsWjRIjRt2rTKvhghISIiIiKiCvbs2YOW
LVti9+7dGDRoENavX18hPyIiAvHx8fj222+xZ88ebNq0Cbm5ufj8888xfPhw7Ny5E6Ghofjss8+q
7YsREiIiIiIiquDSpUsICQkBAPTs2bPShqRjx47w9/fX/l2lUsHMzAyzZs2CXC7Xpslksmr7MtqG
JL201FhdERnESsxAIZmuAG87oYdApJNVYr7QQyBq8Ezlkq29e/di+/btFdIcHR21Gwtra2vk51f8
Py+TySCTyVBaWorZs2djyJAhsLa2hrW1NQDg7t27CA8Px5dffllt/4yQEBERERH9gw0ePBiDBw+u
kPbee++hsLAQAFBYWAhbW9tK9XJzczFlyhQEBgYiNDRUmx4REYGwsDAsX7682vMjADckRERERESC
MOXb/j777LM4efIk2rdvj1OnTiEgIKBCvkKhwKhRozB69GgMHDhQmx4REYElS5bg66+/hoeHR436
EpWXl5fX6ej12GPf2BjdEBkss1Qt9BCI9HqtpZPQQyDSKYaXbJEJ652RJPQQauSSu4/R+gpIjjOo
fHFxMWbNmoWMjAxIpVKsWrUKzs7OWL58OV566SVERkbiiy++qHCOZOnSpZg0aRKUSiWcnZ0BAE2a
NMGiRYuq7IsbEvrH44aETBk3JGSquCEhU9ZQNiR/ehhvQ9IxybANiTHxNC8REREREQmGZ0iIiIiI
iAQgEpvuGRJjYoSEiIiIiIgEwwgJEREREZEATPgmW0bFCAkREREREQmGERIiIiIiIgEwQqLBCAkR
EREREQmGERIiIiIiIgGY8pPajYkREiIiIiIiEgwjJEREREREAmCARIMREiIiIiIiEgw3JERERERE
JBheskVEREREJAAeatdghISIiIiIiATDCAkRERERkQAYINFghISIiIiIiATDCAkRERERkQDEDJEA
YISEiIiIiIgExAgJEREREZEAGCDRYISEiIiIiIgEwwgJEREREZEA+BwSDUZIiIiIiIhIMIyQEBER
EREJQMTQAABGSIiIiIiISECMkBARERERCYBnSDQYISEiIiIiIsEwQkJEREREJAAGSDQYISEiIiIi
IsFwQ0JERERERILhJVtERERERALgoXYNRkiIiIiIiEgwjJAQEREREQmAARINRkiIiIiIiEgwjJAQ
EREREQlAzBAJAEZIiIiIiIhIQIyQEBEREREJgAESDUZIiIiIiIhIMIyQEBEREREJgM8h0WCEhIiI
iIiIBMMICRERERGRABgg0WCEhIiIiIiIBMMICRERERGRABgh0WCEhIiIiIiIBMMICRERERGRAERi
hkgARkiIiIiIiEhA3JAQEREREZFguCFp4Dp9tgKBaz+rlC4Si9Fh4TwMunENbyTeQ7ftm2Hh7Ky3
HUPLE9VEr89X4oUv11RKF4nFCA6bjzF3ohCaFouX/7MFlo2rnp+GlCeqjv28j2G/8JPKGWIxbCdP
h+v/TsHtzCU0WvE5xI0c9TdkaHmiavit+BT+q1dUzhCL0Wz+bPS4HolesbfQbstGmDs76W/I0PIk
CJHIeC9Txg1JA9Zu7iy0GPOOzry2c2bCd9gQRIx/D8dfGQgrdzd037FFb1uGlieqTpf5s9EuZJTO
vMB5H8Lv7aE4NnYi9vcdCBsPd/TbtVVvW4aWJ6qKfMJkWL8xVHfe+PdgNWAQchbMRua7IyBp7IJG
K9fqb8vA8kRVaTprBjxHjdCd9+EHcB8yGFGTpuLSwNdg4eaG9ls36W/LwPJEQuKGpI6Z29uj8+qV
eO1ODF67dxMdPl4AAOj9809oMe7dOunD2scHL/y0H83HvIPChIRK+WKpFK1Cx+HqJ0uQeuIkcq5c
w9l3Q+Ec3AVOgZ2fuDw1XDIHezy/dhVC4m5ibMJtdP1kIQDg9WP/RfvxIXXSh62vD149fADtQkYh
L173/HxmYijOfbwYCb+eRMblqzjyzli4dw2Caxfd89OQ8tQwiWztYD8vDK6/noPbiQjYTvkAAOC0
ZReshw6vkz4kHp5w2rgd1oOHoSwlqXIBMylsho1E3rrVKDl/FqU3opE9+wPIOgbAvEPHJy9PDZKZ
vT38Voaj541reO7WdTRfMBcA0Om/P8ArZHSd9GHp441nf9gLz1EjUZyQWClfJJXCe9y7+GtJOLJP
/o78q9dxbdwE2HcJhF3nTk9cnoQjFomM9jJlvMtWHZLa2eJf/zuE0sJCnH/vfdj5+6HDwnkojE+A
TRNf3Nn+n0p1rL29MPDqJb1t7rFvXCnNuUtnFCUl4+y749F1y1eV8u3btYXUVo7002e1aYXxCSiI
i4NzcBAyL/zxROWpYTK3s8Xg4z9DWViI4xOmwrG1H4LD5iMvLgF2TZsgauvOSnXk3l4YFfOn3jbX
WVcO/7sFBaIgKQlHR49D322Vfxvn3L4tzG3lSDp1RpuWH5+A3Ng4uHcLQur5P56oPDU8Ihs5nLft
QXlxEe6HzYVZs5awmzwNqpQkmHl6oXD/d5XqSNw84Hr4uN42kzr6VUoz79ARqrQUZM/5AI0+rXyp
q7SVH8Q2Nii5eEGbpkpJQllSIsw7BkB55c8nKk8Nj5mtLTofPghVYSGip06Hjb8fms+bjeKERFj6
+iBp5+5KdSy8PNE98rzeNn9x9qiUZte5E0qSknE9dCLabVxfKV/etg3M5HLknHn4Pa1ISERxXDzs
gwKR+8fFJypPJDRuSOpQmw+mwcKlMY517AJldjaSfj6KVhND0eGjeYha8RnUJSWV6hQlJuGHlm0N
6if2u32I/W6f3nwrD3dN28kpFdKLU9K0eU9Snhqmzh9Oh5VLY+xr3xmKrGzcO3wEz7w3Hl3D5uOP
8FVQ6ZifBYlJ2Ny0tUH93PxmL25+s1dvvs2DOVX42HwrTEmF3KPyF7Wh5anhkYeMh8TJCWkD+0B9
/z5w8jfYDH8HtpOnI3/TvwGlslIdVVoKUnp3N6if4sM/ofjwT3rzJS6umrYz0ir2lZEOiYvbE5en
hsd32hSYN3bG2cBBKM3OQebRY/AOHYvm82fj3qrPdX6vK5KScarNMwb1k7pvP1L37debL3PXzKeS
lNQK6SVpabDQ8T1taHkSjokHLoyGG5I61GTYm7iz/T9QZmdr00rz8iG1leP2lu0665Sr1VCkp9fp
OMwsLaFWqVBeVlYhXa0sgcRC9sTlqWHye3sIorbthCLr4fwsycuDzNYW177eprNOuVqNorQ6np9W
VlCrVFA/Nt9USiUkFhZPXJ4aHqsBg1C4f69mM/JAeX4+IJejcO83uiup1VBnZdbpOEQWFihXqYDH
5hqUSohk5k9cnhoe9yGDkbRzN0qzc7RpZfl5MJPbInH7Dt2V1Goo0zPqdBwSS0uU6/qeLlFCLKv8
PW1oeSKhcUNSR+QtmsPC2Rmpv52skC6SiHFrwyaoiop01rPy9EC/iNN6293n2cTgsZQpFBBLJBBJ
JJovywfE5jKU6RiHoeWp4XFo2RxWzs6I//VEhXSRRILL6zfq/Xe28fTA25fO6MwDgK9cfA0eS1lx
sc75JjE3R2lR4ROXp4bFzLcJJI0cURJxtmKGRILC3TtQrijWWU/i6obG3/9Xb7sp3QIMHkt5SQlE
EgkgkQCPzDWYm6O8uPI4DC1PDYtV82Ywd3ZC9slTFdJFYgkSNm2Gukj3v7HMwx3BZ07obfeEb0uD
x6JWKCDS9T0tM9f584Wh5Uk4IoZIAHBDUmdsfLwBoMIhc6fAzrDx8UHOtet66xWnpOJIjxfqdCxF
iZrDmpauLihKStamW7q5oPhw6hOXp4bH1tcHAJAf//CwpGuXzrDz9UHmVf3zszAlFd8EP1+nYyl4
MN+sXV1Q8Mh8s3ZzReGhyvPN0PLUsEg8PAEAqpSH/7bmHTrCzMMTpTdv6K2nykhH+tBX63QsqlTN
ZYESJ2eo0h7OLYlzYyjS0564PDUslg++1xUJD2+AYNe5Eyx9vJF/LUpvPWVqGs4/36dOx6J4sPaZ
u7igJPnh/xWZi0uly7JqU55IaNyQ1JFylRoAYO7goE17JkxzB6OqLhAsV6lQcO9enY7l/vUolObl
o3G3rtqzJtbeXrDx8UH62XNPXJ4aHvWD35BZONhr07ot/kjzh2rmZ+7dup2fGdeioMzLh0ePbtqz
JnJvL9j5+iDp9NknLk8NzIO1U2xnp02ynTpD84eqfnOoUkGVEF+nQym9dQPqggKYB3TWnjWRuHnA
zMMTJZGVDwEbWp4alr8jC9JH1s0WH83T/KGadbP4XmydjiU/Khpl+flw6BqkPWti4eUJSx9v5Jyr
fIDe0PIkHAZINLghqSPZl6+grLgYz4QtRPSq1fB+bRDMHeyR99cd+Lz2KnKjY1Co4xao9UGtVOL2
5q145pOPUZKVBUVmJjqtDEfa6TPIuqi5o5fUVg6x1BwlWVk1Kk8NW8afmvnZbfFH+GPFarR4fRAs
HByQc/sOWg5+FVlR0cg34vy8tmkLui39GMVZWSjOyESv1cuReOoM0v7QzDdzWznE5uZQZGbVqDw1
XKUxUShXKGA7dQbyN2+AZZ9+ENvZoSwuFpZ9+6H09i2odN2it14GU4rCvbthN+1DqO/nQJ2dDfs5
C1Fy8QJKr10BAIhsbCCSSqHOyalReWq48q9cg6q4GM0/mofY1WvhMmggzOztUXjnLlxeHYiCmBgo
dNyitz6UK5VI3LodLcIWoDQ7G8rMTPiFL0POmbPIuxQJAJDI5RCbS1GalV2j8kSmhBuSOqLMyUHE
hMl4JmwBeuzZidzoGJwc8jYadw1GwIpPcf96FGLWfmG08VxdvAxiqRTBG9dDZCZFyvFfcXHGbG3+
s58uQePuXfFT+041Kk8NmyI7B8fGvYeunyxE/+/+g6yoGPz0+jC4dw/Gc5+FI/NaFCJXrzPaeM6F
LYVYKkWfr/8NsVSK+GO/4sT0D7X5PVcshUePbtje+tkalaeGS517HzkLZ8N26gw4rlmP0tu3kTU5
FLKAzrCbvQClt26gYPtmo40n78vPATMpHBavgMjMDIqzvyP300XafLuZ8yDrFIi0V16sUXlquEpz
chA1eRpaLJiLDju3oiDmBi6/NRIOwUFoFb4EBVHRiPvi30Ybz52lyyEyk6LN+nUQS82Q+esJ3Jw1
V5vfaukiOHQNxpmAoBqVJ9PAMyQaovLy8nJjdKTreRpEpiCzVC30EIj0eq1l5We9EJmCmMR8oYdA
pFfvDCNFVp9QVrBhj354Eo7n9J8ZFRojJEREREREAmCAREMs9ACIiIiIiOifixESIiIiIiIB8AyJ
BiMkREREREQkGG5IiIiIiIhIMLxki4iIiIhIACKGBgAwQmI0Hv1ewtDsVPSL+B02TZoIPRwiAECT
V17Ge/npePviGdg15bwk4Vj0egHul6LR+Pv/QuLlLfRw6B/M+aU+eDEtAUGnf4NlE1+hh0P0j8AN
iZFk/XEJZ8aMg7x5MzR7Z3i99CESi9Fh4TwMunENbyTeQ7ftm2Hh7Fzj+p0+W4HAtZ/Vy9jINKX+
cRFH3hkL+xbN0Gb0iHrvr9fnK/HCl2uqLScSixEcNh9j7kQhNC0WL/9nCywb13wuU8OjvHoFObOn
w8zbF9avDq73/uznfQz7hZ9UTBSLYTt5Olz/dwpuZy6h0YrPIW7kWHVDtalDJi33UiSujZsIq2ZN
4TH8rXrvz2/Fp/BfvaJioliMZvNno8f1SPSKvYV2WzbC3NnJ8DJPUp6MQiQSGe1lyrghMRJFRgYS
DvyIjIjzsG/Tul76aDtnJnyHDUHE+Pdw/JWBsHJ3Q/cdW2pUt93cWWgx5p16GReZruL0DPy1/yCS
z56HU9v6mZd/6zJ/NtqFjKpR2cB5H8Lv7aE4NnYi9vcdCBsPd/TbtbVex0fCUmdnofjYESgvR0La
slW99iWfMBnWbwytnD7+PVgNGIScBbOR+e4ISBq7oNHKtVW3VYs6ZNqUGZlIP/gTcs9fgE0b/3rt
q+msGfAcVfmXQU0//ADuQwYjatJUXBr4Gizc3NB+6yaDyzxJeSJj4obEyPL/ugM7v7r/shVLpWgV
Og5XP1mC1BMnkXPlGs6+Gwrn4C5wCuyst561jw9e+Gk/mo95B4UJCXU+LmoY7v91B438/eqlbVtf
H7x6+ADahYxCXnz1c0wsleKZiaE49/FiJPx6EhmXr+LIO2Ph3jUIrl30z2V6OpTFxcKsafN6aVvi
4QmnjdthPXgYylIee4qzmRQ2w0Yib91qlJw/i9Ib0cie/QFkHQNg3qGj7gZrU4cajMI7d2HTqn42
x5Y+3nj2h73wHDUSxQmJFfJEUim8x72Lv5aEI/vk78i/eh3Xxk2AfZdA2HXuVOMyhrZJAhGLjPcy
YTzUbkQyR0d4/d8AyBwcYCa3QVl+gc5y1t5eGHj1kt529tg3rpRm364tpLZypJ8+q00rjE9AQVwc
nIODkHnhD51tOXfpjKKkZJx9dzy6bvnKwHdETwMLJ0c0HzQAFo0cIJXboFTPvJR7e2FUzJ9621ln
rTv07xYUiIKkJBwdPQ59t1X/2zjn9m1hbitH0qkz2rT8+ATkxsbBvVsQUs/rnsvU8IkdHGDZuw/E
dvYQWVujvLBQZzmJmwdcDx/X205SR92ba/MOHaFKS0H2nA/Q6NOKl6dKW/lBbGODkosXtGmqlCSU
JSXCvGMAlFcqz/3a1KGGQerYCC4DXoHUwQESGxuoCnSvixZenugeeV5vO784e+hMt+vcCSVJybge
OhHtNq6vkCdv2wZmcjlyzjz8PlckJKI4Lh72QYHI/eNijcoY2iaRkLghMaKOS8IgEmuCUnZ+fsjS
swAUJSbhh5ZtDWrbysNdUzc5pUJ6cUqaNk+X2O/2Ifa7fQb1RU+XHssWaeelo78fUi/onpcFiUnY
3NTwy7pufrMXN7/ZW+PyNg/ma+Fjc7kwJRVyD91f7vR0sJs+C3gwF6XNWkB59bLOcqq0FKT07m5w
+8WHf0Lx4Z905klcXDVtZ6RV7CsjHRIXtzqrQw1Dy7CPtHPRxq8Vci/q/iWhIikZp9o8Y3D7qfv2
I3Xffp15MnfN3ClJSa2QXpKWBosH62NNyhjaJgnExM92GAsv2TKSxj26wWfw6zg3dgIAVHnZVrla
DUV6ut6XLmaWllCrVCgvK6uQrlaWQGIhq7s3Qk8Vj57d0XLIG/jfmPEAUOVlW+VqNYrS0vW+6oqZ
lRXUKhXUj81llVIJiYVFnfVDpsW8UxdYvjwA2XNnAgDMmlVx2ZZaDXVWpt5XbYgsLFCuUgGPzTso
lRDJzOusDpk+h25d4frGq7g+YTIAwNqvpf7CajWU6Rl6X7UhsbREua7v8xIlxDJZjcsY2iaRkBgh
MQKxVIrOq5bjzradSP7fLyhKSoZ9a/0/+Fl5eqBfxGm9+fs8K9+etUyhgFgigUgi0XxB/t23uQxl
RUVP9gboqSSWSvH8mhWI2rIDsUePoSApGY5t9M9LG08PvH3pjN78r1x862RcZcXFOueyxNwcpUW6
L+GhBs5MCvu5H6Fw/3coOX0SqrRUSJu10Ftc4uqGxt//V29+SrcAg4dQXlICkUQCSCTAI/MO5uYo
Ly6uszpk2kRSKfxWLEPSjl3IOnYciuQU2PjpXxdlHu4IPnNCb/4J3yo2M3qoFQqIdH2fy8yhevB9
XpMyhrZJwjD1u18ZCzckRuA/bQrM5HJcCdPcYjI35gbsqljgilNScaTHCwb1UZSoOaBp6eqCoqRk
bbqlmwuKD6fqq0b/YAEfTIXUVo6zCxcBALKiY6qMkBSmpOKb4OfrfVwFD+aytasLCh6Zy9Zurig8
xLn8NJKPGQuxjQ3y1q4CAJT+dbvKCIkqIx3pQ1+t0zGoUjWXCEqcnKFKezjPJM6NoUhPq7M6ZNp8
p0yCmdwGtz9ZCgAouHED1q30byqUqWk4/3yfOh2D4sG6Z+7igpLkh2ugzMVFe8lVTcoY2ibR4xQK
BQDY17MAACAASURBVGbOnImsrCxYW1sjPDwcjRo1qlAmPDwckZGRKCsrw5AhQ/Dmm29q8y5cuICZ
M2fi5MmT1fbFS7bqmU2TJmg9bQoiZ89DaV4+AOB+dAzsqro0RqVCwb17el+63L8ehdK8fDTu1lWb
Zu3tBRsfH6SfPVe3b4oaPLumTdBpxlT8PnMulA/mZVZUDBxb67/FZblKhdy79/S+6krGtSgo8/Lh
0aObNk3u7QU7Xx8kPXLTBno6SLy8IR89DveXL0H5g4PDpX/dqjJCApUKqoR4va/aKL11A+qCApgH
PLyTm8TNA2YeniiJ1H2uqjZ1yHRZNvGF79T3cHPeR1Dla9bFgugbsPGv4hJrlQrF92L1vmojPyoa
Zfn5cOgapE2z8PKEpY83cs6dr3EZQ9skgZjwXbb27NmDli1bYvfu3Rg0aBDWr694A4aIiAjEx8fj
22+/xZ49e7Bp0ybk5uYCAFJSUrB161aUPX5Jqx6MkNSzTqvCkXbydyQcfHiQMjfmBixdXWBubw/l
/ft10o9aqcTtzVv/n707j4+quv8//rqzZBKyhxACYReooFJZREBA3KVaRFuUzSouRQWtooAbBTcs
lKX26xf9urXyVRbR31dpVaxVFpHNIq6AoCgEAglZCNknM3N/fwwEQzLJBDJzh/B+Ph7zIHPuWT7R
YZgzn3PO5dwnZlCRl0d5bi595swie+2n5B3ZjOdMiMfmjKIiL69RxpRT15C/zCZz1Sd8/3/Lq8ry
tm4jNr0lruQkKgoa53UZjKiEeGxRUZTn+l+XPrebr198hQtmzqAsL4+yg7kMmT+bvWs+JfuzwKfP
yakp6aHpVGzaQPm/P6gq8/ywE3uLNIyERMzDheEJpLKSkmWLSLxvCr5DBfjy80l66I9U/GcTlV9/
WVXNiIvDcDrxFRQE3UZODWfOmkn+J5+Ss/zYcsCS7d/hatkSR1ISnkb697o+ptvN3r+9SpfHplGZ
n487N5czZz1NwafrOLz586Dr2OPjsUU5qczLD6q+yPE2b97MbbfdBsDgwYNrTEh69uxJt27Hvsj0
er04HA4qKiqYPn06TzzxBNddd11QY2lCEkLtf3MtqX3P473+g6qVH9q6FYDE7mdycN2GRhvvqyef
xuZ00v+FBRgOJ/s/+pj/PPBg1fVef3qKtIED+EcPnTl+Ousy4jrSzz+PRX2qn1KU9+02AJp370bW
p+HLqg3+80wyBl3Aq917VZWtf2wmNqeTy196DpvTyZ4PP2bVpClhi0nCI+bKq4j65bnk/PbX1cor
d+4AwNm5C+4wZhoO//cz4HCS/OSfMRwOytd9QuGfHq9WJ3HyI7j69CX7qkuCbiORr+W115DYtw8b
BlZfllq8zf++GNftFxwKYybhh5mzMRxOzlrwX9icDnI/XsV3Ux9uUJ1fzHyc5AH9+bR3v6D7FAtE
yB6SZcuW8eqrr1Yra968OfHx8QDExsZSdCRzeJTL5cLlclFZWcmDDz7IDTfcQGxsLI888gi33HIL
LVu2DHp8wzRN8+R/jfrVdu8MkUiQW+mzOgSRgK7rWvv9XUSstm1vUf2VRCxy6cF99VeKAIcva/gh
HCcq4cOGrTKYOHEiv//97+nRowdFRUWMGjWKf/6z+oEihYWF3HPPPfTt25cJEyaQnZ3N7373O9LS
/J/7t2zZwmWXXcb8+fPrHEsZEhERERERCxgRfAf1Xr16sXr1anr06MGaNWvo3bv65Km8vJybb76Z
cePGMWzYMABatmzJBx8cW4J7wQUX1DsZAW1qFxERERGR44waNYqdO3cyatQoli5dysSJEwGYPXs2
X331FUuWLCEzM5Nly5Zx4403cuONN5KZmXlCY2nJlpz2tGRLIpmWbEmk0pItiWSnypKtoivCt683
/oPIPf1PS7ZERERERKwQIZvaraYlWyIiIiIiYhllSERERERELBDJm9rDSRkSERERERGxjDIkIiIi
IiJW0B4SQBkSERERERGxkDIkIiIiIiJW0B4SQBkSERERERGxkDIkIiIiIiIWMLSHBFCGRERERERE
LKQMiYiIiIiIFbSHBFCGRERERERELKQMiYiIiIiIFbSHBFCGRERERERELKQMiYiIiIiIBQylBgBl
SERERERExEKakIiIiIiIiGW0ZEtERERExAra1A4oQyIiIiIiIhZShkRERERExAKGbowIKEMiIiIi
IiIWUoZERERERMQK2kMCKEMiIiIiIiIWUoZERERERMQK2kMCNDBDUlxcTEVFBQA7duzgpZdeYsOG
DSEJTEREREREmr6gJySrVq1i0KBBbN68mczMTMaMGcOyZcu44447WLJkSShjFBERERFpcgzDCNsj
kgU9IZk/fz7jx4+nf//+vPnmm6SmprJixQrmzJnDyy+/HMoYRURERESkiQp6D8mPP/7INddcg2EY
fPzxx1x66aUYhkG3bt3IyckJZYwiIiIiIk2P9pAADciQpKWlsX37drZv387OnTsZMmQIAGvXriUj
IyNU8YmIiIiISBMWdIZk3Lhx3H333dhsNs4991x69+7NggULWLBgAX/6059CGaOIiIiISNMT4Xs7
wiXoCcmYMWPo1asXu3fvpkuXLng8HgYMGMDFF1/MmWeeGcoYRURERESkiQp6QuLxeFi+fDmvvfYa
Xq+XDz74gL/97W84HA6eeOIJmjVrFso4RURERESalEg//Spcgt5D8swzz7B27VpeeeUVXC4XADfe
eCNbt27Vki0RERERETkhQU9I3n33XWbMmMF5551XVdanTx9mzpzJhx9+GJLgRERERESkaQt6yVZB
QQHNmzevUR4TE0N5eXmjBiUiIiIi0uTp2F+gARmS/v378+KLL2KaZlVZUVER8+bNo1+/fiEJTkRE
REREmragMyTTp09nwoQJ9O/fn4qKCsaPH8/+/ftp06YNzz//fChjFBERERFpcrSp3S/oCUnLli15
8803Wb9+Pbt27cLj8dCxY0cGDhyIzRZ0okVERERERKRK0BOSo/r370///v1DEYuIiIiIyOlDe0iA
BkxIRo8eXWda6fXXX2+UgERERERE5PQR9IRkwIAB1Z57PB4yMzNZvXo1EyZMaPTARERERESaNO0h
ARowIZk4cWKt5W+//Tbvvfce48aNa7SgRERERETk9HDSu9F79+7Nxo0bGyMWEREREZHThmEzwvaI
ZEFnSDIzM2uUlZSU8Morr5CRkdGoQYmIiIiIyOkh6AnJZZddhmEY1W6MCNCqVStmzpzZ6IGJiIiI
iDRp2kMCNGBC8tFHH1V7bhgGTqeT1NRU3dRFREREREROSJ0TktqWaf2c1+tl7969ALRt27bxohIR
ERERaeoifG9HuNQ5ITm6TKsupmliGAbbtm1r1MBERERERKTpq3NCcvwyLRERERERaRza9uBX54Qk
2NOz3G53owQjIiIiIiKnl6A3tefk5PD888+zc+dOfD4f4F+u5Xa72bVrF59//nmd7S9ol3xykYqE
yOKdB60OQSSgVq8+Z3UIIrVKX/+h1SGInPq0hwRowI0RH374YdatW0fPnj358ssv6dWrFy1atGDr
1q1MmjQplDGKiIiIiEgTFXSGZPPmzbzyyiv07NmTTz/9lCFDhtC7d29eeOEFVq5cydixY0MZp4iI
iIiINEFBZ0hM06Rly5YAdO7cma1btwIwdOhQvvnmm9BEJyIiIiLSVBlG+B4RLOgJyVlnncXbb78N
QLdu3Vi7di1Q/71KREREREREAgl6ydYDDzzAHXfcQUxMDMOHD+ell15i6NChZGdnc80114QyRhER
ERGRpifCMxfhUueEpLCwkMTERAB69uzJxx9/TFlZGcnJybz11lv8+9//JikpiaFDh4YlWBERERER
aVrqnJBccMEFDB48mGHDhnHxxRcTGxtLbGwsAC1btmTMmDFhCVJEREREpMlRhgSoZw/Js88+S3x8
PNOmTWPAgAE89NBDrF+/HtM0wxWfiIiIiIg0YXVmSIYMGcKQIUNwu92sWbOGFStWMGHCBGJjY7nq
qqsYNmwY3bt3D1esIiIiIiJNhy3o86WatKA2tUdFRXHppZdy6aWX4na7WbVqFe+//z5jx44lPT2d
YcOGcccdd4Q6VhERERERaWIaPC2Liori8ssvZ+7cufz1r3/F5XLxzDPPhCI2EREREZGmS/chARpw
7C+Ax+Ph008/5V//+hcfffQRdrudK6+8kunTp4cqPhERERERacLqnZC43W4++eQTPvjgA1atWoXH
4+GSSy5h9uzZXHDBBdjt9nDEKSIiIiLStER45iJc6pyQTJo0idWrV1NRUcHAgQOZPn06l1xyCdHR
0eGKT0REREREmrA6JyTZ2dlMnjyZK6+8kqSkpHDFJCIiIiLS9ClDAtQzIXn99dfDFYeIiIiIiJyG
GrSpXUREREREGonuQwKcwLG/IiIiIiIijUUTEhERERERsUzQE5LRo0ezadOmUMYiIiIiInL60I0R
gQZMSMaOHctTTz3FzTffzJYtW6rKd+/ezYABA0ISnIiIiIiING1Bb2pPS0sjMTGRTZs2MXr0aDp2
7Eh0dDSZmZl07tw5lDGKiIiIiDQ9EZy5KC8vZ/LkyeTl5REbG8usWbNISUmpVmfWrFl8/vnneDwe
brjhBq6//npKS0uZMWMGe/fupbKykmnTptGjR486xwo6Q/LII4+Qnp7Oc889xyuvvMKYMWM4ePAg
7dq144UXXjix31RERERERCLO4sWL6dq1K4sWLWL48OEsWLCg2vUNGzawZ88eli5dyuLFi3nxxRcp
LCzk5ZdfpkuXLixatIgnnniCXbt21TtW0BOSAwcOMHHiRC688EL69+/PmDFjeP/990lLS+OJJ55o
+G8pIiIiInI6i+A9JJs3b2bQoEEADB48mPXr11e73rNnT2bOnFn13Ov14nA4WLt2LU6nk1tvvZUF
CxZU9VGXoCckPXv25P33369WFhcXx5QpU/jggw+C7UZERERERCLIsmXLuPrqq6s9ioqKiI+PByA2
NpaioqJqbVwuF4mJiVRWVvLggw9yww03EBsbS0FBAYcPH+bll1/m4osvZtasWfWOH/QekgceeICx
Y8eyY8cORo0axVlnnYXdbuef//wnsbGxDfy1RUREREROcxFyY8QRI0YwYsSIamUTJ06kpKQEgJKS
EhISEmq0Kyws5J577qFv376MHz8egKSkJC6++GIALrrooqC2dgQ9ITn77LNZuHAhs2fPZuzYsRiG
gWEY+Hw+Jk2aFGw3IiIiIiIS4Xr16sXq1avp0aMHa9asoXfv3tWul5eXc/PNNzNu3DiGDRtWVd67
d29Wr17N2WefzWeffRbU4VeGaZpmQwPMzc3lhx9+oKioiDPPPJM2bdrU22ZPj180dBiRsFi886DV
IYgENHnjW1aHIFIrc/2HVocgEpB9/Mz6K0UAz6NjwzaW48nXGlS/rKyMqVOncvDgQZxOJ3PnzqVF
ixbMnj2bK6+8ks8//5xnn32Wbt26VbWZOXMm8fHxPProoxw8eBCHw8GsWbPqnSuc0ITkRGhCIpFK
ExKJZJqQSKTShEQimSYkNTV0QhJOQS/ZEhERERGRRhTB9yEJp8jYSSMiIiIiIqclZUhERERERKyg
DAmgDImIiIiIiFhIGRIREREREQsYEXIfEqvpv4KIiIiIiFhGExIREREREbGMlmyJiIiIiFhBm9oB
ZUhERERERMRCypCIiIiIiFhBGRJAGRIREREREbGQMiQiIiIiIlZQhgRQhkRERERERCykDImIiIiI
iBV0Y0RAGRIREREREbGQMiQiIiIiIlbQHhJAGRIREREREbGQMiQiIiIiIlZQhgRQhkRERERERCyk
DImIiIiIiBWUIQGUIREREREREQspQyIiIiIiYgXdhwTQhKTJSX70MQyHnfwZj9Zd0WYjceK9xF1z
LUZsLOWffkL+U4/jy88LT6ByWrr8r3OxORysuOsPddYzbDYGzXiEc8aOIio+jh8//Jh/3TuZ0pyD
YYpUTjdGRlcwDMy939VfN70TJKeDzQ5F+ZhZO8BTGYYo5XRlDLoGDBvmmv+rp6KBcd5lGF17QZQL
MnfiW7scyorDE6jICdK0rAlJvOse4q8fGVzdO+8mdti15D0ylexxY7G3TCd13n+FOEI5nQ2c9hA9
bx8XXN1HH+TssSN597a7WHTZ1cRntObaxa+GOEI5XRktO2I0zwi6LsnpmJnbMHdtAacLo/3ZIY5Q
TmdGn0uxdT8/uLq9L8Ho2gvfyjfxLX8RYhOwXTY6xBGKnDxNSELMlpBI8rTHyFi9gYxPNpJ07wMA
pP19EXGjxjbKGPaMNqS9tJC460fhydpXfwOHk/gxv6Pwr/Mo37COym1byZ0yiehevYn6Zc9GiUlO
DdHJSVzxX/O4O3Mn9+z7gQufnA7AmI/eo9edtzfKGIkd2jNyxTv0vH0chXsy661vczrpPWE8a/74
JD99vIrsL77inRtvpc2AfmT069soMckpwO7AyOiK0f0CjO4D/VkJwDijJwQ5eahXVDRGp3OheWtM
d3n99Q0DUttgHtgFxQVQVoy5ZytGbBI0S2icmOTU4IrBGDQc2+8ewXbToxjnXwGAbdjvMc7q3zhj
xCdju/o2jO7nYxYV1F/fZsc4ZwDmpn/Bvu8hNwvfv5dgtOoALds1TkzS+AwjfI8IpiVbIWTEx9Py
f5fgKy0lf/rDODt3IemeSXiy9uFs25bit96o0cbeOoOMFR8H7HNPj1/UKHOd2wtv9n5yp04idfa8
euOKOvNMbHFxlP9nU1WZN2sfnn17cfXqg/vLLUH+hnIqcyUmMHblCtzFJbx/x92kdu/GhY9Po3D3
HpI6duDLVxbWaJPQri13fvdlwD5nxaTUKMvo15eivfv4x023M2zhS/XG1fKX5+BKiGfPmrVVZYf3
ZHLop920uaAf+zZsqqO1NAk2B8YZvcDnxdy7HVxx2Fp18k8aomIgf3/NNs5obN0CfxD0fbWyZmGz
RKis8E8q2nWvP67oOAy7A7P40LGyynJMdxnEJkHp4SB+OTnlRUVju+YO8FTgW/0WRkpLbH2vwFd0
CBJSMLd/VrNNXBL2MVMCdun9n4drlBnp7TFLDmF+tATbpUGsfmjeCiMqGl/WrmNlxYcwD+djpHfA
zN4TzG8nYglNSEIo8fY7sTdPJfumy/EdKqBs1cfEj72ZpHsmUfjCAnC7a7TxHtjP3osuaNA4pe8u
p/Td5UHXt7dM94+Vk1197JwcHOnpDRpbTl39p95PbMuWvH5Jb8ry8vn+3RWcd/edXPj4H1n3pzl4
KypqtCnau49nO5zZoHG2LlnG1iXLgq4fn9EagOKs6h86i/cfIL5NI30zLhHNaNkenFGY2zeCtxLI
w2zRBqNVJ8zs3WD6ajaqLMe39dOGDXQoG/NQdv31jnK6jox13N+NSjeG04XZsNHlFGX0GgLN4vAt
+R8oL8XcvR3znIEYfS/H/HwleD01G5UU4l04s0HjmDu/gJ1fBN8gLtH/5/ET49KiY9ck8kR45iJc
NCEJodhhwyn+f8vwHTqWavUVF2GLi6d42ZLaG/l8+PJyQxqXER2D6fWCp/qbplnpxnC5Qjq2RI6z
x4zky78tpCwvv6qs4vBhXAkJfPHi32ptY/p8lGTnhDQuR0wMPq8X33GvT29FBQ5XdEjHlgiRnO7P
gnh/tlHc6wG7A/LqWJbqqfklT6Oy2TFNE46fepg+nZRzGjG69vJnQcpLjxW6yyEqGnPrxtobmWbI
N5YbDiemzwe+4ybsR//uiEQwvUJDxNGhE/aU5pSvr/6NnWG3U/T6Qsyyslrb2dNb0ertdwP2u7df
r5OOzawox7DbwW4Hr/dYbM6ogHFJ05LStQuxaS346aNV1coNu53//Pf/UFlaWmu7+LYZ3Pb5+oD9
zm9x8uuUPeXl2Ox2DLvdP3E+wu5yBYxLmhBXMwxHFL6i/OrlhoGZu7f27Aj4N5d3DbzHyPz2k5OP
zefDMAxMDKpNSgwb+LwBm0kTktQCIyYO397vq5cbBubX6wKfthaXiO36ewN263vlsZMOzfRUYrPZ
/K/Hn/89sTt0Clwk05cZgCYkIeNo0wYAz/6sqrKoX/bEkdEG93fbArbzHszhwIjhIY3Ne8C/FMae
2gJv9oGqcntaGp5VDVi+IKespA7tAf/ejKMy+vUlqUN7cr76OmC74qwD/O38C0MaW9Fe/zfgca3S
q36uep6VFaiZNBVRR7JglT/bZN4sASMqBrOub5gr3Zg7/xPa2I7G5IyqvmzLGYV5OMTZGYkM8cn+
P4t+to+oZTuMhBR8eXW8P5UU4XszxCdZFhf6/2wWDyWFx8qPfy4SgTQhCZUj3+zaEo6t20y6bzIA
Rl3rBb1ePJmh3Xjm/m47vuJiXH36Vu09sbfOwJHRhorNtWzGkybHd+T1GZ2SXFU25KkZ/h/qeH2a
Xi+Hdv0YytDI+eobKg4X0XbggKq9Jwnt2pLUoT171wbOzkgTYR7JPNidgD9ja7Q6I5iG4A5xhre8
GNPr8W9gP7r3xBntnyyVHKq7rTQNRzMP0TFw5DO+7fwr/T/U9W+76YPD+YGvN4a8/ZjucozWHf37
TwDikvyTpf0/hXZsOXHaQwJoQhIy7q3f4isvJ2nSZA6/+DzNrhiKPTGRyp9+pNmVV+HeuQNvMEf0
NhIjLg7D6cRXUACVlRQtXUTy/VPwHSrAm59HyiPTKf9sI+6vAp+gJE3HgS1fUFlWxpCnZrB+1jy6
/fZaopOTyd/5Pd1GXMfBb7ZWy56EWlRCPPaoKMpy8/C63Wx54WUuevpxyvLyKT14kMufmcOeNWvJ
2hTib8DFemVFmD4vRqszMLN3YySlgd2JWVGKkZSGWV5SPXsSaja7fwmMt9I/Wcrb54/NUwleN0br
rpjFBTph63RxcJ9/adT5Q/FtWYlxRg+IboZ5KBfjjB6YeQegOIyT0ygX2BxQXuI/le7bjRj9hvr/
npSVYBs4DDNrF+SE7/1c5ERo4VqI+AoPkf/ogzhaptPirwuI6voLciaO5/BLzxNz0cU0u2JoWONJ
nvoI6YverHpe+OxfKHn3HzSf+WdavrQQT1YWuffXffdsaTrK8wt47/YJxGe05jdvvk6Ls7vz5rU3
sH7WPLpcPZQzfxvaZYPHu3TO09y09qOq52tmPMXWpW9y9SvPM2rFcgr3ZPL26JvDGpNYxOvBzNzm
3xPS4RyIjsP88SvMnN2QkApJaWENx2jdBaNL76rn5oEf4VA2RrtuGJ16+o/93f1tWGMSC1WUYa58
03/DwStuxEhJx/f+q5hbVmJ06IZxxjlhDccYcDW26+6qem5+9iHmzi+wXXQ9tqtvwyw+hO/DRWGN
SRpI9yEBwDBNMywnFdZ2/wyRSLB450GrQxAJaPLGt6wOQaRW5voPrQ5BJCD7+IYds2wV718CH3bQ
2Oz3/iVsYzWUlmyJiIiIiFghwjMX4aIlWyIiIiIiYhllSERERERErKD7kADKkIiIiIiIiIWUIRER
ERERsYL2kADKkIiIiIiIiIU0IQmTmCGX0PaLbbT6v3dxtG1ndTgiAHS+eihTSnK59fP1JHXqaHU4
cjpLSMU4ZwhG174QFWN1NHI6a98N2++fxHb9vZCQYnU0IqcFTUjCpOKrL8ibMglH+w7E/eb60Axi
s5F4zyQyPvqENhs+J3XuM9hSmgfdPPnRx0iZ8WRoYpOIlLXpPyz/3W2kdDmDX97yu5CPd/lf53Ll
gmfqrWfYbAx+fBoTdm3lvoN7GL7o7zRLaxHy+MRCpYWYe74FVwxGSquQD2dkdMVoU/P+WEZ6J4xu
AzDOGoTR7ixwOOvv6wTaSATLycT891JIbI7R7byQD2cMugZj8LXHFRoYfS/HNvZBbLdMx3bZaIiJ
a3idk6kv4aEbIwKakISNLz+P0n+9T8WWzTi7huYmkYl33k3ssGvJe2Qq2ePGYm+ZTuq8/wqu7V33
EH/9yJDEJZGrNOcg2996m73rNpB29lkhHWvgtIfoefu44Oo++iBnjx3Ju7fdxaLLriY+ozXXLn41
pPGJxTyVUHgQSgohOrQfkoyWHTGaZ9RaTnI6ZuY2zF1b/HeLb392vX01tI1EuLJizF1fw4HdGCnp
IR3K6HMptu7n1yzvfQlG1174Vr6Jb/mL/jvDXza6wXVOpr5IOGlCEmae3T/hPKNz43fscBI/5ncU
/nUe5RvWUbltK7lTJhHdqzdRv+wZsJk9ow1pLy0k7vpReLL2NX5cckrI3/kDqd3PDEnfiR3aM3LF
O/S8fRyFezLrrW9zOuk9YTxr/vgkP328iuwvvuKdG2+lzYB+ZPTrG5IYJYJUlEF0bGj6jorG6HQu
NG+N6S6vfs0wILUN5oFdUFzg/1C6ZytGbBI0S6i9vxNpI6cMszAXkluGpvP4ZGxX34bR/XzMooLq
12x2jHMGYG76F+z7HnKz8P17CUarDtCyXfB1GtqnWMOwhe8RwXTKVhjZkpOJuewK7IlJGLGxmCUl
tdazt84gY8XHAfvZ06NmhiXqzDOxxcVR/p9NVWXerH149u3F1asP7i+31NqX69xeeLP3kzt1Eqmz
5zXwN5KmICa1Ob+4dhgxKclExcfjLiqqtV5Cu7bc+d2XAfuZFVP7WuuMfn0p2ruPf9x0O8MWvlRv
PC1/eQ6uhHj2rFlbVXZ4TyaHftpNmwv6sW/DpjpayynN7oTEFhgOJ6bNDj5v7fWc0di69Q/Yje+r
lbVfaJYIlRX+SUO77tWvRcdh2B2YxYeOlVWWY7rLIDYJSg/X7O9E2sipIToWo+PZGNHNwOmCyora
68UlYR8zJWA33v95uNZyI709ZskhzI+WYLv0uNUJzVthREXjy9p1rKz4EObhfIz0DpjZe4Kr09A+
RSykCUkYJT/wIMaRGarzjC64v/qi1nreA/vZe9EFDerb3tKfVvbmZFfvKycHR3rglHPpu8spHfFw
awAAIABJREFUfXd5g8aSpuXiPz2BceTGTKndzyRr42e11ivau49nOzQ8i7J1yTK2LlkWdP34jNYA
FGftr1ZevP8A8W1qLrORpsNo3RmOLnOOjg38gb6yHN/WTxs+wKFszEPZtV9zuo70fdwHz0o3htOF
2Vht5JRg9B96bM19ShpkB8julhTiXTizwf2bO7+AnbV/BiAu0f/n8a//0qJj14Kp09A+xRq2yN7b
ES6Rnb9pQlznnU+zX/2a3IceAMDZuY5lWz4fvrzcgI/aGNExmF4veDzVys1KN4bL1Wi/hzQt7QYP
pPvIEfxj3O8BSO0WeMJh+nyUZOcEfDQWR0wMPq8X33GvZW9FBQ5XdKONIxEmNgmSWmLu2eZ/Xt+y
LY878ONE2OyYpgnHTyNMX+A7KZ9IG4l8rTthdD4X38dvAGDUtWzLNKGsOPDjBBgOJ6bPBz5f9Qte
D9gdQddpaJ8iVtKrMBwcTlIenUHxW29Q/slqPNkHcHbuGrC6Pb0Vrd5+N+D1vf161SgzK8ox7Haw
28F7bJmD4YzCLCs7ufilSbI5nVz+1zl8+fKr7FrxIUX7smhxVreA9ePbZnDb5+sDXp/fonHWIXvK
y7HZ7Rh2u3+SfYTd5aKytLRRxpAIYxgYGV0hPwuK8jDd5Riu2MAZBqfLfzxwAOa3nzQ8Bp8PwzAw
Mag2wTBsgZeOnUgbiWw2O7aB12Bu+wz2fIdZXAgpdUxI4hL9xwMH4HvlsQaHYHoqsdmOrPk3fzaB
sDv8hz8EWaehfYpFInxvR7hoQhIGCbf+HiM2jkN/mQNA5fc769zY7j2Yw4ERwxs0hveAf3mLPbUF
3uwDVeX2tDQ8qwIsUZDTWr8H7iUqIZ5V0/z/YB78dludGZLirAP87fwLQx5X0V7/4QpxrdKrfq56
npUV8vHFAi3ag92Buf8H//OKkrozJJVuzJ3/adwYKo9scndGVV+C5YzCPBwg63IibSSiGedeCFEu
zE0r/AUF2RjJaYEnxyVF+N4M7jTLoBUX+v9sFu8/de6onz8Ppk5D+xSxkCYkIeZo246EW39P3iNT
MYv96dvKnTuIverXgRt5vXgyG7bBzP3ddnzFxbj69K3aE2JvnYEjow0Vm2vfEyCnr6ROHek3+V7e
ve0u3If9m9gPfruVs0aOCNjG9Ho5tOvHkMeW89U3VBwuou3AAVV7TxLatSWpQ3v2rg2coZFTVFQM
Rlo7zMxtx7IK5SWQVNfpRia4GznzW16M6fX4l44d3WfijMaIisEsOdR4bSRyJaRg9LwQ38pl4PZP
MM38Axidzw3cxvTB4fzGjSNvvz9L2Lqjf68JQFwSRkIKvv0/BV+noX2KNSL8/iDhoglJiCU/Mp2K
jesp+3BFVVnl9zuwt0jDlpCI73AjfTNRWUnR0kUk3z8F36ECvPl5pDwynfLPNuL+yn8ykhEXh+F0
4isoqKczaeouf2YOu1eu4bv/905VWe6324hrlU50chLlBeH7MBWVEI89Koqy3DwAvG43W154mYue
fpyyvHxKDx7k8mfmsGfNWrI2NfK34mI5I6Or/8jcwoNVZWZ5CTanC9Pu8K9xDwfThLx9GK3OwPRU
gteN0borZnFB9Y3ANrt/iYW3Mvg2ckqwDbwG9v0Au745VpifjRGbAK4Y/5HU4eDzYn67EaPfUMzy
EigrwTZwGGbWLsjJDL5OlAtsDv8EP5j6IhbShCSEmg29Cte5Pdl/7dXVyt07dwDg7NKFis2N9wGr
8Nm/YDgcNJ/5ZwyHg7JPP6Fg5uNV15OnPkJ0n75kDb2k0caUU0+3668jo995vNx7QLXyg99uBSC1
ezf2fhq+TMSlc56m3eCBPH/msW8h18x4CpvTydWvPI/d6WTXhx/x4b2TwxaThElSGjRLwNxx3FHO
5Uc2A0fHhnU5iXngRwzDwGjXzT/pKMrD3LezWh2jdReIS8LcviHoNhL5jDN6QHo7fG88U63czD+S
+UppCft/Cls85mcfgs2G7aLr/Ycn7N2BuXZ5g+oYA67GaN0J36I/B92nWEAHYABgmP4jQkKutntn
iESCxTsP1l9JxCKTN75ldQgitTLXf2h1CCIB2cc3/DhmK3hfmha2sey3PRG2sRpKGRIRERERESto
Dwmg+5CIiIiIiIiFlCEREREREbGC7kMCKEMiIiIiIiIW0oREREREREQsoyVbIiIiIiJW0KZ2QBkS
ERERERGxkDIkIiIiIiJW0I0RAWVIRERERETEQsqQiIiIiIhYQXtIAGVIRERERETEQsqQiIiIiIhY
QTdGBJQhERERERERCylDIiIiIiJiBZv2kIAyJCIiIiIiYiFlSERERERErKA9JIAyJCIiIiIiYiFl
SERERERErKD7kACakIiIiIiIyHHKy8uZPHkyeXl5xMbGMmvWLFJSUqrVmTVrFp9//jkej4cbbriB
66+/nqysLKZMmYJpmiQmJjJ37lxiYmLqHEtLtkRERERErGDYwvdooMWLF9O1a1cWLVrE8OHDWbBg
QbXrGzZsYM+ePSxdupTFixfz4osvUlhYyN///neGDh3K66+/TpcuXXjzzTfrHUsTEhERERERqWbz
5s0MGjQIgMGDB7N+/fpq13v27MnMmTOrnnu9XhwOB926dePw4cMAFBcX43DUvyBLS7ZERERERE5j
y5Yt49VXX61W1rx5c+Lj4wGIjY2lqKio2nWXy4XL5aKyspIHH3yQG264gdjYWNLT05k7dy7//Oc/
cbvdTJw4sd7xNSEREREREbFChNwYccSIEYwYMaJa2cSJEykpKQGgpKSEhISEGu0KCwu555576Nu3
L+PHjwdg9uzZPP300wwaNIhVq1YxdepUXnjhhTrH15ItERERERGpplevXqxevRqANWvW0Lt372rX
y8vLufnmm/nNb37DhAkTqsoTEhKqMitpaWlVy7fqogyJiIiIiIgVIvjY31GjRjF16lRGjRqF0+lk
7ty5gD8DcuWVV/L555+TmZnJsmXLWLZsGQAzZ85k2rRpPP744/h8PkzT5I9//GO9YxmmaZoh/W2O
2NPjF+EYRqTBFu88aHUIIgFN3viW1SGI1Mpc/6HVIYgEZB8/s/5KEcC7bF7YxrKPmBS2sRpKGRIR
ERERESucwHG8TZH+K4iIiIiIiGWUIRERERERsUKEnLJlNWVIRERERETEMsqQiIiIiIhYQXtIAGVI
RERERETEQsqQiIiIiIhYIYLvQxJOypCIiIiIiIhllCEREREREbGC9pAAypCIiIiIiIiFlCERERER
EbGC7kMCKEMiIiIiIiIW0oREREREREQsoyVbIiIiIiJW0KZ2QBkSERERERGxkDIkIiIiIiJW0I0R
AWVIRERERETEQsqQiIiIiIhYwabcAChDIiIiIiIiFlKGRERERETECtpDAihDIiIiIiIiFlKGRERE
RETECroPCaAMiYiIiIiIWEgZEhERERERK2gPCaAMiYiIiIiIWEgZEhERERERK+g+JIAyJCIiIiIi
YqGwZUhaD+wcrqFEGmTqhk1WhyAS0B2xba0OQaRWk7qkWR2CSEBdx8+0OoTgaA8JoAyJiIiIiIhY
SBMSERERERGxjDa1i4iIiIhYQTdGBJQhERERERERCylDIiIiIiJiBW1qB5QhERERERERCylDIiIi
IiJiBe0hAZQhERERERERCylDIiIiIiJiBZv2kIAyJCIiIiIiYiFlSERERERErKA9JIAyJCIiIiIi
YiFlSERERERErKD7kADKkIiIiIiIiIWUIRERERERsYL2kADKkIiIiIiIiIWUIRERERERsYChPSSA
MiQiIiIiImIhTUhERERERMQyWrIlIiIiImIFbWoHlCERERERERELKUMiIiIiImIFZUgAZUhERERE
RMRCypCIiIiIiFjBpmN/QRkSERERERGxkDIkIiIiIiJW0B4SQBkSERERERGxkDIkIiIiIiJWMLSH
BJQhERERERERCylDIiIiIiJiBe0hAZQhERERERERCylDIiIiIiJiBe0hAZQhERERERERCylDIiIi
IiJiBe0hAZQhERERERERCylDIiIiIiJiBZv2kIAyJCIiIiIiYiFNSERERERExDJasiUiIiIiYgVt
ageUIREREREREQspQyIiIiIiYgXdGBFQhkRERERERCykDImIiIiIiBW0hwRQhkRERERERCykDImI
iIiIiBUieA9JeXk5kydPJi8vj9jYWGbNmkVKSkq1OvPnz2fdunUYhsH999/P+eefT35+Pg888ADl
5eWkpaXx9NNPExMTU+dYypCIiIiIiEg1ixcvpmvXrixatIjhw4ezYMGCate3bt3KF198wRtvvMG8
efN46qmnAFiwYAFXX301ixYtonv37ixdurTesTQhERERERGxgmEL36OBNm/ezKBBgwAYPHgw69ev
r3a9e/fuvPzyyxiGQVZWFgkJCbW2W7duXb1jacmWiIiIiMhpbNmyZbz66qvVypo3b058fDwAsbGx
FBUV1WjncDiYP38+CxcuZNq0aQAUFxfX265GPyf7C4iIiIiIyAmwRcZipREjRjBixIhqZRMnTqSk
pASAkpKSqgzI8e677z5uv/12brjhBvr06UNcXBwlJSVER0fX2e7nIuO/goiIiIiIRIxevXqxevVq
ANasWUPv3r2rXV+/fj2PPfYYAC6XC4fDgWEY9barjSYkIiIiIiIWMAwjbI+GGjVqFDt37mTUqFEs
XbqUiRMnAjB79my++uor+vbti8/nY+TIkYwZM4YxY8bQtm1b7rzzTt59911GjhzJli1bGDt2bP3/
HUzTNBsc4Qnw3HVVOIY5fRk2bMNuxOh3KbhiMLduxrf0OSg6FFRz26gJYLPje/2vIQ408jjmLLI6
hNOD0wWOKMAArwfcZUCQbz/OaP/RiO6yUEYYke6IbWt1CE2aYbNxzZPT6H/zGFzxcWxd8W8WT7if
opyDQbUf/dx8bA4Hr91+d4gjjTyTuqRZHULTZ7ORet8kEq69DltsLCWffELOY9Px5uUF1Tztsccx
HA6yH3k4xIFGnq47vrc6hKCY320I21jGL/qFbayGUoakibBdNRrj/EvwvToX7/ypGMmp2G8P7g3I
dvVYbIN+FeII5bTmdIE9CirKoLzYP7lwNQu+rdMV2vjktHX1jIfpd9No/v678cwdPJSkNhmMf+u1
oNr++rFHGHzHrSGOUE5nze++h4Th13JgymQyx4zGkZ5O62f/O7i29/yBpFGjQxyhnLQIPmUrnCI7
uqaiWRy2UROxz16E/c9LsA0fB4B90myMIb8++f7tDoyLrsG3fCHm9i8g8we8L8/C6HwWdOoWuF3z
dGz3Po0x6FeYeTknH4ecogx/BiIm3v9wRvuLXbFHMhqNwOGCynLwecD0gbsU7A6w2esIyzgWg8/X
OHHIKaVZcjKjn/8Lcw7+yNy83Vz7J/9a5Qc++YAhE8efdP92p5OL/3AH7zz8GNv+vZLMLV/y8shx
dB7Yn079+wZsl9qxA/d9/E8G33krebv3nHQccmqyJSaS9vgTnLFxE2ds+g+pD0wGoO3iJSTdeOPJ
D+B0knTTzeTOm0vpuk+p2Pot++/7AzG9+xDds2fgZm3b0mbhaySOHkPlvn0nH4dIGOiUrVCLicX+
wByoKMf3v89A6/bYr7kJMy8bWrTCXLuiZpuUNBxP/i1glzWWv7XphBHTDHPHV8fK8nMwcw9gnHEW
5q5ttfZjnNENCg7ifWU29lumnMhvJ01BdCyYpn85lGGHqGj/pMFmgwp3zfqGATF1nJhRWlj9uc3u
b+PzHCszTf8kw2YHn7f2fmwOfxxlpcFnU6TJiElMZMq6D6koLmbhLXfR+uzuDJ85nbyf9tDijI6s
ffHvNdo0b9+Op376JmCfdxjVX7dtz+1BTEICO1atrSrL272H3B9/ovOgAexav6nWfjoNOJ+CzH28
POoWblsS+L1ami5bfDztlr6Br6SUAw89hKtrV1In3U/lvn0427ajsJYbwTkyMui0cnXAPnd07Vzt
eXS3btjj4ijdtLGqzLNvH5WZmcT0OY/yLVtq7Se6Zy88B/azf9K9tJr/zAn+hiLhpQlJiNmuvAES
kvFOvx1KDsPXGzEvHo7tmpvwvb8EPJU1GxXk4nmw/g1ARxnJqf4fDh23prQwHyM5NeAqfXPTSsxN
K4MeR5ogZ7Q/jVtehH8/hwecUf7yyvLa25gmlB4OfoyjG+mO365m+upOIXsr/Q85LQ195AES0tOY
1vlySvLy+eof73PJfRMY/vR03ntiNp6Kihpt8jP3MiW9cy291S6pTWsACvZlVSsvzDpASts2Adtt
en0pm16v/87D0nSl3HkX9tQW7Bl1Cb6CAko+/oikm8eRev8D5C/4b0x3zS9zPPv388OA4NfwO9LT
/e2ys6v3k5ODs1WrgO2Klr9D0fJ3gh5HLHYCm82bIk1IQszodwnmpx/4JyNHlZVATCzmJ+/V3sj0
weGC4AeJcmH6vDW+aTY9lf4PlyKBOJzgcVNtc7lpgsGR8kAachaGUXMyUnVJb8RSu343jWbti69S
kpdfVVZWeJiYxATWPP9KrW1Mn4/D2cEvP41q1gyf14vP46lWXllRgSNa+5YksIRrr6PwjaX4Co79
W+0rKsIWH8+hxQEOSvH58ObmBj2GER2D6fXCca9P0+3GcOn1KU2LJiSh1LINRnwSvu3HpVVtNnwr
3wF3zW/4AEhugX3acwG79U76bfUCdwWGze5fYvOztfaGw4npDvAtt8jRTW5ez/EXoLKOyYhhQHR8
4Otlx2dPzMATj/Ac8ienmJa/6EJCWgu2fVg9g2uz2/j4medwl5bW2i65bRumb619mRXAvfGtqz2v
LCvDZrdjs9vxeY99oeN0uXCX1D6GiLNTJxzNm1P66afVyg2bjUOv/h2zrPbTAB2tWtHhvVqWaR/x
fc9fVntulpdj2O1gt8PPXp9GVBS+AH8H5BQU4ZvNw0UTkhAymrcEqL5hvFM3jNR02LsrcMPCPLxP
B3+EpFlw5BuXxBQo+Nm3L4kpmMcv4xI56uiboPmzDeNVE9sA+zrAP4koLw5+nKOTDuO4TIlhA1NL
sqSm1I4dAMj/2YbxTv37ktqxA3u/+Dpgu8Ks/Tx17sCgxynI9G/4TWyVTsHeY5t/E1unc+i4ZVwi
Rznb+JfzVWYde81E9+yJs21bKrbVvmcT/Eutdl8zLOhxPAf2A+BokVb1M4AjLa3GMi6RU50mJKF0
9INe7LFvk+1HTtiqc6mKzwcH9we+frx9uzDLSjG6nHNsT0hKGkZqOubOwBs8RYAjE4UjPx89Yau+
lVRmA0698nn9ExGb49ieEMOof+Ijp62j2YpmKSlVZdfNfsL/Q11vnV4vB3+o48ue4+z98mvKDh+m
y4UDq/aENG/fjtSOHdi5Zl2D45bThNf//mdPTOLoVyotpkz1/1DXv+1eL5V7dgc9TMW27XiLi4np
27dqT4gjIwNn27aUffbZiUQukUhLlwFNSELK3PM9prsC+7Xj8K14A6P3IIiNx8zeh9F7MOa+nyC/
EY7b9Xgw17yL7dpb8RUfxiw6hH3kXf5Tt3767li96GbgcEBxAzYkS9N1dKLgjIbKCv9+EsPwl9ud
x643Bk+F//SuCtM/mYmK8S8VqzEhMWjY/hRpivZs/gJ3WRnXzX6c95+aQ58briM2JZnsHd9z3sjf
kvX11kY5btfjdrN6wUv8Zs6TFOfmUZRzkFEL5rFj1Sf8uPHYB77ohAQcUU6Kc5VxFij/9ht85eWk
TplC/nPPEf+rX2FLTML944/EX3UVFTu+w9MIx+2alW4KF71Oi6kP4i0owJuXR9qMxyjduJHyL7+o
qmeLi8NwRuEtyK+jN5HIpoVroVRShG/hPEhKxXbHNIyMjngXzMC3YilGj37+CUoj8f1jIeZnK7Hd
fD/2e5/GzM/B++LT1erYRozHPvUvjTamnOqOHvVr8x+ra7NDRYl/cmJ3+h+NpbLCf6KcKwai4/yT
korj1kBHHbkmp72S/HxevekOktu05q7lS8jocRbPXjWC95+aQ49rfkXv669ttLGWP/oEm15/g1te
e5FJK/9J/u49/M9vq99D4oZnZvHQZ6sabUw5tfkOHeLA1Ck4W6aT8dzzuH5xJvt+fxv5zy8g7pJL
iR/aeDcazp0/j8P/WE6rOXNo87+v4cnaR9Y9E6vVafHoNNq99f8abUwJM5stfI8IZphmeHaV1rh3
hkiEcMwJcCKKSAS4I7at1SGI1GpSlzSrQxAJqOuO760OISjmj1/UX6mRGB3PDdtYDaUlWyIiIiIi
VtAeEkBLtkRERERExELKkIiIiIiIWEH3IQGUIREREREREQspQyIiIiIiYgXtIQGUIREREREREQtp
QhImRo9+2J/9B/Zpz0GLVlaHI+Jnd0BMgv/+H1rHKhb65bBfscB7iOnfbqLFGZ2sDkdOY7GXXEqX
7Tto/94KnO3aWx2ONHlGGB+RS59AwsT8cTu+V2ZBWga2C64MzSCGDds1N2F/+n+xz3sT220PQXxS
0M1toyZgG3NPaGKTyOT1Hrs5oiMq9OM5o/03QAyqrgti4v0TpqhmRPqbqZycXRs+4+WR40jr2pmB
t98U8vFGPzefsS/+V7Uyw2Zj+MzpzMrawV+Ksvj9soXEp7Wos58TaSORrfyLLey/716iOnQg8frr
Qz5e2mOP0/KpmdULbTZS73+ATmvX0XnLl7T667PYmzdveJ2TqS8SRpqQhEvRIczP18IP30JGh5AM
YbtqNMb5l+B7dS7e+VMxklOx3/5wcG2vHottUOPdXVZOFSZ4K8HnDf1dXJ0u/yPYuvYoqCiD8mL/
GltXs9DGJ5YqyjnI5mX/xw9r15PR4+yQjvXrxx5h8B231ii/esbD9LtpNH//3XjmDh5KUpsMxr/1
Wp19nUgbiWzevDyK33+Pss2bcZ15ZkjHan7PH0gaNbpm+d33kDD8Wg5MmUzmmNE40tNp/ex/N7jO
ydQXCSdNSMLMzMnCaNWu8Tu2OzAuugbf8oWY27+AzB/wvjwLo/NZ0Klb4HbN07Hd+zTGoF9h5uU0
flxyajB9YNhD07dhgCvWn4Hx+YJr43BBZTn4PP7Y3KX+5WW2EMUoESN7x/e0Pis0HwJTO3bgvo//
yeA7byVv955q1+xOJxf/4Q7eefgxtv17JZlbvuTlkePoPLA/nfr3rbW/E2kjpw73Tz8S1blLSPp2
tm1Lm4WvkTh6DJX79h130UnSTTeTO28upes+pWLrt+y/7w/E9O5DdM+ewddpaJ9iDcMI3yOC6ZSt
cIpLwOh5AUZsPETHQHlZ7fVS0nA8+beA3XjuuqpmYZtOGDHNMHd8dawsPwcz9wDGGWdh7tpWa1/G
Gd2g4CDeV2Zjv2VKQ34baTIM/4f9+vaQGIZ/+VQgpYW1l9sc/klFWWlwWQ6b3T+Wz3OszDT9kxmb
3Z/NkSYpLrU5vX47nNiUZKLj4ykvKqq1XvP27Xjqp28C9nOHUfvrtNOA8ynI3MfLo27htiXV32Pb
ntuDmIQEdqxaW1WWt3sPuT/+ROdBA9i1flON/k6kjZwa7MkpxF9xJfakJGyxcfhKimut58jIoNPK
1QH72dG1c63l0T174Tmwn/2T7qXV/GeqX+vWDXtcHKWbNlaVefbtozIzk5g+51G+ZUtQdRrap4iV
NCEJI9tvbjs2Q23VHn7cXnvFglw8D45tUN9Gcqr/h0N51S8U5mMkp2IGaGduWom5aWWDxpImJiqa
qv0ZdX3gN00oPdzw/r2V/kewjv4dMY971Zo+bbxv4n47dyaGzf//v9VZ3fhxQ+0f6PMz9zIlvfYP
enXZ9PpSNr2+tNZrSW1aA1CwL6taeWHWAVLatmm0NnJqaPHQQ1XLWKO6dKH8i9o/sHv27+eHAf0a
3H/R8ncoWv5Ordcc6en+vrOzq4+Vk4OzVaug6zS0T7FIhGcuwkX/uoeJ0bUHxnlD8P19jv95Xcu2
TB8cLgj8qE2UC9PnrfFh0vRUgjMMm5Xl1GSzg90JFaX+5/V+4DfreDQWo+ZkpOqS3ribqq5DBtF3
zPW8MuZ2gDqXbZk+H4ezcwI+TkRUs2b4vF58Hk+18sqKChzRte99OpE2Evlizu9H/K+HceCBSYB/
QhKQz4c3Nzfg40QY0TGYXi8c97oy3W4MlyvoOg3tU8RKypCEg92BbeRdmGtXYH7zGWZBLkbr9oE/
wiW38B8PHIB30m9rFrorMGx2/zc6P1unbzicmO7yk4tfmq6oGPC4/cujji6JCpTNMAyIjg/cV9kJ
ZE9qZQaeeASaqMgpze50Mvq5+Xzywt/45r0PKNi7j9Zndw9YP7ltG6ZvDbwc6t741g2OobKsDJvd
js1ux+c99sWO0+XCXVLaaG0kwjmdtHzsMQqXLqFk1SoqDxzAVceExNGqFR3eWxHw+vc9f9ngEMzy
cgy7Hex2/0mIRxhRUfhKS4Ou09A+xSr6og00IQkL4/IREN0M39t/B8DM2u1fshVIYR7ep+9u0Bhm
wZFvYhJToOBn38okpmAev4xLBPwbxzH8m8cBzHpO2jJN/4lXoXZ00mEclykxbGA2YOmXnDKueHAS
0Qnx/N+DMwDI+mZbnRmSwqz9PHXuwEaNoSDTv7E4sVU6BXuPbTJObJ3OoeOWZJ1MG4lsKb8fjy02
jtw5fwbAvWNHnRkST04Ou68Z1qgxeA7sB8DRIq3qZwBHWlrVkqtg6jS0TxEraclWqLVohe2KEfiW
vQDlR76FyPoJo3UdExKfDw7uD/yozb5dmGWlGF3OOVaWkoaRmo65M/DmTzlNGTb/0bqVPztY4WiG
pC6mL/Cjsfi8/omI7WfflxjGkeyfNrQ3NS3O6MSVD03ijT9MpfywP8u27+tv68yQ+LxeDv6wK+Dj
ROz98mvKDh+my4XHJjrN27cjtWMHdq5Z12htJHI527UnZfwd5Dz1JL5i/5cvFTu+w9W54g6QAAAL
sElEQVSla+BGXi+Ve3YHfJyIim3b8RYXE9P32EltjowMnG3bUvbZZ0HXaWifYhGdsgUoQxJytpF3
YX73JeaWY6ewmFm7sSWmQLM4KG2kb5w9Hsw172K79lZ8xYcxiw5hH3mX/9Stn77z14luBg4HFDfW
0ho5ZUVF+5dpeX+2ntjnBeNI1qRR94QE47gxPRX+GCtM/2QnKsYfqyYkTc6oBfPY/tFqPn/z7aqy
rG+2kdgqnWbJyZQWBNg318g8bjerF7zEb+Y8SXFuHkU5Bxm1YB47Vn3CjxuPfWCLTkjAEeWkODcv
6DZyakib8Ril69dRvOL9qjL3zh040tKwJSbiKwxwkmAjMyvdFC56nRZTH8RbUIA3L88f28aNlH/5
RdB1bHFxGM4ovAX5QdUXsZImJCFk9LkQo2M3vE/eWa3czPrJ/0Pr9vD9t402nu8fC/3rmW++H+wO
zK2b8S05thfFNmI8Rtdz8E67pdHGlFOQ3enPPpQfd6Tq0Q/74c5ERMXUjKeyAjDAFeP/01sJ2gvV
5PQZ+Vs6DejL42edX61839f+98XWZ3fj+0/Cl2lY/ugT2J1ObnntRexOB9+u+DeLJ9xfrc4Nz8yi
65CBPNLxnKDbSOSLv+pqYnr25KerhlYrr/huBwCuLl0p+0/4Jpm58+eBw0GrOXPA4aT0kzVkPzaj
QXVaPDqNZn3P58eLhwTdp1ggshMXYWOY5v9v725jsiz7OI5/L0DRwNKyp1uNWVPTYbm4Vo2oTDYn
3OG00sTaWmsNB72gByOb0XQwphNcc+AqbVo+9LC2XjilWnhBC3jD6EXzgazWIHxoLaexUsDrftFi
I0QF7zi8d38/G2M7z4Pr+HO+++/3P85rZE6Jnve7M6QrQNKGXaFLkAa1ImVK6BKk83ph2g2hS5AG
Nb3tSOgSLkm88/CI7RX514wR22uoTEgkSZKkIIxIwEPtkiRJkgIyIZEkSZJCuMLffjVSTEgkSZIk
BWNCIkmSJIVgQgKYkEiSJEkKyIZEkiRJUjCObEmSJElBOLIFJiSSJEmSAjIhkSRJkkLwUDtgQiJJ
kiQpIBMSSZIkKQgTEjAhkSRJkhSQCYkkSZIUgmdIABMSSZIkSQGZkEiSJEkhmJAAJiSSJEmSAjIh
kSRJkoIwIQETEkmSJEkBmZBIkiRJAUQ8QwKYkEiSJEkKyIREkiRJCsGEBDAhkSRJkhSQDYkkSZKk
YBzZkiRJkoJwZAtMSCRJkiQFZEIiSZIkheChdsCERJIkSVJAJiSSJElSCCYkgAmJJEmSpIBMSCRJ
kqQgTEjAhESSJElSQCYkkiRJUgieIQFMSCRJkiQFZEIiSZIkhWBAApiQSJIkSQrIhESSJEkKwogE
TEgkSZIkBWRCIkmSJIXgW7YAExJJkiRJAdmQSJIkSQrGkS1JkiQpBEe2ABsSSZIkSX/zxx9/sHLl
Sn755RdSUlJYt24d1157bb81GzdupLGxkUgkwosvvsg999xDZ2cnr776Kr29vcTjcdauXcutt956
wb0c2ZIkSZKCiIzgz9Ds3r2b6dOns2vXLhYtWkRNTU2/+wcOHODrr7/mww8/pKqqivLycgDeeOMN
nnzySd577z0KCgqoqqq66F42JJIkSZL6aWlp4f777wfggQceoKmpqd/9WbNmsXXrViKRCJ2dnVx9
9dUAlJSU8OCDDwLQ29tLcnLyRfdyZEuSJEkK4Qo5Q/LRRx+xffv2fteuu+46xo0bB0BKSgqnT58e
8HdJSUls3LiRd999l9deew2gb6zr+++/Z926dVRXV190fxsSSZIk6f/YkiVLWLJkSb9rzz33HF1d
XQB0dXX1JSB/9/zzz/Pss8/y+OOPE41GueWWW2hubmbNmjWsX7/+oudHwJEtSZIkKYxIZOR+huiu
u+6ivr4egIaGBjIyMvrdb2pqYs2aNQAkJyeTlJREJBKhubmZ8vJytmzZwuzZsy9pLxMSSZIkSf3k
5+dTUlJCfn4+o0aNorKyEoD169ezYMEC7r77bmpra1m2bBnnzp3jiSeeYMqUKRQVFdHd3c0rr7wC
wNSpU1m7du0F94rE4/H4P/4fAT2F/x6JbaQhS9qwK3QJ0qBWpEwJXYJ0Xi9MuyF0CdKgprcdCV3C
pek6OXJ7pYwfub2GyJEtSZIkScE4siVJkiSFcIW8ZSs0ExJJkiRJwYzYGRJJkiRJ+jsTEkmSJEnB
2JBIkiRJCsaGRJIkSVIwNiSSJEmSgrEhkaRh2LRpE+np6RQWFnKhd4MsX76c4uLi896LxWKkp6fz
66+/XnCvjo4OZsyYwY8//nhZNUuSdCWyIZGkYXj66ad5/fXX+eKLLzh06NCg6/Ly8qivr+fMmTMD
7u3du5esrCwmTJjwT5YqSdIVzYZEkoYhNTWVxYsXM3r0aA4fPjzougULFtDd3c2XX37Z7/rZs2ep
q6tj4cKF/3SpkiRd0WxIJGmYenp6uOqqq/j2228HXTNhwgSysrKora3td72hoYFz584xb968vmut
ra0sX76cO++8kzlz5vDMM89w/PjxAZ95vhGuTZs2kZ+fD8CxY8coLCxkzpw5zJ07lw0bNnD27Nm+
tTt37iQ7O5vZs2eTl5fH/v37h/0MJEm6XDYkkjRM1dXVnDx5kra2tguue/jhh4nFYv2agn379jF/
/nzGjBkDwG+//UZBQQGZmZns2bOHrVu30tHRwebNm4dUUzwep6ioiGuuuYaPP/6YDRs2EIvFqKqq
AuDAgQNUVFSwatUqamtryc3Npbi4mFOnTg3xv5ck6b/DhkSShqGtrY1t27bx0EMPXTAhAcjOzqa3
t5fGxkYAzpw5Q11dHXl5eX1rfv/9dwoKCigqKmLKlClkZGQwf/58jhw5MqS6mpub6ejooKysjNtu
u41oNEppaSk7duygp6eHn376CYBJkyYxadIkCgoKqK6uZtSoUUN8ApIk/XckhS5Akv7XxONxSktL
Wbp0KZmZmRQWFnL69GnGjRt33vVjx44lOzubTz/9lLlz51JfX09KSgr33ntv35rrr7+exYsXs23b
Ng4ePMiRI0c4fPgwd9xxx5Bq++677zh16hTRaLRfvd3d3XR2dpKVlcWsWbNYtGgR06dPZ968eTz2
2GOMHTt2eA9DkqTLZEMiSUO0e/dujh49ypYtWzh58iTwZ2KSkZEx6N/k5eXx8ssv09PTw969e8nN
zSUxMbHv/vHjx3n00UeZOXMmWVlZLF26lFgsRktLy4DPikQiA6719PT0/U5LS+PNN98csOamm25i
9OjRfPDBB7S0tLB//35qa2vZsWMHO3fu5Pbbbx/ys5Ak6XI5siVJQ3DixAmqqqooLS0lNTWVyZMn
k5qaetFzJPfddx8JCQk0NTVRX18/4O1an3/+OSkpKbz99ts89dRTRKNR2tvbz/sdJ3+NV3V1dfVd
6+joAGDq1KkcO3aM8ePHk5aWRlpaGj///DOVlZXE43FaW1upqakhGo2ycuVK9u3bx8SJE2loaLjc
RyNJ0rDYkEjSEJSVlZGZmUl2dnbftWnTpl30HElSUhI5OTlUVlZy4403kp6e3u/++PHjOXHiBF99
9RXt7e289dZbfPbZZ/0Owv9l4sSJ3Hzzzbzzzju0t7fzySefEIvFAMjKymLy5Mm89NJLHDp0iNbW
VlavXk1CQgLJycmMGTOGmpoa3n//fTo6Oqirq+Po0aMD6pEkaaTYkEjSJYrFYjQ2NrJ69ep+12fM
mHHRhAT+HNs6ePBgv8Psf8nJyWHhwoUUFxfzyCOP0NzczKpVq/jhhx8GfKliQkIC5eXlfPPNN+Tm
5rJnzx4KCwsBSExMZPPmzSQmJrJs2TJWrFhBNBqlrKwMgJkzZ1JRUcH27dvJycmhoqKCkpISMjMz
h/tYJEm6LJH4+eYBJEmSJGkEmJBIkiRJCsaGRJIkSVIwNiSSJEmSgrEhkSRJkhSMDYkkSZKkYGxI
JEmSJAVjQyJJkiQpGBsSSZIkScHYkEiSJEkK5j8wC9EyBozJaQAAAABJRU5ErkJggg==
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [58]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The best parameters were found to be: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;The best parameters were found to be: Result(alpha=0.1, lmbda=10.0).
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Train-a-final-model"&gt;Train a final model&lt;a class="anchor-link" href="#Train-a-final-model"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Using these parameters, let's train a final implicit matrix factorization model using 1,000,000 top songs. Additionally, we'll choose $\epsilon = 1,000$ which seemed to work best in simple experimentation. This experimentation is not shown here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [7]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RatingsMatrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_top_songs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [163]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Our ratings matrix contains &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; countries and &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; unique songs.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Our ratings matrix contains 55 countries and 8663 unique songs.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;implicit_mf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ImplicitMF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Visualize"&gt;Visualize&lt;a class="anchor-link" href="#Visualize"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Let's plot the cosine similarities between all pairs of countries to confirm that things make sense. Intuitively, I'd think that countries in the following groups should be similar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;United States, United Kingdom, Canada, Australia, New Zealand&lt;/li&gt;
&lt;li&gt;Latin American countries&lt;/li&gt;
&lt;li&gt;Sweden, Finland, Norway, Denmark&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;First let's replace the index of our vectors such that it contains the country names.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [165]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;country_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;country_id_to_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;countries_lookup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'id'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;countries_lookup&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;country_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;
&lt;span class="n"&gt;country_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Index&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;country_id_to_name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;country_ids&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_name'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_names&lt;/span&gt;

&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[165]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;dim_0&lt;/th&gt;
&lt;th&gt;dim_1&lt;/th&gt;
&lt;th&gt;dim_2&lt;/th&gt;
&lt;th&gt;dim_3&lt;/th&gt;
&lt;th&gt;dim_4&lt;/th&gt;
&lt;th&gt;dim_5&lt;/th&gt;
&lt;th&gt;dim_6&lt;/th&gt;
&lt;th&gt;dim_7&lt;/th&gt;
&lt;th&gt;dim_8&lt;/th&gt;
&lt;th&gt;dim_9&lt;/th&gt;
&lt;th&gt;...&lt;/th&gt;
&lt;th&gt;dim_20&lt;/th&gt;
&lt;th&gt;dim_21&lt;/th&gt;
&lt;th&gt;dim_22&lt;/th&gt;
&lt;th&gt;dim_23&lt;/th&gt;
&lt;th&gt;dim_24&lt;/th&gt;
&lt;th&gt;dim_25&lt;/th&gt;
&lt;th&gt;dim_26&lt;/th&gt;
&lt;th&gt;dim_27&lt;/th&gt;
&lt;th&gt;dim_28&lt;/th&gt;
&lt;th&gt;dim_29&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;country_name&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;United States&lt;/th&gt;
&lt;td&gt;-1.276051&lt;/td&gt;
&lt;td&gt;-1.058583&lt;/td&gt;
&lt;td&gt;-0.622779&lt;/td&gt;
&lt;td&gt;0.430231&lt;/td&gt;
&lt;td&gt;0.306830&lt;/td&gt;
&lt;td&gt;0.966786&lt;/td&gt;
&lt;td&gt;0.190530&lt;/td&gt;
&lt;td&gt;-0.549356&lt;/td&gt;
&lt;td&gt;0.538788&lt;/td&gt;
&lt;td&gt;-0.722162&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.057478&lt;/td&gt;
&lt;td&gt;0.500288&lt;/td&gt;
&lt;td&gt;0.799253&lt;/td&gt;
&lt;td&gt;0.383243&lt;/td&gt;
&lt;td&gt;0.575051&lt;/td&gt;
&lt;td&gt;0.247066&lt;/td&gt;
&lt;td&gt;0.548018&lt;/td&gt;
&lt;td&gt;-0.238643&lt;/td&gt;
&lt;td&gt;0.114481&lt;/td&gt;
&lt;td&gt;0.731294&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;United Kingdom&lt;/th&gt;
&lt;td&gt;0.106521&lt;/td&gt;
&lt;td&gt;0.346207&lt;/td&gt;
&lt;td&gt;-0.276669&lt;/td&gt;
&lt;td&gt;-0.003249&lt;/td&gt;
&lt;td&gt;-1.294024&lt;/td&gt;
&lt;td&gt;-0.090318&lt;/td&gt;
&lt;td&gt;0.666301&lt;/td&gt;
&lt;td&gt;-0.533962&lt;/td&gt;
&lt;td&gt;0.281620&lt;/td&gt;
&lt;td&gt;-0.268332&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;-0.803658&lt;/td&gt;
&lt;td&gt;-0.498773&lt;/td&gt;
&lt;td&gt;-0.041302&lt;/td&gt;
&lt;td&gt;-0.705815&lt;/td&gt;
&lt;td&gt;0.624482&lt;/td&gt;
&lt;td&gt;-0.007293&lt;/td&gt;
&lt;td&gt;1.335080&lt;/td&gt;
&lt;td&gt;-0.717667&lt;/td&gt;
&lt;td&gt;0.019507&lt;/td&gt;
&lt;td&gt;-1.700016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Argentina&lt;/th&gt;
&lt;td&gt;-0.871966&lt;/td&gt;
&lt;td&gt;0.678408&lt;/td&gt;
&lt;td&gt;0.343354&lt;/td&gt;
&lt;td&gt;-0.968614&lt;/td&gt;
&lt;td&gt;-1.099471&lt;/td&gt;
&lt;td&gt;0.014175&lt;/td&gt;
&lt;td&gt;0.031833&lt;/td&gt;
&lt;td&gt;-0.370821&lt;/td&gt;
&lt;td&gt;-0.114270&lt;/td&gt;
&lt;td&gt;-0.982766&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.435942&lt;/td&gt;
&lt;td&gt;-0.170878&lt;/td&gt;
&lt;td&gt;-0.172491&lt;/td&gt;
&lt;td&gt;-0.158596&lt;/td&gt;
&lt;td&gt;0.445267&lt;/td&gt;
&lt;td&gt;-0.949628&lt;/td&gt;
&lt;td&gt;0.542433&lt;/td&gt;
&lt;td&gt;-0.157643&lt;/td&gt;
&lt;td&gt;0.765119&lt;/td&gt;
&lt;td&gt;0.587947&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Australia&lt;/th&gt;
&lt;td&gt;-1.129085&lt;/td&gt;
&lt;td&gt;0.212391&lt;/td&gt;
&lt;td&gt;0.136416&lt;/td&gt;
&lt;td&gt;-0.665215&lt;/td&gt;
&lt;td&gt;0.452084&lt;/td&gt;
&lt;td&gt;-0.418566&lt;/td&gt;
&lt;td&gt;-0.187979&lt;/td&gt;
&lt;td&gt;0.807976&lt;/td&gt;
&lt;td&gt;-0.693282&lt;/td&gt;
&lt;td&gt;-0.723989&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;-0.961785&lt;/td&gt;
&lt;td&gt;-0.064047&lt;/td&gt;
&lt;td&gt;-0.161033&lt;/td&gt;
&lt;td&gt;0.211444&lt;/td&gt;
&lt;td&gt;0.180285&lt;/td&gt;
&lt;td&gt;0.597487&lt;/td&gt;
&lt;td&gt;-0.143220&lt;/td&gt;
&lt;td&gt;0.336241&lt;/td&gt;
&lt;td&gt;-0.251201&lt;/td&gt;
&lt;td&gt;-1.057264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Austria&lt;/th&gt;
&lt;td&gt;-0.094485&lt;/td&gt;
&lt;td&gt;0.870111&lt;/td&gt;
&lt;td&gt;-0.328650&lt;/td&gt;
&lt;td&gt;-0.594735&lt;/td&gt;
&lt;td&gt;-0.429165&lt;/td&gt;
&lt;td&gt;-0.955718&lt;/td&gt;
&lt;td&gt;0.151173&lt;/td&gt;
&lt;td&gt;0.250305&lt;/td&gt;
&lt;td&gt;-0.302996&lt;/td&gt;
&lt;td&gt;0.718965&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;-0.409435&lt;/td&gt;
&lt;td&gt;-0.381884&lt;/td&gt;
&lt;td&gt;-0.144373&lt;/td&gt;
&lt;td&gt;0.340587&lt;/td&gt;
&lt;td&gt;0.006711&lt;/td&gt;
&lt;td&gt;0.870717&lt;/td&gt;
&lt;td&gt;0.067101&lt;/td&gt;
&lt;td&gt;-0.361675&lt;/td&gt;
&lt;td&gt;0.742727&lt;/td&gt;
&lt;td&gt;1.341254&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 30 columns&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Then, we'll plot the cosine similarities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [166]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"white"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_cosine_similarities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;similarities_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cosine_similarity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similarities_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;triu_indices_from&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;

    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;cmap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diverging_palette&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;220&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;as_cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;heatmap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;similarities_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;vmax&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;xticklabels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;yticklabels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;linewidths&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;cbar_kws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"shrink"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; 
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Cosine Similarity Matrix'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [167]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_cosine_similarities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJEAAAQ6CAYAAAD0leyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I first turn to my favorite country - Colombia - to inspect its similarity with other Latin American countries. The numbers are high for: Peru, Paraguay, Panama, Nicaragua, Gautemala, Ecuador, Dominican Republic and Costa Rica.&lt;/p&gt;
&lt;p&gt;Next, I turn to the United Kingdom: it is most similar with Switzerland, New Zealand, Ireland, Hungary, Belgium and Australia.&lt;/p&gt;
&lt;p&gt;Finally, we see that Sweden is similar enough to Norway, yet not Finland (who, incidentally, seems to have nothing in common with anyone at all).&lt;/p&gt;
&lt;p&gt;Overall, this looks very good to me.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Visualize-with-TSNE"&gt;Visualize with TSNE&lt;a class="anchor-link" href="#Visualize-with-TSNE"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To further inspect similarities between countries, let's explore the 2-dimensional TSNE space.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [205]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'pca'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12345&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dim_1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dim_2'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [206]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_tsne_embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_embeddings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;country_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;country_embeddings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;dim_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_embedding&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xytext&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;textcoords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'offset points'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'right'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'bottom'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Two-Dimensional TSNE Embeddings of Latent Country Vectors'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dimension 1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dimension 2'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [207]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_tsne_embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA34AAANtCAYAAADPa6khAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is not particularly useful. Furthermore, and perhaps most importantly, the plot varies immensely when changing the random seed. This is a worthwhile step nonetheless.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Inspect-predicted-preferences"&gt;Inspect predicted preferences&lt;a class="anchor-link" href="#Inspect-predicted-preferences"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Crucially, we'll now inspect predicted preferences for select countries and confirm that they make sense. I first normalize each of the country and song vectors, then compute predicted preferences via the dot product.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SONG_METADATA_QUERY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;span class="s2"&gt;    SELECT &lt;/span&gt;
&lt;span class="s2"&gt;        songs.title as song_title, &lt;/span&gt;
&lt;span class="s2"&gt;        songs.artist as song_artist,&lt;/span&gt;
&lt;span class="s2"&gt;        songs.id as song_id&lt;/span&gt;
&lt;span class="s2"&gt;    FROM songs&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;

&lt;span class="n"&gt;song_metadata_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SONG_METADATA_QUERY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ENGINE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;song_vectors_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;song_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'inner'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [209]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="United-States"&gt;United States&lt;a class="anchor-link" href="#United-States"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [210]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'display.max_colwidth'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [211]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'United States'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[211]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Dab of Ranch - Recorded at Spotify Studios NYC&lt;/td&gt;
&lt;td&gt;Migos&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Massage In My Room&lt;/td&gt;
&lt;td&gt;Future&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Father Stretch My Hands Pt. 1&lt;/td&gt;
&lt;td&gt;Kanye West&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Keep On&lt;/td&gt;
&lt;td&gt;Kehlani&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;First Day Out&lt;/td&gt;
&lt;td&gt;Tee Grizzley&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;When I Was Broke&lt;/td&gt;
&lt;td&gt;Future&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Star Of The Show&lt;/td&gt;
&lt;td&gt;Thomas Rhett&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Rose Golden&lt;/td&gt;
&lt;td&gt;Kid Cudi&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;oui&lt;/td&gt;
&lt;td&gt;Jeremih&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Everyday We Lit&lt;/td&gt;
&lt;td&gt;YFN Lucci&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Colombia"&gt;Colombia&lt;a class="anchor-link" href="#Colombia"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [212]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Colombia'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[212]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Ya No Me Duele Más&lt;/td&gt;
&lt;td&gt;Silvestre Dangond&lt;/td&gt;
&lt;td&gt;0.941204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Bajo el agua&lt;/td&gt;
&lt;td&gt;Manuel Medrano&lt;/td&gt;
&lt;td&gt;0.919997&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Si No Te Quiere&lt;/td&gt;
&lt;td&gt;Ozuna&lt;/td&gt;
&lt;td&gt;0.916971&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;El Ganador&lt;/td&gt;
&lt;td&gt;Nicky Jam&lt;/td&gt;
&lt;td&gt;0.898030&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Oiga, Mira, Vea&lt;/td&gt;
&lt;td&gt;Guayacán Orquesta&lt;/td&gt;
&lt;td&gt;0.897944&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Mil Lágrimas&lt;/td&gt;
&lt;td&gt;Nicky Jam&lt;/td&gt;
&lt;td&gt;0.897863&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Déjame Te Explico&lt;/td&gt;
&lt;td&gt;Reykon&lt;/td&gt;
&lt;td&gt;0.897396&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Quédate&lt;/td&gt;
&lt;td&gt;Manuel Medrano&lt;/td&gt;
&lt;td&gt;0.896307&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Afuera del planeta&lt;/td&gt;
&lt;td&gt;Manuel Medrano&lt;/td&gt;
&lt;td&gt;0.895686&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Materialista&lt;/td&gt;
&lt;td&gt;Silvestre Dangond&lt;/td&gt;
&lt;td&gt;0.895501&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Turkey"&gt;Turkey&lt;a class="anchor-link" href="#Turkey"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [213]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Turkey'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[213]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Dudak&lt;/td&gt;
&lt;td&gt;Edis&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Benim Karanlık Yanım&lt;/td&gt;
&lt;td&gt;Sezen Aksu&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Affection&lt;/td&gt;
&lt;td&gt;Cigarettes After Sex&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Ateş Böceği&lt;/td&gt;
&lt;td&gt;Mithat Can Özer&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Beni Öyle Bilme&lt;/td&gt;
&lt;td&gt;Ceylan Ertem&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Keyfim Kaçık Acık&lt;/td&gt;
&lt;td&gt;Can Bonomo&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Seni Unutmaya Ömrüm Yeter mi&lt;/td&gt;
&lt;td&gt;Ümit Besen&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Vazgeçtim&lt;/td&gt;
&lt;td&gt;Sezen Aksu&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Yanımda Kal&lt;/td&gt;
&lt;td&gt;Harun Kolçak&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Stanga&lt;/td&gt;
&lt;td&gt;Sagi Abitbul&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Germany"&gt;Germany&lt;a class="anchor-link" href="#Germany"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [214]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Germany'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[214]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Skandale&lt;/td&gt;
&lt;td&gt;Bonez MC&lt;/td&gt;
&lt;td&gt;0.908499&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Ahnma&lt;/td&gt;
&lt;td&gt;Beginner&lt;/td&gt;
&lt;td&gt;0.906591&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Am schönsten&lt;/td&gt;
&lt;td&gt;SDP&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Tage wie diese&lt;/td&gt;
&lt;td&gt;Die Toten Hosen&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Kartell (feat. Azet &amp;amp; Nash)&lt;/td&gt;
&lt;td&gt;Zuna&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Chill mal dein Leben (feat. Moe Phoenix)&lt;/td&gt;
&lt;td&gt;Kianush&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Neuanfang&lt;/td&gt;
&lt;td&gt;Clueso&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Neben der Spur&lt;/td&gt;
&lt;td&gt;Maxwell&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Hochspannung&lt;/td&gt;
&lt;td&gt;Maxwell&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Intro&lt;/td&gt;
&lt;td&gt;Capital Bra&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Taiwan"&gt;Taiwan&lt;a class="anchor-link" href="#Taiwan"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [215]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[215]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;如果的事&lt;/td&gt;
&lt;td&gt;王藍茵&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;小小的&lt;/td&gt;
&lt;td&gt;思衛&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Heathens (feat. MUTEMATH)&lt;/td&gt;
&lt;td&gt;Twenty One Pilots&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;其實都沒有&lt;/td&gt;
&lt;td&gt;楊宗緯&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;對摺 - 國語&lt;/td&gt;
&lt;td&gt;任賢齊&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;最難的是相遇&lt;/td&gt;
&lt;td&gt;Valen Hsu&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;안녕 못해 I'm Not Okay (From “MISSING 9”)&lt;/td&gt;
&lt;td&gt;CHEN&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;未接來電&lt;/td&gt;
&lt;td&gt;楊乃文&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;飛機 - feat. 林俊峰&lt;/td&gt;
&lt;td&gt;林俊傑&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;All On Me&lt;/td&gt;
&lt;td&gt;Devin Dawson&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;These look great.&lt;/p&gt;
&lt;p&gt;One interesting thing to note is the significance of the length of the vectors. To this point, &lt;a href="https://arxiv.org/abs/1508.02297"&gt;Schakel and Wilson&lt;/a&gt; offer the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A word that is consistently used in a similar context will be represented by a longer vector than a word of the same frequency that is used in different contexts.&lt;/p&gt;
&lt;p&gt;Not only the direction, but also the length of word vectors carries important information.&lt;/p&gt;
&lt;p&gt;Word vector length furnishes, in combination with term frequency, a useful measure of word significance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, my guess is that length indicates something like popularity - at least for songs. Let's see which songs and countries have the largest L2 norms.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [216]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[216]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Shape of You&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.687497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Starboy&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.684529&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;I Don’t Wanna Live Forever (Fifty Shades Darker) - From "Fifty Shades Darker (Original Motion Pi...&lt;/td&gt;
&lt;td&gt;ZAYN&lt;/td&gt;
&lt;td&gt;0.682996&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Despacito (Featuring Daddy Yankee)&lt;/td&gt;
&lt;td&gt;Luis Fonsi&lt;/td&gt;
&lt;td&gt;0.682941&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Rockabye (feat. Sean Paul &amp;amp; Anne-Marie)&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;0.682918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.682878&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Call On Me - Ryan Riback Extended Remix&lt;/td&gt;
&lt;td&gt;Starley&lt;/td&gt;
&lt;td&gt;0.682799&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.682442&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.682225&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Closer&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.682175&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [217]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[217]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country_name&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;4.842656&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Sweden&lt;/td&gt;
&lt;td&gt;4.602653&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Finland&lt;/td&gt;
&lt;td&gt;4.415459&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;France&lt;/td&gt;
&lt;td&gt;4.399233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;United Kingdom&lt;/td&gt;
&lt;td&gt;4.374870&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Netherlands&lt;/td&gt;
&lt;td&gt;4.330190&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Denmark&lt;/td&gt;
&lt;td&gt;4.232457&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Italy&lt;/td&gt;
&lt;td&gt;4.218088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Norway&lt;/td&gt;
&lt;td&gt;4.084380&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Germany&lt;/td&gt;
&lt;td&gt;4.076692&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;To reference Taiwan and Colombia, let's inspect predictions using unnormalized vectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [218]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[218]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Bad Things - With Camila Cabello&lt;/td&gt;
&lt;td&gt;Machine Gun Kelly&lt;/td&gt;
&lt;td&gt;0.958864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Just Hold On&lt;/td&gt;
&lt;td&gt;Steve Aoki&lt;/td&gt;
&lt;td&gt;0.958692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Shape of You&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.958325&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Side To Side&lt;/td&gt;
&lt;td&gt;Ariana Grande&lt;/td&gt;
&lt;td&gt;0.957497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Closer&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.957290&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Starboy&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.957090&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.957001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;I Don’t Wanna Live Forever (Fifty Shades Darker) - From "Fifty Shades Darker (Original Motion Pi...&lt;/td&gt;
&lt;td&gt;ZAYN&lt;/td&gt;
&lt;td&gt;0.956933&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.956803&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;The Greatest&lt;/td&gt;
&lt;td&gt;Sia&lt;/td&gt;
&lt;td&gt;0.956746&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [219]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Colombia'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[219]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;I Got You&lt;/td&gt;
&lt;td&gt;Bebe Rexha&lt;/td&gt;
&lt;td&gt;1.019838&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Party Monster&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.993647&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Hello&lt;/td&gt;
&lt;td&gt;Adele&lt;/td&gt;
&lt;td&gt;0.993421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;I Would Like&lt;/td&gt;
&lt;td&gt;Zara Larsson&lt;/td&gt;
&lt;td&gt;0.987099&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Scars To Your Beautiful&lt;/td&gt;
&lt;td&gt;Alessia Cara&lt;/td&gt;
&lt;td&gt;0.987036&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Weak&lt;/td&gt;
&lt;td&gt;AJR&lt;/td&gt;
&lt;td&gt;0.986642&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Too Good&lt;/td&gt;
&lt;td&gt;Drake&lt;/td&gt;
&lt;td&gt;0.982179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Nancy Mulligan&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.979373&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Supermarket Flowers&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.978706&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Hearts Don't Break Around Here&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.978494&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Respectively, there is not a song with Chinese nor Spanish letters in each set of recommendations. Additionally, it seems like the world is fond of Ed Sheeran. It is clear that the global popularity of songs dominates our recommendations when computing the dot product with unnormalized vectors.&lt;/p&gt;
&lt;p&gt;When we do normalize, our predicted preferences can be defined thus:&lt;/p&gt;
$$
\hat{x}_u = \frac{x_u}{\|x_u\|} \quad \hat{y}_i = \frac{y_i}{\|y_i\|}\\
\hat{p}_{u, i} = \hat{x}_u \cdot \hat{y}_i
$$&lt;p&gt;One idea might be to scale the song vectors by their respective lengths, leaving our predicted preferences as:&lt;/p&gt;
$$
\begin{align*}
\hat{p}_{u, i} &amp;amp;= \hat{x}_u \cdot \big(\hat{y}_i \cdot c\|y_i\|\big)\\
&amp;amp;= \hat{x}_u \cdot \Bigg(\hat{y}_i \cdot c\frac{y_i}{\hat{y_i}}\Bigg)\\
&amp;amp;= \hat{x}_u \cdot c y_i
\end{align*}
$$&lt;p&gt;Let's briefly see what this looks like.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [220]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[220]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Bad Things - With Camila Cabello&lt;/td&gt;
&lt;td&gt;Machine Gun Kelly&lt;/td&gt;
&lt;td&gt;0.039601&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Just Hold On&lt;/td&gt;
&lt;td&gt;Steve Aoki&lt;/td&gt;
&lt;td&gt;0.039594&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Shape of You&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.039578&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Side To Side&lt;/td&gt;
&lt;td&gt;Ariana Grande&lt;/td&gt;
&lt;td&gt;0.039544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Closer&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.039536&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Starboy&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.039528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.039524&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;I Don’t Wanna Live Forever (Fifty Shades Darker) - From "Fifty Shades Darker (Original Motion Pi...&lt;/td&gt;
&lt;td&gt;ZAYN&lt;/td&gt;
&lt;td&gt;0.039521&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.039516&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;The Greatest&lt;/td&gt;
&lt;td&gt;Sia&lt;/td&gt;
&lt;td&gt;0.039513&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;About the same. Nevertheless, the length of our vectors can likely offer some value when balancing local recommendations with globally popular songs. I leave this to the reader to explore further for now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Approximating-IMF-with-neural-networks"&gt;Approximating IMF with neural networks&lt;a class="anchor-link" href="#Approximating-IMF-with-neural-networks"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Again, IMF gives a function $f: u, i \rightarrow \hat{p}_{u, i}$ and the following objective function to minimize:&lt;/p&gt;
$$
\underset{x_{*}, y_{*}}{\arg\min}\sum\limits_{u, i}c_{u, i}\big(p_{u, i} - f(u, i)\big)^2 + \lambda\bigg(\sum\limits_u\|x_u\|^2 + \sum\limits_i\|y_u\|^2\bigg)
$$&lt;p&gt;Let's try to do this with neural networks. I'll use the $\alpha$ and $\lambda$ parameters found above, and keep the same dimensionality, $F = 30$, for our latent vectors. This number was chosen in separate experimentation not shown here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [32]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;backend&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Lambda&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LSTM&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers.merge&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.optimizers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Adam&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.regularizers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;l2&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plot_model&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [9]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Distribution of # of Unique Songs Streamed per Country'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Unique Songs Streamed'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'# of Countries'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAe4AAAFoCAYAAACR/hiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The 15th percentile of songs rated by country is &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;The 15th percentile of songs rated by country is 218.20000000000002.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's set our eligibility cutoff at 200 songs and create training and validation matrices.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;more_than_200_ratings_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;split_ratings_matrix_into_training_and_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_200_ratings_mask&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Arrange-our-data"&gt;Arrange our data&lt;a class="anchor-link" href="#Arrange-our-data"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To interact with Keras, we'll need to change our data from wide format to long. Additionally, so as to create our embeddings, we'll need to give each country and song a unique, contiguous index. Finally, I'll do the same with the song artist, as well as tokenize the song title, for later use.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[16]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;5552&lt;/th&gt;
&lt;td&gt;El Perdón&lt;/td&gt;
&lt;td&gt;Nicky Jam&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5553&lt;/th&gt;
&lt;td&gt;Lean On (feat. MØ &amp;amp; DJ Snake)&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5555&lt;/th&gt;
&lt;td&gt;Love Me Like You Do - From "Fifty Shades Of Grey"&lt;/td&gt;
&lt;td&gt;Ellie Goulding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5556&lt;/th&gt;
&lt;td&gt;Uptown Funk&lt;/td&gt;
&lt;td&gt;Mark Ronson&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5557&lt;/th&gt;
&lt;td&gt;Loquita&lt;/td&gt;
&lt;td&gt;Marama&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_id_to_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;c_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="n"&gt;song_id_to_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="n"&gt;song_artist_to_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;artist&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;artist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;())}&lt;/span&gt;

&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_id_to_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_artist_to_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tail&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[17]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_index&lt;/th&gt;
&lt;th&gt;song_artist_index&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;33068&lt;/th&gt;
&lt;td&gt;Affection&lt;/td&gt;
&lt;td&gt;Cigarettes After Sex&lt;/td&gt;
&lt;td&gt;8658&lt;/td&gt;
&lt;td&gt;3699&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33069&lt;/th&gt;
&lt;td&gt;Istemem Soz Sevmeni&lt;/td&gt;
&lt;td&gt;Ferman Akgül&lt;/td&gt;
&lt;td&gt;8659&lt;/td&gt;
&lt;td&gt;3106&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33070&lt;/th&gt;
&lt;td&gt;Skammekroken 2017&lt;/td&gt;
&lt;td&gt;TIX&lt;/td&gt;
&lt;td&gt;8660&lt;/td&gt;
&lt;td&gt;1677&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33071&lt;/th&gt;
&lt;td&gt;Wiosna&lt;/td&gt;
&lt;td&gt;Organek&lt;/td&gt;
&lt;td&gt;8661&lt;/td&gt;
&lt;td&gt;3716&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33072&lt;/th&gt;
&lt;td&gt;Fuckt.&lt;/td&gt;
&lt;td&gt;Strapo&lt;/td&gt;
&lt;td&gt;8662&lt;/td&gt;
&lt;td&gt;3717&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Tokenize-song-title"&gt;Tokenize song title&lt;a class="anchor-link" href="#Tokenize-song-title"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;NUM_WORDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'title_sequence'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Convert-ratings-to-long-format"&gt;Convert ratings to long format&lt;a class="anchor-link" href="#Convert-ratings-to-long-format"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [19]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;input_tuples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;country_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;input_tuples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;ratings_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;country_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;song_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="n"&gt;ratings_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'inner'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_id_to_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-network-parameters"&gt;Define network parameters&lt;a class="anchor-link" href="#Define-network-parameters"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [22]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nunique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;n_songs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nunique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;n_artists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nunique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;
&lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-a-loss-function"&gt;Define a loss function&lt;a class="anchor-link" href="#Define-a-loss-function"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is simply the IMF objective we'd like to minimize.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [23]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-functions-to-generate,-evaluate-and-visualize-predictions"&gt;Define functions to generate, evaluate and visualize predictions&lt;a class="anchor-link" href="#Define-functions-to-generate,-evaluate-and-visualize-predictions"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [24]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
        &lt;span class="s1"&gt;'prediction'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pivot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'prediction'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [25]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_200_ratings_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                         &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;evaluator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ExpectedPercentileRankingsEvaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Train: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_validation&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [74]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visualize_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predicted Preferences Histogram'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predicted Prefence'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Count'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#1"&gt;Network #1&lt;a class="anchor-link" href="#Network-#1"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;A trivially "Siamese" network which first embeds each country and song index into $\mathbb{R}^f$ in parallel then computes a dot-product of the embeddings. This is roughly equivalent to what is being done by IMF.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [162]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [163]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_1.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 1" class="img-responsive" src="https://willwolf.io/figures/neural_implicit_mf_network_1.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [164]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 2s - loss: 64.9000 - val_loss: 0.0185
Epoch 2/5
476465/476465 [==============================] - 8s - loss: 0.0744 - val_loss: 0.0185
Epoch 3/5
476465/476465 [==============================] - 3s - loss: 0.0744 - val_loss: 0.0185
Epoch 4/5
476465/476465 [==============================] - 10s - loss: 0.0745 - val_loss: 0.0185
Epoch 5/5
476465/476465 [==============================] - 8s - loss: 0.0745 - val_loss: 0.0186
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [165]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.5053388101677021
Validation: 0.49071882278215817
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;For reference, let's recall the results we computed when validation our model with &lt;code&gt;best_params&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [166]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[166]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;{'train': 0.04479720392806505, 'validation': 0.18543168148626762}&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Additionally, a &lt;em&gt;random&lt;/em&gt; model should return an expected percentile ranking of ~50%, as noted in &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Hu, Koren and Volinksy&lt;/a&gt;. So, this is really bad. Let's visualize the distribution of predictions before moving on to another model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [167]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;visualize_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnIAAAGDCAYAAACvCP20AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_2"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#2"&gt;Network #2&lt;a class="anchor-link" href="#Network-#2"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as previous, but with a bias embedding for each set, in $\mathbb{R}$, added to the dot-product.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [168]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [169]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_2.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 2" class="img-responsive" src="https://willwolf.io/figures/neural_implicit_mf_network_2.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [170]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 2s - loss: 0.0734 - val_loss: 0.0204
Epoch 2/5
476465/476465 [==============================] - 2s - loss: 0.0563 - val_loss: 0.0216
Epoch 3/5
476465/476465 [==============================] - 2s - loss: 0.0534 - val_loss: 0.0236
Epoch 4/5
476465/476465 [==============================] - 2s - loss: 0.0515 - val_loss: 0.0252
Epoch 5/5
476465/476465 [==============================] - 2s - loss: 0.0502 - val_loss: 0.0269
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [171]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.10900494586591782
Validation: 0.1906236846450195
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This looks a lot better. Now let's visualize our ground-truth and predictions side-by-side.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [172]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'predictions'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;'ratings'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA7MAAAFyCAYAAAAnNY1XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2Y1XWZP/D3YUZUmKGRpF2xdCF1k9JVmsWrFujBiLaN
qyyUhy62wjYz0ygziAQyVCBt1pTQ1Nx2TVJZTam82qsoI8ALzFLX8aHdtkiFCoNyZjQe5pzfH3s1
+8NggJnBM9+Z1+svzjn3zLk/NzPnM+/z/Z5zSpVKpRIAAAAokAHVbgAAAAAOlDALAABA4QizAAAA
FI4wCwAAQOEIswAAABSOMAsAAEDhCLNQAKeeemqeeuqp/Od//mcuuOCCTmsffvjhzJ8/P0n2qx4A
qB77NnSdMAsFctJJJ+Xqq6/utOa///u/85vf/Ga/6wGA6rFvQ9eVKpVKpdpNQF+yfv36fP7zn89f
/MVf5Mknn8xhhx2WxYsX54Ybbsjvf//7PPnkk3njG9+Yj33sY7nyyitz//33p729PaNGjcrFF1+c
urq6/PjHP87ChQtTKpVy0kknZeXKlfn2t7+dp59+OgsXLsy3vvWttLW15dJLL81PfvKT1NTU5C1v
eUumTZuWadOmpaWlJW9961vzrne9q6O+paUll1xySR5//PGUSqWMGzcun/jEJ1JbW5uTTjopH/rQ
h7J27dr89re/zT/+4z/m/e9/f7Zs2ZLZs2dn27ZtSZI3vOENmTVrVpUnDAC93/r163PZZZdl0KBB
aWtry+jRo/Poo4+mra0tlUoll156aYYPH77XfXvOnDmpq6vLE088kV//+tcZOXJkmpqaMnjw4Pzw
hz/MlVdemQEDBuTEE0/MunXrsnz58hx66KH2bfoVR2bhIHj00Uczc+bMfPOb38y73/3uXHTRRUmS
P/7xj/n2t7+diy66KNdff31qampy5513ZuXKlXnZy16WK6+8Mjt27MjHPvaxzJkzJ3fddVdOO+20
/PGPf/yz+7j66quzffv23HPPPbnrrrvyk5/8JL/61a9ywQUXpLGxMYsWLdqt/tJLL01DQ0O++c1v
5o477sgTTzyRm266KUmyY8eOHHHEEbn11ltz9dVX5wtf+EK2b9+e22+/PS9/+cvzjW98I7fccks2
btyYlpaWgz9AAOgD/uu//itf+MIX8rnPfS7btm3LbbfdlnvuuSdnnHFGbrjhhhx11FF73beT5JFH
HslXvvKV3HPPPfntb3+b73znO9m2bVs+9alP5Yorrsjdd9+d0047rePIrn2b/qa22g1AX/SqV70q
jY2NSZL3vOc9+dznPpeXvexlee1rX9tRc++996alpSXr1q1LkuzcuTMvfelL87Of/Sy1tbV53ete
lyR5xzve0fFamv/funXr8ulPfzo1NTWpqanJ1772tSTJnXfeuceeVq9ena9//esplUoZOHBgpk6d
mn/913/Nhz70oSTJ6aefniR59atfnR07duS5557LuHHj8qEPfSibN2/O61//+lx44YWpr6/voSkB
QN921FFH5eijj87RRx+dl7zkJbn11lvz5JNPZv369Rk8ePA+v37cuHEZOHBgkuSEE07IH/7wh/z4
xz/OK1/5yrzqVa9Kkpxxxhm59NJLO+rt2/QnjszCQVBTU7Pb5UqlkgEDBmTQoEEd15XL5cydOzd3
33137r777qxYsSJf/OIXUyqV8sKz/2tr//x5p9ra2pRKpY7Lmzdv7jitaE/K5fKfXd61a1fH5UMP
PTRJOr5npVLJySefnFWrVmXKlCl5+umnc+aZZ+YnP/nJvpYPACQd+/69996bc845J8n/Pnk8bdq0
/fr6ww47rOPff/r7oKam5s/+Thgw4H//pLdv098Is3AQPP7443n88ceTJLfddltGjx6dIUOG7FYz
duzY3HLLLdmxY0fK5XLmzZuXpqamnHDCCalUKvnhD3+YJFm1alX+8Ic//Nl9vO51r8s3vvGNlMvl
7NixIxdccEHuv//+1NTU7BZSX3h/lUolO3bsyO23357Xv/71na7jyiuvzLJly/KWt7wln/nMZ3Lc
ccfll7/8ZRenAgD909q1a/OmN70p06dPz0knnZTvfe97aW9vT5K97tt7M3r06Pzyl7/s+DvjP/7j
P/Lss8+mVCrZt+l3hFk4CI488shcddVVmTRpUr73ve/l85///J/VfOQjH8nRRx+dM844I29/+9tT
qVQyZ86cHHLIIfnSl76UL37xi3nnO9+Z7373u3npS1/6Z1//0Y9+NIccckje+c535l3velfe8IY3
5K1vfWtOPfXU/M///E/OO++83eovvvjibN26NZMmTcqkSZMyYsSIfPjDH+50He973/vy+OOP5x3v
eEfe85735OUvf3ne8Y53dG84ANDPTJ06Nffff38mTZqUKVOm5BWveEWeeuqplMvlve7be9PQ0JCm
pqbMnj07Z5xxRtasWZPa2tocfvjh9m36He9mDD1s/fr1He9ECADQk1pbW7Ns2bKcf/75Ofzww9Pc
3JxzzjknP/rRj3Z7+RH0B94ACgAACqKuri6HHHJIJk+enNra2tTW1uaqq64SZOmXHJkFAACgcLxm
FgAAgMIRZgEAACgcYRYAAIDCKfQbQG3Z0lLtFnrcEUcMyrZtz1W7jcIzx55jlj3DHHvOnmY5bFh9
lbrhYOmpPb6//e71p/X2p7Um1tvXWe/edbbHOzLby9TW1lS7hT7BHHuOWfYMc+w5ZsmB6G8/L/1p
vf1prYn19nXW2zXCLAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIUj
zAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIVTW+0G2N2kC++udgu7uWnOm6vd
AgB0WW/aV+2pAD3LkVkAAAAKR5gFAACgcIRZAAAACkeYBQAAoHCEWQAAAApHmAUAAKBwhFkAAAAK
R5gFAACgcIRZAAAACkeYBQAAoHCEWQAAAApHmAUAAKBwhFkAAAAKR5gFAACgcIRZAAAACkeYBQAA
oHCEWQAAAApHmAUAAKBwaju7cefOnZk7d26efvrp7NixI+eee26OOuqonHPOOfmrv/qrJMm0adPy
9re/PUuXLs29996b2trazJ07NyeffHI2btyYOXPmpFQq5fjjj8+CBQsyYMCAA6oFAACAF+o0zK5c
uTINDQ254oor8vvf/z7vete7ct555+UDH/hAZs6c2VHX3NycDRs2ZMWKFdm8eXPOP//83HHHHVm0
aFFmzZqV0047LfPnz8+qVasyfPjw/a6dMGHCQR8AAAAAxdNpmH3b296WiRMnJkkqlUpqamryyCOP
5Be/+EVWrVqVY489NnPnzs0DDzyQsWPHplQqZfjw4Wlvb8/WrVvT3NycMWPGJEnGjx+ftWvXZsSI
EftdK8wCAACwJ52G2cGDBydJWltbc8EFF2TWrFnZsWNHzjzzzLzmNa/Jtddemy996Uupr69PQ0PD
bl/X0tKSSqWSUqm023Wtra37XbsvRxwxKLW1NQe+avbbsGH11W6hy4rce29jlj3DHHuOWQIAnYbZ
JNm8eXPOO++8TJ8+PZMmTcqzzz6bIUOGJEkmTJiQhQsX5vTTT09bW1vH17S1taW+vn6317y2tbVl
yJAhqaur2+/afdm27bn9WyVdtmXLvp9U6I2GDasvbO+9jVn2DHPsOXuapXALAP1Pp++w9Mwzz2Tm
zJm56KKLMnny5CTJ2WefnYcffjhJct999+XVr351Ro8enTVr1qRcLmfTpk0pl8sZOnRoRo0alfXr
1ydJVq9encbGxgOqBQAAgD3p9Mjsddddl2effTbLli3LsmXLkiRz5szJ5ZdfnkMOOSRHHnlkFi5c
mLq6ujQ2NmbKlCkpl8uZP39+kmT27NmZN29empqaMnLkyEycODE1NTX7XQsAAAB7UqpUKpVqN9FV
ffGUvZmLv1/tFnZz05w3V7uFLnFKZ88xy55hjj3Hacb9Q0/9vvSmffXF2FP702NNf1prYr19nfV2
Xrs3PsgVAACAwhFmAQAAKBxhFgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEWAACAwhFmAaAf+93v
fpc3vOEN+fnPf56NGzdm2rRpmT59ehYsWJByuZwkWbp0aSZPnpypU6fm4YcfTpIeqQWA7hBmAaCf
2rlzZ+bPn5/DDjssSbJo0aLMmjUry5cvT6VSyapVq9Lc3JwNGzZkxYoVaWpqyiWXXNIjtQDQXcIs
APRTS5YsydSpU/Oyl70sSdLc3JwxY8YkScaPH59169blgQceyNixY1MqlTJ8+PC0t7dn69at3a4F
gO6qrXYDAMCL784778zQoUMzbty4XH/99UmSSqWSUqmUJBk8eHBaWlrS2tqahoaGjq/70/Xdrd2X
I44YlNramh5bb28wbFh9n7qf3qA/rTWx3r7Oeg+cMAsA/dAdd9yRUqmU++67L4899lhmz56drVu3
dtze1taWIUOGpK6uLm1tbbtdX19fnwEDBnSrdl+2bXuuu0vsdbZs2XeI765hw+pflPvpDfrTWhPr
7eust/PavXGaMQD0Q7fccku+9rWv5eabb86JJ56YJUuWZPz48Vm/fn2SZPXq1WlsbMzo0aOzZs2a
lMvlbNq0KeVyOUOHDs2oUaO6VQsA3eXILACQJJk9e3bmzZuXpqamjBw5MhMnTkxNTU0aGxszZcqU
lMvlzJ8/v0dqAaC7SpVKpVLtJrqqLx6Kn7n4+9VuYTc3zXlztVvokv52qsbBZJY9wxx7zp5m2d9e
Z9Qf9NTvS2/aV1+MPbU/Pdb0p7Um1tvXWW/ntXvjNGMAAAAKR5gFAACgcIRZAAAACkeYBQAAoHCE
WQAAAApHmAUAAKBwhFkAAAAKR5gFAACgcIRZAAAACkeYBQAAoHCEWQAAAApHmAUAAKBwhFkAAAAK
R5gFAACgcIRZAAAACkeYBQAAoHCEWQAAAApHmAUAAKBwhFkAAAAKR5gFAACgcIRZAAAACkeYBQAA
oHCEWQAAAApHmAUAAKBwhFkAAAAKR5gFAACgcIRZAAAACkeYBQAAoHCEWQAAAApHmAUAAKBwhFkA
AAAKR5gFAACgcIRZAAAACkeYBQAAoHCEWQAAAAqntrMbd+7cmblz5+bpp5/Ojh07cu655+a4447L
nDlzUiqVcvzxx2fBggUZMGBAli5dmnvvvTe1tbWZO3duTj755GzcuLHbtQAAAPBCnabFlStXpqGh
IcuXL8+NN96YhQsXZtGiRZk1a1aWL1+eSqWSVatWpbm5ORs2bMiKFSvS1NSUSy65JEm6XQsAAAB7
0umR2be97W2ZOHFikqRSqaSmpibNzc0ZM2ZMkmT8+PFZu3ZtRowYkbFjx6ZUKmX48OFpb2/P1q1b
u107YcKEg7l2AAAACqrTMDt48OAkSWtray644ILMmjUrS5YsSalU6ri9paUlra2taWho2O3rWlpa
UqlUulW7L0ccMSi1tTUHuGQOxLBh9dVuocuK3HtvY5Y9wxx7jlkCAJ2G2STZvHlzzjvvvEyfPj2T
Jk3KFVdc0XFbW1tbhgwZkrq6urS1te12fX19/W6vee1K7b5s2/bcvldIt2zZsu8nFXqjYcPqC9t7
b2OWPcMce86eZincAkD/0+lrZp955pnMnDkzF110USZPnpwkGTVqVNavX58kWb16dRobGzN69Ois
WbMm5XI5mzZtSrlcztChQ7tdCwAAAHvS6ZHZ6667Ls8++2yWLVuWZcuWJUk+85nP5NJLL01TU1NG
jhyZiRMnpqamJo2NjZkyZUrK5XLmz5+fJJk9e3bmzZvX5VoAAADYk1KlUqlUu4mu6oun7M1c/P1q
t7Cbm+a8udotdIlTOnuOWfYMc+w5TjPuH3rq96U37asvxp7anx5r+tNaE+vt66y389q98UGuAAAA
FI4wCwAAQOEIswAAABSOMAsAAEDhCLMAAAAUjjALAABA4QizAAAAFI4wCwAAQOEIswAAABSOMAsA
AEDhCLMAAAAUjjALAABA4QizAAAAFI4wCwAAQOEIswAAABSOMAsAAEDhCLMAAAAUTm21GwAAXnzt
7e25+OKL84tf/CKlUimXXHJJDj300MyZMyelUinHH398FixYkAEDBmTp0qW59957U1tbm7lz5+bk
k0/Oxo0bu10LAN1hJwGAfugHP/hBkuTWW2/NrFmz8s///M9ZtGhRZs2aleXLl6dSqWTVqlVpbm7O
hg0bsmLFijQ1NeWSSy5Jkm7XAkB3OTILAP3QW97ylrzxjW9MkmzatClDhgzJunXrMmbMmCTJ+PHj
s3bt2owYMSJjx45NqVTK8OHD097enq1bt6a5ublbtRMmTKjKugHoO4RZAOinamtrM3v27Hz3u9/N
1VdfnbVr16ZUKiVJBg8enJaWlrS2tqahoaHja/50faVS6VbtvhxxxKDU1tb05HKrbtiw+j51P71B
f1prYr19nfUeOGEWAPqxJUuW5JOf/GTOOuusbN++veP6tra2DBkyJHV1dWlra9vt+vr6+t1e89qV
2n3Ztu257i6t19myZd8hvruGDat/Ue6nN+hPa02st6+z3s5r98ZrZgGgH7rrrrvy5S9/OUly+OGH
p1Qq5TWveU3Wr1+fJFm9enUaGxszevTorFmzJuVyOZs2bUq5XM7QoUMzatSobtUCQHc5MgsA/dBb
3/rWfPrTn8573/ve7Nq1K3Pnzs0rX/nKzJs3L01NTRk5cmQmTpyYmpqaNDY2ZsqUKSmXy5k/f36S
ZPbs2d2qBYDuKlUqlUq1m+iqvngofubi71e7hd3cNOfN1W6hS/rbqRoHk1n2DHPsOXuaZX97nVF/
0FO/L71pX30x9tT+9FjTn9aaWG9fZ72d1+6N04wBAAAoHGEWAACAwhFmAQAAKBxhFgAAgMIRZgEA
ACgcYRYAAIDCEWYBAAAoHGEWAACAwhFmAQAAKBxhFgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEW
AACAwhFmAQAAKBxhFgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEWAACAwhFmAQAAKBxhFgAAgMIR
ZgEAACgcYRYAAIDCEWYBAAAonP0Ksw899FBmzJiRJHn00Uczbty4zJgxIzNmzMg999yTJFm6dGkm
T56cqVOn5uGHH06SbNy4MdOmTcv06dOzYMGClMvlA64FAACAF6rdV8ENN9yQlStX5vDDD0+SNDc3
5wMf+EBmzpzZUdPc3JwNGzZkxYoV2bx5c84///zccccdWbRoUWbNmpXTTjst8+fPz6pVqzJ8+PD9
rp0wYcLBWzkAAACFtc8js8ccc0yuueaajsuPPPJI7r333rz3ve/N3Llz09ramgceeCBjx45NqVTK
8OHD097enq1bt6a5uTljxoxJkowfPz7r1q07oFoAAADYk30emZ04cWKeeuqpjssnn3xyzjzzzLzm
Na/Jtddemy996Uupr69PQ0NDR83gwYPT0tKSSqWSUqm023Wtra37XbsvRxwxKLW1Nfu/Wg7YsGH1
1W6hy4rce29jlj3DHHuOWQIA+wyzLzRhwoQMGTKk498LFy7M6aefnra2to6atra21NfXZ8CAAbtd
N2TIkNTV1e137b5s2/bcgbbPAdqyZd9PKvRGw4bVF7b33sYse4Y59pw9zVK4BYD+54Dfzfjss8/u
eNOm++67L69+9aszevTorFmzJuVyOZs2bUq5XM7QoUMzatSorF+/PkmyevXqNDY2HlAtAAAA7MkB
H5n97Gc/m4ULF+aQQw7JkUcemYULF6auri6NjY2ZMmVKyuVy5s+fnySZPXt25s2bl6ampowcOTIT
J05MTU3NftcCAADAnpQqlUql2k10VV88ZW/m4u9Xu4Xd3DTnzdVuoUuc0tlzzLJnmGPPcZpx/9BT
vy+9aV99MfbU/vRY05/WmlhvX2e9ndfuzQGfZgwAAADVJswCAABQOMIsAAAAhSPMAgAAUDjCLAAA
AIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAIAAFA4wiwAAACFI8wC
AABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjC
LAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDi11W4AAHjx7dy5M3Pnzs3TTz+d
HTt25Nxzz81xxx2XOXPmpFQq5fjjj8+CBQsyYMCALF26NPfee29qa2szd+7cnHzyydm4cWO3awGg
O+wkANAPrVy5Mg0NDVm+fHluvPHGLFy4MIsWLcqsWbOyfPnyVCqVrFq1Ks3NzdmwYUNWrFiRpqam
XHLJJUnS7VoA6C5HZgGgH3rb296WiRMnJkkqlUpqamrS3NycMWPGJEnGjx+ftWvXZsSIERk7dmxK
pVKGDx+e9vb2bN26tdu1EyZMqM7CAegzhFkA6IcGDx6cJGltbc0FF1yQWbNmZcmSJSmVSh23t7S0
pLW1NQ0NDbt9XUtLSyqVSrdq9+WIIwaltramx9bbGwwbVt+n7qc36E9rTay3r7PeAyfMAkA/tXnz
5px33nmZPn16Jk2alCuuuKLjtra2tgwZMiR1dXVpa2vb7fr6+vrdXvPaldp92bbtue4ur9fZsmXf
Ib67hg2rf1HupzfoT2tNrLevs97Oa/fGa2YBoB965plnMnPmzFx00UWZPHlykmTUqFFZv359kmT1
6tVpbGzM6NGjs2bNmpTL5WzatCnlcjlDhw7tdi0AdJcjswDQD1133XV59tlns2zZsixbtixJ8pnP
fCaXXnppmpqaMnLkyEycODE1NTVpbGzMlClTUi6XM3/+/CTJ7NmzM2/evC7XAkB3lSqVSqXaTXRV
XzwUP3Px96vdwm5umvPmarfQJf3tVI2DySx7hjn2nD3Nsr+9zqg/6Knfl960r74Ye2p/eqzpT2tN
rLevs97Oa/fGacYAAAAUjjALAABA4QizAAAAFI4wCwAAQOEIswAAABSOMAsAAEDhCLMAAAAUjjAL
AABA4QizAAAAFI4wCwAAQOEIswAAABSOMAsAAEDh1Fa7gd5g5uLvV7sFAAAADoAjswAAABTOfoXZ
hx56KDNmzEiSbNy4MdOmTcv06dOzYMGClMvlJMnSpUszefLkTJ06NQ8//HCP1QIAAMAL7TPM3nDD
Dbn44ouzffv2JMmiRYsya9asLF++PJVKJatWrUpzc3M2bNiQFStWpKmpKZdcckmP1AIAAMCe7PM1
s8ccc0yuueaafOpTn0qSNDc3Z8yYMUmS8ePHZ+3atRkxYkTGjh2bUqmU4cOHp729PVu3bu127YQJ
Ezrt7YgjBqW2tqZbA6Bzw4bVV7uFLity772NWfYMc+w5ZgkA7DPMTpw4MU899VTH5UqlklKplCQZ
PHhwWlpa0tramoaGho6aP13f3dp92bbtuf1cJl21Zcu+/x96o2HD6gvbe29jlj3DHHvOnmYp3AJA
/3PAbwA1YMD/fUlbW1uGDBmSurq6tLW17XZ9fX19t2sBAABgTw44zI4aNSrr169PkqxevTqNjY0Z
PXp01qxZk3K5nE2bNqVcLmfo0KHdrgUAAIA9OeDPmZ09e3bmzZuXpqamjBw5MhMnTkxNTU0aGxsz
ZcqUlMvlzJ8/v0dqAQAAYE9KlUqlUu0muqqnXn82c/H3e+T79EU3zXlztVvoEq9P7Dlm2TPMsed4
zWz/0Bf3+BdjT+1PjzX9aa2J9fZ11tt57d4c8GnGAAAAUG3CLAAAAIUjzAIAAFA4wiwAAACFI8wC
AABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjC
LAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAIAAFA4wiwAAACF
I8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAA
UDjCLAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAJAP/bQQw9l
xowZSZKNGzdm2rRpmT59ehYsWJByuZwkWbp0aSZPnpypU6fm4Ycf7rFaAOiO2mo3QO82c/H3q91C
h5vmvLnaLQD0KTfccENWrlyZww8/PEmyaNGizJo1K6eddlrmz5+fVatWZfjw4dmwYUNWrFiRzZs3
5/zzz88dd9zR7doJEyZUefUAFJ0jswDQTx1zzDG55pprOi43NzdnzJgxSZLx48dn3bp1eeCBBzJ2
7NiUSqUMHz487e3t2bp1a7drAaC7HJkFgH5q4sSJeeqppzouVyqVlEqlJMngwYPT0tKS1tbWNDQ0
dNT86fru1u7LEUcMSm1tTY+ss7cYNqy+T91Pb9Cf1ppYb19nvQdOmAUAkiQDBvzfCVttbW0ZMmRI
6urq0tbWttv19fX13a7dl23bnuvucnqdLVv2HeK7a9iw+hflfnqD/rTWxHr7OuvtvHZvnGYMACRJ
Ro0alfXr1ydJVq9encbGxowePTpr1qxJuVzOpk2bUi6XM3To0G7XAkB3OTILACRJZs+enXnz5qWp
qSkjR47MxIkTU1NTk8bGxkyZMiXlcjnz58/vkVoA6K5SpVKpVLuJruqpQ/G96R172bsDeTfj/naq
xsFklj3DHHvOnmbZ315n1B/0xT3+xXhX/v70WNOf1ppYb19nvZ3X7o3TjAEAACgcYRYAAIDCEWYB
AAAonC6/AdQZZ5yRurq6JMnLX/7yTJkyJZdddllqamoyduzYfPSjH025XM5nP/vZPPHEExk4cGAu
vfTSHHvssXnwwQf3uxYAAABeqEthdvv27alUKrn55ps7rnvnO9+Za665Jq94xSvyoQ99KI8++mie
euqp7NixI7fddlsefPDBLF68ONdee20WLFiw37UAAADwQl0Ks48//nief/75zJw5M7t27cr555+f
HTt25Jgqe2sdAAAMaElEQVRjjkmSjB07NuvWrcuWLVsybty4JMkpp5ySRx55JK2trftdCwAAAHvS
pTB72GGH5eyzz86ZZ56ZX/7yl/mnf/qnDBkypOP2wYMH58knn0xra2vHqchJUlNT82fXdVa7a9eu
1NbuvcUjjhiU2tqariyBAjrQj97wUR09xyx7hjn2HLMEALoUZkeMGJFjjz02pVIpI0aMSH19fX7/
+9933N7W1pYhQ4bkj3/8Y9ra2jquL5fLqaur2+26zmo7C7JJsm3bc11pn4I6kM/e6m+f1XUwmWXP
MMee43NmAYCki+9m/O///u9ZvHhxkuQ3v/lNnn/++QwaNCi/+tWvUqlUsmbNmjQ2Nmb06NFZvXp1
kuTBBx/MCSeckLq6uhxyyCH7VQsAAAB70qUjs5MnT86nP/3pTJs2LaVSKZdffnkGDBiQT37yk2lv
b8/YsWPzN3/zNznppJOydu3aTJ06NZVKJZdffnmS5JJLLtnvWgAAAHihLoXZgQMH5gtf+MKfXX/7
7bfvdnnAgAH53Oc+92d1p5xyyn7XAgAAwAt16TRjAAAAqCZhFgAAgMIRZgEAACgcYRYAAIDCEWYB
AAAoHGEWAACAwhFmAQAAKBxhFgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEWAACAwhFmAQAAKBxh
FgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEWAACAwhFmAQAAKBxhFgAAgMIRZgEAACgcYRYAAIDC
EWYBAAAoHGEWAACAwhFmAQAAKBxhFgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEWAACAwhFmAQAA
KBxhFgAAgMIRZgEAACgcYRYAAIDCEWYBAAAoHGEWAACAwqmtdgOwv2Yu/n61W+hw05w3V7sFAADo
1xyZBQAAoHCEWQAAAApHmAUAAKBwhFkAAAAKR5gFAACgcLybMXSBd1YGAIDqcmQWAACAwhFmAQAA
KBynGUPB9aZTnhOnPQMA8OJwZBYAAIDCcWQW6FG96Uixo8QAAH2XMAv0Wb0pWH/zC++sdgsAAH2K
MAvwIph04d3VbmE3jloDAEXXq8JsuVzOZz/72TzxxBMZOHBgLr300hx77LHVbgsA6CZ7fO86W4Ri
8MQjdK5Xhdnvfe972bFjR2677bY8+OCDWbx4ca699tpqtwUAdJM9Hg6cJ0D2TtAn6WVh9oEHHsi4
ceOSJKecckoeeeSRKncE0Df1pj+Q/EHSP9jjgZ5kHyPpZWG2tbU1dXV1HZdramqya9eu1Nbuuc1h
w+p75H69MQtAsfTU4z8vHns8QOf6297WE+vtVZ8zW1dXl7a2to7L5XJ5r5scAFAc9ngAelqvCrOj
R4/O6tWrkyQPPvhgTjjhhCp3BAD0BHs8AD2tVKlUKtVu4k/+9E6HP/vZz1KpVHL55Zfnla98ZbXb
AgC6yR4PQE/rVWEWAAAA9kevOs0YAAAA9ocwCwAAQOEIs1VQLpczf/78TJkyJTNmzMjGjRt3u/32
22/Pu9/97px11ln5wQ9+UKUui2Ffs/zqV7+aM888M2eeeWaWLl1apS57v33N8U81H/zgB/P1r3+9
Ch0Wx75m+cMf/jBnnXVWzjzzzHz2s5+NV3rs2b7meNNNN+Xd73533vOe9+S73/1ulbqkt9qfx7S+
ZOfOnbnooosyffr0TJ48OatWrap2Sy+K3/3ud3nDG96Qn//859Vu5aD78pe/nClTpuTd7353VqxY
Ue12DqqdO3fmwgsvzNSpUzN9+vQ+/f/70EMPZcaMGUmSjRs3Ztq0aZk+fXoWLFiQcrlc5e563v+/
3sceeyzTp0/PjBkzcvbZZ+eZZ57p0vcUZqvge9/7Xnbs2JHbbrstF154YRYvXtxx25YtW3LzzTfn
1ltvzVe+8pU0NTVlx44dVey2d+tslk8++WRWrlyZW2+9NbfffnvWrFmTxx9/vIrd9l6dzfFPrrrq
qjz77LNV6K5YOptla2trrrjiilx33XVZsWJFjj766Gzbtq2K3fZenc3x2Wefzb/927/l1ltvzU03
3ZTLL7+8ip3SG+3PY1pfsnLlyjQ0NGT58uW58cYbs3Dhwmq3dNDt3Lkz8+fPz2GHHVbtVg669evX
56c//Wm+/vWv5+abb86vf/3rard0UP3whz/Mrl27cuutt+a8887LVVddVe2WDoobbrghF198cbZv
354kWbRoUWbNmpXly5enUqn0uSelXrjeyy67LPPmzcvNN9+cCRMm5IYbbujS9xVmq+CBBx7IuHHj
kiSnnHJKHnnkkY7bHn744Zx66qkZOHBg6uvrc8wxxwhgnehsln/5l3+ZG2+8MTU1NSmVStm1a1cO
PfTQarXaq3U2xyT5zne+k1Kp1FHD3nU2y5/+9Kc54YQTsmTJkkyfPj1HHnlkhg4dWq1We7XO5nj4
4Ydn+PDhef755/P888+nVCpVq016qX09pvU1b3vb2/Kxj30sSVKpVFJTU1Pljg6+JUuWZOrUqXnZ
y15W7VYOujVr1uSEE07Ieeedlw9/+MN54xvfWO2WDqoRI0akvb095XI5ra2tffbzqI855phcc801
HZebm5szZsyYJMn48eOzbt26arV2ULxwvU1NTTnxxBOTJO3t7V3+G71v/nT0cq2tramrq+u4XFNT
k127dqW2tjatra2pr6/vuG3w4MFpbW2tRpuF0NksDznkkAwdOjSVSiWf//znM2rUqIwYMaKK3fZe
nc3xZz/7Wb71rW/l6quvzpe+9KUqdlkMnc1y27ZtWb9+fe66664MGjQo733ve3PKKaf4udyDzuaY
JEcddVT+4R/+Ie3t7TnnnHOq1Sa91L5+fvqawYMHJ/nfdV9wwQWZNWtWlTs6uO68884MHTo048aN
y/XXX1/tdg66bdu2ZdOmTbnuuuvy1FNP5dxzz+14krkvGjRoUJ5++un8/d//fbZt25brrruu2i0d
FBMnTsxTTz3VcblSqXT8nw4ePDgtLS3Vau2geOF6//RE1E9+8pN87Wtfyy233NKl79s3H9V7ubq6
urS1tXVcLpfLHRvsC29ra2vbLdyyu85mmSTbt2/P3LlzM3jw4CxYsKAaLRZCZ3O866678pvf/Cbv
e9/78vTTT+eQQw7J0UcfnfHjx1er3V6ts1k2NDTkpJNOyrBhw5IkjY2Neeyxx4TZPehsjqtXr85v
f/vbjlOwzj777IwePTonn3xyVXql99nX3tAXbd68Oeedd16mT5+eSZMmVbudg+qOO+5IqVTKfffd
l8ceeyyzZ8/Otdde2/HY2tc0NDRk5MiRGThwYEaOHJlDDz00W7duzUtf+tJqt3ZQfPWrX83YsWNz
4YUXZvPmzXnf+96Xb37zm33+7LoBA/7vhNm2trYMGTKkit28OO65555ce+21uf7667t8pprTjKtg
9OjRWb16dZLkwQcfzAknnNBx28knn5wHHngg27dvT0tLS37+85/vdju762yWlUolH/nIR/LXf/3X
+dznPtcvTrvqqs7m+KlPfSorVqzIzTffnDPOOCPvf//7BdlOdDbLV7/61fnZz36WrVu3ZteuXXno
oYdy3HHHVavVXq2zOb7kJS/JYYcdloEDB+bQQw9NfX2913Ozm85+fvqiZ555JjNnzsxFF12UyZMn
V7udg+6WW27J1772tdx888058cQTs2TJkj4bZJPkta99bX70ox+lUqnkN7/5TZ5//vk0NDRUu62D
ZsiQIR0Hcl7ykpdk165daW9vr3JXB9+oUaOyfv36JP/7pG1jY2OVOzq47r777o7f41e84hVd/j59
+2nKXmrChAlZu3Ztpk6dmkqlkssvvzz/8i//kmOOOSann356ZsyYkenTp6dSqeTjH/94n38mqjs6
m2W5XM6GDRuyY8eO/OhHP0qSfOITn8ipp55a5a57n339TLL/9jXLCy+8MB/84AeT/O/r3Pr6H9ld
ta85rlu3LmeddVYGDBiQ0aNH5+/+7u+q3TK9yJ5+fvqy6667Ls8++2yWLVuWZcuWJfnfN1vpD2+O
1B+86U1vyv3335/JkyenUqlk/vz5ffoJ+ve///2ZO3dupk+fnp07d+bjH/94Bg0aVO22DrrZs2dn
3rx5aWpqysiRIzNx4sRqt3TQtLe357LLLstRRx2V888/P0nyt3/7t7ngggsO+HuVKj4XAgAAgIJx
mjEAAACFI8wCAABQOMIsAAAAhSPMAgAAUDjCLAAAAIUjzAIAAFA4wiwAAACFI8wCAABQOP8PENe7
KRMqaLIAAAAASUVORK5CYII=
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This model approximates the training distribution much better than the others. Key to this task is delineating between items with 0 ratings, and those with high ratings. Remember, we first down-scaled our ratings with the following transformation to shrink the domain:&lt;/p&gt;
$$\tilde{r}_{u, i} = \log{\bigg(\frac{1 + r_{u, i}}{\epsilon}\bigg)}$$&lt;p&gt;Furthermore, the model scores worse on the training distribution, yet similarly on the validation set. The bias term was clearly helpful.&lt;/p&gt;
&lt;p&gt;Let's try a similar model, but &lt;em&gt;concatenate&lt;/em&gt; the country and song embeddings, then stack fully connected layers, then compute the final linear combination as before.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_3"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#3"&gt;Network #3&lt;a class="anchor-link" href="#Network-#3"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as &lt;a href="#network_1"&gt;#1&lt;/a&gt;, but concatenate the latent vectors instead. Then, stack 3 fully-connected layers with ReLU activations, batch normalization after each, and dropout after the first. Finally, add a 1-unit dense layer on the end, and add bias embeddings to the result. (NB: I wanted to add the bias embeddings to the respective $\mathbb{R}^f$ embeddings at the outset, but couldn't figure out how to do this in Keras.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [185]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;concatenation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;dense_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;concatenation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batch_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batch_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batch_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'predicted_preference'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [190]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_3.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 3" class="img-responsive" src="https://willwolf.io/figures/neural_implicit_mf_network_3.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [191]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 5s - loss: 0.0702 - val_loss: 0.0830
Epoch 2/5
476465/476465 [==============================] - 5s - loss: 0.0472 - val_loss: 0.0601
Epoch 3/5
476465/476465 [==============================] - 5s - loss: 0.0477 - val_loss: 0.0667
Epoch 4/5
476465/476465 [==============================] - 5s - loss: 0.0483 - val_loss: 0.0508
Epoch 5/5
476465/476465 [==============================] - 5s - loss: 0.0491 - val_loss: 0.0513
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [192]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.11102242413459021
Validation: 0.1899202531334214
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;These results are roughly identical to the previous. Choosing between the models, we'd certainly go with the simpler of the two.&lt;/p&gt;
&lt;p&gt;Now, let's try some architectures that benefit from other data sources altogether: the song title, and the song artist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_4"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#4"&gt;Network #4&lt;a class="anchor-link" href="#Network-#4"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as &lt;a href="#network_2"&gt;#2&lt;/a&gt;, except feed in the song title text as well. This text is first tokenized, then padded to a maximum sequence length, then embedded into a fixed-length vector by an LSTM, then reduced to a single value by a dense layer with a ReLU activation. Finally, this scalar is concatenated to the scalar output that &lt;a href="#network_2"&gt;#2&lt;/a&gt; would produce, and the result is fed into a final dense layer with a linear activation - i.e. a linear combination of the two.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [194]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'title_sequence'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;padded_title_sequences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sequences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'title_sequence'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [205]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;title_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int32'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_title_sequence'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;title_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;title_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;title_lstm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;title_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_title_lstm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;title_lstm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_title_lstm&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;final_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [206]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;title_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;final_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_4.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 4" class="img-responsive" src="https://willwolf.io/figures/neural_implicit_mf_network_4.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [207]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padded_title_sequences&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;network_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 124s - loss: 65.4419 - val_loss: 0.0215
Epoch 2/5
476465/476465 [==============================] - 126s - loss: 0.0506 - val_loss: 0.0256
Epoch 3/5
476465/476465 [==============================] - 120s - loss: 0.0476 - val_loss: 0.0282
Epoch 4/5
476465/476465 [==============================] - 119s - loss: 0.0472 - val_loss: 0.0288
Epoch 5/5
476465/476465 [==============================] - 119s - loss: 0.0471 - val_loss: 0.0291
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [208]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.11471700578472085
Validation: 0.18885209182480478
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;We seem to get a pinch of lift from the song title, though we're still underfitting our training set. Perhaps we could make our model bigger, or run it for more epochs.&lt;/p&gt;
&lt;p&gt;Let's plot our ground-truth vs. prediction distributions side-by-side just to make sure we're still on the right track.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [210]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'predictions'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;'ratings'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA7MAAAFyCAYAAAAnNY1XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2YlXWdP/D3YUZUmCGkqJXKFiq3KF2jWbraH2BPRlty
lQrx0MVW2GZlEmUGkUAEiVbOlhKatm57qVSS5UN51RZmBBSYpa6j1tYWpbCFgTkzGiNz7t8fXsyK
4vAwjId75vX6a845nznn8/3AnO+8z32fM5WiKIoAAABAiQyodQMAAACwv4RZAAAASkeYBQAAoHSE
WQAAAEpHmAUAAKB0hFkAAABKR5iFEnjFK16R++67L//1X/+V2bNnd1t75513ZuHChUmyT/UAQO3Y
t+HACbNQIscdd1wuuuiibmt+/etf549//OM+1wMAtWPfhgNXKYqiqHUT0Jds2LAhn/nMZ/Kc5zwn
f/jDH3LEEUfk/PPPz+WXX54HH3wwf/jDH/Ka17wmH/rQh/K5z30ut956azo7OzN69Oice+65aWho
yM9+9rMsWbIklUolxx13XG644YZ85zvfyf33358lS5bk29/+dtrb27N06dL8/Oc/T11dXd7whjdk
+vTpmT59elpbW/PGN74xb3vb27rqW1tbs3jx4tx7772pVCoZP358PvKRj6S+vj7HHXdc3vve92bd
unX505/+lH/+53/Ou971rmzdujVz587N9u3bkyQnnnhi5syZU+MJA8Chb8OGDfn0pz+dQYMGpb29
PWPGjMndd9+d9vb2FEWRpUuXZsSIEU+5b8+bNy8NDQ355S9/mf/93//NqFGj0tzcnMGDB+dHP/pR
Pve5z2XAgAF56UtfmvXr12flypU5/PDD7dv0K47MQi+4++67M2vWrNx444059dRTc8455yRJ/vrX
v+Y73/lOzjnnnFx22WWpq6vLN7/5zdxwww159rOfnc997nPp6OjIhz70ocybNy/XXXddXvWqV+Wv
f/3rkx7joosuyo4dO3LTTTfluuuuy89//vP8/ve/z+zZs9PU1JRly5btVr906dIMHTo0N954Y669
9tr88pe/zBVXXJEk6ejoyFFHHZWvfe1rueiii3LhhRdmx44dueaaa/K85z0v3/rWt3L11Vdn06ZN
aW1t7f0BAkAf8N///d+58MIL86lPfSrbt2/P17/+9dx000055ZRTcvnll+foo49+yn07Se666678
27/9W2666ab86U9/yne/+91s3749H/vYx/LZz342119/fV71qld1Hdm1b9Pf1Ne6AeiLXvKSl6Sp
qSlJctppp+VTn/pUnv3sZ+eVr3xlV80tt9yS1tbWrF+/Pkny6KOP5pnPfGZ+9atfpb6+Pq9+9auT
JCeffHLXe2keb/369fn4xz+eurq61NXV5aqrrkqSfPOb39xjT2vWrMlXv/rVVCqVDBw4MNOmTct/
/Md/5L3vfW+S5PWvf32S5GUve1k6Ojry8MMPZ/z48Xnve9+bLVu25B//8R9z9tlnp7Gx8SBNCQD6
tqOPPjrPfe5z89znPjfPeMYz8rWvfS1/+MMfsmHDhgwePHiv3z9+/PgMHDgwSXLsscfmL3/5S372
s5/lhS98YV7ykpckSU455ZQsXbq0q96+TX/iyCz0grq6ut0uF0WRAQMGZNCgQV3XVavVzJ8/P9df
f32uv/76rFq1Kl/4whdSqVTyxLP/6+uf/LpTfX19KpVK1+UtW7Z0nVa0J9Vq9UmXd+7c2XX58MMP
T5Ku+yyKIscff3xWr16dqVOn5v7778+UKVPy85//fG/LBwCSrn3/lltuyRlnnJHksRePp0+fvk/f
f8QRR3R9vev3g7q6uif9njBgwGO/0tu36W+EWegF9957b+69994kyde//vWMGTMmQ4YM2a1m3Lhx
ufrqq9PR0ZFqtZoFCxakubk5xx57bIqiyI9+9KMkyerVq/OXv/zlSY/x6le/Ot/61rdSrVbT0dGR
2bNn59Zbb01dXd1uIfWJj1cURTo6OnLNNdfkH//xH7tdx+c+97msWLEib3jDG/KJT3wiL3rRi/K7
3/3uAKcCAP3TunXr8trXvjYzZszIcccdlx/84Afp7OxMkqfct5/KmDFj8rvf/a7r94zvfe97eeih
h1KpVOzb9DvCLPSCZz3rWfn85z+fSZMm5Qc/+EE+85nPPKnmAx/4QJ773OfmlFNOyZvf/OYURZF5
8+blsMMOyxe/+MV84QtfyFvf+tZ8//vfzzOf+cwnff8HP/jBHHbYYXnrW9+at73tbTnxxBPzxje+
Ma94xSvyP//zPznzzDN3qz/33HOzbdu2TJo0KZMmTcrIkSPzvve9r9t1vPOd78y9996bk08+Oaed
dlqe97zn5eSTT+7ZcACgn5k2bVpuvfXWTJo0KVOnTs3zn//83HfffalWq0+5bz+VoUOHprm5OXPn
zs0pp5yStWvXpr6+PkceeaR9m37HpxnDQbZhw4auTyIEADiY2trasmLFipx11lk58sgj09LSkjPO
OCM//vGPd3v7EfQHPgAKAABKoqGhIYcddlgmT56c+vr61NfX5/Of/7wgS7/kyCwAAACl4z2zAAAA
lI4wCwAAQOkIswAAAJROqT8AauvW1lq3sN+OOmpQtm9/uNZt1Jw5mMEu5mAGu/RkDsOHNx7kbqi1
g7XH97efr/603v601sR6+zrrfWrd7fGOzD7N6uvrat3CIcEczGAXczCDXcyB3tDf/l/1p/X2p7Um
1tvXWe+BEWYBAAAoHWEWAACA0hFmAQAAKB1hFgAAgNIRZgEAACgdYRYAAIDSEWYBAAAoHWEWAACA
0hFmAQAAKJ367m589NFHM3/+/Nx///3p6OjI+9///hx99NE544wz8rd/+7dJkunTp+fNb35zli9f
nltuuSX19fWZP39+jj/++GzatCnz5s1LpVLJi1/84ixatCgDBgzYr1oAAAB4om7D7A033JChQ4fm
s5/9bB588MG87W1vy5lnnpl3v/vdmTVrVlddS0tLNm7cmFWrVmXLli0566yzcu2112bZsmWZM2dO
XvWqV2XhwoVZvXp1RowYsc+1J510Uq8PAAAAgPLpNsy+6U1vysSJE5MkRVGkrq4ud911V377299m
9erVecELXpD58+fntttuy7hx41KpVDJixIh0dnZm27ZtaWlpydixY5MkEyZMyLp16zJy5Mh9rhVm
AQAA2JNuw+zgwYOTJG1tbZk9e3bmzJmTjo6OTJkyJS9/+ctzySWX5Itf/GIaGxszdOjQ3b6vtbU1
RVGkUqnsdl1bW9s+1+7NUUcNSn193f6vusaGD2+sdQuHBHMwg13MwQx2MQcAYF91G2aTZMuWLTnz
zDMzY8aMTJo0KQ899FCGDBmSJDnppJOyZMmSvP71r097e3vX97S3t6exsXG397y2t7dnyJAhaWho
2Ofavdm+/eF9W+UhZPjwxmzd+tRBfdb5Nz+N3ezdFfNe1yv3u7c59Adm8BhzMINdejIHIZinMuns
62vdQpfe2lMB+qtuP2HpgQceyKxZs3LOOedk8uTJSZLTTz89d955Z5LkJz/5SV72spdlzJgxWbt2
barVajZv3pxqtZphw4Zl9OjR2bBhQ5JkzZo1aWpq2q9aAAAA2JNuj8xeeumleeihh7JixYqsWLEi
STJv3rycd955Oeyww/KsZz0rS5YsSUNDQ5qamjJ16tRUq9UsXLgwSTJ37twsWLAgzc3NGTVqVCZO
nJi6urp9rgUAAIA9qRRFUdS6iQNVxtPynGb8GKdVmsEu5mAGuzjNmMc7WD8Th9K++nScZtyfnk/6
01oT6+3rrLf72qfiD7kCAABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCAABQ
OsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAAUDrCLAAA
AKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wC
AABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCQD/25z//OSeeeGJ+85vfZNOm
TZk+fXpmzJiRRYsWpVqtJkmWL1+eyZMnZ9q0abnzzjuT5KDUAkBPCLMA0E89+uijWbhwYY444ogk
ybJlyzJnzpysXLkyRVFk9erVaWlpycaNG7Nq1ao0Nzdn8eLFB6UWAHpKmAWAfuqCCy7ItGnT8uxn
PztJ0tLSkrFjxyZJJkyYkPXr1+e2227LuHHjUqlUMmLEiHR2dmbbtm09rgWAnqqvdQMAwNPvm9/8
ZoYNG5bx48fnsssuS5IURZFKpZIkGTx4cFpbW9PW1pahQ4d2fd+u63tauzdHHTUo9fV1B229h4Lh
wxv71OMcCvrTWhPr7eusd/8JswDQD1177bWpVCr5yU9+knvuuSdz587Ntm3bum5vb2/PkCFD0tDQ
kPb29t2ub2xszIABA3pUuzfbtz/c0yUecrZu3XuI76nhwxuflsc5FPSntSbW29dZb/e1T8VpxgDQ
D1199dW56qqrcuWVV+alL31pLrjggkyYMCEbNmxIkqxZsyZNTU0ZM2ZM1q5dm2q1ms2bN6darWbY
sGEZPXp0j2oBoKccmQUAkiRz587NggUL0tzcnFGjRmXixImpq6tLU1NTpk6dmmq1moULFx6UWgDo
KWEWAPq5K6+8suvrq6666km3n3XWWTnrrLN2u27kyJE9rgWAnnCaMQAAAKUjzAIAAFA6wiwAAACl
I8wCAABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAA
UDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApVPf3Y2PPvpo5s+fn/vvvz8dHR15//vf
nxe96EWZN29eKpVKXvziF2fRokUZMGBAli9fnltuuSX19fWZP39+jj/++GzatKnHtQAAAPBE3abF
G264IUOHDs3KlSvz5S9/OUuWLMmyZcsyZ86crFy5MkVRZPXq1WlpacnGjRuzatWqNDc3Z/HixUnS
41oAAADYk26PzL7pTW/KxIkTkyRFUaSuri4tLS0ZO3ZskmTChAlZt25dRo4cmXHjxqVSqWTEiBHp
7OzMtm3belx70kkn9ebaAQAAKKluw+zgwYOTJG1tbZk9e3bmzJmTCy64IJVKpev21tbWtLW1ZejQ
obt9X2tra4qi6FHt3hx11KDU19ft55Jrb/jwxlq3sM96s9cyzaG3mMFjzMEMdjEHAGBfdRtmk2TL
li0588wzM2PGjEyaNCmf/exnu25rb2/PkCFD0tDQkPb29t2ub2xs3O09rwdSuzfbtz+89xUeYoYP
b8zWrXsP6oeK3uq1bHPoDWbwGHMwg116MgchGAD6n27fM/vAAw9k1qxZOeecczJ58uQkyejRo7Nh
w4YkyZo1a9LU1JQxY8Zk7dq1qVar2bx5c6rVaoYNG9bjWgAAANiTbo/MXnrppXnooYeyYsWKrFix
IknyiU98IkuXLk1zc3NGjRqViRMnpq6uLk1NTZk6dWqq1WoWLlyYJJk7d24WLFhwwLUAAACwJ5Wi
KIpaN3Ggynha3t5Oo5t1/s1PYzd7d8W81/XK/Tqt0gx2MQcz2MVpxjzewfqZOJT21d7aUx+vPz2f
9Ke1Jtbb11lv97VPxR9yBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAA
oHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkA
AABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeY
BQAAoHSEWQAAAEpHmAUAAKB06mvdwKFg1vk317oFAAAA9oMjswAAAJSOMAsAAEDpCLMAAACUjjAL
AABA6QizAAAAlI4wCwAAQOkIswAAAJSOvzMLAP1QZ2dnzj333Pz2t79NpVLJ4sWLc/jhh2fevHmp
VCp58YtfnEWLFmXAgAFZvnx5brnlltTX12f+/Pk5/vjjs2nTph7XAkBP2EkAoB/64Q9/mCT52te+
ljlz5uRf//Vfs2zZssyZMycrV65MURRZvXp1WlpasnHjxqxatSrNzc1ZvHhxkvS4FgB6ypFZAOiH
3vCGN+Q1r3lNkmTz5s0ZMmRI1q9fn7FjxyZJJkyYkHXr1mXkyJEZN25cKpVKRowYkc7Ozmzbti0t
LS09qj3ppJNqsm4A+g5hFgD6qfr6+sydOzff//73c9FFF2XdunWpVCpJksGDB6e1tTVtbW0ZOnRo
1/fsur4oih7V7s1RRw1KfX3dwVxuzQ0f3tinHudQ0J/WmlhvX2e9+0+YBYB+7IILLshHP/rRvP3t
b8+OHTu6rm9vb8+QIUPS0NCQ9vb23a5vbGzc7T2vB1K7N9u3P9zTpR1ytm7de4jvqeHDG5+WxzkU
9Ke1Jtbb11lv97VPxXtmAaAfuu666/KlL30pSXLkkUemUqnk5S9/eTZs2JAkWbNmTZqamjJmzJis
Xbs21Wo1mzdvTrVazbBhwzJ69Oge1QJATzkyCwD90Bvf+MZ8/OMfzzve8Y7s3Lkz8+fPzwtf+MIs
WLAgzc3NGTVqVCZOnJi6uro0NTVl6tSpqVarWbhwYZJk7ty5PaoFgJ6qFEVR1LqJA3WwDsXPOv/m
g3I/fdEV817XK/fb306l2BMzeIw5mMEuPZlDf3ufUX/QF/f43tpTH68/PZ/0p7Um1tvXWW/3tU/F
acYAAACUjjALAABA6QizAAAAlI4wCwAAQOkIswAAAJSOMAsAAEDpCLMAAACUjjALAABA6QizAAAA
lI4wCwAAQOkIswAAAJTOPoXZO+64IzNnzkyS3H333Rk/fnxmzpyZmTNn5qabbkqSLF++PJMnT860
adNy5513Jkk2bdqU6dOnZ8aMGVm0aFGq1ep+1wIAAMAT1e+t4PLLL88NN9yQI488MknS0tKSd7/7
3Zk1a1ZXTUtLSzZu3JhVq1Zly5YtOeuss3Lttddm2bJlmTNnTl71qldl4cKFWb16dUaMGLHPtSed
dFLvrRwAAIDS2uuR2WOOOSYXX3xx1+W77rort9xyS97xjndk/vz5aWtry2233ZZx48alUqlkxIgR
6ezszLZt29LS0pKxY8cmSSZMmJD169fvVy0AAADsyV6PzE6cODH33Xdf1+Xjjz8+U6ZMyctf/vJc
cskl+eIXv5jGxsYMHTq0q2bw4MFpbW1NURSpVCq7XdfW1rbPtXtz1FGDUl9ft++rZb8NH95Yyvsu
CzN4jDmYwS7mAADsq72G2Sc66aSTMmTIkK6vlyxZkte//vVpb2/vqmlvb09jY2MGDBiw23VDhgxJ
Q0PDPtfuzfbtD+9v++ynrVv3/qLCgRg+vLHX7rsszOAx5mAGu/RkDkIwAPQ/+/1pxqeffnrXhzb9
5Cc/ycte9rKMGTMma9euTbVazebNm1OtVjNs2LCMHj06GzZsSJKsWbMmTU1N+1ULAAAAe7LfR2Y/
+clPZsmSJTnssMPyrGc9K0uWLElDQ0OampoyderUVKvVLFy4MEkyd+7cLFiwIM3NzRk1alQmTpyY
urq6fa4FAACAPakURVHUuokDdbBOy5t1/s0H5X76oivmva5X7tdplWawizmYwS5OM+bx+uIe31t7
6uP1p+eT/rTWxHr7Ouvtvvap7PdpxgAAAFBrwiwAAAClI8wCAABQOsIsAAAApSPMAgAAUDrCLAAA
AKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wC
AABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAAUDrC
LAAAAKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAACl
I8wCAABQOsIsAAAApSPMAgAAUDrCLAAAAKUjzAIAAFA6wiwAAAClI8wCAABQOsIsAAAApSPMAgAA
UDrCLAAAAKVTX+sGAICn36OPPpr58+fn/vvvT0dHR97//vfnRS96UebNm5dKpZIXv/jFWbRoUQYM
GJDly5fnlltuSX19febPn5/jjz8+mzZt6nEtAPSEnQQA+qEbbrghQ4cOzcqVK/PlL385S5YsybJl
yzJnzpysXLkyRVFk9erVaWlpycaNG7Nq1ao0Nzdn8eLFSdLjWgDoKUdmAaAfetOb3pSJEycmSYqi
SF1dXVpaWjJ27NgkyYQJE7Ju3bqMHDky48aNS6VSyYgRI9LZ2Zlt27b1uPakk06qzcIB6DOEWQDo
hwYPHpwkaWtry+zZszNnzpxccMEFqVQqXbe3tramra0tQ4cO3e37WltbUxRFj2r35qijBqW+vu6g
rfdQMHx4Y596nENBf1prYr19nfXuP2EWAPqpLVu25Mwzz8yMGTMyadKkfPazn+26rb29PUOGDElD
Q0Pa29t3u76xsXG397weSO3ebN/+cE+Xd8jZunXvIb6nhg9vfFoe51DQn9aaWG9fZ73d1z4V75kF
gH7ogQceyKxZs3LOOedk8uTJSZLRo0dnw4YNSZI1a9akqakpY8aMydq1a1OtVrN58+ZUq9UMGzas
x7UA0FOOzAJAP3TppZfmoYceyooVK7JixYokySc+8YksXbo0zc3NGTVqVCZOnJi6uro0NTVl6tSp
qVarWbhwYZJk7ty5WbBgwQHXAkBPVYqiKGrdxIE6WIfiZ51/80G5n77oinmv65X77W+nUuyJGTzG
HMxgl57Mob+9z6g/6It7fG/tqY/Xn55P+tNaE+vt66y3+9qn4jRjAAAASkeYBQAAoHSEWQAAAEpH
mAUAAKB09inM3nHHHZk5c2aSZNOmTZk+fXpmzJiRRYsWpVqtJkmWL1+eyZMnZ9q0abnzzjsPWi0A
AAA80V7D7OWXX55zzz03O3bsSJIsW7Ysc+bMycqVK1MURVavXp2WlpZs3Lgxq1atSnNzcxYvXnxQ
agEAAGBP9hpmjznmmFx88cVdl1taWjJ27NgkyYQJE7J+/frcdtttGTduXCqVSkaMGJHOzs5s27at
x7UAAACwJ/V7K5g4cWLuu+++rstFUaRSqSRJBg8enNbW1rS1tWXo0KFdNbuu72nt3hx11KDU19ft
41I5EL35txv9XUgz2MUczGAXcwAA9tVew+wTDRjwfwdz29vbM2TIkDQ0NKS9vX236xsbG3tcuzfb
tz+8v+2zn3rrjzf3tz8MvSdm8BhzMINdejIHIRgA+p/9/jTj0aNHZ8OGDUmSNWvWpKmpKWPGjMna
tWtTrVazefPmVKvVDBs2rMe1AAAAsCf7fWR27ty5WbBgQZqbmzNq1KhMnDgxdXV1aWpqytSpU1Ot
VrNw4cKDUgsAAAB7UimKoqh1EwfqYJ2WN+v8mw/K/fRFV8x7Xa/cr9MqzWAXczCDXZxmzOP1xT2+
t/bUx+tPzyf9aa2J9fZ11tt97VPZ79OMAQAAoNaEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZ
AAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpH
mAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACg
dIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAA
AEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gF
AACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBYB+7I47
7sjMmTOTJJs2bcr06dMzY8aMLFq0KNVqNUmyfPnyTJ48OdOmTcudd9550GoBoCeEWQDopy6//PKc
e+652bFjR5Jk2bJlmTNnTlauXJmiKLJ69eq0tLRk48aNWbVqVZqbm7N48eKDUgsAPSXMAkA/dcwx
x+Tiiy/uutzS0pKxY8cmSSZMmJD169fntttuy7hx41KpVDJixIh0dnZm27ZtPa4FgJ6qr3UDAEBt
TJw4Mffdd1/X5aIoUqlUkiSDBw9Oa2tr2traMnTo0K6aXdf3tHZvjjpqUOrr6w7KOg8Vw4c39qnH
ORT0p7Um1tvXWe/+E2YBgCTJgAH/d8JWe3t7hgwZkoaGhrS3t+92fWNjY49r92b79od7upxDztat
ew/xPTV8eOPT8jiHgv601sR6+zrr7b72qTjNGABIkowePTobNmxIkqxZsyZNTU0ZM2ZM1q5dm2q1
ms2bN6darWbYsGE9rgWAnnJkFgBIksydOzcLFixIc3NzRo0alYkTJ6auri5NTU2ZOnVqqtVqFi5c
eFBqAaCnKkVRFAfyjaecckoaGhqSJM973vMyderUfPrTn05dXV3GjRuXD37wg6lWq/nkJz+ZX/7y
lxk4cGCWLl2aF7zgBbn99tv3ubY7B+tQ/Kzzbz4o99MXXTHvdb1yv/3tVIo9MYPHmIMZ7NKTOfS3
9xn1B31xj++tPfXx+tPzSX9aa2K9fZ31dl/7VA7oyOyOHTtSFEWuvPLKruve+ta35uKLL87zn//8
vPe9783dd9+d++67Lx0dHfn617+e22+/Peeff34uueSSLFq0aJ9rAQAA4IkOKMzee++9eeSRRzJr
1qzs3LkzZ511Vjo6OnLMMcckScaNG5f169dn69atGT9+fJLkhBNOyF133ZW2trZ9rgUAAIA9OaAw
e8QRR+T000/PlClT8rvf/S7/8i//stsnEw4ePDh/+MMf0tbW1nUqcpLU1dU96bruanfu3Jn6+qdu
sS9+bP+EwOmpAAAMxElEQVShpjdP3XNaoBnsYg5msIs5AAD76oDC7MiRI/OCF7wglUolI0eOTGNj
Yx588MGu23d97P5f//rX3T6iv1qt7vFj+5+qtrsgm/TNj+0/1PTWufv97X0Be2IGjzEHM9jFe2YB
gP1xQH+a5xvf+EbOP//8JMkf//jHPPLIIxk0aFB+//vfpyiKrF27tusj+tesWZMkuf3223Psscem
oaEhhx122D7VAgAAwJ4c0JHZyZMn5+Mf/3imT5+eSqWS8847LwMGDMhHP/rRdHZ2Zty4cfn7v//7
HHfccVm3bl2mTZuWoihy3nnnJUkWL168z7UAAADwRAcUZgcOHJgLL7zwSddfc801u10eMGBAPvWp
Tz2p7oQTTtjnWgAAAHiiAzrNGAAAAGpJmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSE
WQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABK
R5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAA
oHSEWQAAAEpHmAUAAKB0hFkAAABKR5gFAACgdIRZAAAASkeYBQAAoHSEWQAAAEqnvtYNcGibdf7N
tW6hyxXzXlfrFgAAgEOEI7MAAACUjjALAABA6QizAAAAlI4wCwAAQOkIswAAAJSOMAsAAEDpCLMA
AACUjjALAABA6QizAAAAlI4wCwAAQOkIswAAAJSOMAsAAEDp1Ne6AdhXs86/udYtdLli3utq3QIA
APRrjswCAABQOsIsAAAApSPMAgAAUDreMwsHwPt3AQCgthyZBQAAoHSEWQAAAErHacZQcofSKc+J
054BAHh6ODILAABA6TgyCxxUh9qRYvbMEXQAoOwOqTBbrVbzyU9+Mr/85S8zcODALF26NC94wQtq
3RYA0EP2eC/2sf+88AjdO6TC7A9+8IN0dHTk61//em6//facf/75ueSSS2rdFgDQQ/Z42H9eAHlq
gj7JIRZmb7vttowfPz5JcsIJJ+Suu+6qcUcAfdOh9AuSX0j6B3s8cDDZx0gOsTDb1taWhoaGrst1
dXXZuXNn6uv33Obw4Y0H5XFvvPCtB+V+AOiZg/W8zqHHHg/Qvf62Bx6M9R5Sn2bc0NCQ9vb2rsvV
avUpNzkAoDzs8QAcbIdUmB0zZkzWrFmTJLn99ttz7LHH1rgjAOBgsMcDcLBViqIoat3ELrs+6fBX
v/pViqLIeeedlxe+8IW1bgsA6CF7PAAH2yEVZgEAAGBfHFKnGQMAAMC+EGYBAAAoHWG2l1Sr1Sxc
uDBTp07NzJkzs2nTpt1uv+aaa3Lqqafm7W9/e374wx/WqMvetbcZfOUrX8mUKVMyZcqULF++vEZd
9r69zWFXzXve85589atfrUGHvW9vM/jRj36Ut7/97ZkyZUo++clPpq+++2Fvc7jiiity6qmn5rTT
Tsv3v//9GnX59Ljjjjsyc+bMJ11/880357TTTsvUqVNzzTXX1KAz+oJ9ed7tSx599NGcc845mTFj
RiZPnpzVq1fXuqWnxZ///OeceOKJ+c1vflPrVnrdl770pUydOjWnnnpqVq1aVet2etWjjz6as88+
O9OmTcuMGTP69L/v4/fCTZs2Zfr06ZkxY0YWLVqUarVa4+4Ovsev95577smMGTMyc+bMnH766Xng
gQcO7E4LesX3vve9Yu7cuUVRFMUvfvGL4n3ve1/XbX/605+Kk08+udixY0fx0EMPdX3d13Q3g9//
/vfFKaecUuzcubOoVqvF1KlTi3vuuadWrfaq7uawy4UXXlhMmTKlWLly5dPd3tOiuxm0trYWb3nL
W4o///nPRVEUxWWXXdb1dV/T3Rz+8pe/FCeeeGKxY8eO4sEHHyxe85rX1KrNXnfZZZcVJ598cjFl
ypTdru/o6Cje8IY3FA8++GCxY8eO4tRTTy22bt1aoy4ps3153u1LvvGNbxRLly4tiqIotm/fXpx4
4om1behp0NHRUXzgAx8o3vjGNxa//vWva91Or/rpT39anHHGGUVnZ2fR1tZWXHTRRbVuqVd9//vf
L2bPnl0URVGsXbu2+OAHP1jjjnrHE/fCM844o/jpT39aFEVRLFiwoPjP//zPWrZ30D1xve94xzuK
u+++uyiKovjqV79anHfeeQd0v47M9pLbbrst48ePT5KccMIJueuuu7puu/POO/OKV7wiAwcOTGNj
Y4455pjce++9tWq113Q3g7/5m7/Jl7/85dTV1aVSqWTnzp05/PDDa9Vqr+puDkny3e9+N5VKpaum
L+puBr/4xS9y7LHH5oILLsiMGTPyrGc9K8OGDatVq72quzkceeSRGTFiRB555JE88sgjqVQqtWqz
1x1zzDG5+OKLn3T9b37zmxxzzDF5xjOekYEDB+aVr3xlbr311hp0SNnt7Xm3r3nTm96UD33oQ0mS
oihSV1dX44563wUXXJBp06bl2c9+dq1b6XVr167NsccemzPPPDPve9/78prXvKbWLfWqkSNHprOz
M9VqNW1tbX3271E/cS9saWnJ2LFjkyQTJkzI+vXra9Var3jiepubm/PSl740SdLZ2XnAOaBv/u84
BLS1taWhoaHrcl1dXXbu3Jn6+vq0tbWlsbGx67bBgwenra2tFm32qu5mcNhhh2XYsGEpiiKf+cxn
Mnr06IwcObKG3fae7ubwq1/9Kt/+9rdz0UUX5Ytf/GINu+xd3c1g+/bt2bBhQ6677roMGjQo73jH
O3LCCSf0yf8P3c0hSY4++ui85S1vSWdnZ84444xatdnrJk6cmPvuu+9J1/eX50Z6395+1vqawYMH
J3ls3bNnz86cOXNq3FHv+uY3v5lhw4Zl/Pjxueyyy2rdTq/bvn17Nm/enEsvvTT33Xdf3v/+93e9
EN4XDRo0KPfff3/+6Z/+Kdu3b8+ll15a65Z6xRP3wqIouv5NBw8enNbW1lq11iueuN5dL0T9/Oc/
z1VXXZWrr776gO63bz6rHwIaGhrS3t7edblarXZtok+8rb29fbdf4PqK7maQJDt27Mj8+fMzePDg
LFq0qBYtPi26m8N1112XP/7xj3nnO9+Z+++/P4cddlie+9znZsKECbVqt1d0N4OhQ4fmuOOOy/Dh
w5MkTU1Nueeee/pkmO1uDmvWrMmf/vSnrve6nX766RkzZkyOP/74mvRaC/3luZHet7f9py/asmVL
zjzzzMyYMSOTJk2qdTu96tprr02lUslPfvKT3HPPPZk7d24uueSSrn2krxk6dGhGjRqVgQMHZtSo
UTn88MOzbdu2PPOZz6x1a73iK1/5SsaNG5ezzz47W7ZsyTvf+c7ceOONffYMvl0GDPi/E2bb29sz
ZMiQGnbz9LjppptyySWX5LLLLjvgs/KcZtxLxowZkzVr1iRJbr/99hx77LFdtx1//PG57bbbsmPH
jrS2tuY3v/nNbrf3Fd3NoCiKfOADH8jf/d3f5VOf+lSfPiWquzl87GMfy6pVq3LllVfmlFNOybve
9a4+F2ST7mfwspe9LL/61a+ybdu27Ny5M3fccUde9KIX1arVXtXdHJ7xjGfkiCOOyMCBA3P44Yen
sbExDz30UK1arYkXvvCF2bRpUx588MF0dHTkZz/7WV7xilfUui1KqLuftb7ogQceyKxZs3LOOedk
8uTJtW6n11199dW56qqrcuWVV+alL31pLrjggj4bZJPkla98ZX784x+nKIr88Y9/zCOPPJKhQ4fW
uq1eM2TIkK4XMp/xjGdk586d6ezsrHFXvW/06NHZsGFDksde4G5qaqpxR73r+uuv7/o5fv7zn3/A
99O3X6asoZNOOinr1q3LtGnTUhRFzjvvvPz7v/97jjnmmLz+9a/PzJkzM2PGjBRFkQ9/+MN98tWm
7mZQrVazcePGdHR05Mc//nGS5CMf+Uif/MV1b/8X+oO9zeDss8/Oe97zniSPvferr/7iubc5rF+/
Pm9/+9szYMCAjBkzJv/v//2/Wrf8tLjxxhvz8MMPZ+rUqZk3b15OP/30FEWR0047Lc95znNq3R4l
tKeftb7s0ksvzUMPPZQVK1ZkxYoVSZLLL788RxxxRI0742B47Wtfm1tvvTWTJ09OURRZuHBhnz4I
8K53vSvz58/PjBkz8uijj+bDH/5wBg0aVOu2et3cuXOzYMGCNDc3Z9SoUZk4cWKtW+o1nZ2d+fSn
P52jjz46Z511VpLkH/7hHzJ79uz9vq9KUfTRv4EBAABAn+U0YwAAAEpHmAUAAKB0hFkAAABKR5gF
AACgdIRZAAAASkeYBQAAoHSEWQAAAEpHmAUAAKB0/j+WeLUMWlEqWAAAAABJRU5ErkJggg==
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Finally, let's add the artist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_5"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#5"&gt;Network #5&lt;a class="anchor-link" href="#Network-#5"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as &lt;a href="#network_4"&gt;#4&lt;/a&gt;, except feed in the song artist index as well. This index is first embedded into a vector, then reduced to a scalar by a dense layer with a ReLU activation. Finally, this scalar is concatenated with the two scalars produced in the second-to-last layer of &lt;a href="#network_4"&gt;#4&lt;/a&gt;, then fed into a final dense layer with a linear activation. Like the previous, this is a linear combination of the three inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [217]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;artist_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'artist'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;artist_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_artists&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;artist_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;artist_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_title_lstm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;final_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [218]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;title_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;artist_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;final_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_5.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 5" class="img-responsive" src="https://willwolf.io/figures/neural_implicit_mf_network_5.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [219]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
    &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
    &lt;span class="n"&gt;padded_title_sequences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 122s - loss: 28.0315 - val_loss: 0.0224
Epoch 2/5
476465/476465 [==============================] - 129s - loss: 0.0484 - val_loss: 0.0269
Epoch 3/5
476465/476465 [==============================] - 121s - loss: 0.0473 - val_loss: 0.0310
Epoch 4/5
476465/476465 [==============================] - 134s - loss: 0.0489 - val_loss: 0.0333
Epoch 5/5
476465/476465 [==============================] - 131s - loss: 0.0485 - val_loss: 0.0300
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[219]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;lt;keras.callbacks.History at 0x12ee912b0&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [220]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.11092262775206822
Validation: 0.19037930919362725
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This seems a bit worse than the last, though similar all the same. I'm a bit surprised we didn't get more lift from artists. Nevertheless, we're still underfitting. Bigger model, more epochs, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="What-went-wrong?"&gt;What went wrong?&lt;a class="anchor-link" href="#What-went-wrong?"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Our best neural network in terms of performance and complexity is network &lt;a href="#network_2"&gt;#2&lt;/a&gt;. Let's refit this model and inspect a few predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [409]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [410]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [411]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 3s - loss: 65.2042 - val_loss: 0.0205
Epoch 2/5
476465/476465 [==============================] - 8s - loss: 0.0559 - val_loss: 0.0212
Epoch 3/5
476465/476465 [==============================] - 3s - loss: 0.0519 - val_loss: 0.0225
Epoch 4/5
476465/476465 [==============================] - 2s - loss: 0.0496 - val_loss: 0.0235
Epoch 5/5
476465/476465 [==============================] - 2s - loss: 0.0482 - val_loss: 0.0248
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [412]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.10905118689288228
Validation: 0.19050085125816704
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [441]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;predictions_unstack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_level_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;)][&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_level_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;)][&lt;/span&gt;&lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_level_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_id_to_name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Colombia"&gt;Colombia&lt;a class="anchor-link" href="#Colombia"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [443]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'Colombia'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[443]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.715299&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.704788&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.683443&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.681572&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.680419&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.674962&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.674799&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.672363&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.671484&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.669374&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="United-States"&gt;United States&lt;a class="anchor-link" href="#United-States"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [444]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'United States'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[444]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.752865&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.742354&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.721009&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.719138&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.717985&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.712528&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.712365&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.709929&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.709051&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.706940&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Turkey"&gt;Turkey&lt;a class="anchor-link" href="#Turkey"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [445]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'Turkey'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[445]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.728861&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.718350&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.697005&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.695134&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.693981&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.688524&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.688361&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.685925&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.685046&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.682936&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Taiwan"&gt;Taiwan&lt;a class="anchor-link" href="#Taiwan"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [446]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[446]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="table dataframe table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.789601&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.779090&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.757745&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.755874&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.754721&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.749264&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.749101&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.746664&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.745786&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.743676&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;As is abundantly clear, the model is simply converging to a popularity baseline. I tried extracting the vectors outright, normalizing as before then computing dot products, but the results made little sense.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Future-work"&gt;Future work&lt;a class="anchor-link" href="#Future-work"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;At this stage, Implicit Matrix Factorization is the clear winner. The algorithm is popular for several good reasons - it's simple, relatively scalable, intuitive (especially given the significance of the interplay between $C$ and $P$, which I did not expressedly discuss) and effective. Herein, I did not produce a neural network approximating this algorithm with superior results.&lt;/p&gt;
&lt;p&gt;My intuition tells me I'm missing something simple; why shouldn't a non-linear function approximator be able to approximate $f(u, i)$ better than a linear matrix decomposition? At this stage, I'm not sure. My best guess is that we should seek to optimize a different loss function. Here, we've mirrored that of IMF; why not optimize a &lt;em&gt;ranking&lt;/em&gt; loss function instead, since that's what's ultimately important? The IMF objective makes the actual optimization process easy for ratings matrices with billions of items; in our case, with far fewer items, we're simply using stochastic gradient descent. To this effect, I did experiment with some composite loss functions; on one side, &lt;code&gt;implicit_mf_loss&lt;/code&gt;, and on the other, a &lt;code&gt;cutoff_loss&lt;/code&gt;, which seeks to enforce that songs which received a rating $\tilde{r}_{u, i}$ above a certain cutoff should be indeed predicted at above that cutoff. It's a crude, binary absolute error. This was unsuccessful.&lt;/p&gt;
&lt;p&gt;Finally, if there is in fact some value in using neural networks for this type of problem, I'm extremely intrigued by the ease and intuitiveness with which we can incorporate multiple inputs into our function. Encoding our song title with an LSTM, and training that LSTM end-to-end with our main minimization objective, is one example of this. Furthermore, Keras makes the construction of these networks straightforward.&lt;/p&gt;
&lt;p&gt;Thanks for reading this work. Your feedback is welcome, and I do hope this serves as a point of inspiration, trial and error should this question interest you.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Ordered Categorical GLMs for Product Feedback Scores</title><link href="https://willwolf.io/2017/03/17/ordered-categorical-glms-for-product-feedback-scores/" rel="alternate"></link><published>2017-03-17T16:55:00-04:00</published><updated>2017-03-17T16:55:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2017-03-17:/2017/03/17/ordered-categorical-glms-for-product-feedback-scores/</id><summary type="html">&lt;p&gt;A follow-up to Erik Bernhardsson's post &lt;a href="https://erikbern.com/2015/12/05/more-mcmc-analyzing-a-small-dataset-with-1-5-ratings/"&gt;"More MCMC – Analyzing a small dataset with 1-5 ratings"&lt;/a&gt; using ordered categorical generalized linear models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;TL;DR: there's a &lt;a href="https://willwolf.shinyapps.io/ordered-categorical-a-b-test/"&gt;Shiny app&lt;/a&gt; too.&lt;/p&gt;
&lt;p&gt;I write this post as a follow-up to Erik Bernhardsson's post &lt;a href="https://erikbern.com/2015/12/05/more-mcmc-analyzing-a-small-dataset-with-1-5-ratings/"&gt;"More MCMC – Analyzing a small dataset with 1-5 ratings."&lt;/a&gt; Therein, Erik builds a simple multinomial regression to model explicit, 1-5 feedback scores for different variants of Better's &lt;a href="https://better.com/"&gt;website&lt;/a&gt;. I like his approach for the rigor and mathematical fidelity it brings to what is a straightforward, ubiquitous use case for any product team.&lt;/p&gt;
&lt;p&gt;I recently learned about ordered categorical generalized linear models (GLMs) and thought back to the post above. Of course, while feedback scores do fall into discrete categories, there is an implicit ordinality therein: choosing integers in &lt;span class="math"&gt;\([1, 5]\)&lt;/span&gt; is different from choosing colors in &lt;span class="math"&gt;\(\{\text{red}, \text{green}, \text{blue}\}\)&lt;/span&gt;. This is because 5 is greater than 4, while green is not "greater" than "blue." (For a royalistic debate on the supremacy of the color green, please make use of the comments section below.)&lt;/p&gt;
&lt;p&gt;In a multinomial regression, we can formulate our problem thus:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*} y &amp;amp;\sim \text{Multinomial}(1, p)\\
p_j &amp;amp;= \frac{e^{\phi_j}}{\sum\limits_{k = 1}^{K} e^{\phi_k}}\\
\phi_j &amp;amp;= \alpha_j + \beta_j X_i\\
\alpha_j &amp;amp;\sim \text{Normal}(0, 10)\\
\beta_j &amp;amp;\sim \text{Normal}(0, 10)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In a model with &lt;span class="math"&gt;\(k\)&lt;/span&gt; categorical outcomes, we typically have &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; linear equations for &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt;. The link function — which you'll recognize as the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; — "squashes" these values such that they sum to 1 (with one of the values of &lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt; fixed at an arbitrary constant). The Normal priors placed on &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; were chosen arbitrarily: we can use any continuous-valued distribution we like.&lt;/p&gt;
&lt;p&gt;In situations where we don't have predictor variables, we can alternatively draw the entire vector &lt;span class="math"&gt;\(p\)&lt;/span&gt; from a &lt;a href="https://en.wikipedia.org/wiki/Dirichlet_distribution"&gt;Dirichlet&lt;/a&gt; distribution outright. As it happens, this offers us a trivially simple analytical solution to the posterior. This fact owes itself to &lt;a href="http://stats.stackexchange.com/questions/44494/why-is-the-dirichlet-distribution-the-prior-for-the-multinomial-distribution"&gt;Dirichlet-Multinomial conjugacy&lt;/a&gt;: given total observed counts &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; of each category &lt;span class="math"&gt;\(k\)&lt;/span&gt; and respective parameters &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; on our prior, our posterior distribution for our belief in &lt;span class="math"&gt;\(p\)&lt;/span&gt; is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$p \sim \text{Dirichlet}(\alpha_1 + x_1, ..., \alpha_k + x_k)$$&lt;/div&gt;
&lt;p&gt;This makes both inference and posterior predictive sampling trivial: a few lines of code for each. Unfortunately, while delightfully simple, the multinomial regression makes a strong concession with respect to the data at hand: the ordinality of our feedback scores is not explicitly preserved. To this effect, let us explore the ordered categorical GLM.&lt;/p&gt;
&lt;p&gt;The ordered categorical GLM can be specified thus:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;y \sim \text{Ordered}(p)\\
&amp;amp;\log{\bigg(\frac{p_k}{1 - p_k}\bigg)} = \alpha_k - \phi_i\\
&amp;amp;\phi_i = \beta X_i\\
&amp;amp;\alpha_k \sim \text{Normal}(0, 10)\\
&amp;amp;\beta \sim \text{Normal}(0, 10)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;There's a few components to clarify:&lt;/p&gt;
&lt;h3&gt;Ordered distribution&lt;/h3&gt;
&lt;p&gt;An Ordered distribution is a vanilla categorical distribution that accepts a vector of &lt;em&gt;cumulative probabilities&lt;/em&gt; &lt;span class="math"&gt;\(p_k = \text{Pr}(y_i \leq k)\)&lt;/span&gt;, as opposed to traditional probabilities &lt;span class="math"&gt;\(p_k = \text{Pr}(y_i = k)\)&lt;/span&gt;. This preserves the ordering among variables.&lt;/p&gt;
&lt;h3&gt;Link function&lt;/h3&gt;
&lt;p&gt;In a typical logistic regression, we model the log-odds of observing a positive outcome as a linear function of the intercept plus weighted input variables. (The inverse of this function which we thereafter employ to obtain the raw probability &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the &lt;em&gt;logistic&lt;/em&gt; function, or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid&lt;/a&gt;&lt;/em&gt; function.) In the ordered categorical GLM, we instead model the &lt;em&gt;log-cumulative-odds&lt;/em&gt; of observing a particular outcome as a linear function of the intercept &lt;em&gt;minus&lt;/em&gt; weighted input variables. We'll dive into the "minus" momentarily.&lt;/p&gt;
&lt;h3&gt;Cumulative probability&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_k\)&lt;/span&gt; in the above equation is defined as &lt;span class="math"&gt;\(p_k = \text{Pr}(y_i \leq k)\)&lt;/span&gt;. For this reason, the left-hand-side of the second line of our model gives the log-cumulative-odds, not the log-odds.&lt;/p&gt;
&lt;h3&gt;Priors&lt;/h3&gt;
&lt;p&gt;Placing Normal priors on &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; was an arbitrary choice. In fact, any prior that produces a continuous value should suffice: the constraint that &lt;span class="math"&gt;\(p_k\)&lt;/span&gt; must be a valid (cumulative) probability, i.e. &lt;span class="math"&gt;\(p_k\)&lt;/span&gt; must lie on the interval &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;, is enforced by the inverse link function.&lt;/p&gt;
&lt;h3&gt;Subtracting &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Ultimately, &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; is the linear model. Should we want to add additional predictors, we would append them here. So, why do we subtract &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; from &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; instead of add? Intuitively, it would make sense for an increase in the value of a predictor variable, given a positive coefficient, to shift probability mass towards &lt;em&gt;larger&lt;/em&gt; ordinal values. This makes for a more fluid interpretation of our model parameters. Subtracting &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; does just this: &lt;em&gt;increasing&lt;/em&gt; the value of a given predictor &lt;em&gt;decreases&lt;/em&gt; the log-cumulative-odds of every outcome value &lt;span class="math"&gt;\(k\)&lt;/span&gt; below the maximum (&lt;em&gt;every&lt;/em&gt; outcome value below the maximum, because we have one linear model &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; which we must subtract, separately, from each intercept &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; in order to compute &lt;span class="math"&gt;\(\log{\big(\frac{p_k}{1 - p_k}\big)}\)&lt;/span&gt;) which shifts probability mass upwards. This way, the desired dynamic — "a bigger predictor should lead to a bigger outcome given a positive coefficient" — holds.&lt;/p&gt;
&lt;p&gt;Let's move to R to fit and compare these models. I'm enjoying R more and more for the ease of plotting with &lt;a href="http://docs.ggplot2.org/current/"&gt;ggplot2&lt;/a&gt;, as well as the functional "chains" offered by &lt;a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html"&gt;magrittr&lt;/a&gt; ("to be pronounced with a sophisticated french accent," apparently) and Hadley Wickham's &lt;a href="https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html"&gt;dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, let's simulate some scores then plot the results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;50&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;feedback&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;rmultinom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="n"&gt;max.col&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="explicit feedback scores" class="img-responsive" src="https://willwolf.io/figures/empirical_distribution_explicit_feedback_scores.png"/&gt;&lt;/p&gt;
&lt;p&gt;Next, let's fit an ordered categorical GLM in the Stan modeling language. Note that we don't have any predictor variables &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;; therefore, the only variables we will be estimating are our intercepts &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;data&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kn"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;ordered&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kn"&gt;model&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;ordered_logistic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kn"&gt;generated quantities&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;real&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;ordered_logistic_lpmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our model estimates four values: &lt;span class="math"&gt;\(\alpha_1, \alpha_2, \alpha_3\)&lt;/span&gt; and &lt;span class="math"&gt;\(\alpha_4\)&lt;/span&gt;. Why four and not five? The cumulative probability of the final outcome, &lt;span class="math"&gt;\(\text{Pr}(y_i \leq 5)\)&lt;/span&gt; is always 1.&lt;/p&gt;
&lt;p&gt;The posterior samples from our model will be vectors of cumulative probabilities, i.e. cumulative distribution functions. Let's examine the variation in these estimates, and plot them against the proportion of each class observed in our original data to see how well our model did. The following plot is constructed with 2,000 samples from our posterior distribution, where each sample is given as &lt;span class="math"&gt;\(\{\alpha_1, \alpha_2, \alpha_3, \alpha_4\}\)&lt;/span&gt;. The dotted red line gives the column-wise mean, and the error band gives the column-wise 92% interval.&lt;/p&gt;
&lt;p&gt;&lt;img alt="posterior cumulative distribution" class="img-responsive" src="https://willwolf.io/figures/mean_posterior_cumulative_distribution.png"/&gt;&lt;/p&gt;
&lt;p&gt;Key points are as follows:&lt;/p&gt;
&lt;h3&gt;The scale of our estimates&lt;/h3&gt;
&lt;p&gt;The marginal distributions of each estimated parameter are on the &lt;em&gt;log-cumulative-odds&lt;/em&gt; scale. This is because &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; — the only set of parameters we are estimating — is set equal to &lt;span class="math"&gt;\(\log{\bigg(\frac{\text{Pr}(y_i \leq k)}{1 - \text{Pr}(y_i \leq k)}\bigg)}\)&lt;/span&gt; in our model above. So, what does a value of, say, &lt;span class="math"&gt;\(\alpha_3 = -1.5\)&lt;/span&gt; say about the probability of receiving a feedback score of 3 from a given user? I have no idea. As such, we necessarily convert these estimates from the &lt;em&gt;log-cumulative-odds&lt;/em&gt; scale to the &lt;em&gt;cumulative probability&lt;/em&gt; scale to make interpretation easier. The sigmoid function gives this conversion.&lt;/p&gt;
&lt;h3&gt;The width of the band&lt;/h3&gt;
&lt;p&gt;The width of the band quantifies the uncertainty in our estimate of the true cumulative distribution function. We used 50 samples: it should be reasonably wide. With 500,000 samples, the red band would be indistinguishable from the dotted line.&lt;/p&gt;
&lt;p&gt;Finally, let's simulate new observations from each model. Our posterior contains a distribution of &lt;em&gt;cumulative distribution functions&lt;/em&gt;: how do we translate this into multinomial samples?&lt;/p&gt;
&lt;p&gt;First, samples from this distribution can be transformed into probability mass functions with the follow code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;simulated_probabilities&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;cbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cutpoint_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nf"&gt;cbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoint_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we can use each sample from &lt;code&gt;simulated_probabilities&lt;/code&gt; to roll a multinomial die. Here, I'll adopt the strategy Erik uses in the original post: with each sample, we'll simulate a large number — I've chosen 500 — of multinomial throws. This will give a histogram of empirical counts — similar to the first plot shown at the top of this post. With this distribution, we'll compute a weighted average. After repeating this for a large number of samples from our posterior, we'll have a distribution of weighted averages. (Incidentally, as Erik highlights in his post, the shape of this distribution can be assumed Normal as given by the &lt;a href="https://en.wikipedia.org/wiki/Central_limit_theorem"&gt;Central Limit Theorem&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The following plot compares the results for both the ordered categorical and multinomial models. Remember, we obtain the posterior of the latter through the Dirichlet-Multinomial conjugacy described above. Sampling from this posterior follows trivially. While vanilla histograms should do the trick, let's plot the inferred densities just to be safe.&lt;/p&gt;
&lt;p&gt;&lt;img alt="comparative posterior density plots" class="img-responsive" src="https://willwolf.io/figures/comparative_posterior_predictive_density_plots.png"/&gt;&lt;/p&gt;
&lt;p&gt;To be frank, this has me a little disappointed! According to the posterior predictive densities of the weighted-average throws, there is no discernible difference between the multinomial and ordered categorical models. To be thorough, let's plot 100 draws from the &lt;em&gt;raw&lt;/em&gt;, respective posteriors: a distribution over cumulative distribution functions for each model.&lt;/p&gt;
&lt;p&gt;&lt;img alt="comparative posterior cumulative distributions" class="img-responsive" src="https://willwolf.io/figures/comparative_posterior_cumulative_distributions.png"/&gt;&lt;/p&gt;
&lt;p&gt;Yep, no difference. So, why do we think this is? What are our takeaways?&lt;/p&gt;
&lt;h3&gt;We didn't use any predictor variables&lt;/h3&gt;
&lt;p&gt;In the ordered categorical case, we estimate &lt;span class="math"&gt;\(k - 1\)&lt;/span&gt; values of &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; and a &lt;em&gt;single set&lt;/em&gt; of predictor coefficients. In the multinomial case, we estimate &lt;span class="math"&gt;\(k - 1\)&lt;/span&gt; sets of &lt;span class="math"&gt;\(\{\alpha_k, \beta_{X, k}\}\)&lt;/span&gt; values (for example, should we have &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; classes and two predictor variables &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, we'd estimate parameters &lt;span class="math"&gt;\(\alpha_1, \beta_{a, 1}, \beta_{b, 1}, \alpha_{2}, \beta_{a, 2}, \beta_{b, 2}\)&lt;/span&gt; in the simplest case). Given that we didn't use any predictor variables, we're simply estimating a set of intercepts &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; in each case. In the former, these values give the log-cumulative-odds of each outcome, while in the latter they give the log-odds outright. Given that the transformation between the two is deterministic, the ordered categorical and multinomial models should be functionally identical.&lt;/p&gt;
&lt;h3&gt;We might want predictor variables&lt;/h3&gt;
&lt;p&gt;It is easy to conceive of a situation in which we'd want predictor variables. In this case, the ordered categorical model becomes a clear choice for ordered categorical data. Revisiting its formulation above, we see that predictors are trivial to add to the model: we just tack them onto the equation for &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The inconvenient realities of measurement&lt;/h3&gt;
&lt;p&gt;There's a quote I like from Richard McElreath (I just finished his textbook, &lt;a href="http://xcelab.net/rm/statistical-rethinking/"&gt;Statistical Rethinking&lt;/a&gt;, which I couldn't recommend much more highly):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Both types of models help us transform our modeling to cope with the inconvenient realities of measurement, rather than transforming measurements to cope with the constraints of our models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this quote, he's describing the ordered categorical GLM (in addition to "zero-inflated" models — models that "mix a binary event with an ordinary GLM likelihood like a Poisson or binomial" — hence the plurality). And therein lies the rub: a model is but an approximation of the world, and if one needs to bend to accommodate the other, the former should be preferred.&lt;/p&gt;
&lt;p&gt;To conclude, I built a &lt;a href="https://willwolf.shinyapps.io/ordered-categorical-a-b-test/"&gt;Shiny app&lt;/a&gt; to be used as an A/B test calculator for ordered categorical data using the methodologies detailed above. Example output looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="a/b comparison plot" class="img-responsive" src="https://willwolf.io/figures/a_b_comparison_plot.png"/&gt;&lt;/p&gt;
&lt;p&gt;Code for this work can be found &lt;a href="https://github.com/cavaunpeu/ordered-categorical-glm"&gt;here&lt;/a&gt;. Thanks for reading.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry></feed>