<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>will wolf</title><link href="https://willwolf.io/" rel="alternate"></link><link href="https://willwolf.io/feeds/all.atom.xml" rel="self"></link><id>https://willwolf.io/</id><updated>2022-08-01T17:00:00-04:00</updated><subtitle>writings on machine learning, crypto, geopolitics, life</subtitle><entry><title>README</title><link href="https://willwolf.io/2022/08/01/readme/" rel="alternate"></link><published>2022-08-01T17:00:00-04:00</published><updated>2022-08-01T17:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2022-08-01:/2022/08/01/readme/</id><summary type="html">&lt;p&gt;A document describing how to work with &lt;em&gt;me.&lt;/em&gt; What am I like? How do I work best? And more.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A README is a document describing how to use a piece of software. Done &lt;a href="https://github.com/pandas-dev/pandas/blob/main/README.md"&gt;well&lt;/a&gt;, it clearly and succinctly communicates how to work with the code in question.&lt;/p&gt;
&lt;p&gt;I recently came across the idea of writing a README not for my project, but for &lt;em&gt;myself&lt;/em&gt;; in other words, a document describing how to work with &lt;em&gt;me&lt;/em&gt;! What am I like? How do I work best? And more.&lt;/p&gt;
&lt;p&gt;In the following text, I endeavour to do exactly this, both as an exercise in self-exploration, and a step towards working more effectively with others in my remote-dominated professional world.&lt;/p&gt;
&lt;p&gt;Without further ado, let's begin.&lt;/p&gt;
&lt;h2&gt;How I type on Slack&lt;/h2&gt;
&lt;p&gt;I generally type in all lowercase letters and with punctuation. For example, I might send the following message:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;heya&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt; &lt;span class="n"&gt;goes&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;your&lt;/span&gt; &lt;span class="n"&gt;weekend&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="n"&gt;what&lt;/span&gt; &lt;span class="k"&gt;are&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;working&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt;

&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="s1"&gt;'m tweaking our model. i should be done this evening. i'&lt;/span&gt;&lt;span class="n"&gt;ll&lt;/span&gt; &lt;span class="n"&gt;let&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;know&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;finished&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;My (textual) tone may seem curt and cold. In reality, it's not. Punctuation helps me keep my thoughts organized and speak succinctly. Capitalization is far too much effort for far too little reward.&lt;/p&gt;
&lt;h2&gt;My communication style&lt;/h2&gt;
&lt;p&gt;I speak directly in (non-sensitive) professional conversations. I ask questions plainly, e.g. "Why did you make that choice? How many hours will that take? What other solutions did you consider?&lt;/p&gt;
&lt;p&gt;In science, explicit communication of hypotheses, methods, and outcomes is crucial. Clarifying questions are sexy. Sanity checks are king. My professional brain lives in this space; speaking directly keeps my feet planted below, and (hopefully!) moves me and my team forward.&lt;/p&gt;
&lt;h2&gt;When I have energy&lt;/h2&gt;
&lt;p&gt;I'm a morning person. My most productive technical hours are between 8 am and 12 pm. Ideally, I spend these hours writing code, undistracted. In the afternoons, I prefer talking to others. Ideally, I spend these hours in 1 on 1's, planning, retros, etc.&lt;/p&gt;
&lt;h2&gt;How to message me&lt;/h2&gt;
&lt;p&gt;Don't overthink it. I don't bite!&lt;/p&gt;
&lt;h2&gt;What I look like when I'm thinking&lt;/h2&gt;
&lt;p&gt;When I'm trying to understand a complex problem, I divert my gaze and stare at the wall for 5-15 seconds. (Conversely, staring directly at the camera clouds my brain.)&lt;/p&gt;
&lt;p&gt;When I'm engaged in a technical conversation, my brow is furrowed. I look intense, exacting, prickly.&lt;/p&gt;
&lt;p&gt;In both cases, please don't intuit rudeness! I'm focused on clarifying details for myself and my teammates.&lt;/p&gt;
&lt;h2&gt;How to teach me&lt;/h2&gt;
&lt;p&gt;At work, I prize learning from others—math, engineering, leadership, you name it—above most else. If you have something to teach me, please do. You aren't bothering me. You're not "preaching." Your wisdom is welcome. Keep it coming.&lt;/p&gt;
&lt;h2&gt;How to learn from me&lt;/h2&gt;
&lt;p&gt;If you value what I have to say, let me know. Ask for more. I love to share what I've been taught by others. To pass the torch, if you will! If you're open to learn, I'm genuinely excited to teach.&lt;/p&gt;
&lt;h2&gt;Anything I missed?&lt;/h2&gt;
&lt;p&gt;Let me know in the comments and I'll do my best to address.&lt;/p&gt;</content><category term="life"></category></entry><entry><title>I Work in Crypto</title><link href="https://willwolf.io/2022/06/11/work-in-crypto/" rel="alternate"></link><published>2022-06-11T08:30:00-04:00</published><updated>2022-06-11T08:30:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2022-06-11:/2022/06/11/work-in-crypto/</id><summary type="html">&lt;p&gt;I now work in crypto—building credit scoring models at &lt;a href="https://www.credprotocol.com/"&gt;Cred Protocol&lt;/a&gt; and simulations of complex systems at &lt;a href="https://block.science/"&gt;Block Science&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I &lt;a href="https://willwolf.io/2021/12/04/leaving-asapp/"&gt;left&lt;/a&gt; ASAPP in December 2021 to "explore": Rust, complex systems, crypto. After ~4 months of study, podcasts, conversations, toy &lt;a href="https://willwolf.io/2021/12/21/crypto-projects/"&gt;projects&lt;/a&gt;, Twitter, meetups, conferences, and general experimentation, I've now re-entered the workforce; I now work in crypto!&lt;/p&gt;
&lt;p&gt;Per our brave new remote world and the breadth of my technical interests, I recently started &lt;em&gt;two&lt;/em&gt; new jobs in this zany new space: building credit scoring models at &lt;a href="https://www.credprotocol.com/"&gt;Cred Protocol&lt;/a&gt; and simulations of complex systems at &lt;a href="https://block.science/"&gt;Block Science&lt;/a&gt;. The former is my primary gig.&lt;/p&gt;
&lt;h1&gt;Cred Protocol&lt;/h1&gt;
&lt;p&gt;Cred aims to enable under-collateralized, capital efficient borrowing of cryptocurrencies by quantifying wallet risk. In other words, much like FICO's "300-850," we're building a credit score. From a technical stance, credit scoring implies systems and algorithms to quantify risk—of tens of millions of crypto wallets—in near real-time. Per my longstanding interests in building systems and statistical products, my current energy to build from the ground up, and the chance to explore a world of new data, new players, new algorithms, new rules—I jumped at the opportunity to join Cred as one of its first few hires. We're currently about ~10 people.&lt;/p&gt;
&lt;h1&gt;Block Science&lt;/h1&gt;
&lt;p&gt;Ten years ago, I wrote an undergraduate &lt;a href="https://honors.libraries.psu.edu/catalog/1947"&gt;thesis&lt;/a&gt; about the simulation of iterated Prisoner's Dilemma games on the surface of a geodesic dome. This work has remained of abiding fascination ever since. Last year, I realized that crypto offers a perfect "arena" for the simulation of similar "complex systems": a blockchain gives a graph data structure indexed by time—with copious amounts of structured, &lt;em&gt;public&lt;/em&gt; data to boot! In other words, simulating the behavior of systems built on blockchains, e.g. a lending-and-borrowing protocol like &lt;a href="https://aave.com/"&gt;Aave&lt;/a&gt;, then aligning simulated results with empirical results, is a highly tractable, useful, and interesting line of work.&lt;/p&gt;
&lt;p&gt;Block Science is a leader in doing exactly this. As a visiting contributor, I build, analyze, and communicate simulations of "cryptoeconomic" systems for a variety of partners. Block Science is comprised of ~30 talented and diverse members—with backgrounds spanning economics, social science, natural science, behavioral science, multi-agent systems, and more.&lt;/p&gt;
&lt;h1&gt;Join us&lt;/h1&gt;
&lt;p&gt;Both companies are hiring. If you're interested in what we do, do reach out!&lt;/p&gt;
&lt;p&gt;Will&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Thumbnail image credit goes &lt;a href="https://jingculturecommerce.com/museum-cryptocurrency-donations/"&gt;here&lt;/a&gt;. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="life"></category></entry><entry><title>Neural Methods in Simulation-Based Inference</title><link href="https://willwolf.io/2022/01/04/neural-methods-in-sbi/" rel="alternate"></link><published>2022-01-04T10:00:00-05:00</published><updated>2022-01-04T10:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2022-01-04:/2022/01/04/neural-methods-in-sbi/</id><summary type="html">&lt;p&gt;A survey of how neural networks are currently being used in simulation-based inference routines.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Bayesian inference is the task of quantifying a posterior belief over parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; given observed data &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;—where &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; was generated from a model &lt;span class="math"&gt;\(p(\mathbf{x}\mid{\boldsymbol{\theta}})\)&lt;/span&gt;—via Bayes' Theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$
    p(\boldsymbol{\theta}\mid\mathbf{x}) = \frac{p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{x})}
$$&lt;/div&gt;
&lt;p&gt;In numerous applications of scientific interest, e.g. cosmological, climatic or urban-mobility phenomena, the likelihood of the data &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; under the data-generating function &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; is intractable to compute, precluding classical inference approaches. Notwithstanding, &lt;em&gt;simulating&lt;/em&gt; new data &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; from this function is often trivial—for example, by coding the generative process in a few lines of Python—&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# some deterministic logic&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# some stochastic logic&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# whatever you want!&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="n"&gt;simulated_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;—motivating the study of &lt;em&gt;simulation-based&lt;/em&gt; Bayesian &lt;em&gt;inference&lt;/em&gt; methods, termed SBI.&lt;/p&gt;
&lt;p&gt;Furthermore, the evidence &lt;span class="math"&gt;\(p(\mathbf{x}) = \int{p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}d\boldsymbol{\theta}\)&lt;/span&gt; is typically intractable to compute as well. This is because the integral has no closed-form solution; or, were the functional form of the likelihood (which we don't have) and the prior (which we do have) available, expanding these terms yields a summation over an "impractically large" number of terms, e.g. the number of possible cluster assignment configurations in a mixture of Gaussians &lt;a href="#10.1080/01621459.2017.1285773" id="ref-10.1080/01621459.2017.1285773-1"&gt;(Blei et al., 2017)&lt;/a&gt;. For this reason, in SBI, we typically estimate the &lt;em&gt;unnormalized&lt;/em&gt; posterior &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x}) = p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta}) \propto \frac{p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{x})}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recent work has explored the use of neural networks to perform key density estimation tasks, i.e. subroutines, of the SBI routine itself. We refer to this work as Neural SBI. In the following sections, we detail the various classes of these estimation tasks. For a more thorough analysis of their respective motivations, behaviors, and tradeoffs, we refer the reader to the original work.&lt;/p&gt;
&lt;h1&gt;Neural Posterior Estimation&lt;/h1&gt;
&lt;p&gt;In this class of models, we estimate &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt; with a conditional neural density estimator &lt;span class="math"&gt;\(q_{\phi}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt;. Simply, this estimator is a neural network with parameters &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; that accepts &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; as input and produces &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; as output. For example, It is trained on data tuples &lt;span class="math"&gt;\(\{\boldsymbol{\theta}_n, \mathbf{x}_n\}_{1:N}\)&lt;/span&gt; sampled from &lt;span class="math"&gt;\(p(\mathbf{x}, \boldsymbol{\theta}) = p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(p(\boldsymbol{\theta})\)&lt;/span&gt; is a prior we choose, and &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; is our &lt;em&gt;simulator&lt;/em&gt;. For example, we can construct this training set as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, we train our network.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, once trained, we can estimate &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)\)&lt;/span&gt;—our posterior belief over parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; given our &lt;em&gt;observed&lt;/em&gt; (not simulated!) data &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt; as &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x}_o) = q_{\phi}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Learning the wrong estimator&lt;/h2&gt;
&lt;p&gt;Ultimately, our goal is to perform the following computation:&lt;/p&gt;
&lt;div class="math"&gt;$$
q_{\phi}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)
$$&lt;/div&gt;
&lt;p&gt;Such that &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; produces an &lt;em&gt;accurate&lt;/em&gt; estimation of the parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; given observed data &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;, we require that &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; be &lt;em&gt;trained&lt;/em&gt; on tuples &lt;span class="math"&gt;\(\{\boldsymbol{\theta}_n, \mathbf{x}_n\}\)&lt;/span&gt; where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{x}_n \sim p(\mathbf{x}\mid\boldsymbol{\theta}_n)\)&lt;/span&gt; via our simulation step.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mid\mathbf{x}_n - \mathbf{x}_o\mid\)&lt;/span&gt; is small, i.e. our simulated are nearby our observed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Otherwise, &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; will learn to estimate a posterior over parameters given data &lt;em&gt;unlike&lt;/em&gt; our own.&lt;/p&gt;
&lt;h2&gt;Learning a better estimator&lt;/h2&gt;
&lt;p&gt;So, how do we obtain parameters &lt;span class="math"&gt;\(\boldsymbol{\theta}_n\)&lt;/span&gt; that produce &lt;span class="math"&gt;\(\mathbf{x}_n \sim p(\mathbf{x}\mid\boldsymbol{\theta}_n)\)&lt;/span&gt; near &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;? We take those that have high (estimated) posterior density given &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;In this vein, we build our training set as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Stitching this all together, our SBI routine becomes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_ROUNDS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generative_process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;posterior_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q_phi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ANY_NUMBER&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Learning the right estimator&lt;/h2&gt;
&lt;p&gt;Unfortunately, we're still left with a problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the first round, we learn &lt;span class="math"&gt;\(q_{\phi, r=0}(\boldsymbol{\theta}\mid\mathbf{x}) \approx p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;, i.e. the &lt;strong&gt;right&lt;/strong&gt; estimator.&lt;/li&gt;
&lt;li&gt;Thereafter, we learn &lt;span class="math"&gt;\(q_{\phi, r}(\boldsymbol{\theta}\mid\mathbf{x}) \approx p(\mathbf{x}\mid\boldsymbol{\theta})q_{\phi, r-1}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt;, i.e. the &lt;strong&gt;wrong&lt;/strong&gt; estimator.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, how do we correct this mistake?&lt;/p&gt;
&lt;p&gt;In &lt;a href="#papamakarios2016" id="ref-papamakarios2016-1"&gt;Papamakarios and Murray (2016)&lt;/a&gt;, the authors adjust the learned posterior &lt;span class="math"&gt;\(q_{\phi, r}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt; by simply dividing it by &lt;span class="math"&gt;\(q_{\phi, r-1}(\boldsymbol{\theta}\mid\mathbf{x})\)&lt;/span&gt; then multiplying it by &lt;span class="math"&gt;\(p(\boldsymbol{\theta})\)&lt;/span&gt;. Furthermore, as they choose &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; to be a &lt;em&gt;Mixture Density Network&lt;/em&gt;—a neural network which outputs the parameters of a mixture of Gaussians—and the prior to be "simple distribution (uniform or Gaussian, as is typically the case in practice)," this adjustment can be done analytically.&lt;/p&gt;
&lt;p&gt;Conversely, &lt;a href="#lueckmann2017" id="ref-lueckmann2017-1"&gt;Lueckmann et al. (2017)&lt;/a&gt; &lt;em&gt;train&lt;/em&gt; &lt;span class="math"&gt;\(q_{\phi}\)&lt;/span&gt; on a target &lt;em&gt;reweighted&lt;/em&gt; to similar effect: instead of maximizing the total (log) likelihood &lt;span class="math"&gt;\(\Sigma_{n} \log q_{\phi}(\boldsymbol{\theta}_n\mid\mathbf{x}_n)\)&lt;/span&gt;, they maximize &lt;span class="math"&gt;\(\Sigma_{n} \log w_n q_{\phi}(\boldsymbol{\theta}_n\mid\mathbf{x}_n)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(w_n = \frac{p(\boldsymbol{\theta}_n)}{q_{\phi, r-1}(\boldsymbol{\theta}_n\mid\mathbf{x}_n)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While both approaches carry further nuance and potential pitfalls, they bring us effective methods for using a neural network to directly estimate a faithful posterior in SBI routines.&lt;/p&gt;
&lt;h1&gt;Neural Likelihood Estimation&lt;/h1&gt;
&lt;p&gt;In neural likelihood estimation (NLE), we use a neural network to directly estimate the (intractable) likelihood function of the simulator &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; itself. We denote this estimator &lt;span class="math"&gt;\(q_{\phi}(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt;. Finally, we compute our desired posterior as &lt;span class="math"&gt;\(\tilde{p}(\boldsymbol{\theta}\mid\mathbf{x}_o) \approx q_{\phi}(\mathbf{x}_o\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similar to Neural Posterior Estimation (NPE) approaches, we'd like to learn our estimator on inputs &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; that produce &lt;span class="math"&gt;\(\mathbf{x}_n \sim p(\mathbf{x}\mid\boldsymbol{\theta}_n)\)&lt;/span&gt; near &lt;span class="math"&gt;\(\mathbf{x}_o\)&lt;/span&gt;. To do this, we again sample them from regions of high approximate posterior density. In each round &lt;span class="math"&gt;\(r\)&lt;/span&gt;, in NPE, this posterior was &lt;span class="math"&gt;\(q_{\phi, r-1}(\boldsymbol{\theta}\mid\mathbf{x} = \mathbf{x}_o)\)&lt;/span&gt;; in NLE, it is &lt;span class="math"&gt;\(q_{\phi, r-1}(\mathbf{x}_o\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt;. In both cases, we draw samples from our approximate posterior density, then feed them to the simulator to generate novel data for training our estimator &lt;span class="math"&gt;\(q_{\phi, r}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a more detailed treatment, please refer to original works &lt;a href="#pmlr-v89-papamakarios19a" id="ref-pmlr-v89-papamakarios19a-1"&gt;Papamakarios et al. (2019)&lt;/a&gt; and &lt;a href="#pmlr-v96-lueckmann19a" id="ref-pmlr-v96-lueckmann19a-1"&gt;Lueckmann et al. (2019)&lt;/a&gt; (among others).&lt;/p&gt;
&lt;h1&gt;Neural Likelihood Ratio Estimation&lt;/h1&gt;
&lt;p&gt;In this final class of models, we instead try to directly draw &lt;em&gt;samples&lt;/em&gt; from the true posterior itself. However, since we can't compute &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; nor &lt;span class="math"&gt;\(p(\mathbf{x})\)&lt;/span&gt;, we first need a sampling algorithm that satisifes these constraints. One such class of algorithms is &lt;em&gt;Markov chain Monte Carlo&lt;/em&gt;, termed MCMC.&lt;/p&gt;
&lt;p&gt;In MCMC, we first &lt;em&gt;propose&lt;/em&gt; parameter samples &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; from a proposal distribution. Then, we evaluate their &lt;em&gt;fitness&lt;/em&gt; by asking the question: "does this sample &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; have higher posterior density than the previous sample &lt;span class="math"&gt;\(\boldsymbol{\theta}_j\)&lt;/span&gt; we drew?" Generally, this question is answered through comparison, e.g.&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{
    p(\boldsymbol{\theta}_i\mid\mathbf{x})
} {
    p(\boldsymbol{\theta}_{j}\mid\mathbf{x})
} = \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)p(\boldsymbol{\theta}_i) / p(\mathbf{x})
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)p(\boldsymbol{\theta}_j) / p(\mathbf{x})
}
$$&lt;/div&gt;
&lt;p&gt;Fortunately, the evidence terms &lt;span class="math"&gt;\(p(\mathbf{x})\)&lt;/span&gt; cancel, and the prior densities &lt;span class="math"&gt;\(p(\boldsymbol{\theta})\)&lt;/span&gt; are evaluable. Though we cannot compute the likelihood terms outright, we can estimate their &lt;em&gt;ratio&lt;/em&gt; and proceed with MCMC as per normal. If &lt;span class="math"&gt;\(\frac{p(\boldsymbol{\theta}_i\mid\mathbf{x})}{p(\boldsymbol{\theta}_j\mid\mathbf{x})} \gt 1\)&lt;/span&gt;, we (are likely to) &lt;em&gt;accept&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; as a valid sample from our target posterior.&lt;/p&gt;
&lt;h2&gt;Estimating the likelihood ratio&lt;/h2&gt;
&lt;p&gt;Let us term the likelihood ratio as&lt;/p&gt;
&lt;div class="math"&gt;$$
r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j) = \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)
}
$$&lt;/div&gt;
&lt;p&gt;Ingeniously, &lt;a href="#cranmer2015" id="ref-cranmer2015-1"&gt;Cranmer et al. (2015)&lt;/a&gt; propose to learn a classifier to discriminate samples &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_i)\)&lt;/span&gt; from &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_j)\)&lt;/span&gt;, then use its predictions to estimate &lt;span class="math"&gt;\(r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To do this, they draw training samples &lt;span class="math"&gt;\((\mathbf{x}, y=1) \sim p(\mathbf{x}\mid\boldsymbol{\theta}_i)\)&lt;/span&gt; and &lt;span class="math"&gt;\((\mathbf{x}, y=0) \sim p(\mathbf{x}\mid\boldsymbol{\theta}_j)\)&lt;/span&gt; then train a binary classifer &lt;span class="math"&gt;\(d(y\mid\mathbf{x})\)&lt;/span&gt; on this data. In this vein, a perfect classifier gives:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
d^*(y=1\mid\mathbf{x})
&amp;amp;= \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_i) + p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} \\
d^*(y=0\mid\mathbf{x})
&amp;amp;= \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_i) + p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} \\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Consequently,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
&amp;amp;= \frac{
    p(\mathbf{x}\mid\boldsymbol{\theta}_i)
} {
    p(\mathbf{x}\mid\boldsymbol{\theta}_j)
} \\
&amp;amp;= \frac{
    d^*(y=1\mid\mathbf{x})
} {
    d^*(y=0\mid\mathbf{x})
} \\
&amp;amp;= \frac{
    d^*(y=1\mid\mathbf{x})
} {
    1 - d^*(y=1\mid\mathbf{x})
}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since our classifier won't be perfect, we simply term it &lt;span class="math"&gt;\(d(y\mid\mathbf{x})\)&lt;/span&gt;, where&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\hat{r}(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
&amp;amp;= \frac{d(y=1\mid\mathbf{x})}{1 - d(y=1\mid\mathbf{x})}\\
&amp;amp;\approx r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;With &lt;span class="math"&gt;\(\hat{r}(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)\)&lt;/span&gt; in hand, we can compare the posterior density of proposed samples &lt;span class="math"&gt;\(\boldsymbol{\theta}_i\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{\theta}_j\)&lt;/span&gt; in our MCMC routine.&lt;/p&gt;
&lt;h2&gt;Generalizing our classifier&lt;/h2&gt;
&lt;p&gt;To use the above classifier in our inference routine, we must &lt;em&gt;retrain&lt;/em&gt; a &lt;em&gt;new&lt;/em&gt; classifier for every &lt;em&gt;unique&lt;/em&gt; set of parameters &lt;span class="math"&gt;\(\{\boldsymbol{\theta}_i, \boldsymbol{\theta}_j\}\)&lt;/span&gt;. Clearly, this is extremely impractical. How can we generalize our classifier such that we only have to train it once?&lt;/p&gt;
&lt;p&gt;In &lt;a href="#cranmer2015" id="ref-cranmer2015-2"&gt;Cranmer et al. (2015)&lt;/a&gt;, the authors learn a &lt;em&gt;single&lt;/em&gt; classifier &lt;span class="math"&gt;\(d(y\mid\mathbf{x}, \boldsymbol{\theta})\)&lt;/span&gt; to discriminate samples &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; from &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; is an &lt;em&gt;arbitrary&lt;/em&gt; parameter value, and &lt;span class="math"&gt;\(\boldsymbol{\theta}_{ref}\)&lt;/span&gt; is a fixed, &lt;em&gt;reference&lt;/em&gt; parameter value. It is trained on data &lt;span class="math"&gt;\((\mathbf{x},  \boldsymbol{\theta}, y=1) \sim p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class="math"&gt;\((\mathbf{x}, \boldsymbol{\theta}_{ref},  y=0) \sim p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;. Once trained, it gives:&lt;/p&gt;
&lt;div class="math"&gt;$$
r(\mathbf{x}\mid\boldsymbol{\theta}, \boldsymbol{\theta}_{ref})
= \frac{
    d^*(y\mid\mathbf{x}, \boldsymbol{\theta})
} {
    1 - d^*(y\mid\mathbf{x}, \boldsymbol{\theta})
}
$$&lt;/div&gt;
&lt;p&gt;Consequently,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_j)
&amp;amp;= \frac{
    r(\mathbf{x}\mid\boldsymbol{\theta}_i, \boldsymbol{\theta}_{ref})
} {
    r(\mathbf{x}\mid\boldsymbol{\theta}_j, \boldsymbol{\theta}_{ref})
} \\
&amp;amp;= \frac{
    d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_i)
} {
    1 - d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_i)
} * \frac{
    1 - d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_j)
} {
    d^*(y\mid\mathbf{x}, \boldsymbol{\theta}_j)
}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;With a &lt;em&gt;single&lt;/em&gt; model, we can now compare the density of two proposed posterior samples.&lt;/p&gt;
&lt;h2&gt;Improving our generalized classifier&lt;/h2&gt;
&lt;p&gt;Once more, our classifier &lt;span class="math"&gt;\(d(y\mid\mathbf{x}, \boldsymbol{\theta})\)&lt;/span&gt; discriminates samples &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; from &lt;span class="math"&gt;\(\mathbf{x} \sim p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;. In this vein, in the case that a given &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; was drawn from neither &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; &lt;em&gt;nor&lt;/em&gt; &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;, what should our classifier do? In &lt;a href="#hermans2019" id="ref-hermans2019-1"&gt;Hermans et al. (2019)&lt;/a&gt;, the authors illustrate this problem—&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/neural-sbi/undefined-classifier.png"/&gt;&lt;/p&gt;
&lt;p&gt;—stressing that "poor inference results occur in the absence of support between &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta}_{ref})\)&lt;/span&gt;."&lt;/p&gt;
&lt;p&gt;In solution, they propose to learn a (neural) classifier that instead discriminates between &lt;em&gt;dependent&lt;/em&gt; sample-parameter pairs &lt;span class="math"&gt;\((\mathbf{x}, \boldsymbol{\theta}) \sim p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt; from &lt;em&gt;independent&lt;/em&gt; sample-parameter pairs &lt;span class="math"&gt;\((\mathbf{x}, \boldsymbol{\theta}) \sim p(\mathbf{x})p(\boldsymbol{\theta})\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class="math"&gt;\(p(\mathbf{x})p(\boldsymbol{\theta})\)&lt;/span&gt; occupy the same space, they share a common support. In other words, the likelihood of a given &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; will &lt;em&gt;always&lt;/em&gt; be positive for &lt;em&gt;some&lt;/em&gt; &lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; in the figure above.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Simulation-based inference is a class of techniques that allows us to perform Bayesian inference where our data-generating model &lt;span class="math"&gt;\(p(\mathbf{x}\mid\boldsymbol{\theta})\)&lt;/span&gt; lacks a tractable likelihood function, yet permits simulation of novel data. In the above sections, we detailed several SBI approaches, and ways in which neural networks are currently being used in each.&lt;/p&gt;
&lt;h2&gt;Credit&lt;/h2&gt;
&lt;p&gt;Credit to &lt;a href="https://www.processmaker.com/wp-content/uploads/2021/07/simulation-modeling-process-mining.jpg"&gt;ProcessMaker&lt;/a&gt; for social card image.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr/&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id="10.1080/01621459.2017.1285773"&gt;David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.
&lt;span class="bibtex-protected"&gt;Variational Inference: A Review for Statisticians&lt;/span&gt;.
&lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;, 112(518):859–877, 2017.
&lt;a href="https://arxiv.org/abs/1601.00670"&gt;arXiv:1601.00670&lt;/a&gt;, &lt;a href="https://doi.org/10.1080/01621459.2017.1285773"&gt;doi:10.1080/01621459.2017.1285773&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-10.1080/01621459.2017.1285773-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="cranmer2015"&gt;Kyle Cranmer, Juan Pavez, and Gilles Louppe.
&lt;span class="bibtex-protected"&gt;Approximating Likelihood Ratios with Calibrated Discriminative Classifiers&lt;/span&gt;.
&lt;em&gt;arXiv&lt;/em&gt;, 2015.
&lt;a href="https://arxiv.org/abs/1506.02169"&gt;arXiv:1506.02169&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-cranmer2015-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-cranmer2015-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-cranmer2015-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id="hermans2019"&gt;Joeri Hermans, Volodimir Begy, and Gilles Louppe.
&lt;span class="bibtex-protected"&gt;Likelihood-free MCMC with Amortized Approximate Ratio Estimators&lt;/span&gt;.
&lt;em&gt;arXiv&lt;/em&gt;, 2019.
&lt;a href="https://arxiv.org/abs/1903.04057"&gt;arXiv:1903.04057&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-hermans2019-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="pmlr-v96-lueckmann19a"&gt;Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H. Macke.
Likelihood-free inference with emulator networks.
In Francisco Ruiz, Cheng Zhang, Dawen Liang, and Thang Bui, editors, &lt;em&gt;Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference&lt;/em&gt;, volume 96 of Proceedings of Machine Learning Research, 32–53. 02 Dec 2019. PMLR.
URL: &lt;a href="http://proceedings.mlr.press/v96/lueckmann19a.html"&gt;http://proceedings.mlr.press/v96/lueckmann19a.html&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-pmlr-v96-lueckmann19a-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="lueckmann2017"&gt;Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan Öcal, Marcel Nonnenmacher, and Jakob H Macke.
&lt;span class="bibtex-protected"&gt;Flexible statistical inference for mechanistic models of neural dynamics&lt;/span&gt;.
&lt;em&gt;arXiv&lt;/em&gt;, 2017.
&lt;a href="https://arxiv.org/abs/1711.01861"&gt;arXiv:1711.01861&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-lueckmann2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="papamakarios2016"&gt;George Papamakarios and Iain Murray.
Fast ε-free inference of simulation models with bayesian conditional density estimation.
In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, volume 29. Curran Associates, Inc., 2016.
URL: &lt;a href="https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf"&gt;https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-papamakarios2016-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id="pmlr-v89-papamakarios19a"&gt;George Papamakarios, David Sterratt, and Iain Murray.
Sequential neural likelihood: fast likelihood-free inference with autoregressive flows.
In Kamalika Chaudhuri and Masashi Sugiyama, editors, &lt;em&gt;Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, volume 89 of Proceedings of Machine Learning Research, 837–848. PMLR, 16–18 Apr 2019.
URL: &lt;a href="http://proceedings.mlr.press/v89/papamakarios19a.html"&gt;http://proceedings.mlr.press/v89/papamakarios19a.html&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-pmlr-v89-papamakarios19a-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</content><category term="machine-learning"></category></entry><entry><title>Things I've Built in Crypto</title><link href="https://willwolf.io/2021/12/21/crypto-projects/" rel="alternate"></link><published>2021-12-21T18:00:00-05:00</published><updated>2021-12-21T18:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2021-12-21:/2021/12/21/crypto-projects/</id><summary type="html">&lt;p&gt;A running list of things I've built in and around the crypto space.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A running list of things I've built in and around this space.&lt;/p&gt;
&lt;h1&gt;simsim&lt;/h1&gt;
&lt;p&gt;A (sim)ple (sim)ulation &lt;a href="https://github.com/cavaunpeu/simsim"&gt;framework&lt;/a&gt; for generalized dynamical systems, in Rust.&lt;/p&gt;
&lt;p&gt;&lt;img alt="simsim" class="img-responsive" src="https://willwolf.io/images/simsim.png"/&gt;&lt;/p&gt;
&lt;h1&gt;solsim&lt;/h1&gt;
&lt;p&gt;A &lt;a href="https://github.com/cavaunpeu/solsim"&gt;simulator&lt;/a&gt; for complex systems on the Solana blockchain.&lt;/p&gt;
&lt;p&gt;&lt;img alt="solsim results app screenshot" class="img-responsive" src="https://raw.githubusercontent.com/cavaunpeu/solsim/main/img/results_explorer_app.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Solana Economic Simulator&lt;/h1&gt;
&lt;p&gt;A Streamlit &lt;a href="https://share.streamlit.io/cavaunpeu/solana-economics/main/app/main.py"&gt;app&lt;/a&gt; for simulating Solana validator economics.&lt;/p&gt;
&lt;p&gt;&lt;img alt="solana economics simulation screenshot" class="img-responsive" src="https://raw.githubusercontent.com/cavaunpeu/solana-economics/main/screenshot.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Anchor Escrow Program&lt;/h1&gt;
&lt;p&gt;A toy implementation of a Solana escrow &lt;a href="https://anchor-escrow-program.netlify.app/"&gt;program&lt;/a&gt; in Anchor and React.&lt;/p&gt;
&lt;p&gt;&lt;img alt="escrow program screenshot" class="img-responsive" src="https://anchor-escrow-program.netlify.app/card_image.png"/&gt;&lt;/p&gt;</content><category term="crypto"></category></entry><entry><title>Exploring, Crypto</title><link href="https://willwolf.io/2021/12/10/exploring-crypto/" rel="alternate"></link><published>2021-12-10T10:00:00-05:00</published><updated>2021-12-10T10:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2021-12-10:/2021/12/10/exploring-crypto/</id><summary type="html">&lt;p&gt;I'm taking some time to explore: Rust, complex systems, crypto.&lt;/p&gt;</summary><content type="html">&lt;p&gt;To "explore vs. exploit" is a classic dilemma in decision-making. To illustrate, imagine it’s date-night in downtown Manhattan and you’re choosing where to eat.&lt;/p&gt;
&lt;p&gt;To &lt;em&gt;exploit&lt;/em&gt; is to dine at your favorite restaurant. The food is well-cooked. The servers are attentive. You'll have a great meal and you know it.&lt;/p&gt;
&lt;p&gt;To &lt;em&gt;explore&lt;/em&gt; is to try something new. It's to implicitly concede that while your favorite restaurant is great, it may not be the best overall. Your new choice may very well end in vomit and tears. However, you'll never find something better unless you try.&lt;/p&gt;
&lt;h2&gt;ASAPP&lt;/h2&gt;
&lt;p&gt;Four and a half years at &lt;a href="https://willwolf.io/2021/12/04/leaving-asapp/"&gt;ASAPP&lt;/a&gt; was my career on "exploit." ASAPP is a company solving important problems replete with wonderful colleagues and ample opportunity for personal growth. I benefited from this environment each and every day. I kept choosing ASAPP because it unfailingly was, and remains, an excellent place to call my professional home.&lt;/p&gt;
&lt;p&gt;I joined ASAPP at age 28. I'm now 32. And I'd like to see what else there is. Where else I can learn, enjoy, create, and grow. In simple terms, I'd like to explore.&lt;/p&gt;
&lt;h2&gt;What's next&lt;/h2&gt;
&lt;p&gt;On the technical front:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn &lt;strong&gt;Rust&lt;/strong&gt; and smart-contract development in Solana.&lt;/li&gt;
&lt;li&gt;Study the theory and applications of &lt;a href="https://en.wikipedia.org/wiki/Complex_system"&gt;&lt;strong&gt;complex systems&lt;/strong&gt;&lt;/a&gt; (per my long-standing &lt;a href="https://honors.libraries.psu.edu/files/final_submissions/970"&gt;interest&lt;/a&gt; in this space).&lt;/li&gt;
&lt;li&gt;Apply these tools to the &lt;strong&gt;crypto&lt;/strong&gt; space at large.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the personal front:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Study &lt;strong&gt;Russian.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Spend time with friends.&lt;/li&gt;
&lt;li&gt;Travel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PhD?&lt;/h2&gt;
&lt;p&gt;In addition, I've applied for Ph.D. programs in statistical ML. Hopefully, I'll have a difficult choice to make in March.&lt;/p&gt;
&lt;h2&gt;In sum&lt;/h2&gt;
&lt;p&gt;In the spirit of exploration, I won't rigidly structure this period of my life. Instead, I'll see where my interests lead and follow them vigorously.&lt;/p&gt;
&lt;p&gt;Please don't hesitate to say 👋 if any of the above resonates with you!&lt;/p&gt;
&lt;p&gt;Will&lt;/p&gt;</content><category term="life"></category></entry><entry><title>Leaving ASAPP</title><link href="https://willwolf.io/2021/12/04/leaving-asapp/" rel="alternate"></link><published>2021-12-04T12:00:00-05:00</published><updated>2021-12-04T12:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2021-12-04:/2021/12/04/leaving-asapp/</id><summary type="html">&lt;p&gt;I left &lt;a href="https://www.asapp.com/"&gt;ASAPP&lt;/a&gt; after 4+ wonderful years.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I joined ASAPP in August 2017 as its 78th hire. I'd just finished my Open-Source ML &lt;a href="https://willwolf.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;"Master's"&lt;/a&gt; in Morocco; I wanted to &lt;a href="https://willwolf.io/2022/10/25/my-next-role/"&gt;join&lt;/a&gt; "a small company with ML at its core." I was referred by an old colleague. We barely had a &lt;a href="https://web.archive.org/web/20170825060205/https://www.asapp.com/"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;51 months went by quickly. ASAPP was a magical place. To my colleagues, know that you made a profound personal, professional, intellectual, and creative impact on my life. That our time by the whiteboard with fickle black markers, the monsoon swell of positive energy and "is there an extra chair for me?" that were our daily lunches, our unfettered excitement for the field and optimism for the technologies we build are golden memories that I'll hold close to my heart for years to come.&lt;/p&gt;
&lt;p&gt;All great stories must one day end. For me, that was yesterday. It was peculiar and bittersweet. I'm sincerely lucky to have worked at ASAPP, and I look forward to seeing all they accomplish in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From the bottom of my heart, thank you.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From here, I'm starting something new! I'll share an update soon.&lt;/p&gt;
&lt;p&gt;@will&lt;/p&gt;</content><category term="life"></category></entry><entry><title>Halford Mackinder on Artificial Intelligence</title><link href="https://willwolf.io/2020/08/16/mackinder-on-ai/" rel="alternate"></link><published>2020-08-16T16:00:00-04:00</published><updated>2020-08-16T16:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2020-08-16:/2020/08/16/mackinder-on-ai/</id><summary type="html">&lt;p&gt;What would preeminent 20th century geographer Halford J. Mackinder say about the coming revolution in artificial intelligence and its impact on our current ideological war?&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="en.wikipedia.org/wiki/Halford_Mackinder"&gt;Halford J. Mackinder&lt;/a&gt; was an English geographer, academic and politician. Born in the 1860s and deceased shortly after the end of World War II, he is largely known for a reading at the Royal Geographical Society of his 1904 paper, &lt;em&gt;&lt;a href="https://www.iwp.edu/wp-content/uploads/2019/05/20131016_MackinderTheGeographicalJournal.pdf"&gt;The Geographical Pivot of History&lt;/a&gt;&lt;/em&gt;. In this work, Mackinder describes in sweeping detail the preceding two thousands years of human activity—how geography, along with climate, religion, commerce and war, shaped the world of the time of his writing and facilitated the meteoric rise of the then-dominant Continental European powers. On this foundation, he then delivers, in an inspiring if admonitory crescendo, his famous "Heartland Theory"—a geopolitical analysis of the importance of these powers controlling what is modern Central Asia in the coming (20th) century—for which he is most singularly remembered.&lt;/p&gt;
&lt;p&gt;The goal of this post is to reimagine The Pivot in content and form for the 21st century. First, I'll reintroduce the paper's main ideas. Then, I'll play with the following question: &lt;strong&gt;what might Mackinder write if he were alive today?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The Geographical Pivot of History&lt;/h2&gt;
&lt;p&gt;Mackinder begins his famous work by asserting that in 1904, the world stands at the &lt;em&gt;exit&lt;/em&gt; of the Columbian epoch: a 400-year period of European expansionism, in which European states—principally French, Spanish, Dutch, English and Portuguese—sent their generals, scientists and engineers to explore, detail, and ultimately subjugate people and land in the furthest reaches of the globe—transforming themselves from provincial, land-based powers into global empires, now dominant at sea, and transforming the "known world map" from an ill-lit, half-built cabin deep in an unnamed wood to an immaculate Grand Hotel with a different European powerhouse in every single room. As such, in 1904, his continent's powers sat atop a now-familiar world, shoulder-to-shoulder, with nowhere to go but &lt;em&gt;down.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With the stage set thus, Mackinder invites us to step back, inhale deeply, and reflect—with a bias towards Continental Europe, no doubt, owing implicitly to its then-dominant stature, and perhaps to Mackinder being English himself—on some of history's broadest currents through the lens of geography: what was, what is, and what will be utterly crucial in the coming 20th century—his "Geographical Pivot."&lt;/p&gt;
&lt;p&gt;The Continental European landmass comprises an intricate series of mountains, valleys and peninsulas. These features, in turn, provide Europeans with ample physical barriers between which a multitude of politically and culturally distinct nation-states could and did take root. In stark contrast, Mackinder then describes the Russian landmass, one of an altogether different geographic profile: dense mixed and deciduous forest to the North; dry, arid steppe to the South, relatively inhospitable to both life and agriculture; unendingly flat throughout. Unlike Europe, this geography bred a different type of person and society: regional capitals like Kiev, Moscow and Kazan took root just north of this dividing forest-steppe frontier, whereas the sandy steppe itself spawned, well, a thousand years of nomadic invaders—Huns, Avers, Bulgarians, Magyars, Khazars, Patzinaks, Cumans, Kalmuks, Genghis Khan's Mongol hordes, and others—hardened by its brutal conditions, in sync with the land's silent whisper of "keep on moving, life won't survive here."&lt;/p&gt;
&lt;p&gt;In its first 1,500 years following the death of Jesus Christ, Continental Europe evolved largely on land. Corralled from the West by the Atlantic, as yet uncrossed, the North by the Scandinavian tundra, the South by, in effect, the immense sweep of the then-impenetrable Sahara, and the East by roughly 7,500 kilometers of remaining Eurasia comprised primarily of hostile Central Asian Steppe, separating Berlin from Beijing and the Pacific Ocean. As such, the bulk of Europe's military and political growth remained on its home turf. And implicitly, as the only game in town, &lt;em&gt;land power&lt;/em&gt; reigned supreme.&lt;/p&gt;
&lt;p&gt;On the heels of the Renaissance around 1,500 A.D., things changed. Emboldened and energized by cultural and economic development at home, European explorers took to the seas in zealous pursuit of additional sources of wealth and knowledge abroad. In 1498, &lt;a href="https://en.wikipedia.org/wiki/Vasco_da_Gama"&gt;Vasco de Gama&lt;/a&gt; rounded the Cape of Good Hope, traveling directly from Europe to India without traversing the Middle East. In 1492, &lt;a href="https://en.wikipedia.org/wiki/Christopher_Columbus"&gt;Columbus&lt;/a&gt;, the namesake of the Columbian age itself, crossed the Atlantic and reached the Americas. Over the next 400 years, European nation-states swiftly amassed empires of global proportions, subjugating previous inhabitants under their economic, military and political control, and becoming, new in history, the preeminent titans of &lt;em&gt;sea power&lt;/em&gt; throughout.&lt;/p&gt;
&lt;p&gt;The following map shows European exploration routes during this era. (Though it only covers years 1492-1682, the general trend it shows continued until the time of Mackinder's writing.)&lt;/p&gt;
&lt;p&gt;&lt;img alt="european exploration map" class="img-responsive" src="http://callisto.ggsrv.com/imgsrv/FastFetch/UBER2/00018293"/&gt;&lt;/p&gt;
&lt;p&gt;Simply, Europeans went everywhere! Except the Russian Empire. Except Mongolia. Except the Central Asian Steppe.&lt;/p&gt;
&lt;p&gt;In 1904, following 400 years of global domination, European historians were wont to consider their own stories, self-development and influence as superior to that of others. In response, in his paper, Mackinder implores us to reconsider the map of pre-Columbian Continental Europe, and the ways in which, owing to this map, the development of European society itself was shaped considerably from the &lt;em&gt;outside&lt;/em&gt; in.&lt;/p&gt;
&lt;p&gt;Indeed, until Columbus, the Atlantic Ocean remained uncrossed, thereby serving as an impregnable natural border to the West. Likewise, the menacing infinity of the Sahara to the South remained uncrossed until the mid-19th century. Conversely, the cold and cheerless tundra to the North played host to the &lt;a href="https://en.wikipedia.org/wiki/Viking_Age"&gt;Viking Age&lt;/a&gt;, a 200-year period of disruption and settlement in and around the North Sea—the British Isles, Ireland, Iceland, Greenland, the Faroe Islands, and others—as well as descents down the rivers Dnieper and Volga wherein the Vikings lived and traded with peoples in Kievan Rus and the Eastern Roman Empire.&lt;/p&gt;
&lt;p&gt;Finally, east of the Carpathians (in present-day Romania), sweeping north-east to the Ural Mountains (approximated by Chelyabinsk, Russia on the map below) then south-east through both Kazakhstan and Mongolia, is, well, what? What protected and will henceforth protect Europe from an intruding Eastern neighbor? On what natural borders can it count? Here, Mackinder (figuratively) smiles, stands up, cooly eyes the crowd, and concludes: "Nothing." Then, he reminds European historians what actually happened before the recent Columbian epoch that has so warped their perception and egos.&lt;/p&gt;
&lt;p&gt;&lt;img alt="central asian steppe" class="img-responsive" src="https://willwolf.io/images/mackinder-on-ai/central-asian-steppe.png"/&gt;&lt;/p&gt;
&lt;p&gt;In reality, the 5th to 16th centuries saw unending progressions of inveterate Asiatic invaders crossing the ceaseless Central Asian Steppe and &lt;em&gt;hammering&lt;/em&gt; the European land powers like &lt;em&gt;pestle to mortar.&lt;/em&gt; Furthermore, there is evidence that these invasions sparked the formation of many of the modern Continental European states themselves! Mackinder writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A large part of modern history might be written as a commentary upon the changes directly or indirectly ensuing from these raids. The Angles and Saxons, it is quite possible, were then driven to cross the seas to found England in Britain. The Franks, the Goths, and the Roman provincials were compelled, for the first time, to stand shoulder to shoulder on the battlefield of Chalons, making common cause against the Asiatics, who were unconsciously welding together modern France. Venice was founded from the destruction of Aquileia and Padua; and even the Papacy owed a decisive prestige to the successful mediation of Pope Leo with &lt;a href="https://en.wikipedia.org/wiki/Attila"&gt;Attila&lt;/a&gt; at Milan. Such was the harvest of results produced by a cloud of ruthless and idealess horsemen sweeping over the unimpeded plain—a blow, as it were, from the great Asiatic hammer striking freely through the vacant space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, though European society had matured significantly in the preceding 2,000 years, evolving from land-based principalities to sea-faring, nation-state empires of global reach, its &lt;em&gt;geography&lt;/em&gt; had, and will always, stay the same. In 1904, transcontinental dominion notwithstanding, the Steppe to Europe's East lay flat, barren, uninterrupted—&lt;em&gt;and un-mastered by Europe&lt;/em&gt; as ever.&lt;/p&gt;
&lt;p&gt;&lt;img alt="european colonial possessions 1900" class="img-responsive" src="https://apworldunit5.weebly.com/uploads/7/6/7/6/76761401/701214.gif?880"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;N.B. By 1900, the majority of Latin American states, their own colonial history predominantly that of Spanish, Dutch and Portuguese imperialism, had already gained independence.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And finally, in 1904, why was this so significant? Among his esteemed and learned colleagues at the Royal Geographical Society, representing in turn the titans of contemporary global order, why was Mackinder so acutely preoccupied with the final "piece of the puzzle"—the "Heartland"—so influential in Europe's pre-Columbian history?&lt;/p&gt;
&lt;p&gt;As a barren steppe, the Heartland lacks the stone and timber necessary for road-making. As such, throughout the majority of history, it was best traversed in the same fashion as that of the Asiatic invaders: on lunky horse and camel. However, in 1904, a recent technology was poised to &lt;em&gt;radically&lt;/em&gt; alter this fact, to substantially accelerate the pace with which the Steppe could be crossed, to transform the perennial Steppe Invasion from a dainty pestle into an earth-fracturing pneumatic drill. This technology, &lt;a href="https://en.wikipedia.org/wiki/History_of_rail_transport"&gt;introduced&lt;/a&gt; in Britain around 1830, was the &lt;em&gt;railroad&lt;/em&gt;, and the &lt;em&gt;steam-powered trains&lt;/em&gt; to which it played host.&lt;/p&gt;
&lt;p&gt;The following graph shows the growing number of rail passengers per year at the time of Mackinder's writing, which we can understand as a proxy for the growing number of miles of railroad itself:&lt;/p&gt;
&lt;p&gt;&lt;img alt="rail passengers by year" class="img-responsive" src="https://upload.wikimedia.org/wikipedia/commons/1/15/GBR_rail_passengers_by_year_1830-2015.png"/&gt;&lt;/p&gt;
&lt;p&gt;Here, Mackinder concludes, delivering his key insight as well as the Heartland Theory itself in one grand sweep:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A generation ago steam and the Suez canal appeared to have increased the mobility of sea-power relatively to land-power. Railways acted chiefly as feeders to ocean-going commerce. But trans-continental railways are now transmuting the conditions of land-power, and nowhere can they have such effect as in the closed heart-land of Euro-Asia, in vast areas of which neither timber nor accessible stone was available for road-making. Railways work the greater wonders in the steppe, because they directly replace horse and camel mobility, the road stage of development having here been omitted. [...] As we consider this rapid review of the broader currents of history, does not a certain persistence of geographical relationship become evident? Is not the pivot region of the world's politics that vast area of Euro-Asia which is inaccessible to ships, but in antiquity lay open to the horse-riding nomads, as is to-day about to be covered with a network of railways?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="geographical pivot" class="img-responsive" src="https://willwolf.io/images/mackinder-on-ai/geographical-pivot.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Zooming out&lt;/h2&gt;
&lt;p&gt;Though Mackinder's thesis is specific to 20th century Continental Europe, I think there is a broader framework at play. Personally, I read Mackinder to be saying the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In the present day, there is a global hegemon. This hegemon is content, if haughty, with its place in the world. Modern history presented this hegemon with a task, a challenge, an "unclaimed space on which to plant its flags," and the hegemon did just that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Well, not quite: there is still &lt;em&gt;some&lt;/em&gt; space it failed to claim. And this particular space, as it goes, facilitated singular and outsized destruction in the past. So, hegemon beware!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is today a novel technology neatly poised to reify this destruction on a much grander scale than ever before.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this framework in mind, what might Mackinder write if he were alive today?&lt;/p&gt;
&lt;h2&gt;The United States&lt;/h2&gt;
&lt;p&gt;In the past 100 years, the United States has transformed itself from a non-interventionist island nation of lofty ideals into a heavyweight champion of military might, economic preponderance, and cultural dominance at global scale. To wit, 1918 saw President Woodrow Wilson delivering his &lt;a href="https://www.theworldwar.org/learn/peace/fourteen-points"&gt;Fourteen Points&lt;/a&gt; touting "freedom of the seas and free trade and the concept of national self-determination," while 2020 America maintains ~1,000 &lt;a href="https://www.thenation.com/article/archive/the-us-has-military-bases-in-172-countries-all-of-them-must-close/"&gt;military bases&lt;/a&gt; in ~100 countries, controls ~30% of the &lt;a href="https://www.visualcapitalist.com/all-of-the-worlds-wealth-in-one-visualization/#:~:text=Leading%20the%20pack%20is%20the,share%20of%20the%20global%20total."&gt;world's wealth&lt;/a&gt;, and cannot count on &lt;a href="https://www.washingtonpost.com/news/monkey-cage/wp/2016/12/23/the-cia-says-russia-hacked-the-u-s-election-here-are-6-things-to-learn-from-cold-war-attempts-to-change-regimes/"&gt;seven sets of hands&lt;/a&gt; the number of times its CIA has &lt;a href="https://www.amazon.com/Killing-Hope-C-I-Interventions-II-Updated/dp/1567512526"&gt;intervened in foreign countries&lt;/a&gt; since its &lt;a href="https://www.cia.gov/about-cia/history-of-the-cia"&gt;establishment in 1947&lt;/a&gt;. Though its detractors are harsh, and though President Donald Trump's foppishness and distinctly isolationist bent may indeed alter this course, the fact remains that the United States &lt;em&gt;is&lt;/em&gt; Mackinder's unambiguous contemporary hegemon—his Continental Europe of 1904—eminently comfortable, if &lt;a href="https://www.nytimes.com/2020/02/07/opinion/sunday/western-society-decadence.html"&gt;decaying&lt;/a&gt;, in its gold-rimmed seat on top of the world.&lt;/p&gt;
&lt;p&gt;Continental Europe achieved its own preponderance by winning the war of global &lt;em&gt;land&lt;/em&gt;: occupying—with lust, zeal and greed—as much of the map as it could throughout the Columbian era. In turn, the United States reached its current seat by winning the war of global &lt;em&gt;ideas&lt;/em&gt;, which Europe lost: from the brutal, hellish ruins of a 77-year Battle Royal from 1914 to 1991 between fascism, communism and free-market democracy—in other words, not over who gets what of an empty map, but how we live where we currently are in a map already full, encompassing the two World Wars and the Cold War—the United States and the ideas it represented emerged victorious, while Continental Europe fell to ruin. From then on, the United States became the global imperial much like the Columbian Europeans, patrolling the space of ideas with weapons and money—both as a beachhead-plus-lighthouse for the ideals of democracy, along with frequent &lt;a href="https://en.wikipedia.org/wiki/Foreign_interventions_by_the_United_States#Post-Cold_War"&gt;foreign interventions&lt;/a&gt; to this effect—instead of the seas themselves, cementing its role as global hegemon in the process.&lt;/p&gt;
&lt;h2&gt;The modern war of ideas&lt;/h2&gt;
&lt;p&gt;Whereas the 20th century fought over what I will call "macro-ideology"—how nation-states should self-organize in the broadest of political, economic and social terms—2020 America is home to vicious wars over, in comparison, "micro-ideology": conduct in the workplace, gender identity and bias, police brutality, racial justice, who "belongs" where, and an infinity of other like topics that animate a sizable chunk of our current social discourse.&lt;/p&gt;
&lt;p&gt;In this fight, people have taken to the street. To wit, the May 2020 murder of &lt;a href="https://en.wikipedia.org/wiki/George_Floyd"&gt;George Floyd&lt;/a&gt; at the hands of Minneapolis police officer Derek Chauvin sparked humungous, long-lasting protests around the country and world. In 2017, the day after Donald Trump's inauguration, the &lt;a href="https://en.wikipedia.org/wiki/2017_Women%27s_March"&gt;Women's March&lt;/a&gt; protesting his anti-women rhetoric became the largest single-day protest in U.S. history. Throughout the Trump presidency, white nationalist groups have variously assembled—the infamous 2017 Unite the Right rally in &lt;a href="https://en.wikipedia.org/wiki/Unite_the_Right_rally"&gt;Charlottesville&lt;/a&gt; as the most prominent example—declaiming in fire and fury, well, who really knows—to much public attention and even &lt;a href="https://www.bbc.com/news/world-us-canada-40980175"&gt;counter-protest&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nevertheless, for all of the issues over which we currently fight, do we not, as Ross Douthat writes in &lt;a href="https://www.amazon.com/Decadent-Society-Became-Victims-Success/dp/1476785244"&gt;&lt;em&gt;The Decadent Society&lt;/em&gt;&lt;/a&gt;, conduct the &lt;em&gt;majority&lt;/em&gt; of these battles on our &lt;em&gt;screens&lt;/em&gt; rather than in the streets themselves? Are the &lt;a href="https://en.wikipedia.org/wiki/Sturmabteilung"&gt;Brownshirts&lt;/a&gt; of our time not smashing the heads of their adversaries in the public square, but instead patrolling the forums of Reddit and the "I don't know who needs to hear this; Thread 🧵:"'s of Twitter, throwing flame in the form of an ironic tweet at whoever disagrees? Has physical violence not largely been replaced by &lt;em&gt;reputational&lt;/em&gt; violence, i.e. our modern &lt;a href="https://www.dictionary.com/e/pop-culture/cancel-culture/#:~:text=Cancel%20culture%20refers%20to%20the,the%20form%20of%20group%20shaming."&gt;&lt;em&gt;cancel culture&lt;/em&gt;&lt;/a&gt;? For the &lt;em&gt;majority&lt;/em&gt; of Americans, is the intensity of our political beliefs &lt;em&gt;really&lt;/em&gt; commensurate with our corresponding actions in &lt;a href="https://www.urbandictionary.com/define.php?term=meatspace"&gt;"meatspace"&lt;/a&gt;? Or do we say these things to our like-minded friends, feel these things when we come home from work, and then, out for blood and looking to fight, we take to the internet for twenty minutes to throw a few punches and be punched right back, then ice cream, Netflix, and early to bed?&lt;/p&gt;
&lt;p&gt;Protest and demonstration in the streets is not dead. However, in terms of dynamism, large-scale impact, and sheer number of combatants, the Internet truly is the preeminent battlefield of our time.&lt;/p&gt;
&lt;h2&gt;The remaining space on the map&lt;/h2&gt;
&lt;p&gt;As the dust settled over the world in &lt;a href="https://history.state.gov/milestones/1989-1992/collapse-soviet-union#:~:text=On%20December%2025%2C%201991%2C%20the,the%20newly%20independent%20Russian%20state."&gt;1991&lt;/a&gt; following 77 brutal years of physical and psychological war over macro-ideology, the United States emerged on top, much like Mackinder's post-Columbian Europe following its own implicit victory—territorial and economic expansion unprecedented in history. Nevertheless, in our modern day, there exists a &lt;em&gt;new&lt;/em&gt; space on the map of human interaction that the United States has &lt;em&gt;not&lt;/em&gt; conquered, the space where our current war for micro-ideology is largely waged, and, as it was ultimately a fight for ideology that felled the previous global powers, a space of critical importance. This space, of course, is the Internet: the Mackinderian Digital Heartland.&lt;/p&gt;
&lt;h2&gt;The practical nature of battles online&lt;/h2&gt;
&lt;p&gt;Mackinder's dictum about the importance of the Central Asian heart-land stemmed from the impending technological revolution in its harmful potential: whereas the invaders used to crosse the Steppe on camel and horse—slowly, in other words—they would soon be racing under starry desert skies on steam-powered train.&lt;/p&gt;
&lt;p&gt;Presently, ideological battles online take place with text, video and audio authored manually by humans; in other words, slowly! It is humans that write tweets and think-pieces to which other humans respond. It is humans that start digital media campaigns, create YouTube content, stoke the fires that lead to real-life demonstration and violence, and even coordinate large-scale protests themselves.&lt;/p&gt;
&lt;p&gt;So, what happens when a technology comes along that stands to turbo-charge the speed and chaos with which these battles take place? What happens when the text, video and audio that comprise these battles are created not by humans, but by &lt;em&gt;algorithms&lt;/em&gt;—unaccountable, indefatigable, at times sloppy, and brutally fast? Finally, what happens when human combatants, who we currently "eliminate" from battle via &lt;em&gt;reputational&lt;/em&gt; assassination, are replaced by algorithms that are immune to this deadliest weapon?&lt;/p&gt;
&lt;h2&gt;Mackinder on artifical intelligence&lt;/h2&gt;
&lt;p&gt;A 21st century Halford Mackinder would look at the deafening roll of artificial intelligence innovation in models that generate video, audio and language and be rightfully crapping his pants. Because simply, these models have the ability to do to our already-chaotic online debate not what trains, but 3D-printable intercontinental ballistic missiles, would have done for the Steppe Invaders of centuries past.&lt;/p&gt;
&lt;p&gt;To make this claim, we don't have to go much past the recent advent of &lt;a href="https://openai.com/blog/openai-api/"&gt;OpenAI's GPT-3&lt;/a&gt;, neatly explained in the following video:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=SboKeK6FFHQ" target="_blank" title="GPT-3 - explained in layman terms."&gt;&lt;img alt="Click me!" class="img-responsive" src="http://img.youtube.com/vi/SboKeK6FFHQ/0.jpg"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In essence, and borrowing from the video, "GPT-3 is an extremely sophisticated text predictor. A human gives it a chunk of text as input, and the model generates its best guess as to what the next chunk of text should be." In other words, and in terms of online battle, it reads what's written, and responds. However, crucially, whereas a human might reply to 1 or 2 posts over a 5-minute period, GPT-3 could create or reply to thousands.&lt;/p&gt;
&lt;p&gt;To demonstrate the ability of these models, &lt;a href="https://www.theverge.com/2020/8/16/21371049/gpt3-hacker-news-ai-blog"&gt;college student Liam Porr recently generated&lt;/a&gt; the following &lt;a href="https://adolos.substack.com/p/feeling-unproductive-maybe-you-should"&gt;blog post&lt;/a&gt; using GPT-3. In effect, he fed a title and prompt (about human productivity) to the model, asked it for a few different drafts, selected the best one, edited it minimally, then hit publish. The post itself quickly earned the top-spot on vaunted &lt;a href="https://news.ycombinator.com/"&gt;Hacker News&lt;/a&gt;, seemingly on the merits of its content alone. Take a look and see for yourself.&lt;/p&gt;
&lt;p&gt;Finally, what happens when our online battles are fueled by such &lt;em&gt;high-quality content generated by models like GPT-3 at warp speed?&lt;/em&gt; In a world whose news cycle already changes by the hour? In which social media is already such a dominant force?&lt;/p&gt;
&lt;p&gt;Honestly, your guess, and Mackinder's guess, are as good as mine.&lt;/p&gt;
&lt;h2&gt;How to control the Digital Heartland, and what's at stake&lt;/h2&gt;
&lt;p&gt;Implicit in Mackinder's admonition is that control of the Central Asian Heartland means the same type of control with which European then-empires came to dominate the rest of the world: economic, political or outright military dominion (and to wit, this is in effect what Hitler then tried to do). Unfortunately, he neglects to offer more specific guidance—especially with respect to the impending explosion in railroad construction in which he places so much concern. With this in mind, how might a 21st century Mackinder encourage the United States to "control" the Digital Heartland? By crafting then backing the internet's dominant ideological current, a la victory in the Cold War? By creating a monopoly on the internet's infrastructure and data, i.e. controlling the servers on which online war is waged, and owning the data that it generates? By training, employing and overseeing the bulk of the researchers that build the artificial intelligence itself? Or perhaps by gaining ownership over the specific platforms, like Twitter and Reddit, on which combatants clash?&lt;/p&gt;
&lt;p&gt;To me, the answer is not necessarily clear, and a deeper analysis of this question is the subject of a future post. Nonetheless, it is clear that the eventual conclusion of this war—the "1991" of our current age—has the real potential to spawn some sort of parallel, supranational organization or hegemony rooted more in a sense of identity and ideological affiliation, in which your figurative "digital passport" does not have the name of a sovereign physical territory embroidered on its front, but instead, a way of thought or belief picked from amongst the micro-ideological multiplex that is the modern internet.&lt;/p&gt;
&lt;h2&gt;A closing warning&lt;/h2&gt;
&lt;p&gt;As 1904 Continental Europe stood at, in his view, the exit of the Columbian era, Mackinder leaves us with one final warning about the practical reality of the coming world order. Whereas pre-Columbian Europe was "pent into a narrow region and threatened by external barbarism," and Columbian Europe was characterized by "the expansion of Europe against almost negligible resistances," a now post-Columbian Europe will take an altogether different hue:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From the present time forth, in the post-Columbian age, we shall again have to deal with a closed political system, and none the less that it will be one of worldwide scope. Every explosion of social forces, instead of being dissipated in a surrounding circuit of unknown space and barbaric chaos, will be sharply re-echoed from the far side of the globe, and weak elements in the political and economic organism of the world will be shattered in consequence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To me, his second sentence is utterly jarring: if Mackinder worries thus about the consequences of an "explosion of social forces" in a "closed political system," what would he say about combat on our modern internet—in which social forces, quite literally, can reach and ignite the remotest corners of the world &lt;em&gt;minutes&lt;/em&gt; after their birth? In which further explosions of these forces are, as the military-industrial complex is to traditional warfare, bolstered by the advertising industry, wherein wealthy and powerful executives have a direct incentive to throw kerosene on the flames? And how would he feel about the fact that, in a broad sense, this fiery battlefield is increasingly hard to leave—as we learn more, bank more, socialize more, and work more online?&lt;/p&gt;
&lt;h2&gt;On fatalism&lt;/h2&gt;
&lt;p&gt;To paraphrase Robert Kaplan in Chapter 9 of &lt;a href="https://www.amazon.com/Revenge-Geography-Coming-Conflicts-Against/dp/0812982223"&gt;&lt;em&gt;The Revenge of Geography&lt;/em&gt;&lt;/a&gt;, "though Mackinder was often attacked as an arch-deterministic, a charge not wholly unfair, no doubt, owing to the fact that geography was his subject, and geography itself can by its very nature be deterministic, Mackinder was no mere fatalist: ultimately, he believed that geography and the environment could be overcome, but only if we treat those subjects with the greatest knowledge and respect."&lt;/p&gt;
&lt;p&gt;In other words, a 21st century Mackinder might say that the technology that we currently build does not have to invite an invasion of the Huns. It does not have to poison social order. And finally, as artificial intelligence combatants are increasingly whisked off to wage internet war, he'd hope that we don't inadvertently burn it all down before we get much further.&lt;/p&gt;
&lt;h2&gt;Credit&lt;/h2&gt;
&lt;p&gt;Many thanks to Abishur Prakash for reviewing an earlier draft of this piece.&lt;/p&gt;
&lt;h2&gt;Additional references&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;“Are We on the Road to Civilisation Collapse?” BBC Future, BBC, www.bbc.com/future/article/20190218-are-we-on-the-road-to-civilisation-collapse. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Chan, Amy. “Behind the Lines: When Empires Collapse.” HistoryNet, HistoryNet, 15 May 2017, www.historynet.com/behind-lines-empires-collapse.htm. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Holland, Tom. Why Empires Fall: from Ancient Rome to Putin's Russia, 23 May 2014, www.newstatesman.com/politics/2014/05/why-empires-fall-ancient-rome-putins-russia. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Mackinder, Halford J. “The Geographical Pivot of History.” The Geographical Journal, vol. 23, no. 4, 1904, p. 421., doi:10.2307/1775498. &lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Mihir Shukla, Automation Anywhere. “The Role of Automation in the Aging Workforce.” VentureBeat, VentureBeat, 19 Apr. 2018, venturebeat.com/2018/04/19/the-role-of-automation-in-the-aging-workforce/. &lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;“Why Do Empires Decline?” Quora, www.quora.com/Why-do-empires-decline. &lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="geopolitics"></category></entry><entry><title>Soft Power in the Age of Deepfakes</title><link href="https://willwolf.io/2020/03/12/soft-power-in-the-age-of-generative-models/" rel="alternate"></link><published>2020-03-12T08:00:00-04:00</published><updated>2020-03-12T08:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2020-03-12:/2020/03/12/soft-power-in-the-age-of-generative-models/</id><summary type="html">&lt;p&gt;What happens when a post-Trump, reputationally-bruised United States, and improved generative models (the technology behind "deepfakes") collide head-on?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Nations exert influence in two key ways: hard power, and soft power. Hard power is &lt;a href="https://www.history.com/topics/world-war-ii/bombing-of-hiroshima-and-nagasaki"&gt;dropping the atomic bomb&lt;/a&gt; on Hiroshima and Nagasaki during World War II, compelling the Japanese to surrender. Conversely, soft power is building American libraries in foreign universities, placing American films in foreign theaters, and sending &lt;a href="https://en.wikipedia.org/wiki/Peace_Corps"&gt;United States Peace Corps&lt;/a&gt; volunteers to rural communities worldwide, compelling a favorable image of our country abroad.&lt;/p&gt;
&lt;p&gt;In other words, hard power is to influence via economic and military violence; soft power is to influence via &lt;em&gt;attraction.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Soft power through media&lt;/h2&gt;
&lt;p&gt;A key subset of soft power is that delivered through media.&lt;/p&gt;
&lt;p&gt;For example, during the Franco-Prussian War, the French dropped leaflets over enemy territory by &lt;a href="https://en.wikipedia.org/wiki/Airborne_leaflet_propaganda"&gt;airborne balloon&lt;/a&gt; touting the mutual benefits of ceasefire. During World War I, all parties dropped such leaflets by &lt;a href="https://en.wikipedia.org/wiki/Airborne_leaflet_propaganda#First_World_War"&gt;airplane&lt;/a&gt;. In World War II, the Nazis pioneered the use of &lt;a href="https://en.wikipedia.org/wiki/Radio_propaganda#Nazi_Germany"&gt;radio&lt;/a&gt; as a propaganda machine. Finally, in 1969, the Americans live-broadcast Neil Armstrong’s moon landing to &lt;a href="https://www.youtube.com/watch?v=cwZb2mqId0A"&gt;television&lt;/a&gt; screens worldwide, showcasing the "derring-do and genius of American ingenuity, and putting one over the Soviets at the same time."&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Today, such media is more numerous and diverse: American shows on Netflix, American music on Spotify, American “influencers” on YouTube, Hollywood movies and more—all distributed globally.&lt;/p&gt;
&lt;h2&gt;The cost of media&lt;/h2&gt;
&lt;p&gt;The cost of media can be split into two broad camps: creation and distribution.&lt;/p&gt;
&lt;p&gt;Since the days of leaflets by balloons, the marginal cost of distribution has become extremely cheap. Today, Trump tweets, and his words instantaneously appear on the timelines of his 75 million followers. Were he to have 150 instead, the additional cost of reaching these additional 75 would be, practically speaking, nothing.&lt;/p&gt;
&lt;p&gt;In contrast, the creation of this media itself remains a wholly manual exercise. To create a news broadcast, humans write scripts, set the stage, select the costumes, apply makeup, and read the news itself. Furthermore, creating twice as many news broadcasts costs, crucially, roughly twice as much.&lt;/p&gt;
&lt;p&gt;The dwindling marginal cost of its distribution makes soft power media efficient and appealing. However, it is the “creation bottleneck” that stands in the way of true scale.&lt;/p&gt;
&lt;h2&gt;Generative models&lt;/h2&gt;
&lt;p&gt;Generative models are those that empower &lt;a href="https://en.wikipedia.org/wiki/Deepfake"&gt;“deepfakes.”&lt;/a&gt; They are algorithms that generate realistic &lt;a href="https://thispersondoesnotexist.com/"&gt;images&lt;/a&gt; (refresh this link for more), &lt;a href="https://medium.com/syncedreview/deepmind-dvd-gan-impressive-step-toward-realistic-video-synthesis-12027d942e53"&gt;video&lt;/a&gt;, &lt;a href="https://openai.com/blog/musenet/"&gt;audio&lt;/a&gt;, &lt;a href="https://talktotransformer.com/"&gt;text&lt;/a&gt;, or other types of rich media. Presently, generative models excel in creating art, music, literature, news reports, and textiles. &lt;a href="https://news.microsoft.com/apac/features/much-more-than-a-chatbot-chinas-xiaoice-mixes-ai-with-emotions-and-wins-over-millions-of-fans/"&gt;Microsoft’s Xiaoice&lt;/a&gt;, a multi-purpose &lt;a href="https://en.wikipedia.org/wiki/Chatbot"&gt;chatbot&lt;/a&gt; (which currently dialogs with over 660 million registered users), pulls all this off and more:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Xiaoice’s framework is learning to write literature as well as compose and perform songs. Last year she published a book of poems and helps her followers write their own. She can sing her own songs in styles based on existing popular performers. There are plans to release an album of pop tunes soon. And she is able to author tailor-made stories for children and reads them out in voices suited to each of the characters she has created.&lt;br/&gt;&lt;br/&gt;She’s painting images based on keywords and other inputs. She’s also gone into mainstream media as a host of dozens of TV and radio programs that are broadcast across China. She reads news stories and provides commentary. And, she is generating multiple reports based on information from China’s financial markets and used by investors and traders who subscribe to Wind, a major financial information service.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;In short, generative models &lt;strong&gt;automate&lt;/strong&gt; the creation of media itself—smashing the “creation bottleneck” outright.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Coming soon&lt;/h2&gt;
&lt;p&gt;At present, generative models succeed at tasks like: &lt;a href="https://www.youtube.com/watch?v=PCBTZh41Ris"&gt;transferring dance moves&lt;/a&gt; from professionals to amateurs, &lt;a href="https://github.com/elliottwu/sText2Image"&gt;translating pencil sketches&lt;/a&gt; to high-resolution images with the guidance of text, and making Obama lip sync the words of American actor Jordan Peele.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=cQ54GDm1eL0" title="Click me!"&gt;&lt;img alt="Click me!" class="img-responsive" src="http://img.youtube.com/vi/cQ54GDm1eL0/0.jpg"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These feats are impressive. However, to achieve widespread impact, these algorithms must allow for more “control” over the media they create.&lt;/p&gt;
&lt;p&gt;For example, consider a system that, given English text, instantaneously generates a video of this text being recited in an arbitrary language. To be truly effective as a soft power tool, this system must ultimately operate as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;: “What kind of video would you like?”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;: “I’d like a video of the President reciting the uploaded text. I'd like him to speak slowly, be wearing green US Army fatigues, and be seated in front of a blue Boeing CH-47 Chinook. Make me two, actually: one in Farsi, the other in Greek.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;: “Coming right up.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Such generative models are not quite here. However, given the speed at which this &lt;a href="http://www.arxiv-sanity.com/search?q=video+generation"&gt;research&lt;/a&gt; moves, they will reliably arrive within 5 years.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;The United States used to invest in soft power institutions&lt;/h2&gt;
&lt;p&gt;Historically, the United States invested substantially in soft power institutions.&lt;/p&gt;
&lt;p&gt;During World War I, Woodrow Wilson established the &lt;a href="https://en.wikipedia.org/wiki/Committee_on_Public_Information"&gt;Committee on Public Information&lt;/a&gt;, which ultimately trafficked in outright propaganda.&lt;/p&gt;
&lt;p&gt;During World War II, Roosevelt established the &lt;a href="https://en.wikipedia.org/wiki/Public_diplomacy_of_the_United_States"&gt;Division of Cultural Relations&lt;/a&gt; and Office of Inter-American Affairs in response to fascist propaganda in Latin America. Following the bombing of Pearl Harbor, the &lt;a href="https://en.wikipedia.org/wiki/Voice_of_America"&gt;Voice of America&lt;/a&gt;—a globally-distributed radio broadcast of non-military promotional content—the &lt;a href="https://en.wikipedia.org/wiki/United_States_Office_of_War_Information"&gt;Office of War Information&lt;/a&gt;—distributing media like “newspapers, posters, photographs, films”&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8"&gt;8&lt;/a&gt;&lt;/sup&gt; to civilian communities abroad—and the &lt;a href="https://en.wikipedia.org/wiki/Office_of_Strategic_Services"&gt;Office of Strategic Services&lt;/a&gt;—the predecessor to the CIA—were all established as well.&lt;/p&gt;
&lt;p&gt;During the Cold War, the United States flexed its “peacetime” soft power muscle, establishing myriad organizations to promote international cultural and educational exchanges, including the &lt;a href="https://en.wikipedia.org/wiki/Fulbright_Program"&gt;Fulbright Program&lt;/a&gt; in 1947 and the &lt;a href="https://en.wikipedia.org/wiki/Public_diplomacy_of_the_United_States#U.S._Information_Agency_(USIA)"&gt;United States Information Agency&lt;/a&gt; (USIA) in 1953.&lt;/p&gt;
&lt;h2&gt;Dwindling interest and opportunity missed&lt;/h2&gt;
&lt;p&gt;Following the Cold War, however, American soft power institutions began to slowly decline.&lt;/p&gt;
&lt;p&gt;In 1999, the USIA was absorbed into the US State Department; its staff, and budget for key projects, were cut roughly in half.&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt; “From 1995 to 2001, academic and cultural exchanges dropped from 45,000 to 29,000 annually, and many accessible cultural centers and libraries were closed.”&lt;sup id="fnref2:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt; Finally, “while government-funded radio broadcasts reached between 70 and 80 percent of the populace of Eastern Europe during the Cold War, around year 2000, a mere 2 percent of Arabs heard the VOA.”&lt;sup id="fnref3:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;At present, the United States has a largely &lt;a href="https://www.nationalreview.com/2017/01/trump-foreign-policy-isolationsim-america-first-allies-nato-trans-pacific-partnership/"&gt;isolationist president&lt;/a&gt; in Donald Trump. Since the beginning of his term, American soft power has fallen globally according to the Portland &lt;a href="https://softpower30.com/"&gt;Soft Power 30&lt;/a&gt;. Trump himself is unconcerned with others actively &lt;a href="https://www.rollcall.com/2018/09/25/world-leaders-laugh-as-trump-boasts-of-accomplishments/"&gt;laughing&lt;/a&gt; at him and his country.&lt;/p&gt;
&lt;p&gt;By 2018, Statista estimates that there were 1.67 billion "pay TV households" worldwide.&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9"&gt;9&lt;/a&gt;&lt;/sup&gt; By 2019, they estimate that half of private households had a computer.&lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10"&gt;10&lt;/a&gt;&lt;/sup&gt; Finally, Pew Research estimates that "more than 5 billion people have mobile devices, and over half of these connections are smartphones."&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11"&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In &lt;a href="https://en.wikipedia.org/wiki/Information_Age"&gt;Information Age&lt;/a&gt;, a dwindling interest in soft power institutions is a &lt;strong&gt;major&lt;/strong&gt; opportunity missed.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a recent article, Joseph Nye, the Harvard professor who coined the term “soft power” itself, neatly summarizes this phenomenon:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“U.S. President Donald Trump’s administration has shown little interest in public diplomacy. And yet public diplomacy—a government’s efforts to communicate directly with other countries’ publics—is one of the key instruments policymakers use to generate soft power, and the current information revolution makes such instruments more important than ever.”&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;A renewed interest in soft power&lt;/h2&gt;
&lt;p&gt;Truly impactful generative models, and Trump’s exit from office, will soon collide head-on.&lt;/p&gt;
&lt;p&gt;At this point, the United States—with its blemished reputation, exorbitant military budget&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;, longstanding technical leadership&lt;sup id="fnref:13"&gt;&lt;a class="footnote-ref" href="#fn:13"&gt;13&lt;/a&gt;&lt;/sup&gt; and forever proactive drive—will be sitting on transformative media tech and a clear-cut mission: rebuild its image at home and abroad.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To this effect, I predict that the United States will soon make a concerted investment in generative models for soft power at true scale.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here’s what this might look like.&lt;/p&gt;
&lt;h2&gt;Soft power through media, in the age of generative models&lt;/h2&gt;
&lt;p&gt;The two keys areas which generative models will impact are state-to-public, and state-to-individual media.&lt;/p&gt;
&lt;h3&gt;State-to-public media&lt;/h3&gt;
&lt;p&gt;Current examples of state-to-public soft power media, like Presidential addresses to the American people, English-language CNN news broadcasts to foreign viewers, and front-line conflict reporting will see their use both quickened and expanded by generative models.&lt;/p&gt;
&lt;p&gt;Presently, Presidential addresses happen once every few weeks or months. Conversely, Donald Trump writes on &lt;a href="https://twitter.com/realDonaldTrump"&gt;Twitter&lt;/a&gt; hourly. With generative models, the next President will &lt;em&gt;instantaneously&lt;/em&gt; generate videos of herself reciting the tweet instead. Americans at large devour the President's tweets; if a generated video legitimately looks and sounds just like her, they’re likely to devour it as well.&lt;/p&gt;
&lt;p&gt;Sponsorship deals, like &lt;a href="https://www.espn.com/nba/story/_/id/14314807/lebron-james-signs-life-deal-nike"&gt;Lebrons’ lifetime appointment&lt;/a&gt; with Nike, will crucially expand to include the right to generate content with the athlete’s likeness. Politicians will travel less to campaign; instead, they’ll send generated holograms—of them, their spouses, their key supporters—to campaign instead—each one personalized to the venue in question.&lt;/p&gt;
&lt;p&gt;Finally, a generative model will be trained to ingest footage of world events—for example, the &lt;a href="https://www.cnn.com/2019/09/14/middleeast/yemen-houthi-rebels-drone-attacks-saudi-aramco-intl/index.html"&gt;attack&lt;/a&gt; on the Saudi oil refinery—and produce text describing what happened. This text will then be used to generate a news report. Humans won’t be on the front line to capture this footage either; why not send a drone instead?&lt;/p&gt;
&lt;h3&gt;State-to-individual media&lt;/h3&gt;
&lt;p&gt;Microsoft’s Xiaoice demonstrates the enormous traction of personalized chatbots. From here, the jump to state-built, propaganda-(subtly)-infused chatbots is small.&lt;/p&gt;
&lt;p&gt;In general, the public distrusts governments; to this effect, humanitarian missions are typically run by NGOs. As such, states will deploy these chatbots on behalf of related entities: tourism boards, online universities, cultural centers, etc.&lt;/p&gt;
&lt;p&gt;Finally, current state-to-individual media is largely &lt;em&gt;unidirectional&lt;/em&gt;: media is sent from the former to the latter, and rarely vice versa. Chatbots powered by generative models, however, will solicit &lt;em&gt;feedback&lt;/em&gt; from their viewer in a personalized way; for example, “Are you enjoying this media? What do you like about American schools as compared to your own? What do you think about democracy?”&lt;/p&gt;
&lt;p&gt;In the same way that the Chinese government &lt;em&gt;passively&lt;/em&gt; learns about its citizens by &lt;a href="http://harvardpolitics.com/world/wechat-the-people-technology-and-social-control-in-china/"&gt;monitoring WeChat data&lt;/a&gt;, generative models will &lt;em&gt;actively solicit information&lt;/em&gt; re the efficacy of their soft power plays. Then, like a savvy marketer, they’ll double down on the stuff that works best.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;For over a century, states have exercised soft power through media. Throughout, the marginal cost of distributing this media has trended towards zero. Conversely, to date, the marginal cost of its creation remains high.&lt;/p&gt;
&lt;p&gt;Generative models are a powerful technology which trend the latter towards zero as well. They're here and they work. However, to be truly impactful, there's still a short ways to go.&lt;/p&gt;
&lt;p&gt;In 5 years' time, a dazed, reputationally-bruised United States, and robust, flexible, practical generative models, will collide head-on.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;At this point, soft power media—and human politics in general—are likely to change forever.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Credit&lt;/h2&gt;
&lt;p&gt;Many thanks to Abishur Prakash for reviewing earlier drafts of this piece.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Brands, Hal. “Not Even Trump Can Obliterate America's Soft Power.” Bloomberg.com, Bloomberg, www.bloomberg.com/opinion/articles/2018-01-18/not-even-trump-can-obliterate-america-s-soft-power. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Chakravarti, Sudeshna. Soft Power: The Culture Weapon in The Cold War and South Asia. www.culturaldiplomacy.org/academy/content/pdf/participant-papers/academy/Sudeshna-Khasnobis-Soft-Power-The-Culture-Weapon-in-The-Cold-War-and-South-Asia.pdf. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Khandelwal, Aakash. “Economics of Digital Goods.” LinkedIn SlideShare, 23 Nov. 2016, www.slideshare.net/aakashkhandelwal921/economics-of-digital-goods. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Nye, Joseph S. “No, President Trump: You've Weakened America's Soft Power.” The New York Times, The New York Times, 25 Feb. 2020, www.nytimes.com/2020/02/25/opinion/trump-soft-power.html. &lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Shah, Ritula. “Is US Monopoly on the Use of Soft Power at an End?” BBC News, BBC, 19 Nov. 2014, www.bbc.com/news/world-29536648. &lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;Nye, Joseph S. Soft Power: the Means to Success in World Politics. Knowledge World, 2012. &lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;Harris, Paul. “Man on the Moon: Moment of Greatness That Defined the American Century.” The Guardian, Guardian News and Media, 25 Aug. 2012, www.theguardian.com/science/2012/aug/25/man-moon-american-century. &lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;“United States Office of War Information.” Wikipedia, Wikimedia Foundation, 7 Jan. 2020, en.wikipedia.org/wiki/United_States_Office_of_War_Information. &lt;a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;Watson, Amy. “TV Households Worldwide.” Statista, 4 Dec. 2019, www.statista.com/statistics/268695/number-of-tv-households-worldwide/. &lt;a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:10"&gt;
&lt;p&gt;Statista Research Department. “How Many People Have Access to a Computer 2018.” Statista, 2 Mar. 2020, www.statista.com/statistics/748551/worldwide-households-with-computer/. &lt;a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:11"&gt;
&lt;p&gt;Silver, Laura. “Smartphone Ownership Is Growing Rapidly Around the World, but Not Always Equally.” Pew Research Center's Global Attitudes Project, Pew Research Center, 30 Dec. 2019, www.pewresearch.org/global/2019/02/05/smartphone-ownership-is-growing-rapidly-around-the-world-but-not-always-equally/. &lt;a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:12"&gt;
&lt;p&gt;Cancian, Mark F. “U.S. Military Forces in FY 2020: The Strategic and Budget Context.” U.S. Military Forces in FY 2020: The Strategic and Budget Context | Center for Strategic and International Studies, 12 Mar. 2020, www.csis.org/analysis/us-military-forces-fy-2020-strategic-and-budget-context?gclid=CjwKCAjwgbLzBRBsEiwAXVIygLkZlNLc3zV8EVVhPPwjOoqrUoQ4kjTeaTHN5vFktRWET2wsnHCwRhoCm2QQAvD_BwE. &lt;a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:13"&gt;
&lt;p&gt;“Report Shows United States Leads in Science and Technology as China Rapidly Advances.” ScienceDaily, ScienceDaily, 24 Jan. 2018, www.sciencedaily.com/releases/2018/01/180124113951.htm. &lt;a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="geopolitics"></category></entry><entry><title>On Saudi Drone Strikes and Adversarial AI</title><link href="https://willwolf.io/2019/09/22/saudi-drone-strikes-and-adversarial-ai/" rel="alternate"></link><published>2019-09-22T08:30:00-04:00</published><updated>2019-09-22T08:30:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2019-09-22:/2019/09/22/saudi-drone-strikes-and-adversarial-ai/</id><summary type="html">&lt;p&gt;In world of weaponized drones piloted by algorithms, what new strategic opportunities arise?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week, Houthi rebels—a militant political group in Yemen—claimed credit for exploding an oil refinery in Saudi Arabia. The explosion was the work of a missile, fired from a drone.&lt;/p&gt;
&lt;p&gt;Though the group responsible is closely allied with Iran—and to wit, the American government does declaim Iran as responsible for facilitating the strike itself—it is by all accounts a non-state actor, notable for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As the cost of full-scale war increases due to, at a trivial minimum, the strength of weapons available, small, “surgical” operations become more common. This attack falls in directly with this trend.&lt;/li&gt;
&lt;li&gt;You do not have to be the state to desire to affect world politics. As weapons become more potent, and the “density” of potential targets—population density, in the case of traditional military offensives, or density of nodes in an information-sharing network, in the case of cyber offensives, as two examples—increases, the ability to affect outsized change increases as well. As such, incentives for, and frequency of, strategic action from non-state actors will likely continue to rise—this being yet another example.&lt;/li&gt;
&lt;li&gt;The technologies required to effectuate the attack in question are increasingly easy to obtain.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In sum, last week’s strike is yet another example of an isolated punch by a non-state actor with technologies (drones, at a minimum) ultimately available in the public domain. Moving forward, it seems safe to expect more of the same.&lt;/p&gt;
&lt;h2&gt;Enter machine learning&lt;/h2&gt;
&lt;p&gt;Throughout history, much of war has been about destroying the enemy’s stuff. There are many ways to do this—each involving a distinct set of tradeoffs.&lt;/p&gt;
&lt;p&gt;To this effect, drones are particularly interesting: they allow for precise, close-proximity strikes, without risking the lives of the aggressors. Presently, the majority of such drones are piloted remotely by humans; moving forward, they—in conceptual step with, say, self-driving cars—will pilot themselves, allowing for larger and larger deployments.&lt;/p&gt;
&lt;p&gt;To do this, engineers will equip drones with cameras, and machine learning algorithms that make the drone’s operational and tactical decisions conditional on what these cameras show. As such, the defensive objective is still, as per usual, thwart the aggressor; &lt;strong&gt;however, the aggressor is now the machine learning algorithms controlling the aggressor’s tech.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For drones, what do these algorithms look like? How can they be thwarted? What risks and opportunities do they imply for the defense of critical infrastructure?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;While these questions might seem futuristic, that future is indeed approaching fast.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The supervised classification model&lt;/h2&gt;
&lt;p&gt;To begin the discussion, let’s look at the basic paradigm by which self-driving cars operate: the supervised classification model, which works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Engineers collect a large amount of “labeled data” in the form of &lt;code&gt;(cameras_images, what_the_driver_did)&lt;/code&gt; pairs. This data is considered “labeled,” as each input, &lt;code&gt;cameras_images&lt;/code&gt;, is coupled with an output, &lt;code&gt;what_the_driver_did&lt;/code&gt;. An example of the former might be 360-degree images of a curving, barren road in Arizona; the latter, i.e. what the driver did in this moment in time, might be: depress the gas pedal by five degrees, and rotate the steering wheel counter-clockwise by seven.”&lt;/li&gt;
&lt;li&gt;“Teach” a computer to, given photographic input, predict the correct driver action to be taken.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Roughly speaking, a self-piloting drone would not be different: it’d require equivalent labeled data from human-controlled flights, on which a supervised classification model would be trained.&lt;/p&gt;
&lt;h2&gt;A likely progression of offensive drone tech&lt;/h2&gt;
&lt;p&gt;Weaponized drone deployment will likely progress as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy drones that pilot themselves; however, “finish” decisions are still executed by humans.&lt;/li&gt;
&lt;li&gt;Deploy drones that pilot themselves; “finish” decisions are executed by the drone—when it is extremely confident in its choice.&lt;/li&gt;
&lt;li&gt;Deploy swarms of drones that work together to more swiftly and efficiently achieve the aforementioned—attacking, exchanging data, self-sacrificing optimally, etc. throughout their attack.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;In a brave new world of machine-learned aggressors, what new opportunities for defense arise?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Adversarial examples in machine learning&lt;/h2&gt;
&lt;p&gt;Adversarial examples for supervised image classification models are inputs, i.e. images, whose pixels have been perturbed in a way imperceptible to a human eye, yet cause the classifier to change its prediction entirely.&lt;/p&gt;
&lt;p&gt;An example&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/images/saudi-adversarial/panda-adversarial-example.png"/&gt;&lt;/p&gt;
&lt;p&gt;Initially, the classifier is 57.7% confident that the image contains a panda; after adding noise, resulting in the image on the right—still, inarguably, a panda, to the human eye—the classifier has now changed its prediction to "gibbon" with 99.3% confidence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As such, knowledge of the data on which an aggressive drone was trained gives unique opportunity to algorithmically design adversarial examples, like the above, that confuse its classifier aboard.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Concrete opportunities would include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;At “find”&lt;/strong&gt;: defensive lasers pointed at critical infrastructure dynamically perturb, by just a few pixels, its appearance, tricking the drone into thinking it has not found what it actually has.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At “fix”&lt;/strong&gt;: these lasers dynamically concoct sequences of perturbations, tricking the drone into thinking its target is moving erratically when it’s actually still.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At “finish”&lt;/strong&gt;: adversarial examples make the drone believe its strike would, put simply, cause more damage than it intends.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As adversarial examples are, once more, built from real examples, perturbed minimally, in a way imperceptible to the human eye, they are in fact difficult to, even a priori, teach our classifier to ignore. Said differently, well-designed adversarial examples are extremely effective.&lt;/p&gt;
&lt;h2&gt;Out-of-distribution data&lt;/h2&gt;
&lt;p&gt;Machine learning algorithms are notoriously bad at “knowing what they don’t know.” Said differently, these algorithms only learn to make decisions about the type of data on which they’re trained. Were a classification model piloting a drone to be presented with an image of an ice-cream cone, it would, barring careful design, attempt to make a decision about this data all the same.&lt;/p&gt;
&lt;p&gt;In a world of autonomous fly-and-finish drones, one would hope that its finish decisions are taken with extreme care. Fundamentally, this dovetails quickly into the notion of “out-of-distribution” data, i.e. data that the classifier knows it has not seen before, and about which it therefore neglects to make a prediction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As such, insight into the data on which an enemy’s system was trained naturally implies “defense by what’s different”&lt;/strong&gt;: show the drone images you know that it hasn’t seen before, and thereby increase its uncertainty around the decision at hand—buying time, and keeping your stuff in tact.&lt;/p&gt;
&lt;h2&gt;Learning optimal defense via reinforcement learning&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;Reinforcement learning&lt;/a&gt;, though exceedingly powerful, and often overhyped, is a relatively simple idea: given an environment and its state, try an action, observe a reward, and repeat the actions that give high rewards; additionally, periodically explore new actions you’ve never tried before just to see how it goes.&lt;/p&gt;
&lt;p&gt;Reinforcement learning, or RL, requires vast amounts of data. As such, as a point of “meta defensive strategy”, trying out different types of adversarial-machine-learning attacks against a drone, seeing which work best, then repeating the best performers, while plausible in theory, would not work in practice if one drone approaches your one oil refinery but once a year.&lt;/p&gt;
&lt;p&gt;This said, swarms of drones might change the game; &lt;em&gt;what if, in 20 years, Houthi militants deployed one autonomous, armed drone for every square mile of Saudi territory?&lt;/em&gt; And what if we could simulate this scenario a priori, and learn how to optimally defend against the swarm?&lt;/p&gt;
&lt;p&gt;To do this, you'd require:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A heavy investment into sensors&lt;/strong&gt;, on all of: the infrastructure you’re trying to protect, atmospheric conditions in which the drones are flying, tools to monitor the drones’ speeds and movements, etc. In other words, any and all technologies that capture the state of the environment, and the rewards, to as granular level of detail as possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simulation environments.&lt;/strong&gt; In certain RL problems, this one potentially included, one has the delicious ability to generate data from which the algorithm can learn—by letting it play games with itself. &lt;a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch"&gt;AlphaGo Zero&lt;/a&gt; is one famous such example. Learning to optimally defend against a swarm of drones might fit the bit as well: deploy thousands of your own into a broad swath of desert, instruct them to “capture your flag”, then let your defensive systems get to work: taking actions, observing the subsequent reward—“Did I divert the drone away from my flag?”; “Did the drone hesitate more than usual before acting?”; etc.—and repeat those actions that work best.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;As the years roll forward, machine learning algorithms will continue to direct more and more of our most critical systems—financial markets, healthcare, transportation, and alas—the offensive technologies of political groups.&lt;/p&gt;
&lt;p&gt;To wit, an understanding of the workings of these algorithms sheds light on the new risks these systems will introduce, and the new strategic opportunities that will thereafter arise.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Goodfellow, I., Shlens, J., Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples https://arxiv.org/abs/1412.6572 &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;(Thumbnail) “Adversarial AI: As New Attack Vector Opens, Researchers Aim to Defend Against It.” DataProtectionCenter.com - Tech and Security, 17 Apr. 2018, dataprotectioncenter.com/malware/adversarial-ai-as-new-attack-vector-opens-researchers-aim-to-defend-against-it/. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="geopolitics"></category></entry><entry><title>Artificial Intelligence and Geopolitics</title><link href="https://willwolf.io/2019/09/21/ai-and-geopolitics/" rel="alternate"></link><published>2019-09-21T21:00:00-04:00</published><updated>2019-09-21T21:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2019-09-21:/2019/09/21/ai-and-geopolitics/</id><summary type="html">&lt;p&gt;I'm beginning to write about the intersection of artificial intelligence and geopolitics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Machine learning and artificial intelligence have long been passions; to date, I've spent many years—my entire professional career included—developing relevant expertise. This has been the historical focus of this blog.&lt;/p&gt;
&lt;p&gt;Recently, I've been reading heavily in other areas as well: history, political theory, strategy, economics and geography; geopolitics, in sum.&lt;/p&gt;
&lt;p&gt;The geopolitics of yesterday—the subject of my reading—has its experts and central themes: collective security, nuclear armament, and oil, to name a few. Conversely, the geopolitics of tomorrow will take form from entirely new ideas; chief among them: artificial intelligence.&lt;/p&gt;
&lt;p&gt;As such, I plan to merge the two: to suffuse, through writing, my exploration of the former with my expertise in the latter. &lt;strong&gt;Specifically, I'll connect deep, technical knowledge of AI algorithms, as well as the likely implications of their widespread use, to the risks—and opportunities—that AI will bring to the geopolitics of tomorrow.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My writing will be objective, i.e. devoid of overt political bias. My intended audience is non-technical, i.e. geopolitics-first. Your comments—especially with respect to my evolving understanding of geopolitics itself—are encouraged as always.&lt;/p&gt;
&lt;p&gt;Will&lt;/p&gt;</content><category term="geopolitics"></category></entry><entry><title>Deriving Mean-Field Variational Bayes</title><link href="https://willwolf.io/2018/11/23/mean-field-variational-bayes/" rel="alternate"></link><published>2018-11-23T10:00:00-05:00</published><updated>2018-11-23T10:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-11-23:/2018/11/23/mean-field-variational-bayes/</id><summary type="html">&lt;p&gt;A detailed derivation of Mean-Field Variational Bayes, its connection to Expectation-Maximization, and its implicit motivation for the "black-box variational inference" methods born in recent years.&lt;/p&gt;</summary><content type="html">&lt;p&gt;"Mean-Field Variational Bayes" (MFVB), is similar to &lt;a href="https://willwolf.io/2018/11/11/em-for-lda/"&gt;expectation-maximization&lt;/a&gt; (EM) yet distinct in two key ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We do not minimize &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt;, i.e. perform the E-step, as [in the problems in which we employ mean-field] the posterior distribution &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt; "is too complex to work with,"™ i.e. it has no analytical form.&lt;/li&gt;
&lt;li&gt;Our variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; is a &lt;em&gt;factorized distribution&lt;/em&gt;, i.e.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
q(\mathbf{Z}) = \prod\limits_i^{M} q_i(\mathbf{Z}_i)
$$&lt;/div&gt;
&lt;p&gt;for all latent variables &lt;span class="math"&gt;\(\mathbf{Z}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Briefly, factorized distributions are cheap to compute: if each &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt; is Gaussian, &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; requires optimization of &lt;span class="math"&gt;\(2M\)&lt;/span&gt; parameters (a mean and a variance for each factor); conversely, a non-factorized &lt;span class="math"&gt;\(q(\mathbf{Z}) = \text{Normal}(\mu, \Sigma)\)&lt;/span&gt; would require optimization of &lt;span class="math"&gt;\(M\)&lt;/span&gt; parameters for the mean and &lt;span class="math"&gt;\(\frac{M^2 + M}{2}\)&lt;/span&gt; parameters for the covariance. Following intuition, this gain in computational efficiency comes at the cost of decreased accuracy in approximating the true posterior over latent variables.&lt;/p&gt;
&lt;h2&gt;So, what is it?&lt;/h2&gt;
&lt;p&gt;Mean-field Variational Bayes is an iterative maximization of the ELBO. More precisely, it is an iterative M-step with respect to the variational factors &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the simplest case, we posit a variational factor over every latent variable, &lt;em&gt;as well as every parameter&lt;/em&gt;. In other words, as compared to the log-marginal decomposition in EM, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is absorbed into &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\quad \text{(EM)}
$$&lt;/div&gt;
&lt;p&gt;becomes&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X})} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\quad \text{(MFVB)}
$$&lt;/div&gt;
&lt;p&gt;From there, we simply maximize the ELBO, i.e. &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]\)&lt;/span&gt;, by &lt;em&gt;iteratively maximizing with respect to each variational factor &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt;&lt;/em&gt; in turn.&lt;/p&gt;
&lt;h2&gt;What's this do?&lt;/h2&gt;
&lt;p&gt;Curiously, we note that &lt;span class="math"&gt;\(\log{p(\mathbf{X})}\)&lt;/span&gt; is a &lt;em&gt;fixed quantity&lt;/em&gt; with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;: updating our variational factors &lt;em&gt;will not change&lt;/em&gt; the marginal log-likelihood of our data.&lt;/p&gt;
&lt;p&gt;This said, we note that the ELBO and &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\)&lt;/span&gt; trade off linearly: when one goes up by &lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;, the other goes down by &lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As such, (iteratively) maximizing the ELBO in MFVB is akin to minimizing the divergence between the true posterior over the latent variables given data and our factorized variational approximation thereof.&lt;/p&gt;
&lt;h2&gt;Derivation&lt;/h2&gt;
&lt;p&gt;So, what do these updates look like?&lt;/p&gt;
&lt;p&gt;First, let's break the ELBO into its two main components:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \int{q(\mathbf{Z})\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}}d\mathbf{Z}\\
&amp;amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z} - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;amp;= A + B
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Next, rewrite this expression in a way that isolates a single variational factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, i.e. the factor with respect to which we'd like to maximize the ELBO in a given iteration.&lt;/p&gt;
&lt;h2&gt;Expanding the first term&lt;/h2&gt;
&lt;div class="math"&gt;$$
\begin{align*}
A
&amp;amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}d\mathbf{Z}}\\
&amp;amp;= \int{\prod\limits_{i}q_i(\mathbf{Z}_i)\log{p(\mathbf{X, Z})}d\mathbf{Z}_i}\\
&amp;amp;= \int{q_j(\mathbf{Z}_j)\bigg[\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i\bigg]}d\mathbf{Z}_j\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Following Bishop&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;'s derivation, we've introduced the notation:&lt;/p&gt;
&lt;div class="math"&gt;$$
\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;A few things to note, and in case this looks strange:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Were the left-hand side to read &lt;span class="math"&gt;\(\int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z}\)&lt;/span&gt;, this would look like the perfectly vanilla expectation &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;An expectation maps a function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, e.g. &lt;span class="math"&gt;\(\log{p(\mathbf{X, Z})}\)&lt;/span&gt;, to a single real number. As our expression reads &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; as opposed to &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;, we're conspicuously unable to integrate over the remaining factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;As such, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; gives a function of the value of &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;&lt;/strong&gt; which itself maps to the aforementioned real number.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To further illustrate, let's employ some toy Python code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Suppose `Z = [Z_0, Z_1, Z_2]`, with corresponding (discrete) variational distributions `q_0`, `q_1`, `q_2`&lt;/span&gt;

&lt;span class="n"&gt;q_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;  &lt;span class="c1"&gt;# q_0(1) = .2&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;q_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;q_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Next, suppose we'd like to isolate Z_2&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;, written &lt;code&gt;E_i_neq_j_log_p_X_Z&lt;/code&gt; below, can be computed as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;E_i_neq_j_log_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;Z_i_neq_j_dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;comb&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Z_i_neq_j_dists&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;comb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;comb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Z_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;comb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Continuing with our notes, it was not immediately obvious to me how and why we're able to introduce a second integral sign on line 3 of the derivation above. Notwithstanding, the reason is quite simple; a simple exercise of nested for-loops is illustrative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before beginning, we remind the definition of an integral in code. In its simplest example, &lt;span class="math"&gt;\(\int{ydx}\)&lt;/span&gt; can be written as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lower_lim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper_lim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_ticks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;integral&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# ...where `n_ticks` approaches infinity.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With this in mind, the following confirms the self-evidence of the second integral sign:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# some dummy expression&lt;/span&gt;


&lt;span class="c1"&gt;# Line 2 of `Expanding the first term`&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt;
            &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt;
            &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt;
            &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;TOTAL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;


&lt;span class="c1"&gt;# Line 3 of `Expanding the first term`&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt;
            &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt;
            &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;_total&lt;/span&gt;


&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;TOTAL&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In effect, isolating &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; is akin to the penultimate line &lt;code&gt;total += prob_z_0 * _total&lt;/code&gt;, i.e. multiplying &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; by an intermediate summation &lt;code&gt;_total&lt;/code&gt;.  Therefore, the second integral sign is akin to &lt;code&gt;_total += prob_z_1 * prob_z_2 * ln_p_X_Z(X, Z)&lt;/code&gt;, i.e. the computation of this intermediate summation itself.&lt;/p&gt;
&lt;p&gt;More succinctly, a multi-dimensional integral can be thought of as a nested-for-loop which commutes a global sum. Herein, we are free to compute intermediate sums at will.&lt;/p&gt;
&lt;h2&gt;Expanding the second term&lt;/h2&gt;
&lt;p&gt;Next, let's expand &lt;span class="math"&gt;\(B\)&lt;/span&gt;. We note that this is the entropy of the full variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
B
&amp;amp;= - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\prod\limits_{i}q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)} + \sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q_{i \neq j}(\mathbf{Z}_i)}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] + \text{const}\\
&amp;amp;= - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As we'll be maximizing w.r.t. just &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, we can set all terms that don't include this factor to constants.&lt;/p&gt;
&lt;h2&gt;Putting it back together&lt;/h2&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= A + B\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;One final pseudonym&lt;/h2&gt;
&lt;p&gt;Were we able to replace the expectation in &lt;span class="math"&gt;\(A\)&lt;/span&gt; with the &lt;span class="math"&gt;\(\log\)&lt;/span&gt; of some density &lt;span class="math"&gt;\(D\)&lt;/span&gt;, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
= \int{q_j(\mathbf{Z}_j){ \log{D} }\ d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(A + B\)&lt;/span&gt; could be rewritten as &lt;span class="math"&gt;\(-\text{KL}(q_j(\mathbf{Z}_j)\Vert D)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Acknowledging that &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; is an unnormalized log-likelihood written as a function of &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;, we temporarily rewrite it as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] = \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j})
$$&lt;/div&gt;
&lt;p&gt;As such:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j}) }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \log{\frac{\tilde{p}(\mathbf{X}, \mathbf{Z}_j)}{q_j(\mathbf{Z}_j)}} }d\mathbf{Z}_j} + \text{const}\\
&amp;amp;= - \text{KL}\big(q_j(\mathbf{Z}_j)\Vert \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\big) + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Finally, per this expression, the ELBO reaches its minimum when:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;amp;= \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\\
&amp;amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Or equivalently:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Summing up:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iteratively minimizing the divergence between &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\tilde{p}(\mathbf{X}, \mathbf{Z}_j)\)&lt;/span&gt; for all factors &lt;span class="math"&gt;\(j\)&lt;/span&gt; is our mechanism for maximizing the ELBO&lt;/li&gt;
&lt;li&gt;In turn, maximizing the ELBO is our mechanism for minimizing the KL divergence between the full factorized posterior &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; and the true posterior &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X})\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, as the optimal density &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; relies on those of &lt;span class="math"&gt;\(q_{i \neq j}(\mathbf{Z}_{i})\)&lt;/span&gt;, this optimization algorithm is necessarily &lt;em&gt;iterative&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Normalization constant&lt;/h2&gt;
&lt;p&gt;Nearing the end, we note that &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j) = \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}\)&lt;/span&gt; is not necessarily a normalized density (over &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;). "By inspection," we compute:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;amp;= \frac{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}}{\int{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}d\mathbf{Z}_j}}\\
&amp;amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)} + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;How to actually employ this thing&lt;/h2&gt;
&lt;p&gt;First, plug in values for the right-hand side of:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;Then, attempt to rearrange this expression such that:&lt;/p&gt;
&lt;p&gt;Once exponentiated, giving &lt;span class="math"&gt;\(\exp{\big(\log{q_j(\mathbf{Z}_j)}\big)} = q_j(\mathbf{Z}_j)\)&lt;/span&gt;, we are left with something that, once normalized (by inspection), resembles a known density function (e.g. a Gaussian, a Gamma, etc.).&lt;/p&gt;
&lt;p&gt;NB: This may require significant computation.&lt;/p&gt;
&lt;h1&gt;Approximating a Gaussian&lt;/h1&gt;
&lt;p&gt;Here, we'll approximate a 2D multivariate Gaussian with a factorized mean-field approximation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-3.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-4.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-5.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Summing up&lt;/h1&gt;
&lt;p&gt;Mean-Field Variational Bayes is an iterative optimization algorithm for maximizing a lower-bound of the marginal likelihood of some data &lt;span class="math"&gt;\(\mathbf{X}\)&lt;/span&gt; under a given model with latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;. It accomplishes this task by positing a factorized variational distribution over all latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt; and parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, then computes, &lt;em&gt;analytically&lt;/em&gt;, the algebraic forms and parameters of each factor which maximize this bound.&lt;/p&gt;
&lt;p&gt;In practice, this process can be cumbersome and labor-intensive. As such, in recent years, "black-box variational inference" techniques were born, which &lt;em&gt;fix&lt;/em&gt; the forms of each factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, then optimize its parameters via gradient descent.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Deriving Expectation-Maximization</title><link href="https://willwolf.io/2018/11/11/em-for-lda/" rel="alternate"></link><published>2018-11-11T16:00:00-05:00</published><updated>2018-11-11T16:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-11-11:/2018/11/11/em-for-lda/</id><summary type="html">&lt;p&gt;Deriving the expectation-maximization algorithm, and the beginnings of its application to LDA. Once finished, its intimate connection to variational inference is apparent.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Consider a model with parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; and latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;; the expectation-maximization algorithm (EM) is a mechanism for computing the values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that, under this model, maximize the likelihood of some observed data &lt;span class="math"&gt;\(\mathbf{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The joint probability of our model can be written as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\mathbf{X}, \mathbf{Z}\vert \theta) = p(\mathbf{X}\vert \mathbf{Z}, \theta)p(\mathbf{Z}\vert \theta)
$$&lt;/div&gt;
&lt;p&gt;where, once more, our stated goal is to maximize the marginal likelihood of our data:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \log{\sum_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}
$$&lt;/div&gt;
&lt;p&gt;An example of a latent variable model is the Latent Dirichlet Allocation&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; (LDA) model for uncovering latent topics in documents of text. Once finished deriving the general EM equations, we'll (begin to) apply them to this model.&lt;/p&gt;
&lt;h2&gt;Why not maximum likelihood estimation?&lt;/h2&gt;
&lt;p&gt;As the adage goes, computing the MLE with respect to this marginal is "hard." For one, it requires summing over an (implicitly) humongous number of configurations of latent variables &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Further, as Bishop&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A key observation is that the summation over the latent variables appears inside the logarithm. Even if the joint distribution &lt;span class="math"&gt;\(p(\mathbf{X, Z}\vert\theta)\)&lt;/span&gt; belongs to the exponential family, the marginal distribution &lt;span class="math"&gt;\(p(\mathbf{X}\vert\theta)\)&lt;/span&gt; typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;We'll want something else to maximize instead.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;A lower bound&lt;/h2&gt;
&lt;p&gt;Instead of maximizing the log-marginal &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; (with respect to model parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;), let's maximize a lower-bound with a less-problematic form.&lt;/p&gt;
&lt;p&gt;Perhaps, we'd work with &lt;span class="math"&gt;\(\log{p(\mathbf{X}, \mathbf{Z}\vert \theta)}\)&lt;/span&gt; which, almost tautologically, removes the summation over latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As such, let's derive a lower-bound which features this term. As &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; is often called the log-"evidence," we'll call our expression the "evidence lower-bound," or ELBO.&lt;/p&gt;
&lt;h2&gt;Jensen's inequality&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"&gt;Jensen's inequality&lt;/a&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; generalizes the statement that the line secant to a &lt;strong&gt;concave function&lt;/strong&gt; lies below this function. An example is illustrative:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://alliance.seas.upenn.edu/~cis520/dynamic/2017/wiki/uploads/Lectures/jensen.png"/&gt;&lt;/p&gt;
&lt;p&gt;First, we note that the red line is below the blue for all points for which it is defined.&lt;/p&gt;
&lt;p&gt;Second, working through the example, and assuming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(f(x) = \exp(-(x - 2)^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(v_1 = 1; v_2 = 2.5; \alpha = .3\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{align*}
f(v_1) &amp;amp;\approx .3679\\
f(v_2) &amp;amp;\approx .7788\\
\alpha f(v_1) + (1 - \alpha)f(v_2) &amp;amp;\approx \bf{.6555}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\alpha v_1 + (1 - \alpha)v_2 &amp;amp;= 2.05\\
f(\alpha v_1 + (1 - \alpha)v_2) &amp;amp;\approx \bf{.9975}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;we see that &lt;strong&gt;&lt;span class="math"&gt;\(\alpha f(v_1) + (1 - \alpha)f(v_2) \leq f(\alpha v_1 + (1 - \alpha)v_2)\)&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we arrive at a general form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}_{v}}[f(v)] \leq f(\mathop{\mathbb{E}_{v}}[v])
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(p(v) = \alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Deriving the ELBO&lt;/h2&gt;
&lt;p&gt;In trying to align &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}
= \log{\sum\limits_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}\)&lt;/span&gt; with &lt;span class="math"&gt;\(f(\mathop{\mathbb{E}_{v}}[v])\)&lt;/span&gt;, we see a function &lt;span class="math"&gt;\(f = \log\)&lt;/span&gt; yet no expectation inside. However, given the summation over &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;, introducing some distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; would give us the expectation we desire.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{p(\mathbf{X}\vert\theta)}
&amp;amp;= \log{\sum_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}\\
&amp;amp;= \log{\sum_{\mathbf{Z}}q(\mathbf{Z})\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\\
&amp;amp;= \log{\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; is some distribution over &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt; with parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; (omitted for cleanliness) and known form (e.g. a Gaussian). It is often referred to as a &lt;strong&gt;variational distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;From here, via Jensen's inequality, we can derive the lower-bound:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{p(\mathbf{X}\vert\theta)} = \log{\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}\bigg]}
&amp;amp;\geq \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + R
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Et voilà&lt;/em&gt;, we see that this term contains &lt;span class="math"&gt;\(\log{p(\mathbf{X, Z}\vert\theta)}\)&lt;/span&gt;; the ELBO should now be easier to optimize with respect to our parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;So, what's &lt;span class="math"&gt;\(R\)&lt;/span&gt;?&lt;/h1&gt;
&lt;div class="math"&gt;$$
\begin{align*}
R
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta) - \log{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X}\vert\theta)}\bigg] -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg(\log{p(\mathbf{X}\vert\theta)} - \log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} - \log{p(\mathbf{X}\vert\theta)} + \log{q(\mathbf{Z})}\bigg)\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg( - \log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{q(\mathbf{Z})}\bigg)\\
&amp;amp;=
\sum_{\mathbf{Z}}q(\mathbf{Z})\log{\frac{q(\mathbf{Z})}{p(\mathbf{Z}\vert\mathbf{X}, \theta)}}\\
&amp;amp;= \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Putting it back together:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)
$$&lt;/div&gt;
&lt;h2&gt;The EM algorithm&lt;/h2&gt;
&lt;p&gt;The algorithm can be described by a few simple observations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; is a divergence metric which is strictly non-negative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; does not depend on &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;—if we decrease &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; by changing &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;, the ELBO must increase to compensate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we increase the ELBO by changing &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; will increase as well. In addition, as &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt; now (likely) diverges from &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; in non-zero amount, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; will increase even more.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;The EM algorithm is a repeated alternation between Step 2 (E-step) and Step 3 (M-step).&lt;/strong&gt; After each M-Step, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; is guaranteed to increase (unless it is already at a maximum)&lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;A graphic&lt;sup id="fnref3:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; is further illustrative.&lt;/p&gt;
&lt;h3&gt;Initial decomposition&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/initial_decomp.png"/&gt;&lt;/p&gt;
&lt;p&gt;Here, the ELBO is written as &lt;span class="math"&gt;\(\mathcal{L}(q, \theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;E-step&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/e_step.png"/&gt;&lt;/p&gt;
&lt;p&gt;In other words, holding the parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; constant, minimize &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;. Remember, as &lt;span class="math"&gt;\(q\)&lt;/span&gt; is a distribution with a fixed functional form, this amounts to updating its parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The caption implies that we can always compute &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt;. We will show below that this is not the case for LDA, nor for many interesting models.&lt;/p&gt;
&lt;h3&gt;M-step&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/m_step.png"/&gt;&lt;/p&gt;
&lt;p&gt;In other words, in the M-step, maximize the ELBO with respect to the model parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Expanding the ELBO:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta)}\bigg] + \mathbf{H}[q(\mathbf{Z})]
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;we see that it decomposes into an expectation of the joint distribution over data and latent variables given parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; with respect to the variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;, plus the entropy of &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As our task is to maximize this expression with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we can treat the latter term as a constant.&lt;/p&gt;
&lt;h2&gt;EM for LDA&lt;/h2&gt;
&lt;p&gt;To give an example of the above, we'll examine the classic Latent Dirichlet Allocation&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; paper.&lt;/p&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/em-for-lda/lda_formulation.png"/&gt;&lt;/p&gt;
&lt;p&gt;"Given the parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, the joint distribution of a topic mixture &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; topics &lt;span class="math"&gt;\(\mathbf{z}\)&lt;/span&gt;, and a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; words &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; is given by:"&lt;sup id="fnref3:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta, \mathbf{z}, \mathbf{w}\vert \alpha, \beta) = p(\theta\vert \alpha)\prod\limits_{n=1}^{N}p(z_n\vert \theta)p(w_n\vert z_n, \beta)
$$&lt;/div&gt;
&lt;h3&gt;Log-evidence&lt;/h3&gt;
&lt;p&gt;The (problematic) log-evidence of a single document:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{w}\vert \alpha, \beta)} = \log{\int p(\theta\vert \alpha)\prod\limits_{n=1}^{N}\sum\limits_{z_n} p(z_n\vert \theta)p(w_n\vert z_n, \beta)d\theta}
$$&lt;/div&gt;
&lt;p&gt;NB: The parameters of our model are &lt;span class="math"&gt;\(\{\alpha,  \beta\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{\theta, \mathbf{z}\}\)&lt;/span&gt; are our latent variables.&lt;/p&gt;
&lt;h3&gt;ELBO&lt;/h3&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\bigg(\frac{p(\theta\vert \alpha)}{q(\mathbf{Z})}}\prod\limits_{n=1}^{N}p(z_n\vert \theta)p(w_n\vert z_n, \beta)\bigg)\bigg]
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{Z} = \{\theta, \mathbf{z}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;KL term&lt;/h3&gt;
&lt;div class="math"&gt;$$
\text{KL}\big(q(\mathbf{Z})\Vert \frac{p(\theta, \mathbf{z}, \mathbf{w}\vert \alpha, \beta)}{p(\mathbf{w}\vert \alpha, \beta)}\big)
$$&lt;/div&gt;
&lt;p&gt;Peering at the denominator, we see that it includes an integration over all values &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, which we assume is intractable to compute. As such, the "ideal" E-step solution &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\theta, \mathbf{z}\vert \mathbf{w}, \alpha, \beta)\)&lt;/span&gt; will elude us as well.&lt;/p&gt;
&lt;p&gt;In the next post, we'll cover how to minimize this KL term with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; in detail. This effort will begin with the derivation of the mean-field algorithm.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we motivated the expectation-maximization algorithm then derived its general form. We then applied this framework to the LDA model.&lt;/p&gt;
&lt;p&gt;In the next post, we'll expand this logic into mean-field variational Bayes, and eventually, variational inference more broadly.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Wikipedia contributors. "Jensen's inequality." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 29 Oct. 2018. Web. 11 Nov. 2018. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Additional Strategies for Confronting the Partition Function</title><link href="https://willwolf.io/2018/10/29/additional-strategies-partition-function/" rel="alternate"></link><published>2018-10-29T22:00:00-04:00</published><updated>2018-10-29T22:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-10-29:/2018/10/29/additional-strategies-partition-function/</id><summary type="html">&lt;p&gt;Stochastic maximum likelihood, contrastive divergence, negative contrastive estimation and negative sampling for improving or avoiding the computation of the gradient of the log-partition function. (Oof, that's a mouthful.)&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="https://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/"&gt;previous post&lt;/a&gt; we introduced Boltzmann machines and the infeasibility of computing the gradient of its log-partition function &lt;span class="math"&gt;\(\nabla\log{Z}\)&lt;/span&gt;. To this end, we explored one strategy for its approximation: Gibbs sampling. Gibbs sampling is a viable alternative because the expression for our gradient simplifies to an expectation over the model distribution, which can be approximated with Monte Carlo samples.&lt;/p&gt;
&lt;p&gt;In this post, we'll highlight the imperfections of this approximate approach itself, then present more preferable alternatives.&lt;/p&gt;
&lt;h1&gt;Pitfalls of Gibbs sampling&lt;/h1&gt;
&lt;p&gt;To refresh, the two gradients we seek to compute in a reasonable amount of time are:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{Z} = \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\\
\nabla_{b_{i}}\log{Z} = \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$&lt;/div&gt;
&lt;p&gt;Via Gibbs sampling, we approximate each by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Burning in a Markov chain with respect to our model, then selecting &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples from this chain&lt;/li&gt;
&lt;li&gt;Evaluating both functions (&lt;span class="math"&gt;\(x_i  x_j\)&lt;/span&gt;, and &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;) at these samples&lt;/li&gt;
&lt;li&gt;Taking the average of each&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Concretely:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{Z} \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}\\
\nabla_{b_{i}}\log{Z} \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;We perform this sampling process at each gradient step.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The cost of burning in each chain&lt;/h2&gt;
&lt;p&gt;Initializing a Markov chain at a random sample incurs a non-trivial "burn-in" cost. If paying this cost at each gradient step, it begins to add up. How can we do better?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the remainder of the post, we'll explore two new directives for approximating the negative phase more cheaply, and the algorithms they birth.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Directive #1: Cheapen the burn-in process&lt;/h1&gt;
&lt;h2&gt;Stochastic maximum likelihood&lt;/h2&gt;
&lt;p&gt;SML assumes the premise: let's initialize our chain at a point already close to the model's true distribution—reducing or perhaps eliminating the cost of burn-in altogether.  &lt;strong&gt;In this vein, at what sample do we initialize the chain?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In SML, we simply initialize at the terminal value of the previous chain (i.e. the one we manufactured to compute the gradients of the previous mini-batch). &lt;strong&gt;As long as the model has not changed significantly since, i.e. as long as the previous parameter update (gradient step) was not too large, this sample should exist in a region of high probability under the current model.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Implications&lt;/h3&gt;
&lt;p&gt;Per the expression for the full log-likelihood gradient, e.g. &lt;span class="math"&gt;\(\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, the negative phase works to "reduce the probability of the points in which the model strongly, yet wrongly, believes".&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; Since we approximate this term at each parameter update with samples &lt;em&gt;roughly from&lt;/em&gt; the current model's true distribution, &lt;strong&gt;we do not encroach on this foundational task.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Contrastive divergence&lt;/h2&gt;
&lt;p&gt;Alternatively, in the contrastive divergence algorithm, we initialize the chain at each gradient step with a &lt;em&gt;random sample&lt;/em&gt; from the data distribution.&lt;/p&gt;
&lt;h3&gt;Implications&lt;/h3&gt;
&lt;p&gt;With no guarantee that the data distribution resembles the model distribution, we may systematically fail to sample, and thereafter "suppress," points that are incorrectly likely under the latter (as they do not appear in the former!). &lt;strong&gt;This incurs the growth of "spurious modes"&lt;/strong&gt; in our model, aptly named.&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;p&gt;Cheapening the burn-in phase indeed gives us a more efficient training routine. Moving forward, what are some even more aggressive strategies we might explore?&lt;/p&gt;
&lt;h1&gt;Directive #2: Skip the computation of &lt;span class="math"&gt;\(Z\)&lt;/span&gt; altogether&lt;/h1&gt;
&lt;p&gt;Canonically, we write the log-likelihood of our Boltzmann machine as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{\mathcal{L}(x)}
&amp;amp;= \log{\frac{\exp{(H(x))}}{Z}}\\
&amp;amp;= \log{\big(\exp{(H(x))}\big)} - \log{Z}\\
&amp;amp;= H(x) - \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Instead, what if we simply wrote this as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{\mathcal{L}(x)} = H(x) - c
$$&lt;/div&gt;
&lt;p&gt;or, more generally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p_{\text{model}}(x)} = \log{\tilde{p}_{\text{model}}(x; \theta)} - c
$$&lt;/div&gt;
&lt;p&gt;and estimated &lt;span class="math"&gt;\(c\)&lt;/span&gt; as a parameter?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Immediately, we remark that if we optimize this model with maximum likelihood, our algorithm will, trivially, make &lt;span class="math"&gt;\(c\)&lt;/span&gt; arbitrarily negative.&lt;/strong&gt; In other words, the easiest way to increase &lt;span class="math"&gt;\(\log{p_{\text{model}}(x)}\)&lt;/span&gt; is to decrease &lt;span class="math"&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How might we better phrase this problem?&lt;/p&gt;
&lt;h2&gt;Noise contrastive estimation&lt;/h2&gt;
&lt;p&gt;Ingeniously, NCE proposes an alternative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Posit two distributions: the model, and a noise distribution&lt;/li&gt;
&lt;li&gt;Given a data point, predict the distribution (i.e. binary classification) from which this point was generated&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's unpack this a bit.&lt;/p&gt;
&lt;p&gt;Under an (erroneous) MLE formulation, we would optimize the following objective:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x \sim p_{\text{data}}} [\log{p_{\text{model}}}(x)]
$$&lt;/div&gt;
&lt;p&gt;Under NCE, we're going to replace two pieces so as to perform the binary classification task described above (with 1 = "model", and 0 = "noise").&lt;/p&gt;
&lt;p&gt;First, let's swap &lt;span class="math"&gt;\(\log{p_{\text{model}}}(x)\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log{p_{\text{joint}}}(y = 0\vert x)\)&lt;/span&gt;, where:&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(x\vert y) =
\begin{cases}
p_{\text{noise}}(x)\quad y = 0\\
p_{\text{model}}(x)\quad y = 1\\
\end{cases}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(x, y)
= p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(y = 0\vert x)
= \frac{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)}
$$&lt;/div&gt;
&lt;p&gt;Finally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x \sim p_{\text{data}}} [\log{p_{\text{joint}}(y = 0\vert x)}]
$$&lt;/div&gt;
&lt;p&gt;From here, we need to update &lt;span class="math"&gt;\(x \sim p_{\text{data}}\)&lt;/span&gt; to include &lt;span class="math"&gt;\(y\)&lt;/span&gt;. We'll do this in two pedantic steps.&lt;/p&gt;
&lt;p&gt;First, let's write:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x, y=0\ \sim\ p_{\text{noise}}} [\log{p_{\text{joint}}(y\vert x)}]
$$&lt;/div&gt;
&lt;p&gt;This equation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Builds a classifier that discriminates between samples generated from the model distribution and noise distribution &lt;strong&gt;trained only on samples from the latter.&lt;/strong&gt; (Clearly, this will not make for an effective classifier.)&lt;/li&gt;
&lt;li&gt;To train this classifier, we note that the equation asks us to maximize the likelihood of the noise samples under the noise distribution—where the noise distribution itself has no actual parameters we intend to train!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In solution, we trivially expand our expectation to one over both noise samples, and data samples. In doing so, in predicting &lt;span class="math"&gt;\(\log{p_{\text{joint}}(y = 1\vert x)} = 1 - \log{p_{\text{joint}}(y = 0\vert x)}\)&lt;/span&gt;, &lt;strong&gt;we'll be maximizing the likelihood of the data under the model.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x, y\ \sim\ p_{\text{train}}} [\log{p_{\text{joint}}(y \vert x)}]
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\text{train}}(x\vert y) =
\begin{cases}
p_{\text{noise}}(x)\quad y = 0\\
p_{\text{data}}(x)\quad y = 1\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;As a final step, we'll expand our object into something more elegant:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)}\\
&amp;amp;= \frac{1}{1 + \frac{p_{\text{joint}}(y = 1)p_{\text{model}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;em&gt;a priori&lt;/em&gt; that &lt;span class="math"&gt;\(p_{\text{joint}}(x, y)\)&lt;/span&gt; is &lt;span class="math"&gt;\(k\)&lt;/span&gt; times more likely to generate a noise sample, i.e. &lt;span class="math"&gt;\(\frac{p_{\text{joint}}(y = 1)}{p_{\text{joint}}(y = 0)} = \frac{1}{k}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{p_{\text{noise}}(x)\cdot k}}\\
&amp;amp;= \frac{1}{1 + \exp\big(\log{\frac{p_{\text{model}}(x)}{{p_{\text{noise}}(x)\cdot k}}}\big)}\\
&amp;amp;= \sigma\bigg(-\log{\frac{p_{\text{model}}(x)}{{p_{\text{noise}}(x)\cdot k}}}\bigg)\\
&amp;amp;= \sigma\bigg(\log{k} + \log{p_{\text{noise}}(x)} - \log{p_{\text{model}}(x)}\bigg)\\
p_{\text{joint}}(y = 1\vert x)
&amp;amp;= 1 - \sigma\bigg(\log{k} + \log{p_{\text{noise}}(x)} - \log{p_{\text{model}}(x)}\bigg)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Given a joint training distribution over &lt;span class="math"&gt;\((X_{\text{data}}, y=1)\)&lt;/span&gt; and &lt;span class="math"&gt;\((X_{\text{noise}}, y=0)\)&lt;/span&gt;, this is the target we'd like to maximize.&lt;/p&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;For our training data, &lt;strong&gt;we require the ability to sample from our noise distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For our target, &lt;strong&gt;we require the ability to compute the likelihood of some data under our noise distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Therefore, these criteria do place practical restrictions on the types of noise distributions that we're able to consider.&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;We briefly alluded to the fact that our noise distribution is non-parametric. However, there is nothing stopping us from evolving this distribution and giving it trainable parameters, then updating these parameters such that it generates increasingly "optimal" samples.&lt;/p&gt;
&lt;p&gt;Of course, we would have to design what "optimal" means. One interesting approach is called &lt;a href="https://arxiv.org/abs/1805.03642"&gt;Adversarial Contrastive Estimation
&lt;/a&gt;, wherein the authors adapt the noise distribution to generate increasingly "harder negative examples, which forces the main model to learn a better representation of the data."&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;Negative sampling is the same as NCE except:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We consider noise distributions whose likelihood we cannot evaluate&lt;/li&gt;
&lt;li&gt;To accommodate, we simply set &lt;span class="math"&gt;\(p_{\text{noise}}(x) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{p_{\text{noise}}(x)\cdot k}}\\
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{ k}}\\
&amp;amp;= \frac{1}{1 + \exp\big(\log\frac{p_{\text{model}}(x)}{ k}\big)}\\
&amp;amp;=\sigma(-\log\frac{p_{\text{model}}(x)}{ k})\\
&amp;amp;=\sigma(\log{k} - \log{p_{\text{model}}(x)})\\
p_{\text{joint}}(y = 1\vert x)
&amp;amp;= 1 - \sigma(\log{k} - \log{p_{\text{model}}(x)})
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;In code&lt;/h2&gt;
&lt;p&gt;Since I learn best by implementing things, let's play around. Below, we train Boltzmann machines via noise contrastive estimation and negative sampling.&lt;/p&gt;
&lt;h2&gt;Load data&lt;/h2&gt;
&lt;p&gt;For this exercise, we'll fit a Boltzmann machine to the &lt;a href="https://www.kaggle.com/zalando-research/fashionmnist"&gt;Fashion MNIST&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/additional-strategies-partition-function/output_3_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Define model&lt;/h2&gt;
&lt;p&gt;Below, as opposed to in the previous post, I offer a vectorized implementation of the Boltzmann energy function.&lt;/p&gt;
&lt;p&gt;This said, the code is still imperfect: especially re: the line in which I iterate through data points individually to compute the joint likelihood.&lt;/p&gt;
&lt;p&gt;Finally, in &lt;code&gt;Model._H&lt;/code&gt;, I divide by 1000 to get this thing to train. The following is only a toy exercise (like many of my posts); I did not spend much time tuning parameters.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xavier_uniform_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diagonal_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diagonal_mask&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;triu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;            where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;            for which to compute the joint likelihood&lt;/span&gt;

&lt;span class="sd"&gt;        :return: the likelihood, or log-likelihood if `log=True`&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
        &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;

        &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# make copy&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Noise contrastive estimation&lt;/h2&gt;
&lt;p&gt;Train a model using noise contrastive estimation. For our noise distribution, we'll start with a diagonal multivariate Gaussian, from which we can sample, and whose likelihood we can evaluate (as of PyTorch 0.4!).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# define model, noise distribution&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MultivariateNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;covariance_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# define classifier. we add a multiplicative constant to make training more stable.&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# define noise generator&lt;/span&gt;
&lt;span class="n"&gt;noise_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;noiseloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise_sample&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define optimizer, loss&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BCELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

            &lt;span class="c1"&gt;# points from data distribution&lt;/span&gt;
            &lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
            &lt;span class="n"&gt;X_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

            &lt;span class="c1"&gt;# points from noise distribution&lt;/span&gt;
            &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;
            &lt;span class="n"&gt;X_noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

            &lt;span class="c1"&gt;# stack into single input&lt;/span&gt;
            &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_noise&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Batch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; | Loss: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'weights.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'bias.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'c: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'weights.grad.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'bias.grad.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'c.grad: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train model&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.305&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0887&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0794&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0603&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0525&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0503&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0414&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.038&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.034&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0312&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;Next, we'll try negative sampling using some actual images as negative samples&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;noiseset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'data/mnist'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noiseloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noiseset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get some random training images&lt;/span&gt;
&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# show images&lt;/span&gt;
&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torchvision&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/additional-strategies-partition-function/output_12_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Train model&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# define model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define classifier&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# i had to change this learning rate to get this to train&lt;/span&gt;

&lt;span class="c1"&gt;# train&lt;/span&gt;
&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.304&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.027&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0111&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00611&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00505&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00318&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00284&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0029&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0023&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00217&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Sampling&lt;/h1&gt;
&lt;p&gt;Once more, the (ideal) goal of this model is to fit a function &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; to some data, such that we can:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluate its likelihood (wherein it actually tells us that data on which the model was fit is more likely than data on which it was not)&lt;/li&gt;
&lt;li&gt;Draw realistic samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From a Boltzmann machine, our primary strategy for drawing samples is via Gibbs sampling. It's slow, and I do not believe it's meant to work particularly well. Let's draw 5 samples and see how we do.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;

&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;CPU times: user 4min 10s, sys: 4.09 s, total: 4min 14s&lt;/span&gt;
&lt;span class="err"&gt;Wall time: 4min 17s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Takes forever!&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/additional-strategies-partition-function/output_18_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Nothing great. These samples are highly correlated, if perfectly identical, as expected.&lt;/p&gt;
&lt;p&gt;To generate better images, we'll have to let this run for a lot longer and "thin" the chain (taking every &lt;code&gt;every_n&lt;/code&gt; samples, where &lt;code&gt;every_n&lt;/code&gt; is on the order of 1, 10, or 100, roughly).&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, we discussed four additional strategies for both speeding up, as well as outright avoiding, the computation of the gradient of the log-partition function &lt;span class="math"&gt;\(\nabla\log{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While we only presented toy models here, these strategies see successful application in larger undirected graphical models, as well as directed conditional models for &lt;span class="math"&gt;\(p(y\vert x)\)&lt;/span&gt;. One key example of the latter is a language model; though the partition function is a sum over distinct values of &lt;span class="math"&gt;\(y\)&lt;/span&gt; (labels) instead of configurations of &lt;span class="math"&gt;\(x\)&lt;/span&gt; (inputs), it can still be intractable to compute! This is because there are as many distinct values of &lt;span class="math"&gt;\(y\)&lt;/span&gt; as there are tokens in the given language's vocabulary, which is typically on the order of millions.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/boltzmann-machines"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-2.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;@book{Goodfellow-et-al-2016,
title={Deep Learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
publisher={MIT Press},
note={\url{http://www.deeplearningbook.org}},
year={2016}
} &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1805.03642"&gt;Adversarial Contrastive Estimation&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>A Thorough Introduction to Boltzmann Machines</title><link href="https://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/" rel="alternate"></link><published>2018-10-20T14:00:00-04:00</published><updated>2018-10-20T14:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-10-20:/2018/10/20/thorough-introduction-to-boltzmann-machines/</id><summary type="html">&lt;p&gt;A pedantic walk through Boltzmann machines, with focus on the computational thorn-in-side of the partition function.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The principal task of machine learning is to fit a model to some data. In programming terms, this model is an object with two methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;How likely is the query point &lt;span class="math"&gt;\(x\)&lt;/span&gt; under our model? In other words, how likely was it that our model produced &lt;span class="math"&gt;\(x\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Note: The likelihood gives a value proportional to a valid probability, but is not necessarily a valid probability itself.&lt;/p&gt;
&lt;h2&gt;Sample&lt;/h2&gt;
&lt;p&gt;Draw a sample datum &lt;span class="math"&gt;\(x\)&lt;/span&gt; from the model.&lt;/p&gt;
&lt;h2&gt;Notation&lt;/h2&gt;
&lt;p&gt;Canonically, we denote an instance of our &lt;code&gt;Model&lt;/code&gt; in mathematical syntax as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
x \sim p(x)
$$&lt;/div&gt;
&lt;p&gt;Again, this simple notation implies two powerful methods: that we can evaluate the &lt;code&gt;likelihood&lt;/code&gt; of having observed &lt;span class="math"&gt;\(x\)&lt;/span&gt; under our model &lt;span class="math"&gt;\(p\)&lt;/span&gt;, and that we can &lt;code&gt;sample&lt;/code&gt; a new value &lt;span class="math"&gt;\(x\)&lt;/span&gt; from our model &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Often, we work instead with &lt;em&gt;conditional&lt;/em&gt; models, e.g. &lt;span class="math"&gt;\(y \sim p(y\vert x)\)&lt;/span&gt;, in classification and regression tasks. The &lt;code&gt;likelihood&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt; methods apply all the same.&lt;/p&gt;
&lt;h2&gt;Boltzmann machines&lt;/h2&gt;
&lt;p&gt;A Boltzmann machine is one of the simplest mechanisms for modeling &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. It is an undirected graphical model where every dimension &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; of a given observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; influences every other dimension. As such, we might use it to model data which we believe to exhibit this property, e.g. an image (where intuitively, pixel values influence neighboring pixel values). For &lt;span class="math"&gt;\(x \in R^3\)&lt;/span&gt;, our model would look as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/boltzmann-machine.svg"/&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(x \in R^n\)&lt;/span&gt;, a given node &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; would have &lt;span class="math"&gt;\(n - 1\)&lt;/span&gt; outgoing connections in total—one to every other node &lt;span class="math"&gt;\(x_j\ \forall\ j \neq i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, a Boltzmann machine strictly operates on &lt;em&gt;binary&lt;/em&gt; data. This keeps things simple.&lt;/p&gt;
&lt;h2&gt;Computing the likelihood&lt;/h2&gt;
&lt;p&gt;A Boltzmann machines admits the following formula for computing the &lt;code&gt;likelihood&lt;/code&gt; of data points &lt;span class="math"&gt;\(x^{(1)}, ..., x^{(n)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x) = \sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p(x) = \frac{\exp{(H(x))}}{Z}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{i=1}^n p(x^{(i)})
$$&lt;/div&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since our weights can be negative, &lt;span class="math"&gt;\(H(x)\)&lt;/span&gt; can be negative. Since our likelihood is proportional to a valid probability, we'd prefer it to be non-negative.&lt;/li&gt;
&lt;li&gt;To enforce this constraint, we exponentiate &lt;span class="math"&gt;\(H(x)\)&lt;/span&gt; in the second equation.&lt;/li&gt;
&lt;li&gt;To normalize, we divide by the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt;, i.e. the sum of the likelihoods of all possible values of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Computing the partition function by hand&lt;/h2&gt;
&lt;p&gt;In the case of 2-dimensional binary datum &lt;span class="math"&gt;\(x\)&lt;/span&gt;, there are 4 possible "configurations": &lt;span class="math"&gt;\([0, 0], [0, 1], [1, 0], [1, 1]\)&lt;/span&gt;. As such, to compute the likelihood of one of these configurations, e.g.&lt;/p&gt;
&lt;div class="math"&gt;$$
p([1, 0]) = \frac{\exp{(H([1, 0]))}}{\exp{(H([0, 0]))} + \exp{(H([0, 1]))} + \exp{(H([1, 0]))} + \exp{(H([1, 1]))}}
$$&lt;/div&gt;
&lt;p&gt;we see that the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt; is a sum of 4 terms.&lt;/p&gt;
&lt;p&gt;More generally, given &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional &lt;span class="math"&gt;\(x\)&lt;/span&gt;, where each &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; can assume one of &lt;span class="math"&gt;\(v\)&lt;/span&gt; distinct values, computing &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; implies a summation over &lt;span class="math"&gt;\(v^d\)&lt;/span&gt; terms. &lt;strong&gt;With a non-trivially large &lt;span class="math"&gt;\(v\)&lt;/span&gt; or &lt;span class="math"&gt;\(d\)&lt;/span&gt; this becomes intractable to compute.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below, we'll demonstrate how "tractability," i.e. "can we actually compute &lt;span class="math"&gt;\(Z\)&lt;/span&gt; before the end of the universe?" changes with varying &lt;span class="math"&gt;\(d\)&lt;/span&gt; for our Boltzmann machine (of &lt;span class="math"&gt;\(v = 2\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;The likelihood function in code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;        where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;        for which to compute the joint likelihood.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
    &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="c1"&gt;# compute partition function&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;NB: In mathematical Python code, for-loops are bad; we should prefer &lt;code&gt;numpy&lt;/code&gt; instead. Nevertheless, I've used for-loops here because they are easier to read.&lt;/p&gt;
&lt;p&gt;This code block is longer than you might expect because it includes a few supplementary behaviors, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computing the likelihood of one or more points&lt;/li&gt;
&lt;li&gt;Avoiding redundant computation of &lt;code&gt;Z&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Optionally computing the log-likelihood&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Training the model&lt;/h2&gt;
&lt;p&gt;At the outset, the parameters &lt;code&gt;self.weights&lt;/code&gt; and &lt;code&gt;self.biases&lt;/code&gt; of our model are initialized at random. Trivially, such that the values returned by &lt;code&gt;likelihood&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt; are useful, we must first update these parameters by fitting this model to observed data.&lt;/p&gt;
&lt;p&gt;To do so, we will employ the principal of maximum likelihood: compute the parameters that make the observed data maximally likely under the model, via gradient ascent.&lt;/p&gt;
&lt;h2&gt;Gradients&lt;/h2&gt;
&lt;p&gt;Since our model is simple, we can derive exact gradients by hand. We will work with the log-likelihood instead of the true likelihood to avoid issues of computational underflow. Below, we simplify this expression, then compute its various gradients.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(\log{\mathcal{L}}\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{k=1}^n \frac{\exp{(H(x^{(k)})}}{Z}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{\mathcal{L}(x^{(1)}, ..., x^{(n)})}
&amp;amp;= \sum\limits_{k=1}^n \log{\frac{\exp{(H(x^{(k)})}}{Z}}\\
&amp;amp;= \sum\limits_{k=1}^n \log{\big(\exp{(H(x^{(k)})}\big)} - \log{Z}\\
&amp;amp;= \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This gives the total likelihood. Our aim is to maximize the expected likelihood with respect to the data generating distribution.&lt;/p&gt;
&lt;h3&gt;Expected likelihood&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{x \sim p_{\text{data}}}\big[ \mathcal{L}(x) \big]
&amp;amp;= \sum\limits_{k=1}^N p_{\text{data}}(x = x^{(k)}) \mathcal{L(x^{(k)})}\\
&amp;amp;\approx \sum\limits_{k=1}^N \frac{1}{N} \mathcal{L(x^{(k)})}\\
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^N  \mathcal{L(x^{(k)})}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In other words, we wish to maximize the average likelihood of our data under the model. Henceforth, we will refer to this quantity as &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, i.e. &lt;span class="math"&gt;\(\mathcal{L} = \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, deriving the gradient with respect to our weights:&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(\nabla_{w_{i, j}}\log{\mathcal{L}}\)&lt;/span&gt;:&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\nabla_{w_{i, j}} \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)}) - \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;First term:&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)})
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \bigg[ \sum\limits_{i \neq j} w_{i, j} x_i^{(k)} x_j^{(k)} + \sum\limits_i b_i x_i^{(k)} \bigg]\\
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\\
&amp;amp;\approx \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j]
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Second term:&lt;/h3&gt;
&lt;p&gt;NB: &lt;span class="math"&gt;\(\sum\limits_{\mathcal{x}}\)&lt;/span&gt; implies a summation over all &lt;span class="math"&gt;\(v^d\)&lt;/span&gt; possible configurations of &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\nabla_{w_{i, j}} \log{Z}
&amp;amp;= \nabla_{w_{i, j}} \log{\sum\limits_{\mathcal{x}}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{\sum\limits_{\mathcal{x}} \exp{(H(x))}} \nabla_{w_{i, j}} \sum\limits_{\mathcal{x}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \nabla_{w_{i, j}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \exp{(H(x))} \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} \frac{\exp{(H(x))}}{Z} \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} p(x) \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} p(x) [x_i  x_j]\\
&amp;amp;= \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Putting it back together&lt;/h3&gt;
&lt;p&gt;Combining these constituent parts, we arrive at the following formula:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
$$&lt;/div&gt;
&lt;p&gt;Finally, following the same logic, we derive the exact gradient with respect to our biases:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{b_i}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$&lt;/div&gt;
&lt;p&gt;The first and second terms of each gradient are called, respectively, &lt;strong&gt;the positive and negative phases.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Computing the positive phase&lt;/h2&gt;
&lt;p&gt;In the following toy example, our data are small: we can compute the positive phase using all of the training data, i.e. &lt;span class="math"&gt;\(\frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\)&lt;/span&gt;. Were our data bigger, we could approximate this expectation with a mini-batch of training data (much like SGD).&lt;/p&gt;
&lt;h2&gt;Computing the negative phase&lt;/h2&gt;
&lt;p&gt;Again, this term asks us to compute then sum the log-likelihood over every possible data configuration in the support of our model, which is &lt;span class="math"&gt;\(O(v^d)\)&lt;/span&gt;. &lt;strong&gt;With non-trivially large &lt;span class="math"&gt;\(v\)&lt;/span&gt; or &lt;span class="math"&gt;\(d\)&lt;/span&gt;, this becomes intractable to compute.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below, we'll begin our toy example by computing the true negative-phase, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, with varying data dimensionalities &lt;span class="math"&gt;\(d\)&lt;/span&gt;. Then, once this computation becomes slow, we'll look to approximate this expectation later on.&lt;/p&gt;
&lt;h2&gt;Parameter updates in code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train model, visualize model distribution&lt;/h2&gt;
&lt;p&gt;Finally, we're ready to train. Using the true negative phase, let's train our model for 100 epochs with &lt;span class="math"&gt;\(d=3\)&lt;/span&gt; then visualize results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset_data_and_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Generate training data, weights, biases, and a list of all data configurations&lt;/span&gt;
&lt;span class="sd"&gt;    in our model's support.&lt;/span&gt;

&lt;span class="sd"&gt;    In addition, generate a list of tuples of the indices of adjacent nodes, which&lt;/span&gt;
&lt;span class="sd"&gt;    we'll use to update parameters without duplication.&lt;/span&gt;

&lt;span class="sd"&gt;    For example, with `n_units=3`, we generate a matrix of weights with shape (3, 3);&lt;/span&gt;
&lt;span class="sd"&gt;    however, there are only 3 distinct weights in this matrix that we'll actually&lt;/span&gt;
&lt;span class="sd"&gt;    want to update: those connecting Node 0 --&amp;gt; Node 1, Node 1 --&amp;gt; Node 2, and&lt;/span&gt;
&lt;span class="sd"&gt;    Node 0 --&amp;gt; Node 2. This function returns a list containing these tuples&lt;/span&gt;
&lt;span class="sd"&gt;    named `var_combinations`.&lt;/span&gt;

&lt;span class="sd"&gt;    :param n_units: the dimensionality of our data `d`&lt;/span&gt;
&lt;span class="sd"&gt;    :param n_obs: the number of observations in our training set&lt;/span&gt;
&lt;span class="sd"&gt;    :param p: a vector of the probabilities of observing a 1 in each index&lt;/span&gt;
&lt;span class="sd"&gt;        of the training data. The length of this vector must equal `n_units`&lt;/span&gt;

&lt;span class="sd"&gt;    :return: weights, biases, var_combinations, all_configs, data&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialize data&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# initialize parameters&lt;/span&gt;
    &lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# a few other pieces we'll need&lt;/span&gt;
    &lt;span class="n"&gt;var_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;all_configs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nd"&gt;@staticmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_inv_logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;            where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;            for which to compute the joint likelihood.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
        &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

        &lt;span class="c1"&gt;# compute partition function&lt;/span&gt;
        &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Can't burn in for more samples than there are in the chain"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_inv_logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# make copy&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reset_data_and_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Epoch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; | Likelihood: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;209.63758306786653&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;162.04280784271083&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;160.49961381649555&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;159.79539070373576&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;159.2853717231018&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.90186293631422&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.6084020645482&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.38094343579155&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.20287017780586&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.06232196551673&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Visualize samples&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_n_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    NB: We add some jitter to the points so as to better visualize density in a given corner of the model.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;111&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;projection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'3d'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 0'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_zlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="n"&gt;plot_n_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; Samples from Model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_7_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The plot roughly matches the data-generating distribution: most points assume values of either &lt;span class="math"&gt;\([1, 0, 1]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([1, 0, 0]\)&lt;/span&gt; (given &lt;span class="math"&gt;\(p=[.8, .1, .5]\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Sampling, via Gibbs&lt;/h2&gt;
&lt;p&gt;The second, final method we need to implement is &lt;code&gt;sample&lt;/code&gt;. In a Boltzmann machine, we typically do this via &lt;a href="http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf"&gt;Gibbs sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To effectuate this sampling scheme, we'll need a model of each data dimension conditional on the other data dimensions. For example, for &lt;span class="math"&gt;\(d=3\)&lt;/span&gt;, we'll need to define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_0\vert x_1, x_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_1\vert x_0, x_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_2\vert x_0, x_1)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that each dimension must assume a 0 or a 1, the above 3 models must necessarily return the probability of observing a 1 (where 1 minus this value gives the probability of observing a 0).&lt;/p&gt;
&lt;p&gt;Let's derive these models using the workhorse axiom of conditional probability, starting with the first:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p(x_0 = 1\vert x_1, x_2)
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_1, x_2)}\\
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{\sum\limits_{x_0 \in [0, 1]} p(x_0, x_1, x_2)}\\
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_0 = 0, x_1, x_2) + p(x_0 = 1, x_1, x_2)}\\
&amp;amp;= \frac{1}{1 + \frac{p(x_0 = 0, x_1, x_2)}{p(x_0 = 1, x_1, x_2)}}\\
&amp;amp;= \frac{1}{1 + \frac{\exp{(H(x_0 = 0, x_1, x_2)))}}{\exp{(H(x_0 = 1, x_1, x_2)))}}}\\
&amp;amp;= \frac{1}{1 + \exp{(H(x_0 = 0, x_1, x_2) - H(x_0 = 1, x_1, x_2))}}\\
&amp;amp;= \frac{1}{1 + \exp{(\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i - (\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i))}}\\
&amp;amp;= \frac{1}{1 + \exp{(-\sum\limits_{j \neq i = 0} w_{i, j} x_j - b_i)}}\\
&amp;amp;= \sigma\bigg(\sum\limits_{j \neq i = 0} w_{i, j} x_j + b_i\bigg)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Pleasantly enough, this model resolves to a simple Binomial GLM, i.e. logistic regression, involving only its neighboring units and the weights that connect them.&lt;/p&gt;
&lt;p&gt;With the requisite conditionals in hand, let's run this chain and compare it with our (trained) model's true probability distribution.&lt;/p&gt;
&lt;h2&gt;True probability distribution&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[0, 0, 0]: 0.07327&lt;/span&gt;
&lt;span class="err"&gt;[0, 0, 1]: 0.09227&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 0]: 0.01366&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 1]: 0.01938&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 0]: 0.3351&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 1]: 0.3622&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 0]: 0.04693&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 1]: 0.05715&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Empirical probability distribution, via Gibbs&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;empirical_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;empirical_probability&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (true), &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;empirical_probability&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (empirical)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[0, 0, 0]: 0.07327 (true), 0.05102 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 0, 1]: 0.09227 (true), 0.09184 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 0]: 0.01366 (true), 0.0102 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 1]: 0.01938 (true), 0.02041 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 0]: 0.3351 (true), 0.3673 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 1]: 0.3622 (true), 0.398 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 0]: 0.04693 (true), 0.03061 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 1]: 0.05715 (true), 0.03061 (empirical)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Close, ish enough.&lt;/p&gt;
&lt;h2&gt;Scaling up, and hitting the bottleneck&lt;/h2&gt;
&lt;p&gt;With data of vary dimensionality &lt;code&gt;n_units&lt;/code&gt;, the following plot gives the time in seconds that it takes to train this model for 10 epochs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_15_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;To reduce computational burden, and/or to fit a Boltzmann machine to data of non-trivial dimensionality (e.g. a 28x28 grey-scale image, which implies a random variable with 28x28=784 dimensions), we need to compute the positive and/or negative phase of our gradient faster than we currently are.&lt;/p&gt;
&lt;p&gt;To compute the former more quickly, we could employ mini-batches as in canonical stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;In this post, we'll instead focus on ways to speed up the latter. Revisiting its expression, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, we readily see that we can create an unbiased estimator for this value by drawing Monte Carlo samples from our model, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j] \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$&lt;/div&gt;
&lt;p&gt;So, now we just need a way to draw these samples. Luckily, we have a Gibbs sampler to tap!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instead of computing the true negative phase, i.e. summing &lt;span class="math"&gt;\(x_i  x_j\)&lt;/span&gt; over all permissible configurations &lt;span class="math"&gt;\(X\)&lt;/span&gt; under our model, we can approximate it by evaluating this expression for a few model samples, then taking the mean.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We define this update mechanism here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                          &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we'll define a function that we can parameterize by an optimization algorithm (computing the true negative phase, or approximating it via Gibbs sampling, in the above case) which will train a model for &lt;span class="math"&gt;\(n\)&lt;/span&gt; epochs and return data requisite for plotting.&lt;/p&gt;
&lt;h2&gt;How does training progress for varying data dimensionalities?&lt;/h2&gt;
&lt;p&gt;Finally, for data of &lt;code&gt;n_units&lt;/code&gt; 3, 4, 5, etc., let’s train models for 100 epochs and plot likelihood curves.&lt;/p&gt;
&lt;p&gt;When training with the approximate negative phase, we’ll:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Derive model samples from a &lt;strong&gt;1000-sample Gibbs chain. Of course, this is a parameter we can tune, which will affect both model accuracy and training runtime. However, we don’t explore that in this post;&lt;/strong&gt; instead, we just pick something reasonable and hold this value constant throughout our experiments.&lt;/li&gt;
&lt;li&gt;Train several models for a given &lt;code&gt;n_units&lt;/code&gt;; Seaborn will average results for us then plot a single line.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_epochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_epochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_23_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When we let each algorithm run for 100 epochs, the true negative phase gives a model which assigns higher likelihood to the observed data in all of the above training runs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Notwithstanding, the central point is that 100 epochs of the true negative phase takes a long time to run.&lt;/p&gt;
&lt;p&gt;As such, let’s run each for an equal amount of time, and plot results. Below, we define a function to train models for &lt;span class="math"&gt;\(n\)&lt;/span&gt; seconds (or 1 epoch—whichever comes first).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;n_seconds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;How many epochs do we actually get through?&lt;/h2&gt;
&lt;p&gt;Before plotting results, let’s examine how many epochs each algorithm completes in its allotted time. In fact, for some values of &lt;code&gt;n_units&lt;/code&gt;, we couldn’t even complete a single epoch (when computing the true negative phase) in &lt;span class="math"&gt;\(\leq 1\)&lt;/span&gt; second.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_28_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Finally, we look at performance. With &lt;code&gt;n_units &amp;lt;= 7&lt;/code&gt;, we see that 1 second of training with the true negative phase yields a better model. Conversely, &lt;strong&gt;using 7 or more units, the added performance given by using the true negative phase is overshadowed by the amount of time it takes the model to train.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/thorough-introduction-to-boltzmann-machines/output_31_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Of course, we re-stress that the exact ablation results are conditional (amongst other things) on &lt;strong&gt;the number of Gibbs samples we chose to draw. Changing this will change the results, but not that about which we care the most: the overall trend.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Throughout this post, we've given a thorough introduction to a Boltzmann machine: what it does, how it trains, and some of the computational burdens and considerations inherent.&lt;/p&gt;
&lt;p&gt;In the next post, we'll look at cheaper, more inventive algorithms for avoiding the computation of the negative phase, and describe how they're used in common machine learning models and training routines.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/boltzmann-machines"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-1.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec19.pdf"&gt;CSC321 Lecture 19: Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/"&gt;Derivation: Maximum Likelihood for Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf"&gt;Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>From Gaussian Algebra to Gaussian Processes, Part 2</title><link href="https://willwolf.io/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/" rel="alternate"></link><published>2018-06-12T08:00:00-04:00</published><updated>2018-06-12T08:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:willwolf.io,2018-06-12:/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/</id><summary type="html">&lt;p&gt;Introducing the RBF kernel, and motivating its ubiquitous use in Gaussian processes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, we covered the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions (i.e. function evaluations)&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian (just pluck out the pieces of interest)&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset gives another Gaussian (a simple algebraic formula)&lt;/li&gt;
&lt;li&gt;Posterior over functions (the linear map of the posterior over weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X_{*})^T\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Covariances (the second thing we need in order to specify a multivariate Gaussian)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;If any of the above is still not clear, please look no further, and re-visit the &lt;a href="https://willwolf.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/"&gt;previous post&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Conversely, we did not directly cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we'll explain these two.&lt;/p&gt;
&lt;h2&gt;The more features we use, the more expressive our model&lt;/h2&gt;
&lt;p&gt;We concluded the previous post by plotting posteriors over function evaluations given various &lt;code&gt;phi_func&lt;/code&gt;s, i.e. a function that creates "features" &lt;span class="math"&gt;\(\phi(X)\)&lt;/span&gt; given an input &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 inputs&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 corresponding outputs, which we'll use later on&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# makes D=2 features for each input&lt;/span&gt;


&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One common such set of features are those given by "radial basis functions", a.k.a. the "squared exponential" function, defined as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, the choice of which features to use is ultimately arbitrary, i.e. a choice left to the modeler.&lt;/p&gt;
&lt;p&gt;Throughout the exercise, we saw that the larger the dimensionality &lt;span class="math"&gt;\(d\)&lt;/span&gt; of our feature function &lt;code&gt;phi_func&lt;/code&gt;, the more expressive, i.e. less endemically prone to overfitting, our model became.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, how far can we take this?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Computing features is expensive&lt;/h2&gt;
&lt;p&gt;Ideally, we'd compute as many features as possible for each input element, i.e. employ &lt;code&gt;phi_func(x, D=some_huge_number)&lt;/code&gt;. Unfortunately, the cost of doing so adds up, and ultimately becomes intractable past meaningfully-large values of &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perhaps there's a better way?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How are these things used?&lt;/h2&gt;
&lt;p&gt;Let's bring back our GP equations, and prepare ourselves to &lt;em&gt;squint&lt;/em&gt;! In the previous post, we outlined the following modeling process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define prior distribution over weights and function evaluations, &lt;span class="math"&gt;\(P(w, y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Marginalizing &lt;span class="math"&gt;\(P(w, y)\)&lt;/span&gt; over &lt;span class="math"&gt;\(y\)&lt;/span&gt;, i.e. &lt;span class="math"&gt;\(\int P(w, y)dy\)&lt;/span&gt;, and given some observed function evaluations &lt;span class="math"&gt;\(y\)&lt;/span&gt;, compute the posterior distribution over weights, &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;After linear-mapping &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; onto some new, transformed test input &lt;span class="math"&gt;\(\phi(X_*)^T\)&lt;/span&gt;, compute the posterior distribution over function evaluations, &lt;span class="math"&gt;\(P(y_*\ \vert\ y) = P(\phi(X_{*})^Tw\ \vert\ y)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, let's unpack #2 and #3.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First, the mathematical equation:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(w\vert y)
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}\Sigma_y^{-1}(y - \mu_y), \Sigma_w - \Sigma_{wy}\Sigma_y^{-1}\Sigma_{wy}^T)\\
    \\
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}(y - \mu_w^T \phi(X)), \Sigma_w - \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}\Sigma_{wy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Next, this equation in code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define initial parameters&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# dimensionality of `phi_func`&lt;/span&gt;

&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# often a vector of zeros, though it doesn't have to be&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# often the identity matrix, though it doesn't have to be&lt;/span&gt;

&lt;span class="c1"&gt;# Featurize `X_train`&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Params of prior distribution over function evals&lt;/span&gt;
&lt;span class="n"&gt;mu_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt;
     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cov_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;

&lt;span class="c1"&gt;# Params of posterior distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;
&lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;
           &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(P(y_*\ \vert\ y) = P(\phi(X_{*})^Tw\ \vert\ y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(X_*\)&lt;/span&gt; is a set of test points, e.g. &lt;code&gt;np.linspace(-10, 10, 200)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition, let's call &lt;span class="math"&gt;\(X_* \rightarrow\)&lt;/span&gt; &lt;code&gt;X_test&lt;/code&gt; and &lt;span class="math"&gt;\(y_* \rightarrow\)&lt;/span&gt; &lt;code&gt;y_test&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mathematical equations in code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Featurize `X_test`&lt;/span&gt;
&lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# The following two equations were defined above&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;
&lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;

&lt;span class="c1"&gt;# The mean of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

&lt;span class="c1"&gt;# The covariance of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Never alone&lt;/h2&gt;
&lt;p&gt;Squinting at the equations for &lt;code&gt;mu_y_test_post&lt;/code&gt; and &lt;code&gt;cov_y_test_post&lt;/code&gt;, we see that &lt;code&gt;phi_x&lt;/code&gt; and &lt;code&gt;phi_x_test&lt;/code&gt; appear &lt;strong&gt;only in the presence of another &lt;code&gt;phi_x&lt;/code&gt;, or &lt;code&gt;phi_x_test&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These four distinct such terms are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In mathematical notation, they are (respectively):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Simplifying further&lt;/h2&gt;
&lt;p&gt;These are nothing more than &lt;em&gt;scaled&lt;/em&gt; (via the &lt;span class="math"&gt;\(\Sigma_w\)&lt;/span&gt; term) dot products in some expanded feature space &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Until now, we've explicitly chosen what this &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; function is.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If the scaling matrix &lt;span class="math"&gt;\(\Sigma_w\)&lt;/span&gt; is &lt;a href="https://en.wikipedia.org/wiki/Positive-definite_matrix"&gt;positive definite&lt;/a&gt;, we can state the following, using &lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X)\)&lt;/span&gt;, i.e. &lt;code&gt;phi_x.T @ cov_w @ phi_x&lt;/code&gt;, as an example:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\Sigma_w = (\sqrt{\Sigma_w})^2
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\phi(X)^T \Sigma_w \phi(X)
    &amp;amp;= \big(\sqrt{\Sigma_w}\phi(X)\big)^T\big(\sqrt{\Sigma_w}\phi(X)\big)\\
    &amp;amp;= \varphi(X)^T\varphi(X)\\
    &amp;amp;= \varphi(X) \cdot \varphi(X)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As such, our four distinct scaled-dot-product terms can be rewritten as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*) = \varphi(X_*) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X) = \varphi(X_*) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X) = \varphi(X) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*) = \varphi(X) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;In other words, these terms can be equivalently written as dot-products in some space &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NB: we have &lt;strong&gt;not&lt;/strong&gt; explicitly chosen what this &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; function is.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Kernels&lt;/h2&gt;
&lt;p&gt;A "kernel" is a function which gives the similarity between individual elements in two sets, i.e. a Gram matrix.&lt;/p&gt;
&lt;p&gt;For instance, imagine we have two sets of countries, &lt;span class="math"&gt;\(\{\text{France}, \text{Germany}, \text{Iceland}\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{\text{Morocco}, \text{Denmark}\}\)&lt;/span&gt;, and that similarity is given by an integer value in &lt;span class="math"&gt;\([1, 5]\)&lt;/span&gt;, where 1 is the least similar, and 5 is the most. Applying a kernel to these sets might give a Gram matrix such as:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped=""&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe table table-striped table-hover"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;France&lt;/th&gt;
&lt;th&gt;Germany&lt;/th&gt;
&lt;th&gt;Iceland&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;Morocco&lt;/th&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Denmark&lt;/th&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;When you hear the term "kernel" in the context of machine learning, think "similarity between things in lists."&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NB: A "list" could be a list of vectors, i.e. a matrix. A vector, or a matrix, are the canonical inputs to a kernel.&lt;/p&gt;
&lt;h2&gt;Mercer's Theorem&lt;/h2&gt;
&lt;p&gt;Mercer's Theorem has as a key result that any kernel function can be expressed as a dot product, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
K(X, X') = \varphi(X) \cdot \varphi (X')
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; is some function that creates &lt;span class="math"&gt;\(d\)&lt;/span&gt; features out of &lt;span class="math"&gt;\(X\)&lt;/span&gt; (in the same vein as &lt;code&gt;phi_func&lt;/code&gt; from above).&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;To illustrate, I'll borrow an example from &lt;a href="https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is"&gt;CrossValidated&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;"For example, consider a simple polynomial kernel &lt;span class="math"&gt;\(K(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2\)&lt;/span&gt; with &lt;span class="math"&gt;\(\mathbf x, \mathbf y \in \mathbb R^2\)&lt;/span&gt;. This doesn't seem to correspond to any mapping function &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;, it's just a function that returns a real number. Assuming that &lt;span class="math"&gt;\(\mathbf x = (x_1, x_2)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf y = (y_1, y_2)\)&lt;/span&gt;, let's expand this expression:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
K(\mathbf x, \mathbf y)
    &amp;amp;= (1 + \mathbf x^T \mathbf y)^2\\
    &amp;amp;= (1 + x_1 \, y_1  + x_2 \, y_2)^2\\
    &amp;amp;= 1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 + 2 x_2 y_2 + 2 x_1 x_2 y_1 y_2
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Note that this is nothing else but a dot product between two vectors &lt;span class="math"&gt;\((1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)\)&lt;/span&gt; and &lt;span class="math"&gt;\((1, y_1^2, y_2^2, \sqrt{2} y_1, \sqrt{2} y_2, \sqrt{2} y_1 y_2)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\varphi(\mathbf x) = \varphi(x_1, x_2) = (1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)\)&lt;/span&gt;. So the kernel &lt;span class="math"&gt;\(K(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2 = \varphi(\mathbf x) \cdot \varphi(\mathbf y)\)&lt;/span&gt; computes a dot product in 6-dimensional space without explicitly visiting this space."&lt;/p&gt;
&lt;h3&gt;What this means&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/kernels-for-gaussian-processes.svg"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with inputs &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Our goal is to compute the similarity between then, &lt;span class="math"&gt;\(\text{Sim}(X, Y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Bottom path&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Lifting these inputs into some feature space, then computing their dot-product in that space, i.e. &lt;span class="math"&gt;\(\varphi(X) \cdot \varphi (Y)\)&lt;/span&gt; (where &lt;span class="math"&gt;\(F = \varphi\)&lt;/span&gt;, since I couldn't figure out how to draw a &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; in &lt;a href="http://draw.io"&gt;draw.io&lt;/a&gt;), is one strategy for computing this similarity.&lt;/li&gt;
&lt;li&gt;Unfortunately, this robustness comes at a cost: &lt;strong&gt;the computation is extremely expensive.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Top Path&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A valid kernel computes similarity between inputs. The function it employs might be extremely simple, e.g. &lt;span class="math"&gt;\((X - Y)^{2}\)&lt;/span&gt;; &lt;strong&gt;the computation is extremely cheap.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Mercer!&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mercer's Theorem tells us that every valid kernel, i.e. the top path, is &lt;em&gt;implicitly traversing the bottom path.&lt;/em&gt; &lt;strong&gt;In other words, kernels allow us to directly compute the result of an extremely expensive computation, extremely cheaply.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How does this help?&lt;/h2&gt;
&lt;p&gt;Once more, the Gaussian process equations are littered with the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*) = \varphi(X_*) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X) = \varphi(X_*) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X) = \varphi(X) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*) = \varphi(X) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, we previously established that the more we increase the dimensionality &lt;span class="math"&gt;\(d\)&lt;/span&gt; of our given feature function, the more flexible our model becomes.&lt;/p&gt;
&lt;p&gt;Finally, past any meaningfully large value of &lt;span class="math"&gt;\(d\)&lt;/span&gt;, and irrespective of what &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; actually is, &lt;strong&gt;this computation becomes intractably expensive.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Kernels!&lt;/h3&gt;
&lt;p&gt;You know where this is going.&lt;/p&gt;
&lt;p&gt;Given Mercer's theorem, we can state the following equalities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X_*) \cdot \varphi(X_*) = K(X_*, X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X_*) \cdot \varphi(X) = K(X_*, X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X) \cdot \varphi(X) = K(X, X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X) \cdot \varphi(X_*) = K(X, X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Which kernels to choose?&lt;/h2&gt;
&lt;p&gt;At the outset, we stated that our primary goal was to increase &lt;span class="math"&gt;\(d\)&lt;/span&gt;. As such, &lt;strong&gt;let's pick the kernel whose implicit &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; has the largest dimensionality possible.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we saw that the kernel &lt;span class="math"&gt;\(k(\mathbf x, \mathbf y)\)&lt;/span&gt; was implicitly computing a &lt;span class="math"&gt;\(d=6\)&lt;/span&gt;-dimensional dot-product. Which kernels compute a &lt;span class="math"&gt;\(d=100\)&lt;/span&gt;-dimensional dot-product? &lt;span class="math"&gt;\(d=1000\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How about &lt;span class="math"&gt;\(d=\infty\)&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Radial basis function, a.k.a. the "squared-exponential"&lt;/h2&gt;
&lt;p&gt;This kernel is implicitly computing a &lt;span class="math"&gt;\(d=\infty\)&lt;/span&gt;-dimensional dot-product. That's it. &lt;strong&gt;That's why it's so ubiquitous in Gaussian processes.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Rewriting our equations&lt;/h2&gt;
&lt;p&gt;With all of the above in mind, let's rewrite the equations for the parameters of our posterior distribution over function evaluations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The mean of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

               &lt;span class="c1"&gt;# Now, substituting in our kernels&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

&lt;span class="c1"&gt;# The covariance of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;

                &lt;span class="c1"&gt;# Now, substituting in our kernels&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Defining the kernel in code&lt;/h2&gt;
&lt;p&gt;Mathematically, the RBF kernel is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
K(X, Y) = \exp(-\frac{1}{2}\vert X - Y \vert ^2)
$$&lt;/div&gt;
&lt;p&gt;To conclude, let's define a Python function for the parameters of our posterior over function evaluations, using this RBF kernel as &lt;code&gt;k&lt;/code&gt;, then plot the resulting distribution.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 inputs&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 corresponding outputs, which we'll use below&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# vector of test inputs&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rbf_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (len(x), 1)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (1, len(y))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (len(x), len(y))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rbf_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# The following quantity is used in both `mu_y_test_post` and `cov_y_test_post`;&lt;/span&gt;
&lt;span class="c1"&gt;# we extract it into a separate variable for readability&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;            
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Visualizing results&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_17_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;And for good measure, with some samples from the posterior:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_19_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;p&gt;In this post, we've unpacked the notion of a kernel, and its ubiquitous use in Gaussian Processes.&lt;/p&gt;
&lt;p&gt;In addition, we've introduced the RBF kernel, i.e. "squared exponential" kernel, and motivated its widespread application in these models.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/gaussian-processes"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/gaussian-processes/blob/master/gaussian-processes-part-2.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://gaussianprocess.org/gpml/?"&gt;Gaussian Processes for Machine Learning&lt;/a&gt;. Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Mercers-Theorem"&gt;What is an intuitive explanation of Mercer's Theorem?&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry></feed>