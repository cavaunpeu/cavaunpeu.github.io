<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>will wolf</title><link href="https://cavaunpeu.github.io/" rel="alternate"></link><link href="https://cavaunpeu.github.io/feeds/all.atom.xml" rel="self"></link><id>https://cavaunpeu.github.io/</id><updated>2020-03-12T08:00:00-04:00</updated><subtitle>writings on machine learning, geopolitics, life</subtitle><entry><title>Soft Power in the Age of Deepfakes</title><link href="https://cavaunpeu.github.io/2020/03/12/soft-power-in-the-age-of-generative-models/" rel="alternate"></link><published>2020-03-12T08:00:00-04:00</published><updated>2020-03-12T08:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2020-03-12:/2020/03/12/soft-power-in-the-age-of-generative-models/</id><summary type="html">&lt;p&gt;What happens when a post-Trump, reputationally-bruised United States, and improved generative models (the technology behind "deepfakes") collide head-on?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Nations exert influence in two key ways: hard power, and soft power. Hard power is &lt;a href="https://www.history.com/topics/world-war-ii/bombing-of-hiroshima-and-nagasaki"&gt;dropping the atomic bomb&lt;/a&gt; on Hiroshima and Nagasaki during World War II, compelling the Japanese to surrender. Conversely, soft power is building American libraries in foreign universities, placing American films in foreign theaters, and sending &lt;a href="https://en.wikipedia.org/wiki/Peace_Corps"&gt;United States Peace Corps&lt;/a&gt; volunteers to rural communities worldwide, compelling a favorable image of our country abroad.&lt;/p&gt;
&lt;p&gt;In other words, hard power is to influence via economic and military violence; soft power is to influence via &lt;em&gt;attraction.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Soft power through media&lt;/h2&gt;
&lt;p&gt;A key subset of soft power is that delivered through media.&lt;/p&gt;
&lt;p&gt;For example, during the Franco-Prussian War, the French dropped leaflets over enemy territory by &lt;a href="https://en.wikipedia.org/wiki/Airborne_leaflet_propaganda"&gt;airborne balloon&lt;/a&gt; touting the mutual benefits of ceasefire. During World War I, all parties dropped such leaflets by &lt;a href="https://en.wikipedia.org/wiki/Airborne_leaflet_propaganda#First_World_War"&gt;airplane&lt;/a&gt;. In World War II, the Nazis pioneered the use of &lt;a href="https://en.wikipedia.org/wiki/Radio_propaganda#Nazi_Germany"&gt;radio&lt;/a&gt; as a propaganda machine. Finally, in 1969, the Americans live-broadcast Neil Armstrong’s moon landing to &lt;a href="https://www.youtube.com/watch?v=cwZb2mqId0A"&gt;television&lt;/a&gt; screens worldwide, showcasing the "derring-do and genius of American ingenuity, and putting one over the Soviets at the same time."&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Today, such media is more numerous and diverse: American shows on Netflix, American music on Spotify, American “influencers” on YouTube, Hollywood movies and more—all distributed globally.&lt;/p&gt;
&lt;h2&gt;The cost of media&lt;/h2&gt;
&lt;p&gt;The cost of media can be split into two broad camps: creation and distribution.&lt;/p&gt;
&lt;p&gt;Since the days of leaflets by balloons, the marginal cost of distribution has become extremely cheap. Today, Trump tweets, and his words instantaneously appear on the timelines of his 75 million followers. Were he to have 150 instead, the additional cost of reaching these additional 75 would be, practically speaking, nothing.&lt;/p&gt;
&lt;p&gt;In contrast, the creation of this media itself remains a wholly manual exercise. To create a news broadcast, humans write scripts, set the stage, select the costumes, apply makeup, and read the news itself. Furthermore, creating twice as many news broadcasts costs, crucially, roughly twice as much.&lt;/p&gt;
&lt;p&gt;The dwindling marginal cost of its distribution makes soft power media efficient and appealing. However, it is the “creation bottleneck” that stands in the way of true scale.&lt;/p&gt;
&lt;h2&gt;Generative models&lt;/h2&gt;
&lt;p&gt;Generative models are those that empower &lt;a href="https://en.wikipedia.org/wiki/Deepfake"&gt;“deepfakes.”&lt;/a&gt; They are algorithms that generate realistic &lt;a href="https://thispersondoesnotexist.com/"&gt;images&lt;/a&gt; (refresh this link for more), &lt;a href="https://medium.com/syncedreview/deepmind-dvd-gan-impressive-step-toward-realistic-video-synthesis-12027d942e53"&gt;video&lt;/a&gt;, &lt;a href="https://openai.com/blog/musenet/"&gt;audio&lt;/a&gt;, &lt;a href="https://talktotransformer.com/"&gt;text&lt;/a&gt;, or other types of rich media. Presently, generative models excel in creating art, music, literature, news reports, and textiles. &lt;a href="https://news.microsoft.com/apac/features/much-more-than-a-chatbot-chinas-xiaoice-mixes-ai-with-emotions-and-wins-over-millions-of-fans/"&gt;Microsoft’s Xiaoice&lt;/a&gt;, a multi-purpose &lt;a href="https://en.wikipedia.org/wiki/Chatbot"&gt;chatbot&lt;/a&gt; (which currently dialogs with over 660 million registered users), pulls all this off and more:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Xiaoice’s framework is learning to write literature as well as compose and perform songs. Last year she published a book of poems and helps her followers write their own. She can sing her own songs in styles based on existing popular performers. There are plans to release an album of pop tunes soon. And she is able to author tailor-made stories for children and reads them out in voices suited to each of the characters she has created.&lt;br/&gt;&lt;br/&gt;She’s painting images based on keywords and other inputs. She’s also gone into mainstream media as a host of dozens of TV and radio programs that are broadcast across China. She reads news stories and provides commentary. And, she is generating multiple reports based on information from China’s financial markets and used by investors and traders who subscribe to Wind, a major financial information service.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;In short, generative models &lt;strong&gt;automate&lt;/strong&gt; the creation of media itself—smashing the “creation bottleneck” outright.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Coming soon&lt;/h2&gt;
&lt;p&gt;At present, generative models succeed at tasks like: &lt;a href="https://www.youtube.com/watch?v=PCBTZh41Ris"&gt;transferring dance moves&lt;/a&gt; from professionals to amateurs, &lt;a href="https://github.com/elliottwu/sText2Image"&gt;translating pencil sketches&lt;/a&gt; to high-resolution images with the guidance of text, and making Obama lip sync the words of American actor Jordan Peele.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=cQ54GDm1eL0" title="Click me!"&gt;&lt;img alt="Click me!" class="img-responsive" src="http://img.youtube.com/vi/cQ54GDm1eL0/0.jpg"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These feats are impressive. However, to achieve widespread impact, these algorithms must allow for more “control” over the media they create.&lt;/p&gt;
&lt;p&gt;For example, consider a system that, given English text, instantaneously generates a video of this text being recited in an arbitrary language. To be truly effective as a soft power tool, this system must ultimately operate as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;: “What kind of video would you like?”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User&lt;/strong&gt;: “I’d like a video of the President reciting the uploaded text. I'd like him to speak slowly, be wearing green US Army fatigues, and be seated in front of a blue Boeing CH-47 Chinook. Make me two, actually: one in Farsi, the other in Greek.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;: “Coming right up.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Such generative models are not quite here. However, given the speed at which this &lt;a href="http://www.arxiv-sanity.com/search?q=video+generation"&gt;research&lt;/a&gt; moves, they will reliably arrive within 5 years.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;The United States used to invest in soft power institutions&lt;/h2&gt;
&lt;p&gt;Historically, the United States invested substantially in soft power institutions.&lt;/p&gt;
&lt;p&gt;During World War I, Woodrow Wilson established the &lt;a href="https://en.wikipedia.org/wiki/Committee_on_Public_Information"&gt;Committee on Public Information&lt;/a&gt;, which ultimately trafficked in outright propaganda.&lt;/p&gt;
&lt;p&gt;During World War II, Roosevelt established the &lt;a href="https://en.wikipedia.org/wiki/Public_diplomacy_of_the_United_States"&gt;Division of Cultural Relations&lt;/a&gt; and Office of Inter-American Affairs in response to fascist propaganda in Latin America. Following the bombing of Pearl Harbor, the &lt;a href="https://en.wikipedia.org/wiki/Voice_of_America"&gt;Voice of America&lt;/a&gt;—a globally-distributed radio broadcast of non-military promotional content—the &lt;a href="https://en.wikipedia.org/wiki/United_States_Office_of_War_Information"&gt;Office of War Information&lt;/a&gt;—distributing media like “newspapers, posters, photographs, films”&lt;sup id="fnref:8"&gt;&lt;a class="footnote-ref" href="#fn:8"&gt;8&lt;/a&gt;&lt;/sup&gt; to civilian communities abroad—and the &lt;a href="https://en.wikipedia.org/wiki/Office_of_Strategic_Services"&gt;Office of Strategic Services&lt;/a&gt;—the predecessor to the CIA—were all established as well.&lt;/p&gt;
&lt;p&gt;During the Cold War, the United States flexed its “peacetime” soft power muscle, establishing myriad organizations to promote international cultural and educational exchanges, including the &lt;a href="https://en.wikipedia.org/wiki/Fulbright_Program"&gt;Fulbright Program&lt;/a&gt; in 1947 and the &lt;a href="https://en.wikipedia.org/wiki/Public_diplomacy_of_the_United_States#U.S._Information_Agency_(USIA)"&gt;United States Information Agency&lt;/a&gt; (USIA) in 1953.&lt;/p&gt;
&lt;h2&gt;Dwindling interest and opportunity missed&lt;/h2&gt;
&lt;p&gt;Following the Cold War, however, American soft power institutions began to slowly decline.&lt;/p&gt;
&lt;p&gt;In 1999, the USIA was absorbed into the US State Department; its staff, and budget for key projects, were cut roughly in half.&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt; “From 1995 to 2001, academic and cultural exchanges dropped from 45,000 to 29,000 annually, and many accessible cultural centers and libraries were closed.”&lt;sup id="fnref2:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt; Finally, “while government-funded radio broadcasts reached between 70 and 80 percent of the populace of Eastern Europe during the Cold War, around year 2000, a mere 2 percent of Arabs heard the VOA.”&lt;sup id="fnref3:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;At present, the United States has a largely &lt;a href="https://www.nationalreview.com/2017/01/trump-foreign-policy-isolationsim-america-first-allies-nato-trans-pacific-partnership/"&gt;isolationist president&lt;/a&gt; in Donald Trump. Since the beginning of his term, American soft power has fallen globally according to the Portland &lt;a href="https://softpower30.com/"&gt;Soft Power 30&lt;/a&gt;. Trump himself is unconcerned with others actively &lt;a href="https://www.rollcall.com/2018/09/25/world-leaders-laugh-as-trump-boasts-of-accomplishments/"&gt;laughing&lt;/a&gt; at him and his country.&lt;/p&gt;
&lt;p&gt;By 2018, Statista estimates that there were 1.67 billion "pay TV households" worldwide.&lt;sup id="fnref:9"&gt;&lt;a class="footnote-ref" href="#fn:9"&gt;9&lt;/a&gt;&lt;/sup&gt; By 2019, they estimate that half of private households had a computer.&lt;sup id="fnref:10"&gt;&lt;a class="footnote-ref" href="#fn:10"&gt;10&lt;/a&gt;&lt;/sup&gt; Finally, Pew Research estimates that "more than 5 billion people have mobile devices, and over half of these connections are smartphones."&lt;sup id="fnref:11"&gt;&lt;a class="footnote-ref" href="#fn:11"&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In &lt;a href="https://en.wikipedia.org/wiki/Information_Age"&gt;Information Age&lt;/a&gt;, a dwindling interest in soft power institutions is a &lt;strong&gt;major&lt;/strong&gt; opportunity missed.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a recent article, Joseph Nye, the Harvard professor who coined the term “soft power” itself, neatly summarizes this phenomenon:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“U.S. President Donald Trump’s administration has shown little interest in public diplomacy. And yet public diplomacy—a government’s efforts to communicate directly with other countries’ publics—is one of the key instruments policymakers use to generate soft power, and the current information revolution makes such instruments more important than ever.”&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;A renewed interest in soft power&lt;/h2&gt;
&lt;p&gt;Truly impactful generative models, and Trump’s exit from office, will soon collide head-on.&lt;/p&gt;
&lt;p&gt;At this point, the United States—with its blemished reputation, exorbitant military budget&lt;sup id="fnref:12"&gt;&lt;a class="footnote-ref" href="#fn:12"&gt;12&lt;/a&gt;&lt;/sup&gt;, longstanding technical leadership&lt;sup id="fnref:13"&gt;&lt;a class="footnote-ref" href="#fn:13"&gt;13&lt;/a&gt;&lt;/sup&gt; and forever proactive drive—will be sitting on transformative media tech and a clear-cut mission: rebuild its image at home and abroad.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To this effect, I predict that the United States will soon make a concerted investment in generative models for soft power at true scale.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here’s what this might look like.&lt;/p&gt;
&lt;h2&gt;Soft power through media, in the age of generative models&lt;/h2&gt;
&lt;p&gt;The two keys areas which generative models will impact are state-to-public, and state-to-individual media.&lt;/p&gt;
&lt;h3&gt;State-to-public media&lt;/h3&gt;
&lt;p&gt;Current examples of state-to-public soft power media, like Presidential addresses to the American people, English-language CNN news broadcasts to foreign viewers, and front-line conflict reporting will see their use both quickened and expanded by generative models.&lt;/p&gt;
&lt;p&gt;Presently, Presidential addresses happen once every few weeks or months. Conversely, Donald Trump writes on &lt;a href="https://twitter.com/realDonaldTrump"&gt;Twitter&lt;/a&gt; hourly. With generative models, the next President will &lt;em&gt;instantaneously&lt;/em&gt; generate videos of herself reciting the tweet instead. Americans at large devour the President's tweets; if a generated video legitimately looks and sounds just like her, they’re likely to devour it as well.&lt;/p&gt;
&lt;p&gt;Sponsorship deals, like &lt;a href="https://www.espn.com/nba/story/_/id/14314807/lebron-james-signs-life-deal-nike"&gt;Lebrons’ lifetime appointment&lt;/a&gt; with Nike, will crucially expand to include the right to generate content with the athlete’s likeness. Politicians will travel less to campaign; instead, they’ll send generated holograms—of them, their spouses, their key supporters—to campaign instead—each one personalized to the venue in question.&lt;/p&gt;
&lt;p&gt;Finally, a generative model will be trained to ingest footage of world events—for example, the &lt;a href="https://www.cnn.com/2019/09/14/middleeast/yemen-houthi-rebels-drone-attacks-saudi-aramco-intl/index.html"&gt;attack&lt;/a&gt; on the Saudi oil refinery—and produce text describing what happened. This text will then be used to generate a news report. Humans won’t be on the front line to capture this footage either; why not send a drone instead?&lt;/p&gt;
&lt;h3&gt;State-to-individual media&lt;/h3&gt;
&lt;p&gt;Microsoft’s Xiaoice demonstrates the enormous traction of personalized chatbots. From here, the jump to state-built, propaganda-(subtly)-infused chatbots is small.&lt;/p&gt;
&lt;p&gt;In general, the public distrusts governments; to this effect, humanitarian missions are typically run by NGOs. As such, states will deploy these chatbots on behalf of related entities: tourism boards, online universities, cultural centers, etc.&lt;/p&gt;
&lt;p&gt;Finally, current state-to-individual media is largely &lt;em&gt;unidirectional&lt;/em&gt;: media is sent from the former to the latter, and rarely vice versa. Chatbots powered by generative models, however, will solicit &lt;em&gt;feedback&lt;/em&gt; from their viewer in a personalized way; for example, “Are you enjoying this media? What do you like about American schools as compared to your own? What do you think about democracy?”&lt;/p&gt;
&lt;p&gt;In the same way that the Chinese government &lt;em&gt;passively&lt;/em&gt; learns about its citizens by &lt;a href="http://harvardpolitics.com/world/wechat-the-people-technology-and-social-control-in-china/"&gt;monitoring WeChat data&lt;/a&gt;, generative models will &lt;em&gt;actively solicit information&lt;/em&gt; re the efficacy of their soft power plays. Then, like a savvy marketer, they’ll double down on the stuff that works best.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;For over a century, states have exercised soft power through media. Throughout, the marginal cost of distributing this media has trended towards zero. Conversely, to date, the marginal cost of its creation remains high.&lt;/p&gt;
&lt;p&gt;Generative models are a powerful technology which trend the latter towards zero as well. They're here and they work. However, to be truly impactful, there's still a short ways to go.&lt;/p&gt;
&lt;p&gt;In 5 years' time, a dazed, reputationally-bruised United States, and robust, flexible, practical generative models, will collide head-on.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;At this point, soft power media—and human politics in general—are likely to change forever.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Credit&lt;/h2&gt;
&lt;p&gt;Many thanks to Abishur Prakash for reviewing earlier drafts of this piece.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Brands, Hal. “Not Even Trump Can Obliterate America's Soft Power.” Bloomberg.com, Bloomberg, www.bloomberg.com/opinion/articles/2018-01-18/not-even-trump-can-obliterate-america-s-soft-power. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Chakravarti, Sudeshna. Soft Power: The Culture Weapon in The Cold War and South Asia. www.culturaldiplomacy.org/academy/content/pdf/participant-papers/academy/Sudeshna-Khasnobis-Soft-Power-The-Culture-Weapon-in-The-Cold-War-and-South-Asia.pdf. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Khandelwal, Aakash. “Economics of Digital Goods.” LinkedIn SlideShare, 23 Nov. 2016, www.slideshare.net/aakashkhandelwal921/economics-of-digital-goods. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Nye, Joseph S. “No, President Trump: You've Weakened America's Soft Power.” The New York Times, The New York Times, 25 Feb. 2020, www.nytimes.com/2020/02/25/opinion/trump-soft-power.html. &lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Shah, Ritula. “Is US Monopoly on the Use of Soft Power at an End?” BBC News, BBC, 19 Nov. 2014, www.bbc.com/news/world-29536648. &lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;Nye, Joseph S. Soft Power: the Means to Success in World Politics. Knowledge World, 2012. &lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;Harris, Paul. “Man on the Moon: Moment of Greatness That Defined the American Century.” The Guardian, Guardian News and Media, 25 Aug. 2012, www.theguardian.com/science/2012/aug/25/man-moon-american-century. &lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:8"&gt;
&lt;p&gt;“United States Office of War Information.” Wikipedia, Wikimedia Foundation, 7 Jan. 2020, en.wikipedia.org/wiki/United_States_Office_of_War_Information. &lt;a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:9"&gt;
&lt;p&gt;Watson, Amy. “TV Households Worldwide.” Statista, 4 Dec. 2019, www.statista.com/statistics/268695/number-of-tv-households-worldwide/. &lt;a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:10"&gt;
&lt;p&gt;Statista Research Department. “How Many People Have Access to a Computer 2018.” Statista, 2 Mar. 2020, www.statista.com/statistics/748551/worldwide-households-with-computer/. &lt;a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:11"&gt;
&lt;p&gt;Silver, Laura. “Smartphone Ownership Is Growing Rapidly Around the World, but Not Always Equally.” Pew Research Center's Global Attitudes Project, Pew Research Center, 30 Dec. 2019, www.pewresearch.org/global/2019/02/05/smartphone-ownership-is-growing-rapidly-around-the-world-but-not-always-equally/. &lt;a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:12"&gt;
&lt;p&gt;Cancian, Mark F. “U.S. Military Forces in FY 2020: The Strategic and Budget Context.” U.S. Military Forces in FY 2020: The Strategic and Budget Context | Center for Strategic and International Studies, 12 Mar. 2020, www.csis.org/analysis/us-military-forces-fy-2020-strategic-and-budget-context?gclid=CjwKCAjwgbLzBRBsEiwAXVIygLkZlNLc3zV8EVVhPPwjOoqrUoQ4kjTeaTHN5vFktRWET2wsnHCwRhoCm2QQAvD_BwE. &lt;a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:13"&gt;
&lt;p&gt;“Report Shows United States Leads in Science and Technology as China Rapidly Advances.” ScienceDaily, ScienceDaily, 24 Jan. 2018, www.sciencedaily.com/releases/2018/01/180124113951.htm. &lt;a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="geopolitics"></category></entry><entry><title>On Saudi Drone Strikes and Adversarial AI</title><link href="https://cavaunpeu.github.io/2019/09/22/saudi-drone-strikes-and-adversarial-ai/" rel="alternate"></link><published>2019-09-22T08:30:00-04:00</published><updated>2019-09-22T08:30:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2019-09-22:/2019/09/22/saudi-drone-strikes-and-adversarial-ai/</id><summary type="html">&lt;p&gt;In world of weaponized drones piloted by algorithms, what new strategic opportunities arise?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week, Houthi rebels—a militant political group in Yemen—claimed credit for exploding an oil refinery in Saudi Arabia. The explosion was the work of a missile, fired from a drone.&lt;/p&gt;
&lt;p&gt;Though the group responsible is closely allied with Iran—and to wit, the American government does declaim Iran as responsible for facilitating the strike itself—it is by all accounts a non-state actor, notable for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As the cost of full-scale war increases due to, at a trivial minimum, the strength of weapons available, small, “surgical” operations become more common. This attack falls in directly with this trend.&lt;/li&gt;
&lt;li&gt;You do not have to be the state to desire to affect world politics. As weapons become more potent, and the “density” of potential targets—population density, in the case of traditional military offensives, or density of nodes in an information-sharing network, in the case of cyber offensives, as two examples—increases, the ability to affect outsized change increases as well. As such, incentives for, and frequency of, strategic action from non-state actors will likely continue to rise—this being yet another example.&lt;/li&gt;
&lt;li&gt;The technologies required to effectuate the attack in question are increasingly easy to obtain.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In sum, last week’s strike is yet another example of an isolated punch by a non-state actor with technologies (drones, at a minimum) ultimately available in the public domain. Moving forward, it seems safe to expect more of the same.&lt;/p&gt;
&lt;h2&gt;Enter machine learning&lt;/h2&gt;
&lt;p&gt;Throughout history, much of war has been about destroying the enemy’s stuff. There are many ways to do this—each involving a distinct set of tradeoffs.&lt;/p&gt;
&lt;p&gt;To this effect, drones are particularly interesting: they allow for precise, close-proximity strikes, without risking the lives of the aggressors. Presently, the majority of such drones are piloted remotely by humans; moving forward, they—in conceptual step with, say, self-driving cars—will pilot themselves, allowing for larger and larger deployments.&lt;/p&gt;
&lt;p&gt;To do this, engineers will equip drones with cameras, and machine learning algorithms that make the drone’s operational and tactical decisions conditional on what these cameras show. As such, the defensive objective is still, as per usual, thwart the aggressor; &lt;strong&gt;however, the aggressor is now the machine learning algorithms controlling the aggressor’s tech.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For drones, what do these algorithms look like? How can they be thwarted? What risks and opportunities do they imply for the defense of critical infrastructure?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;While these questions might seem futuristic, that future is indeed approaching fast.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The supervised classification model&lt;/h2&gt;
&lt;p&gt;To begin the discussion, let’s look at the basic paradigm by which self-driving cars operate: the supervised classification model, which works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Engineers collect a large amount of “labeled data” in the form of &lt;code&gt;(cameras_images, what_the_driver_did)&lt;/code&gt; pairs. This data is considered “labeled,” as each input, &lt;code&gt;cameras_images&lt;/code&gt;, is coupled with an output, &lt;code&gt;what_the_driver_did&lt;/code&gt;. An example of the former might be 360-degree images of a curving, barren road in Arizona; the latter, i.e. what the driver did in this moment in time, might be: depress the gas pedal by five degrees, and rotate the steering wheel counter-clockwise by seven.”&lt;/li&gt;
&lt;li&gt;“Teach” a computer to, given photographic input, predict the correct driver action to be taken.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Roughly speaking, a self-piloting drone would not be different: it’d require equivalent labeled data from human-controlled flights, on which a supervised classification model would be trained.&lt;/p&gt;
&lt;h2&gt;A likely progression of offensive drone tech&lt;/h2&gt;
&lt;p&gt;Weaponized drone deployment will likely progress as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy drones that pilot themselves; however, “finish” decisions are still executed by humans.&lt;/li&gt;
&lt;li&gt;Deploy drones that pilot themselves; “finish” decisions are executed by the drone—when it is extremely confident in its choice.&lt;/li&gt;
&lt;li&gt;Deploy swarms of drones that work together to more swiftly and efficiently achieve the aforementioned—attacking, exchanging data, self-sacrificing optimally, etc. throughout their attack.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;In a brave new world of machine-learned aggressors, what new opportunities for defense arise?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Adversarial examples in machine learning&lt;/h2&gt;
&lt;p&gt;Adversarial examples for supervised image classification models are inputs, i.e. images, whose pixels have been perturbed in a way imperceptible to a human eye, yet cause the classifier to change its prediction entirely.&lt;/p&gt;
&lt;p&gt;An example&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/images/saudi-adversarial/panda-adversarial-example.png"/&gt;&lt;/p&gt;
&lt;p&gt;Initially, the classifier is 57.7% confident that the image contains a panda; after adding noise, resulting in the image on the right—still, inarguably, a panda, to the human eye—the classifier has now changed its prediction to "gibbon" with 99.3% confidence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As such, knowledge of the data on which an aggressive drone was trained gives unique opportunity to algorithmically design adversarial examples, like the above, that confuse its classifier aboard.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Concrete opportunities would include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;At “find”&lt;/strong&gt;: defensive lasers pointed at critical infrastructure dynamically perturb, by just a few pixels, its appearance, tricking the drone into thinking it has not found what it actually has.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At “fix”&lt;/strong&gt;: these lasers dynamically concoct sequences of perturbations, tricking the drone into thinking its target is moving erratically when it’s actually still.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At “finish”&lt;/strong&gt;: adversarial examples make the drone believe its strike would, put simply, cause more damage than it intends.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As adversarial examples are, once more, built from real examples, perturbed minimally, in a way imperceptible to the human eye, they are in fact difficult to, even a priori, teach our classifier to ignore. Said differently, well-designed adversarial examples are extremely effective.&lt;/p&gt;
&lt;h2&gt;Out-of-distribution data&lt;/h2&gt;
&lt;p&gt;Machine learning algorithms are notoriously bad at “knowing what they don’t know.” Said differently, these algorithms only learn to make decisions about the type of data on which they’re trained. Were a classification model piloting a drone to be presented with an image of an ice-cream cone, it would, barring careful design, attempt to make a decision about this data all the same.&lt;/p&gt;
&lt;p&gt;In a world of autonomous fly-and-finish drones, one would hope that its finish decisions are taken with extreme care. Fundamentally, this dovetails quickly into the notion of “out-of-distribution” data, i.e. data that the classifier knows it has not seen before, and about which it therefore neglects to make a prediction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As such, insight into the data on which an enemy’s system was trained naturally implies “defense by what’s different”&lt;/strong&gt;: show the drone images you know that it hasn’t seen before, and thereby increase its uncertainty around the decision at hand—buying time, and keeping your stuff in tact.&lt;/p&gt;
&lt;h2&gt;Learning optimal defense via reinforcement learning&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;Reinforcement learning&lt;/a&gt;, though exceedingly powerful, and often overhyped, is a relatively simple idea: given an environment and its state, try an action, observe a reward, and repeat the actions that give high rewards; additionally, periodically explore new actions you’ve never tried before just to see how it goes.&lt;/p&gt;
&lt;p&gt;Reinforcement learning, or RL, requires vast amounts of data. As such, as a point of “meta defensive strategy”, trying out different types of adversarial-machine-learning attacks against a drone, seeing which work best, then repeating the best performers, while plausible in theory, would not work in practice if one drone approaches your one oil refinery but once a year.&lt;/p&gt;
&lt;p&gt;This said, swarms of drones might change the game; &lt;em&gt;what if, in 20 years, Houthi militants deployed one autonomous, armed drone for every square mile of Saudi territory?&lt;/em&gt; And what if we could simulate this scenario a priori, and learn how to optimally defend against the swarm?&lt;/p&gt;
&lt;p&gt;To do this, you'd require:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A heavy investment into sensors&lt;/strong&gt;, on all of: the infrastructure you’re trying to protect, atmospheric conditions in which the drones are flying, tools to monitor the drones’ speeds and movements, etc. In other words, any and all technologies that capture the state of the environment, and the rewards, to as granular level of detail as possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simulation environments.&lt;/strong&gt; In certain RL problems, this one potentially included, one has the delicious ability to generate data from which the algorithm can learn—by letting it play games with itself. &lt;a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch"&gt;AlphaGo Zero&lt;/a&gt; is one famous such example. Learning to optimally defend against a swarm of drones might fit the bit as well: deploy thousands of your own into a broad swath of desert, instruct them to “capture your flag”, then let your defensive systems get to work: taking actions, observing the subsequent reward—“Did I divert the drone away from my flag?”; “Did the drone hesitate more than usual before acting?”; etc.—and repeat those actions that work best.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;As the years roll forward, machine learning algorithms will continue to direct more and more of our most critical systems—financial markets, healthcare, transportation, and alas—the offensive technologies of political groups.&lt;/p&gt;
&lt;p&gt;To wit, an understanding of the workings of these algorithms sheds light on the new risks these systems will introduce, and the new strategic opportunities that will thereafter arise.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Goodfellow, I., Shlens, J., Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples https://arxiv.org/abs/1412.6572 &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;(Thumbnail) “Adversarial AI: As New Attack Vector Opens, Researchers Aim to Defend Against It.” DataProtectionCenter.com - Tech and Security, 17 Apr. 2018, dataprotectioncenter.com/malware/adversarial-ai-as-new-attack-vector-opens-researchers-aim-to-defend-against-it/. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="geopolitics"></category></entry><entry><title>Artificial Intelligence and Geopolitics</title><link href="https://cavaunpeu.github.io/2019/09/21/ai-and-geopolitics/" rel="alternate"></link><published>2019-09-21T21:00:00-04:00</published><updated>2019-09-21T21:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2019-09-21:/2019/09/21/ai-and-geopolitics/</id><summary type="html">&lt;p&gt;I'm beginning to write about the intersection of artificial intelligence and geopolitics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Machine learning and artificial intelligence have long been passions; to date, I've spent many years—my entire professional career included—developing relevant expertise. This has been the historical focus of this blog.&lt;/p&gt;
&lt;p&gt;Recently, I've been reading heavily in other areas as well: history, political theory, strategy, economics and geography; geopolitics, in sum.&lt;/p&gt;
&lt;p&gt;The geopolitics of yesterday—the subject of my reading—has its experts and central themes: collective security, nuclear armament, and oil, to name a few. Conversely, the geopolitics of tomorrow will take form from entirely new ideas; chief among them: artificial intelligence.&lt;/p&gt;
&lt;p&gt;As such, I plan to merge the two: to suffuse, through writing, my exploration of the former with my expertise in the latter. &lt;strong&gt;Specifically, I'll connect deep, technical knowledge of AI algorithms, as well as the likely implications of their widespread use, to the risks—and opportunities—that AI will bring to the geopolitics of tomorrow.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My writing will be objective, i.e. devoid of overt political bias. My intended audience is non-technical, i.e. geopolitics-first. Your comments—especially with respect to my evolving understanding of geopolitics itself—are encouraged as always.&lt;/p&gt;
&lt;p&gt;Will&lt;/p&gt;</content><category term="geopolitics"></category></entry><entry><title>Deriving Mean-Field Variational Bayes</title><link href="https://cavaunpeu.github.io/2018/11/23/mean-field-variational-bayes/" rel="alternate"></link><published>2018-11-23T10:00:00-05:00</published><updated>2018-11-23T10:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-11-23:/2018/11/23/mean-field-variational-bayes/</id><summary type="html">&lt;p&gt;A detailed derivation of Mean-Field Variational Bayes, its connection to Expectation-Maximization, and its implicit motivation for the "black-box variational inference" methods born in recent years.&lt;/p&gt;</summary><content type="html">&lt;p&gt;"Mean-Field Variational Bayes" (MFVB), is similar to &lt;a href="https://cavaunpeu.github.io/2018/11/11/em-for-lda/"&gt;expectation-maximization&lt;/a&gt; (EM) yet distinct in two key ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We do not minimize &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt;, i.e. perform the E-step, as [in the problems in which we employ mean-field] the posterior distribution &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt; "is too complex to work with,"™ i.e. it has no analytical form.&lt;/li&gt;
&lt;li&gt;Our variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; is a &lt;em&gt;factorized distribution&lt;/em&gt;, i.e.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
q(\mathbf{Z}) = \prod\limits_i^{M} q_i(\mathbf{Z}_i)
$$&lt;/div&gt;
&lt;p&gt;for all latent variables &lt;span class="math"&gt;\(\mathbf{Z}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Briefly, factorized distributions are cheap to compute: if each &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt; is Gaussian, &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; requires optimization of &lt;span class="math"&gt;\(2M\)&lt;/span&gt; parameters (a mean and a variance for each factor); conversely, a non-factorized &lt;span class="math"&gt;\(q(\mathbf{Z}) = \text{Normal}(\mu, \Sigma)\)&lt;/span&gt; would require optimization of &lt;span class="math"&gt;\(M + \frac{M^2 + M}{2}\)&lt;/span&gt; parameters for the mean and covariance respectively. Following intuition, this gain in computational efficiency comes at the cost of decreased accuracy in approximating the true posterior over latent variables.&lt;/p&gt;
&lt;h2&gt;So, what is it?&lt;/h2&gt;
&lt;p&gt;Mean-field Variational Bayes is an iterative maximization of the ELBO. More precisely, it is an iterative M-step with respect to the variational factors &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the simplest case, we posit a variational factor over every latent variable, &lt;em&gt;as well as every parameter&lt;/em&gt;. In other words, as compared to the log-marginal decomposition in EM, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is absorbed into &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\quad \text{(EM)}
$$&lt;/div&gt;
&lt;p&gt;becomes&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X})} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\quad \text{(MFVB)}
$$&lt;/div&gt;
&lt;p&gt;From there, we simply maximize the ELBO, i.e. &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]\)&lt;/span&gt;, by &lt;em&gt;iteratively maximizing with respect to each variational factor &lt;span class="math"&gt;\(q_i(\mathbf{Z}_i)\)&lt;/span&gt;&lt;/em&gt; in turn.&lt;/p&gt;
&lt;h2&gt;What's this do?&lt;/h2&gt;
&lt;p&gt;Curiously, we note that &lt;span class="math"&gt;\(\log{p(\mathbf{X})}\)&lt;/span&gt; is a &lt;em&gt;fixed quantity&lt;/em&gt; with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;: updating our variational factors &lt;em&gt;will not change&lt;/em&gt; the marginal log-likelihood of our data.&lt;/p&gt;
&lt;p&gt;This said, we note that the ELBO and &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X})\big)\)&lt;/span&gt; trade off linearly: when one goes up by &lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;, the other goes down by &lt;span class="math"&gt;\(\Delta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As such, (iteratively) maximizing the ELBO in MFVB is akin to minimizing the divergence between the true posterior over the latent variables given data and our factorized variational approximation thereof.&lt;/p&gt;
&lt;h2&gt;Derivation&lt;/h2&gt;
&lt;p&gt;So, what do these updates look like?&lt;/p&gt;
&lt;p&gt;First, let's break the ELBO into its two main components:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \int{q(\mathbf{Z})\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}}d\mathbf{Z}\\
&amp;amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z} - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;amp;= A + B
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Next, rewrite this expression in a way that isolates a single variational factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, i.e. the factor with respect to which we'd like to maximize the ELBO in a given iteration.&lt;/p&gt;
&lt;h2&gt;Expanding the first term&lt;/h2&gt;
&lt;div class="math"&gt;$$
\begin{align*}
A
&amp;amp;= \int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}d\mathbf{Z}}\\
&amp;amp;= \int{\prod\limits_{i}q_i(\mathbf{Z}_i)\log{p(\mathbf{X, Z})}d\mathbf{Z}_i}\\
&amp;amp;= \int{q_j(\mathbf{Z}_j)\bigg[\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i\bigg]}d\mathbf{Z}_j\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Following Bishop&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;'s derivation, we introduce the notation:&lt;/p&gt;
&lt;div class="math"&gt;$$
\int{\prod\limits_{i \neq j}q_i(\mathbf{Z}_{i})\log{p(\mathbf{X, Z})}}d\mathbf{Z}_i = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;A few things to note, and in case this looks strange:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Were the left-hand side to read &lt;span class="math"&gt;\(\int{q(\mathbf{Z})\log{p(\mathbf{X, Z})}}d\mathbf{Z}\)&lt;/span&gt;, this would look like the perfectly vanilla expectation &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;An expectation maps a function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, e.g. &lt;span class="math"&gt;\(\log{p(\mathbf{X, Z})}\)&lt;/span&gt;, to a single real number. As our expression reads &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; as opposed to &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;, we're conspicuously unable to integrate over the remaining factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;As such, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; gives a function of the value of &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;&lt;/strong&gt; which itself maps to the aforementioned real number.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To further illustrate, let's employ some toy Python code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Suppose `Z = [Z_1, Z_2, Z_3]`, with corresponding variational distributions `q_1`, `q_2`, `q_3`&lt;/span&gt;

&lt;span class="n"&gt;q_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;q_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;q_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Next, suppose we'd like to isolate Z_2&lt;/span&gt;
&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt;, written &lt;code&gt;E_i_neq_j_log_p_X_Z&lt;/code&gt; below, can be computed as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;E_i_neq_j_log_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;Z_i_neq_j_dists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;comb&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;Z_i_neq_j_dists&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;comb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;comb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dists&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Z_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;comb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z_i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Continuing with our notes, it was not immediately obvious to me how and why we're able to introduce a second integral sign on line 3 of the derivation above. Notwithstanding, the reason is quite simple; a simple exercise of nested for-loops is illustrative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before beginning, we remind the definition of an integral in code. In its simplest example, &lt;span class="math"&gt;\(\int{ydx}\)&lt;/span&gt; can be written as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lower_lim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper_lim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_ticks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;integral&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;dx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# ...where `n_ticks` approaches infinity.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With this in mind, the following confirms the self-evidence of the second integral sign:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# some dummy expression&lt;/span&gt;


&lt;span class="c1"&gt;# Line 2 of `Expanding the first term`&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt;
            &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt;
            &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt;
            &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;TOTAL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;


&lt;span class="c1"&gt;# Line 3 of `Expanding the first term`&lt;/span&gt;
&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;q_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_1&lt;/span&gt;
            &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z_2&lt;/span&gt;
            &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;val_z_0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_z_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;prob_z_2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ln_p_X_Z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;prob_z_0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;_total&lt;/span&gt;


&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;TOTAL&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In effect, isolating &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; is akin to the penultimate line &lt;code&gt;total += prob_z_0 * _total&lt;/code&gt;, i.e. multiplying &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; by an intermediate summation &lt;code&gt;_total&lt;/code&gt;.  Therefore, the second integral sign is akin to &lt;code&gt;_total += prob_z_1 * prob_z_2 * ln_p_X_Z(X, Z)&lt;/code&gt;, i.e. the computation of this intermediate summation itself.&lt;/p&gt;
&lt;p&gt;More succinctly, a multi-dimensional integral can be thought of as a nested-for-loop which commutes a global sum. Herein, we are free to compute intermediate sums at will.&lt;/p&gt;
&lt;h2&gt;Expanding the second term&lt;/h2&gt;
&lt;p&gt;Next, let's expand &lt;span class="math"&gt;\(B\)&lt;/span&gt;. We note that this is the entropy of the full variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
B
&amp;amp;= - \int{q(\mathbf{Z})\log{q(\mathbf{Z})}}d\mathbf{Z}\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\prod\limits_{i}q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)} + \sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] - \mathop{\mathbb{E}}_{q_{i \neq j}(\mathbf{Z}_i)}\bigg[\sum\limits_{i \neq j}\log{q_i(\mathbf{Z}_i)}\bigg]\\
&amp;amp;= - \mathop{\mathbb{E}}_{q_j(\mathbf{Z}_j)}\bigg[\log{q_j(\mathbf{Z}_j)}\bigg] + \text{const}\\
&amp;amp;= - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As we'll be maximizing w.r.t. just &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, we can set all terms that don't include this factor to constants.&lt;/p&gt;
&lt;h2&gt;Putting it back together&lt;/h2&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= A + B\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;One final pseudonym&lt;/h2&gt;
&lt;p&gt;Were we able to replace the expectation in &lt;span class="math"&gt;\(A\)&lt;/span&gt; with the &lt;span class="math"&gt;\(\log\)&lt;/span&gt; of some density &lt;span class="math"&gt;\(D\)&lt;/span&gt;, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
= \int{q_j(\mathbf{Z}_j){ \log{D} }\ d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(A + B\)&lt;/span&gt; could be rewritten as &lt;span class="math"&gt;\(-\text{KL}(q_j(\mathbf{Z}_j)\Vert D)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Acknowledging that &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\)&lt;/span&gt; is an unnormalized log-likelihood written as a function of &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;, we temporarily rewrite it as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}] = \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j})
$$&lt;/div&gt;
&lt;p&gt;As such:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z})}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \log{\tilde{p}(\mathbf{X}, \mathbf{Z}_j}) }d\mathbf{Z}_j} - \int{q_j(\mathbf{Z}_j)\log{q_j(\mathbf{Z}_j)}}d\mathbf{Z}_j + \text{const}\\
&amp;amp;= \int{q_j(\mathbf{Z}_j){ \log{\frac{\tilde{p}(\mathbf{X}, \mathbf{Z}_j)}{q_j(\mathbf{Z}_j)}} }d\mathbf{Z}_j} + \text{const}\\
&amp;amp;= - \text{KL}\big(q_j(\mathbf{Z}_j)\Vert \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\big) + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Finally, per this expression, the ELBO reaches its minimum when:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;amp;= \tilde{p}(\mathbf{X}, \mathbf{Z}_j)\\
&amp;amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Or equivalently:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Summing up:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iteratively minimizing the divergence between &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\tilde{p}(\mathbf{X}, \mathbf{Z}_j)\)&lt;/span&gt; for all factors &lt;span class="math"&gt;\(j\)&lt;/span&gt; is our mechanism for maximizing the ELBO&lt;/li&gt;
&lt;li&gt;In turn, maximizing the ELBO is our mechanism for minimizing the KL divergence between the full factorized posterior &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; and the true posterior &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X})\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, as the optimal density &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt; relies on those of &lt;span class="math"&gt;\(q_{i \neq j}(\mathbf{Z}_{i})\)&lt;/span&gt;, this optimization algorithm is necessarily &lt;em&gt;iterative&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Normalization constant&lt;/h2&gt;
&lt;p&gt;Nearing the end, we note that &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j) = \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}\)&lt;/span&gt; is not necessarily a normalized density (over &lt;span class="math"&gt;\(\mathbf{Z}_j\)&lt;/span&gt;). "By inspection," we compute:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
q_j(\mathbf{Z}_j)
&amp;amp;= \frac{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}}{\int{\exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)}d\mathbf{Z}_j}}\\
&amp;amp;= \exp{\bigg(\mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]\bigg)} + \text{const}\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;How to actually employ this thing&lt;/h2&gt;
&lt;p&gt;First, plug in values for the right-hand side of:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{q_j(\mathbf{Z}_j)} = \mathop{\mathbb{E}}_{i \neq j}[\log{p(\mathbf{X, Z})}]
$$&lt;/div&gt;
&lt;p&gt;Then, attempt to rearrange this expression such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Once exponentiated, giving &lt;span class="math"&gt;\(\exp{\big(\log{q_j(\mathbf{Z}_j)}\big)} = q_j(\mathbf{Z}_j)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;We are left with something that, once normalized (by inspection),&lt;/li&gt;
&lt;li&gt;Resembles a known density function (e.g. a Gaussian, a Gamma, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NB: This may require significant computation.&lt;/p&gt;
&lt;h1&gt;Approximating a Gaussian&lt;/h1&gt;
&lt;p&gt;Here, we'll approximate a 2D multivariate Gaussian with a factorized mean-field approximation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/mean-field-variational-bayes/mv-gaussian-approx-1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/mean-field-variational-bayes/mv-gaussian-approx-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/mean-field-variational-bayes/mv-gaussian-approx-3.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/mean-field-variational-bayes/mv-gaussian-approx-4.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/mean-field-variational-bayes/mv-gaussian-approx-5.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Summing up&lt;/h1&gt;
&lt;p&gt;Mean-Field Variational Bayes is an iterative optimization algorithm for maximizing a lower-bound of the marginal likelihood of some data &lt;span class="math"&gt;\(\mathbf{X}\)&lt;/span&gt; under a given model with latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;. It accomplishes task by positing a factorized variational distribution over all latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt; and parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, then computes, &lt;em&gt;analytically&lt;/em&gt;, the algebraic forms and parameters of each factor which maximize this bound.&lt;/p&gt;
&lt;p&gt;In practice, this process can be cumbersome and labor-intensive. As such, in recent years, "black-box variational inference" techniques were born, which &lt;em&gt;fix&lt;/em&gt; the forms of each factor &lt;span class="math"&gt;\(q_j(\mathbf{Z}_j)\)&lt;/span&gt;, then optimize its parameters via gradient descent.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Deriving Expectation-Maximization</title><link href="https://cavaunpeu.github.io/2018/11/11/em-for-lda/" rel="alternate"></link><published>2018-11-11T16:00:00-05:00</published><updated>2018-11-11T16:00:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-11-11:/2018/11/11/em-for-lda/</id><summary type="html">&lt;p&gt;Deriving the expectation-maximization algorithm, and the beginnings of its application to LDA. Once finished, its intimate connection to variational inference is apparent.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Consider a model with parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; and latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;; the expectation-maximization algorithm (EM) is a mechanism for computing the values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that, under this model, maximize the likelihood of some observed data &lt;span class="math"&gt;\(\mathbf{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The joint probability of our model can be written as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\mathbf{X}, \mathbf{Z}\vert \theta) = p(\mathbf{X}\vert \mathbf{Z}, \theta)p(\mathbf{Z}\vert \theta)
$$&lt;/div&gt;
&lt;p&gt;where, once more, our stated goal is to maximize the marginal likelihood of our data:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \log{\sum_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}
$$&lt;/div&gt;
&lt;p&gt;An example of a latent variable model is the Latent Dirichlet Allocation&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; (LDA) model for uncovering latent topics in documents of text. Once finished deriving the general EM equations, we'll (begin to) apply them to this model.&lt;/p&gt;
&lt;h2&gt;Why not maximum likelihood estimation?&lt;/h2&gt;
&lt;p&gt;As the adage goes, computing the MLE with respect to this marginal is "hard." I loosely understand why. In any case, Bishop&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A key observation is that the summation over the latent variables appears inside the logarithm. Even if the joint distribution &lt;span class="math"&gt;\(p(\mathbf{X, Z}\vert\theta)\)&lt;/span&gt; belongs to the exponential family, the marginal distribution &lt;span class="math"&gt;\(p(\mathbf{X}\vert\theta)\)&lt;/span&gt; typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;We'll want something else to maximize instead.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;A lower bound&lt;/h2&gt;
&lt;p&gt;Instead of maximizing the log-marginal &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; (with respect to model parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;), let's maximize a lower-bound with a less-problematic form.&lt;/p&gt;
&lt;p&gt;One candidate for this form is &lt;span class="math"&gt;\(\log{p(\mathbf{X}, \mathbf{Z}\vert \theta)}\)&lt;/span&gt; which, almost tautologically, removes the summation over latent variables &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As such, let's derive a lower-bound which features this term. As &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; is often called the log-"evidence," we'll call our expression the "evidence lower-bound," or ELBO.&lt;/p&gt;
&lt;h2&gt;Jensen's inequality&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality"&gt;Jensen's inequality&lt;/a&gt;&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; generalizes the statement that the line secant to a &lt;strong&gt;concave function&lt;/strong&gt; lies below this function. An example is illustrative:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://alliance.seas.upenn.edu/~cis520/dynamic/2017/wiki/uploads/Lectures/jensen.png"/&gt;&lt;/p&gt;
&lt;p&gt;First, we note that the red line is below the blue for all points for which it is defined.&lt;/p&gt;
&lt;p&gt;Second, working through the example, and assuming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(y = f(x) = \exp(-(x - 2)^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(v_1 = 1; v_2 = 2.5; \alpha = .3\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{align*}
f(v_1) &amp;amp;\approx .3679\\
f(v_2) &amp;amp;\approx .7788\\
\alpha f(v_1) + (1 - \alpha)f(v_2) &amp;amp;\approx \bf{.6555}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\alpha v_1 + (1 - \alpha)v_2 &amp;amp;= 2.05\\
f(\alpha v_1 + (1 - \alpha)v_2) &amp;amp;\approx \bf{.9975}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;we see that &lt;strong&gt;&lt;span class="math"&gt;\(\alpha f(v_1) + (1 - \alpha)f(v_2) \leq f(\alpha v_1 + (1 - \alpha)v_2)\)&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we arrive at a general form:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}_{v}}[f(v)] \leq f(\mathop{\mathbb{E}_{v}}[v])
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(p(v) = \alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Deriving the ELBO&lt;/h2&gt;
&lt;p&gt;In trying to align &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}
= \log{\sum\limits_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}\)&lt;/span&gt; with &lt;span class="math"&gt;\(f(\mathop{\mathbb{E}_{v}}[v])\)&lt;/span&gt;, we see a function &lt;span class="math"&gt;\(f = \log\)&lt;/span&gt; yet no expectation inside. However, given the summation over &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt;, introducing some distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; would give us the expectation we desire.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{p(\mathbf{X}\vert\theta)}
&amp;amp;= \log{\sum_{\mathbf{Z}}p(\mathbf{X, Z}\vert\theta)}\\
&amp;amp;= \log{\sum_{\mathbf{Z}}q(\mathbf{Z})\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\\
&amp;amp;= \log{\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; is some distribution over &lt;span class="math"&gt;\(\mathbf{Z}\)&lt;/span&gt; with parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; (omitted for cleanliness) and known form (e.g. a Gaussian). It is often referred to as a &lt;strong&gt;variational distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;From here, via Jensen's inequality, we can derive the lower-bound:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{p(\mathbf{X}\vert\theta)} = \log{\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}\bigg]}
&amp;amp;\geq \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + R
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Et voilà&lt;/em&gt;, we see that this term contains &lt;span class="math"&gt;\(\log{p(\mathbf{X, Z}\vert\theta)}\)&lt;/span&gt;; the ELBO should be easy to optimize with respect to our parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;So, what's &lt;span class="math"&gt;\(R\)&lt;/span&gt;?&lt;/h1&gt;
&lt;div class="math"&gt;$$
\begin{align*}
R
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \sum_{\mathbf{Z}}q(\mathbf{Z})\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Before expanding further, let's briefly restate basic results of Bayes' theorem as applied to our model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta) = \frac{p(\mathbf{X}, \mathbf{Z}\vert\theta)}{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\log{p(\mathbf{X}, \mathbf{Z}\vert\theta)} = \log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, we note that as &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; does not depend on &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X}\vert\theta)}\bigg] = \log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuing:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
R
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \sum_{\mathbf{Z}}q(\mathbf{Z})\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg(\log{p(\mathbf{X, Z}\vert\theta) - \log{q(\mathbf{Z})}}\bigg)\\
&amp;amp;= \log{p(\mathbf{X}\vert\theta)} -  \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg(\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\bigg)\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})\log{p(\mathbf{X}\vert\theta)} -  \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg(\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\bigg)\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})\bigg(\log{p(\mathbf{X}\vert\theta)} -  \big(\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} + \log{p(\mathbf{X}\vert\theta)} - \log{q(\mathbf{Z})}\big)\bigg)\\
&amp;amp;= \sum_{\mathbf{Z}}q(\mathbf{Z})-  \big(\log{p(\mathbf{Z}\vert\mathbf{X}, \theta)} - \log{q(\mathbf{Z})}\big)\\
&amp;amp;=
-\sum_{\mathbf{Z}}q(\mathbf{Z})\log{\frac{p(\mathbf{Z}\vert\mathbf{X}, \theta)}{q(\mathbf{Z})}}\\
&amp;amp;= \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Putting it back together:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{X}\vert\theta)} = \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg] + \text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)
$$&lt;/div&gt;
&lt;h2&gt;The EM algorithm&lt;/h2&gt;
&lt;p&gt;The algorithm can be described by a few simple observations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; is a divergence metric which is strictly non-negative.&lt;/li&gt;
&lt;li&gt;As &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; does not depend on &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;—if we decrease &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; &lt;em&gt;by changing &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;&lt;/em&gt;, the ELBO must increase to compensate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(For intuition, imagine we're able to decrease &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; to 0, which occurs when setting &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt;.)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If we increase the ELBO &lt;em&gt;by changing &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/em&gt;, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; will increase as well. &lt;em&gt;In addition, as &lt;span class="math"&gt;\(p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt; now (likely) diverges from &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; in non-zero amount, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; will increase even more.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;The EM algorithm is a repeated alternation between Step 2 (E-step) and Step 3 (M-step).&lt;/strong&gt; After each M-Step, &lt;span class="math"&gt;\(\log{p(\mathbf{X}\vert\theta)}\)&lt;/span&gt; is guaranteed to increase (unless it is already at a maximum)&lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;A graphic&lt;sup id="fnref3:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; (&lt;em&gt;Pattern Recognition and Machine Learning, Chapter 9&lt;/em&gt;) is further illustrative.&lt;/p&gt;
&lt;h3&gt;Initial decomposition&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/em-for-lda/initial_decomp.png"/&gt;&lt;/p&gt;
&lt;p&gt;Here, the ELBO is written as &lt;span class="math"&gt;\(\mathcal{L}(q, \theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;E-step&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/em-for-lda/e_step.png"/&gt;&lt;/p&gt;
&lt;p&gt;Holding the parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; constant, minimize &lt;span class="math"&gt;\(\text{KL}\big(q(\mathbf{Z})\Vert p(\mathbf{Z}\vert\mathbf{X}, \theta)\big)\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;. Remember, as &lt;span class="math"&gt;\(q\)&lt;/span&gt; is a distribution with a fixed functional form, this amounts to updating its parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The caption implies that we can always compute &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\mathbf{Z}\vert\mathbf{X}, \theta)\)&lt;/span&gt;. We will below that this is not the case for LDA, nor for many interesting models.&lt;/p&gt;
&lt;h3&gt;M-step&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/em-for-lda/m_step.png"/&gt;&lt;/p&gt;
&lt;p&gt;In the M-step, maximize the ELBO with respect to the model parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Expanding the ELBO:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\frac{p(\mathbf{X, Z}\vert\theta)}{q(\mathbf{Z})}}\bigg]
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta)}\bigg] - \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{q(\mathbf{Z})}\bigg]\\
&amp;amp;= \mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{p(\mathbf{X, Z}\vert\theta)}\bigg] + \mathbf{H}[q(\mathbf{Z})]
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;we see that it decomposes into an expectation of the joint distribution over data and latent variables with respect to the variational distribution &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;, plus the entropy of &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Maximizing this expression with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we treat the latter as a constant.&lt;/p&gt;
&lt;h2&gt;EM for LDA&lt;/h2&gt;
&lt;p&gt;In the next few posts, we'll use the Latent Dirichlet Allocation (LDA) model as a running example.&lt;/p&gt;
&lt;p&gt;Since the original paper&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; is beautiful, I'll default to citing its passages as much as possible.&lt;/p&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/em-for-lda/lda_formulation.png"/&gt;&lt;/p&gt;
&lt;p&gt;"Given the parameters &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, the joint distribution of a topic mixture &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, a set of of &lt;span class="math"&gt;\(N\)&lt;/span&gt; topics &lt;span class="math"&gt;\(\mathbf{z}\)&lt;/span&gt;, and a set of &lt;span class="math"&gt;\(N\)&lt;/span&gt; words &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; is given by:"&lt;sup id="fnref3:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta, \mathbf{z}, \mathbf{w}\vert \alpha, \beta) = p(\theta\vert \alpha)\prod\limits_{n=1}^{N}p(z_n\vert \theta)p(w_n\vert z_n, \beta)
$$&lt;/div&gt;
&lt;h3&gt;Log-evidence&lt;/h3&gt;
&lt;p&gt;The (problematic) log-evidence of a single document:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p(\mathbf{w}\vert \alpha, \beta)} = \log{\int p(\theta\vert \alpha)\prod\limits_{n=1}^{N}\sum\limits_{z_n} p(z_n\vert \theta)p(w_n\vert z_n, \beta)d\theta}
$$&lt;/div&gt;
&lt;p&gt;NB: The parameters of our model are &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\{\theta, \mathbf{z}\}\)&lt;/span&gt; are our &lt;em&gt;latent variables.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;ELBO&lt;/h3&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{q(\mathbf{Z})}\bigg[\log{\bigg(\frac{p(\theta\vert \alpha)}{q(\mathbf{Z})}}\prod\limits_{n=1}^{N}p(z_n\vert \theta)p(w_n\vert z_n, \beta)\bigg)\bigg]
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{Z} = \{\theta, \mathbf{z}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;KL term&lt;/h3&gt;
&lt;div class="math"&gt;$$
\text{KL}\big(q(\mathbf{Z})\Vert \frac{p(\theta, \mathbf{z}, \mathbf{w}\vert \alpha, \beta)}{p(\mathbf{w}\vert \alpha, \beta)}\big)
$$&lt;/div&gt;
&lt;p&gt;Peering at the denominator, we see that the expression under the integral is exponential in the number of words &lt;span class="math"&gt;\(N\)&lt;/span&gt;; for any non-trivial &lt;span class="math"&gt;\(N\)&lt;/span&gt; and number of topics, it is intractable to compute. As such, the "ideal" E-step solution &lt;span class="math"&gt;\(q(\mathbf{Z}) = p(\theta, \mathbf{z}\vert \mathbf{w}, \alpha, \beta)\)&lt;/span&gt; admits no analytical form.&lt;/p&gt;
&lt;p&gt;In the next post, we'll cover how to minimize this KL term with respect to &lt;span class="math"&gt;\(q(\mathbf{Z})\)&lt;/span&gt; in detail. This effort will begin with the derivation of the mean-field algorithm.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we motivated the expectation-maximization algorithm then derived its general form. We then, briefly, applied it to the LDA model.&lt;/p&gt;
&lt;p&gt;In the next post, we'll expand this logic into mean-field variational Bayes, and eventually, variational inference more broadly.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;C. M. Bishop. Pattern recognition and machine learning,
page 229. Springer-Verlag New York, 2006. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Wikipedia contributors. "Jensen's inequality." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 29 Oct. 2018. Web. 11 Nov. 2018. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Additional Strategies for Confronting the Partition Function</title><link href="https://cavaunpeu.github.io/2018/10/29/additional-strategies-partition-function/" rel="alternate"></link><published>2018-10-29T22:00:00-04:00</published><updated>2018-10-29T22:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-10-29:/2018/10/29/additional-strategies-partition-function/</id><summary type="html">&lt;p&gt;Stochastic maximum likelihood, contrastive divergence, negative contrastive estimation and negative sampling for improving or avoiding the computation of the gradient of the log-partition function. (Oof, that's a mouthful.)&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="https://cavaunpeu.github.io/2018/10/20/thorough-introduction-to-boltzmann-machines/"&gt;previous post&lt;/a&gt; we introduced Boltzmann machines and the infeasibility of computing the gradient of its log-partition function &lt;span class="math"&gt;\(\nabla_{\theta}\log{Z}\)&lt;/span&gt;. To this end, we explored one strategy for its approximation: Gibbs sampling. Gibbs sampling is a viable alternative because the expression for this gradient simplifies to an expectation over the model distribution, which can be approximated with Monte Carlo samples.&lt;/p&gt;
&lt;p&gt;In this post, we'll highlight the imperfections of even this approach, then present more preferable alternatives.&lt;/p&gt;
&lt;h1&gt;Pitfalls of Gibbs sampling&lt;/h1&gt;
&lt;p&gt;To refresh, the two gradients we seek to compute in a reasonable amount of time are:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{Z} = \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\\
\nabla_{b_{i}}\log{Z} = \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$&lt;/div&gt;
&lt;p&gt;Via Gibbs sampling, we approximate each by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Burning in a Markov chain w.r.t. our model, then selecting &lt;span class="math"&gt;\(n\)&lt;/span&gt; samples from this chain&lt;/li&gt;
&lt;li&gt;Evaluating both functions (&lt;span class="math"&gt;\(x_i  x_j\)&lt;/span&gt;, and &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;) at these samples&lt;/li&gt;
&lt;li&gt;Taking the average of each&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Concretely:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{Z} \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}\\
\nabla_{b_{i}}\log{Z} \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;We perform this sampling process at each gradient step.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;The cost of burning in each chain&lt;/h2&gt;
&lt;p&gt;Initializing a Markov chain at a random sample incurs a "burn-in" process which comes at non-trivial cost. If paying this cost at each gradient step, it begins to add up. How can we do better?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In the remainder of the post, we'll explore two new directives for approximating the negative phase more cheaply, and the algorithms they birth.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Directive #1: Cheapen the burn-in process&lt;/h1&gt;
&lt;h2&gt;Stochastic maximum likelihood&lt;/h2&gt;
&lt;p&gt;SML assumes the premise: let's initialize our chain at a point already close to the model's true distribution—reducing or perhaps eliminating the cost of burn-in altogether.  &lt;strong&gt;This given, at what sample do we initialize the chain?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In SML, we simply initialize at the terminal value of the previous chain (i.e. the one we manufactured to compute the gradients of the previous mini-batch). &lt;strong&gt;As long as the model has not changed significantly since, i.e. as long as the previous parameter update (gradient step) was not too large, this sample should exist in a region of high probability under the current model.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In code, this might look like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_obs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;  &lt;span class="c1"&gt;# X holds all of our observations&lt;/span&gt;

&lt;span class="c1"&gt;# Vanilla Gibbs sampling&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# SML&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;previous_samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;Per the expression for the full log-likelihood gradient, e.g. &lt;span class="math"&gt;\(\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, the negative phase works to "reduce the probability of the points in which the model strongly, yet wrongly, believes".&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; Since we approximate this term at each parameter update with samples &lt;em&gt;roughly from&lt;/em&gt; the current model's true distribution, &lt;strong&gt;we do not encroach on this foundational task.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Contrastive divergence&lt;/h2&gt;
&lt;p&gt;Alternatively, in the contrastive divergence algorithm, we initialize the chain at each gradient step with a sample from the data distribution.&lt;/p&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;With no guarantee that the data distribution resembles the model distribution, we may systematically fail to sample, and thereafter "suppress," points that are incorrectly likely under the latter (as they do not appear in the former!). &lt;strong&gt;This incurs the growth of "spurious modes"&lt;/strong&gt; in our model, aptly named.&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;In code, this might look like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Vanilla Gibbs sampling&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# SML&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;previous_samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="c1"&gt;# Contrastive divergence&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_obs&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Cheapening the burn-in phase indeed gives us a more efficient training routine. Moving forward, what are some even more aggressive strategies we can explore?&lt;/p&gt;
&lt;h1&gt;Directive #2: Skip the computation of &lt;span class="math"&gt;\(Z\)&lt;/span&gt; altogether&lt;/h1&gt;
&lt;p&gt;Canonically, we write the log-likelihood of our Boltzmann machine as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{\mathcal{L}(x)}
&amp;amp;= \log{\frac{\exp{(H(x))}}{Z}}\\
&amp;amp;= \log{\big(\exp{(H(x))}\big)} - \log{Z}\\
&amp;amp;= H(x) - \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Instead, what if we simply wrote this as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{\mathcal{L}(x)} = H(x) - c
$$&lt;/div&gt;
&lt;p&gt;or, more generally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\log{p_{\text{model}}(x)} = \log{\tilde{p}_{\text{model}}(x; \theta)} - c
$$&lt;/div&gt;
&lt;p&gt;and estimated &lt;span class="math"&gt;\(c\)&lt;/span&gt; as a parameter?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Immediately, we remark that if we optimize this model with maximum likelihood, our algorithm will, trivially, make &lt;span class="math"&gt;\(c\)&lt;/span&gt; arbitrarily negative.&lt;/strong&gt; In other words, the quickest way to increase the thing on the left is to decrease &lt;span class="math"&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How might we better phrase this problem?&lt;/p&gt;
&lt;h2&gt;Noise contrastive estimation&lt;/h2&gt;
&lt;p&gt;Ingeniously, NCE proposes an alternative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Posit two distributions: the model, and a noise distribution&lt;/li&gt;
&lt;li&gt;Given a data point, predict from which distribution this point was generated&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's unpack this a bit.&lt;/p&gt;
&lt;p&gt;Under an (erroneous) MLE formulation, we would optimize the following objective:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x \sim p_{\text{data}}} [\log{p_{\text{model}}}(x)]
$$&lt;/div&gt;
&lt;p&gt;Under NCE, we're going to replace two pieces so as to perform the binary classification task described above (with 1 = "model", and 0 = "noise").&lt;/p&gt;
&lt;p&gt;First, let's swap &lt;span class="math"&gt;\(\log{p_{\text{model}}}(x)\)&lt;/span&gt; with &lt;span class="math"&gt;\(\log{p_{\text{joint}}}(y = 0\vert x)\)&lt;/span&gt;, where:&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(x\vert y) =
\begin{cases}
p_{\text{noise}}(x)\quad y = 0\\
p_{\text{model}}(x)\quad y = 1\\
\end{cases}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(x, y)
= p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p_{\text{joint}}(y = 0\vert x)
= \frac{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)}
$$&lt;/div&gt;
&lt;p&gt;Finally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x \sim p_{\text{data}}} [\log{p_{\text{joint}}(y = 0\vert x)}]
$$&lt;/div&gt;
&lt;p&gt;From here, we need to update &lt;span class="math"&gt;\(x \sim p_{\text{data}}\)&lt;/span&gt; to include &lt;span class="math"&gt;\(y\)&lt;/span&gt;. We'll do this in two pedantic steps.&lt;/p&gt;
&lt;p&gt;First, let's write:&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x, y=0\ \sim\ p_{\text{noise}}} [\log{p_{\text{joint}}(y\vert x)}]
$$&lt;/div&gt;
&lt;p&gt;This equation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Builds a classifier that discriminates between samples generated from the model distribution and noise distribution &lt;strong&gt;trained only on samples from the latter.&lt;/strong&gt; (Clearly, this will not make for an effective classifier.)&lt;/li&gt;
&lt;li&gt;To train this classifier, we note that the equation asks us to maximize the likelihood of the noise samples under the noise distribution—where the noise distribution itself has no actual parameters we intend to train!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In solution, we trivially expand our expectation to one over both noise samples, and data samples. In doing so, in predicting &lt;span class="math"&gt;\(\log{p_{\text{joint}}(y = 1\vert x)} = 1 - \log{p_{\text{joint}}(y = 0\vert x)}\)&lt;/span&gt;, &lt;strong&gt;we'll be maximizing the likelihood of the data under the model.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta, c = \underset{\theta, c}{\arg\max}\  \mathbb{E}_{x, y\ \sim\ p_{\text{train}}} [\log{p_{\text{joint}}(y \vert x)}]
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\text{train}}(x\vert y) =
\begin{cases}
p_{\text{noise}}(x)\quad y = 0\\
p_{\text{data}}(x)\quad y = 1\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;As a final step, we'll expand our object into something more elegant:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x) + p_{\text{joint}}(y = 1)p_{\text{model}}(x)}\\
&amp;amp;= \frac{1}{1 + \frac{p_{\text{joint}}(y = 1)p_{\text{model}}(x)}{p_{\text{joint}}(y = 0)p_{\text{noise}}(x)}}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Assuming &lt;em&gt;a priori&lt;/em&gt; that &lt;span class="math"&gt;\(p_{\text{joint}}(x, y)\)&lt;/span&gt; is &lt;span class="math"&gt;\(k\)&lt;/span&gt; times more likely to generate a noise sample, i.e. &lt;span class="math"&gt;\(\frac{p_{\text{joint}}(y = 1)}{p_{\text{joint}}(y = 0)} = \frac{1}{k}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{p_{\text{noise}}(x)\cdot k}}\\
&amp;amp;= \frac{1}{1 + \exp\big(\log{\frac{p_{\text{model}}(x)}{{p_{\text{noise}}(x)\cdot k}}}\big)}\\
&amp;amp;= \sigma\bigg(-\log{\frac{p_{\text{model}}(x)}{{p_{\text{noise}}(x)\cdot k}}}\bigg)\\
&amp;amp;= \sigma\bigg(\log{k} + \log{p_{\text{noise}}(x)} - \log{p_{\text{model}}(x)}\bigg)\\
p_{\text{joint}}(y = 1\vert x)
&amp;amp;= 1 - \sigma\bigg(\log{k} + \log{p_{\text{noise}}(x)} - \log{p_{\text{model}}(x)}\bigg)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Given a joint training distribution over &lt;span class="math"&gt;\((X_{\text{data}}, y=1)\)&lt;/span&gt; and &lt;span class="math"&gt;\((X_{\text{noise}}, y=0)\)&lt;/span&gt;, this is the target we'd like to maximize.&lt;/p&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;For our training data, &lt;strong&gt;we require the ability to sample from our noise distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For our target, &lt;strong&gt;we require the ability to compute the likelihood of some data under our noise distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Therefore, these criterion do place practical restrictions on the types of noise distributions that we're able to consider.&lt;/p&gt;
&lt;h2&gt;Extensions&lt;/h2&gt;
&lt;p&gt;We briefly alluded to the fact that our noise distribution is non-parametric. However, there is nothing stopping us from evolving this distribution and giving it trainable parameters, then updating these parameters such that it generates increasingly "optimal" samples.&lt;/p&gt;
&lt;p&gt;Of course, we would have to design what "optimal" means. One interesting approach is called &lt;a href="https://arxiv.org/abs/1805.03642"&gt;Adversarial Contrastive Estimation
&lt;/a&gt;, wherein the authors adapt the noise distribution to generate increasingly "harder negative examples, which forces the main model to learn a better representation of the data."&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;Negative sampling is the same as NCE except:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We consider noise distributions whose likelihood we cannot evaluate&lt;/li&gt;
&lt;li&gt;To accommodate, we simply set &lt;span class="math"&gt;\(p_{\text{noise}}(x) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p_{\text{joint}}(y = 0\vert x)
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{p_{\text{noise}}(x)\cdot k}}\\
&amp;amp;= \frac{1}{1 + \frac{p_{\text{model}}(x)}{ k}}\\
&amp;amp;=\sigma(-\frac{p_{\text{model}}(x)}{ k})\\
&amp;amp;=\sigma(\log{k} - \log{p_{\text{model}}(x)})\\
p_{\text{joint}}(y = 1\vert x)
&amp;amp;= 1 - \sigma(\log{k} - \log{p_{\text{model}}(x)})
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;In code&lt;/h2&gt;
&lt;p&gt;Since I learn best by implementing things, let's play around. Below, we train Boltzmann machines via noise contrastive estimation and negative sampling.&lt;/p&gt;
&lt;h2&gt;Load data&lt;/h2&gt;
&lt;p&gt;For this exercise, we'll fit a Boltzmann machine to the &lt;a href="https://www.kaggle.com/zalando-research/fashionmnist"&gt;Fashion MNIST&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/additional-strategies-partition-function/output_3_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Define model&lt;/h2&gt;
&lt;p&gt;Below, as opposed to in the previous post, I offer a vectorized implementation of the Boltzmann energy function.&lt;/p&gt;
&lt;p&gt;This said, the code is still imperfect: especially re: the line in which I iterate through data points individually to compute the joint likelihood.&lt;/p&gt;
&lt;p&gt;Finally, in &lt;code&gt;Model._H&lt;/code&gt;, I divide by 1000 to get this thing to train. The following is only a toy exercise (like many of my posts); I did not spend much time tuning parameters.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xavier_uniform_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diagonal_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diagonal_mask&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;triu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;            where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;            for which to compute the joint likelihood&lt;/span&gt;

&lt;span class="sd"&gt;        :return: the likelihood, or log-likelihood if `log=True`&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
        &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;

        &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# make copy&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Noise contrastive estimation&lt;/h2&gt;
&lt;p&gt;Train a model using noise contrastive estimation. For our noise distribution, we'll start with a diagonal multivariate Gaussian, from which we can sample, and whose likelihood we can evaluate (as of PyTorch 0.4!).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# define model, noise distribution&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MultivariateNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;covariance_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# define classifier. we add a multiplicative constant to make training more stable.&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# define noise generator&lt;/span&gt;
&lt;span class="n"&gt;noise_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;noiseloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noise_sample&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define optimizer, loss&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BCELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'inf'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

            &lt;span class="c1"&gt;# points from data distribution&lt;/span&gt;
            &lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
            &lt;span class="n"&gt;X_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

            &lt;span class="c1"&gt;# points from noise distribution&lt;/span&gt;
            &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;
            &lt;span class="n"&gt;X_noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_noise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

            &lt;span class="c1"&gt;# stack into single input&lt;/span&gt;
            &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;X_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_noise&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_noise&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Batch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; | Loss: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'weights.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'bias.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'c: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'weights.grad.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'bias.grad.mean(): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'c.grad: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train model&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.305&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0887&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0794&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0603&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0525&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0503&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0414&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.038&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.034&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0312&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;Next, we'll try negative sampling using some actual images as negative samples&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;noiseset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'data/mnist'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noiseloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noiseset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get some random training images&lt;/span&gt;
&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# show images&lt;/span&gt;
&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torchvision&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make_grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/additional-strategies-partition-function/output_12_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Train model&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# define model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define classifier&lt;/span&gt;
&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# i had to change this learning rate to get this to train&lt;/span&gt;

&lt;span class="c1"&gt;# train&lt;/span&gt;
&lt;span class="n"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noiseloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.304&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.027&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0111&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00611&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00505&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00318&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00284&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0029&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0023&lt;/span&gt;
&lt;span class="n"&gt;Batch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.00217&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Sampling&lt;/h1&gt;
&lt;p&gt;Once more, the (ideal) goal of this model is to fit a function &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; to some data, such that we can:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluate its likelihood (wherein it actually tells us that data to which the model was fit is more likely than data to which it was not)&lt;/li&gt;
&lt;li&gt;Draw realistic samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From a Boltzmann machine, our primary strategy for drawing samples is via Gibbs sampling. It's slow, and I do not believe it's meant to work particularly well. Let's draw 5 samples and see how we do.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;

&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;CPU times: user 4min 10s, sys: 4.09 s, total: 4min 14s&lt;/span&gt;
&lt;span class="err"&gt;Wall time: 4min 17s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Takes forever!&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/additional-strategies-partition-function/output_18_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Nothing great. These samples are highly correlated, if perfectly identical, as expected.&lt;/p&gt;
&lt;p&gt;To generate better images, we'll have to let this run for a lot longer and "thin" the chain (taking every &lt;code&gt;every_n&lt;/code&gt; samples, where &lt;code&gt;every_n&lt;/code&gt; is on the order of 1, 10, or 100, roughly).&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, we discussed four additional strategies for both speeding up, as well as outright avoiding, the computation of the gradient of the log-partition function &lt;span class="math"&gt;\(\nabla_{\theta}\log{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While we only presented toy models here, these strategies see successful application in larger undirected graphical models, as well as directed conditional models for &lt;span class="math"&gt;\(p(y\vert x)\)&lt;/span&gt;. One key example of the latter is a language model; though the partition function is a sum over distinct values of &lt;span class="math"&gt;\(y\)&lt;/span&gt; (labels) instead of configurations of &lt;span class="math"&gt;\(x\)&lt;/span&gt; (inputs), it can still be intractable to compute! This is because there are as many distinct values of &lt;span class="math"&gt;\(y\)&lt;/span&gt; as there are tokens in the given language's vocabulary, which is typically on the order of millions.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/boltzmann-machines"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-2.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;@book{Goodfellow-et-al-2016,
title={Deep Learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
publisher={MIT Press},
note={\url{http://www.deeplearningbook.org}},
year={2016}
} &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1805.03642"&gt;Adversarial Contrastive Estimation&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>A Thorough Introduction to Boltzmann Machines</title><link href="https://cavaunpeu.github.io/2018/10/20/thorough-introduction-to-boltzmann-machines/" rel="alternate"></link><published>2018-10-20T14:00:00-04:00</published><updated>2018-10-20T14:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-10-20:/2018/10/20/thorough-introduction-to-boltzmann-machines/</id><summary type="html">&lt;p&gt;A pedantic walk through Boltzmann machines, with focus on the computational thorn-in-side of the partition function.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The principal task of machine learning is to fit a model to some data. Thinking on the level of APIs, a model is an object with two methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;How likely is the query point(s) &lt;span class="math"&gt;\(x\)&lt;/span&gt; under our model? In other words, how likely was it that our model produced &lt;span class="math"&gt;\(x\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The likelihood gives a value proportional to a valid probability, but is not necessarily a valid probability itself.&lt;/p&gt;
&lt;p&gt;(Finally, the likelihood is often used as an umbrella term for, or interchangeably with, the probability density. The mainstream machine learning community would do well to agree to the use of one of these terms, and to sunset the other; while their definitions may differ slightly, the confusion brought about by their dual used in almost identical contexts simply does not warrant by the pedagogical purity we maintain by keeping them distinct.&lt;/p&gt;
&lt;h2&gt;Sample&lt;/h2&gt;
&lt;p&gt;Draw samples from the model.&lt;/p&gt;
&lt;h2&gt;Denotation&lt;/h2&gt;
&lt;p&gt;Canonically, we denote an instance of our &lt;code&gt;Model&lt;/code&gt; in mathematical syntax as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
x \sim p(x)
$$&lt;/div&gt;
&lt;p&gt;Again, this simple denotation implies two methods: that we can evaluate the likelihood of having observed &lt;span class="math"&gt;\(x\)&lt;/span&gt; under our model &lt;span class="math"&gt;\(p\)&lt;/span&gt;, and that we can sample a new value &lt;span class="math"&gt;\(x\)&lt;/span&gt; from our model &lt;span class="math"&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Often, we work with &lt;em&gt;conditional&lt;/em&gt; models, such as &lt;span class="math"&gt;\(y \sim p(y\vert x)\)&lt;/span&gt;, in classification and regression tasks. The same two implicit methods apply.&lt;/p&gt;
&lt;h2&gt;Boltzmann machines&lt;/h2&gt;
&lt;p&gt;A Boltzmann machine is one of the simplest mechanisms for modeling &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. It is an undirected graphical model where every dimension &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; of a given observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; influences every other dimension. &lt;strong&gt;As such, we might use it to model data which we believe to exhibit this property, e.g. an image.&lt;/strong&gt; For &lt;span class="math"&gt;\(x \in R^3\)&lt;/span&gt;, our model would look as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/thorough-introduction-to-boltzmann-machines/boltzmann-machine.svg"/&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(x \in R^n\)&lt;/span&gt;, a given node &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; would have &lt;span class="math"&gt;\(n - 1\)&lt;/span&gt; outgoing connections in total—one to each of the other nodes &lt;span class="math"&gt;\(x_j\)&lt;/span&gt; for &lt;span class="math"&gt;\(j \neq i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, a Boltzmann machine strictly operates on &lt;em&gt;binary&lt;/em&gt; data. This keeps things simple.&lt;/p&gt;
&lt;h2&gt;Computing the likelihood&lt;/h2&gt;
&lt;p&gt;A Boltzmann machines admits the following formula for computing the likelihood of data points &lt;span class="math"&gt;\(x^{(1)}, ..., x^{(n)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x) = \sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
p(x) = \frac{\exp{(H(x))}}{Z}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{i=1}^n p(x^{(i)})
$$&lt;/div&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since our weights can be negative, &lt;span class="math"&gt;\(H(x)\)&lt;/span&gt; can be negative. As a likelihood gives an optionally-normalized probability, it must be non-negative.&lt;/li&gt;
&lt;li&gt;To enforce this constraint, we exponentiate &lt;span class="math"&gt;\(H(x)\)&lt;/span&gt; in the second equation.&lt;/li&gt;
&lt;li&gt;To normalize, we divide by the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt;, i.e. the sum of the likelihoods of all possible values of &lt;span class="math"&gt;\(x^{(1)}, ..., x^{(n)}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Computing the partition function, with examples&lt;/h2&gt;
&lt;p&gt;In the case of 2-dimensional binary &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the only possible "configurations" of &lt;span class="math"&gt;\(x\)&lt;/span&gt; are: &lt;span class="math"&gt;\([0, 0], [0, 1], [1, 0], [1, 1]\)&lt;/span&gt;, i.e. 4 distinct values. This means that in evaluating the likelihood of one datum &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt; would be a sum of 4 terms.&lt;/p&gt;
&lt;p&gt;Now, with two data points &lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt; and &lt;span class="math"&gt;\(x^{(2)}\)&lt;/span&gt;, there are 16 possible "configurations":&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x^{(1)} = [0, 0]\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^{(2)} = [0, 0]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x^{(1)} = [0, 0]\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^{(2)} = [0, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x^{(1)} = [0, 0]\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^{(2)} = [1, 0]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x^{(1)} = [0, 0]\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^{(2)} = [1, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x^{(1)} = [0, 1]\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^{(2)} = [0, 0]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x^{(1)} = [0, 1]\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^{(2)} = [0, 1]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This means that in evaluating the likelihood of &lt;span class="math"&gt;\(\mathcal{L}(x^{(1)}, x^{(2)})\)&lt;/span&gt;, the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt; would be a sum of 16 terms.&lt;/p&gt;
&lt;p&gt;More generally, given &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional &lt;span class="math"&gt;\(x\)&lt;/span&gt;, where each &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; can assume one of &lt;span class="math"&gt;\(v\)&lt;/span&gt; distinct values, and &lt;span class="math"&gt;\(n\)&lt;/span&gt; data points &lt;span class="math"&gt;\(x^{(1)}, ..., x^{(n)}\)&lt;/span&gt;—in evaluating the likelihood of &lt;span class="math"&gt;\(\mathcal{L}(x^{(1)}, ..., x^{(n)})\)&lt;/span&gt; the normalization constant &lt;span class="math"&gt;\(Z\)&lt;/span&gt; would be a sum of &lt;span class="math"&gt;\((v^d)^n\)&lt;/span&gt; terms. &lt;strong&gt;With a non-trivially large &lt;span class="math"&gt;\(v\)&lt;/span&gt; or &lt;span class="math"&gt;\(d\)&lt;/span&gt; (in the discrete case), or a non-trivially large &lt;span class="math"&gt;\(k\)&lt;/span&gt; in the continuous case, this becomes intractable to compute.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the case of a Boltzmann machine, &lt;span class="math"&gt;\(v = 2\)&lt;/span&gt;, which is not large. Below, we will vary &lt;span class="math"&gt;\(d\)&lt;/span&gt; and examine its impact on the tractability (in terms of, "can we actually compute &lt;span class="math"&gt;\(Z\)&lt;/span&gt; before the end of the universe?") of inference.&lt;/p&gt;
&lt;h2&gt;The likelihood function in code&lt;/h2&gt;
&lt;p&gt;In code, the likelihood function looks as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;        where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;        for which to compute the joint likelihood.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
    &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="c1"&gt;# compute partition function&lt;/span&gt;
    &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code block is longer than you might expect because it includes a few supplementary behaviors, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computing the likelihood of one or more points&lt;/li&gt;
&lt;li&gt;Avoiding redundant computation of &lt;code&gt;Z&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Optionally computing the log-likelihood&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Above all, note that: the likelihood is a function of the model's parameters, i.e. &lt;code&gt;self.weights&lt;/code&gt; and &lt;code&gt;self.biases&lt;/code&gt;, which we can vary, and the data &lt;code&gt;x&lt;/code&gt;, which we can't.&lt;/p&gt;
&lt;h2&gt;Training the model&lt;/h2&gt;
&lt;p&gt;At the outset, the parameters &lt;code&gt;self.weights&lt;/code&gt; and &lt;code&gt;self.biases&lt;/code&gt; of our model are initialized at random. Trivially, such that the values returned by &lt;code&gt;likelihood&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt; are useful, we must first update these parameters by fitting this model to observed data.&lt;/p&gt;
&lt;p&gt;To do so, we will employ the principal of maximum likelihood: compute the parameters that make the observed data maximally likely under the model, via gradient ascent.&lt;/p&gt;
&lt;h2&gt;Gradients&lt;/h2&gt;
&lt;p&gt;Since our model is simple, we can derive exact gradients by hand. We will work with the log-likelihood instead of the true likelihood to avoid issues of computational underflow. Below, we simplify this expression, then compute its various gradients.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(\log{\mathcal{L}}\)&lt;/span&gt;&lt;/h3&gt;
&lt;div class="math"&gt;$$
\mathcal{L}(x^{(1)}, ..., x^{(n)}) = \prod\limits_{k=1}^n \frac{\exp{(H(x^{(k)})}}{Z}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{\mathcal{L}(x^{(1)}, ..., x^{(n)})}
&amp;amp;= \sum\limits_{k=1}^n \log{\frac{\exp{(H(x^{(k)})}}{Z}}\\
&amp;amp;= \sum\limits_{k=1}^n \log{\big(\exp{(H(x^{(k)})}\big)} - \log{Z}\\
&amp;amp;= \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This gives the total likelihood. Our aim is to maximize the expected likelihood with respect to the data generating distribution.&lt;/p&gt;
&lt;h3&gt;Expected likelihood&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathop{\mathbb{E}}_{x \sim p_{\text{data}}}\big[ \mathcal{L}(x) \big]
&amp;amp;= \sum\limits_{k=1}^N p_{\text{data}}(x = x^{(k)}) \mathcal{L(x^{(k)})}\\
&amp;amp;= \sum\limits_{k=1}^N \frac{1}{N} \mathcal{L(x^{(k)})}\\
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^N  \mathcal{L(x^{(k)})}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In other words, the average. We will continue to denote this as &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, i.e. &lt;span class="math"&gt;\(\mathcal{L} = \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, deriving the gradient with respect to our weights:&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(\nabla_{w_{i, j}}\log{\mathcal{L}}\)&lt;/span&gt;:&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\nabla_{w_{i, j}} \frac{1}{N} \sum\limits_{k=1}^n H(x^{(k)}) - \log{Z}
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)}) - \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \log{Z}
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;First term:&lt;/h3&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}}  H(x^{(k)})
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n \nabla_{w_{i, j}} \sum\limits_{i \neq j} w_{i, j} x_i^{(k)} x_j^{(k)} + \sum\limits_i b_i x_i^{(k)}\\
&amp;amp;= \frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\\
&amp;amp;= \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j]
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Second term:&lt;/h3&gt;
&lt;p&gt;NB: &lt;span class="math"&gt;\(\sum\limits_{\mathcal{x}}\)&lt;/span&gt; implies a summation over all &lt;span class="math"&gt;\((v^d)^n\)&lt;/span&gt; possible configurations of values that &lt;span class="math"&gt;\(x^{(1)}, ..., x^{(n)}\)&lt;/span&gt; can assume.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\nabla_{w_{i, j}} \log{Z}
&amp;amp;= \nabla_{w_{i, j}} \log{\sum\limits_{\mathcal{x}}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{\sum\limits_{\mathcal{x}} \exp{(H(x))}} \nabla_{w_{i, j}} \sum\limits_{\mathcal{x}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{Z} \nabla_{w_{i, j}} \sum\limits_{\mathcal{x}} \exp{(H(x))}\\
&amp;amp;= \frac{1}{Z} \sum\limits_{\mathcal{x}} \exp{(H(x))} \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} \frac{\exp{(H(x))}}{Z} \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} P(x) \nabla_{w_{i, j}} H(x)\\
&amp;amp;= \sum\limits_{\mathcal{x}} P(x) [x_i  x_j]\\
&amp;amp;= \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Putting it back together&lt;/h3&gt;
&lt;p&gt;Combining these constituent parts, we arrive at the following formula:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{w_{i, j}}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i  x_j] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]
$$&lt;/div&gt;
&lt;p&gt;Finally, following the same logic, we derive the exact gradient with respect to our biases:&lt;/p&gt;
&lt;div class="math"&gt;$$
\nabla_{b_i}\log{\mathcal{L}} = \mathop{\mathbb{E}}_{x \sim p_{\text{data}}} [x_i] - \mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i]
$$&lt;/div&gt;
&lt;p&gt;The first and second terms of each gradient are called, respectively, &lt;strong&gt;the positive and negative phases.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Computing the positive phase&lt;/h2&gt;
&lt;p&gt;In the following toy example, our data are small: we can compute the positive phase using all of the training data, i.e. &lt;span class="math"&gt;\(\frac{1}{N} \sum\limits_{k=1}^n x_i^{(k)} x_j^{(k)}\)&lt;/span&gt;. Were our data bigger, we could approximate this expectation with a mini-batch of training data and we do in SGD.&lt;/p&gt;
&lt;h2&gt;Computing the negative phase&lt;/h2&gt;
&lt;p&gt;Again, this term asks us to compute then sum the log-likelihood over every possible data configuration in the support of our model, which is &lt;span class="math"&gt;\(O(nv^d)\)&lt;/span&gt;. &lt;strong&gt;With non-trivially large &lt;span class="math"&gt;\(v\)&lt;/span&gt; or &lt;span class="math"&gt;\(d\)&lt;/span&gt;, this becomes intractable to compute.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below, we'll begin our toy example computing the true negative-phase, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, with varying data dimensionalities &lt;span class="math"&gt;\(d\)&lt;/span&gt;. Then, once this computation becomes slow, we'll look to approximate this expectation later on.&lt;/p&gt;
&lt;h2&gt;Parameter updates in code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train model, visualize model distribution&lt;/h2&gt;
&lt;p&gt;Finally, we're ready to train. Using the true negative phase, let's train our model for 100 epochs with &lt;span class="math"&gt;\(d=3\)&lt;/span&gt; then visualize results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reset_data_and_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_obs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Generate training data, weights, biases, and a list of all data configurations&lt;/span&gt;
&lt;span class="sd"&gt;    in our model's support.&lt;/span&gt;

&lt;span class="sd"&gt;    In addition, generate a list of tuples of the indices of adjacent nodes, which&lt;/span&gt;
&lt;span class="sd"&gt;    we'll use to update parameters without duplication.&lt;/span&gt;

&lt;span class="sd"&gt;    For example, with `n_units=3`, we generate a matrix of weights with shape (3, 3);&lt;/span&gt;
&lt;span class="sd"&gt;    however, there are only 3 distinct weights in this matrix that we'll actually&lt;/span&gt;
&lt;span class="sd"&gt;    want to update: those connecting Node 0 --&amp;gt; Node 1, Node 1 --&amp;gt; Node 2, and&lt;/span&gt;
&lt;span class="sd"&gt;    Node 0 --&amp;gt; Node 2. This function returns a list containing these tuples&lt;/span&gt;
&lt;span class="sd"&gt;    named `var_combinations.&lt;/span&gt;

&lt;span class="sd"&gt;    :param n_units: the dimensionality of our data `d`&lt;/span&gt;
&lt;span class="sd"&gt;    :param n_obs: the number of observations in our training set&lt;/span&gt;
&lt;span class="sd"&gt;    :param p: a vector of the probabilities of observing a 1 in each index&lt;/span&gt;
&lt;span class="sd"&gt;        of the training data. The length of this vector must equal `n_units`&lt;/span&gt;

&lt;span class="sd"&gt;    :return: weights, biases, var_combinations, all_configs, data&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialize data&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# initialize parameters&lt;/span&gt;
    &lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# a few other pieces we'll need&lt;/span&gt;
    &lt;span class="n"&gt;var_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;all_configs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nd"&gt;@staticmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_inv_logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_H&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        :param x: a vector of shape (n_units,) or (n, n_units),&lt;/span&gt;
&lt;span class="sd"&gt;            where the latter is a matrix of multiple data points&lt;/span&gt;
&lt;span class="sd"&gt;            for which to compute the joint likelihood.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Please pass 1 or more points of `n_units` dimensions'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# compute unnormalized likelihoods&lt;/span&gt;
        &lt;span class="n"&gt;multiple_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiple_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

        &lt;span class="c1"&gt;# compute partition function&lt;/span&gt;
        &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_unnormalized_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;burn_in&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Can't burn in for more samples than there are in the chain"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;init_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;init_sample&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_inv_logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# make copy&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_gibbs_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;burn_in&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;every_n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reset_data_and_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;lik&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Epoch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; | Likelihood: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;lik&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;209.63758306786653&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;162.04280784271083&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;160.49961381649555&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;159.79539070373576&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;159.2853717231018&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.90186293631422&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.6084020645482&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;70&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.38094343579155&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.20287017780586&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;90&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;158.06232196551673&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Visualize samples&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_n_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    NB: We add some jitter to the points so as to better visualize density in a given corner of the model.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;111&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;projection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'3d'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 0'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_zlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Node 2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;

&lt;span class="n"&gt;plot_n_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; Samples from Model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/thorough-introduction-to-boltzmann-machines/output_7_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The plot roughly matches the data-generating distribution: most points assume values of either &lt;span class="math"&gt;\([1, 0, 1]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([1, 0, 0]\)&lt;/span&gt; (given &lt;span class="math"&gt;\(p=[.8, .1, .5]\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Sampling, via Gibbs&lt;/h2&gt;
&lt;p&gt;The second, final method we need to implement is &lt;code&gt;sample&lt;/code&gt;. In a Boltzmann machine, we typically do this via &lt;a href="http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf"&gt;Gibbs sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To effectuate this sampling scheme, we'll need a model of each data dimension conditional on the other data dimensions. For example, for &lt;span class="math"&gt;\(d=3\)&lt;/span&gt;, we'll need to define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_0\vert x_1, x_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_1\vert x_0, x_2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p(x_2\vert x_0, x_1)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that each dimension must assume a 0 or a 1, the above 3 models must necessarily return the probability of observing a 1 (where 1 minus this value gives the probability of observing a 0).&lt;/p&gt;
&lt;p&gt;Let's derive these models using the workhorse axiom of conditional probability, starting with the first:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
p(x_0 = 1\vert x_1, x_2)
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_1, x_2)}\\
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{\sum\limits_{x_0 \in [0, 1]} p(x_0, x_1, x_2)}\\
&amp;amp;= \frac{p(x_0 = 1, x_1, x_2)}{p(x_0 = 0, x_1, x_2) + p(x_0 = 1, x_1, x_2)}\\
&amp;amp;= \frac{1}{1 + \frac{p(x_0 = 0, x_1, x_2)}{p(x_0 = 1, x_1, x_2)}}\\
&amp;amp;= \frac{1}{1 + \frac{\exp{(H(x_0 = 0, x_1, x_2)))}}{\exp{(H(x_0 = 1, x_1, x_2)))}}}\\
&amp;amp;= \frac{1}{1 + \exp{(H(x_0 = 0, x_1, x_2) - H(x_0 = 1, x_1, x_2))}}\\
&amp;amp;= \frac{1}{1 + \exp{(\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i - (\sum\limits_{i \neq j} w_{i, j} x_i x_j + \sum\limits_i b_i x_i))}}\\
&amp;amp;= \frac{1}{1 + \exp{(-\sum\limits_{j \neq i = 0} w_{i, j} x_j - b_i)}}\\
&amp;amp;= \sigma\bigg(\sum\limits_{j \neq i = 0} w_{i, j} x_j + b_i\bigg)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Pleasantly enough, this model resolves to a simple Binomial GLM, i.e. logistic regression, involving only its neighboring units and the weights that connect them.&lt;/p&gt;
&lt;p&gt;With the requisite conditionals in hand, let's run this chain and compare it with our (trained) model's true probability distribution.&lt;/p&gt;
&lt;h2&gt;True probability distribution&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[0, 0, 0]: 0.07327&lt;/span&gt;
&lt;span class="err"&gt;[0, 0, 1]: 0.09227&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 0]: 0.01366&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 1]: 0.01938&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 0]: 0.3351&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 1]: 0.3622&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 0]: 0.04693&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 1]: 0.05715&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Empirical probability distribution, via Gibbs&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;empirical_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;empirical_probability&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;empirical_dist&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (true), &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;empirical_probability&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.4&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (empirical)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;[0, 0, 0]: 0.07327 (true), 0.05102 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 0, 1]: 0.09227 (true), 0.09184 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 0]: 0.01366 (true), 0.0102 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[0, 1, 1]: 0.01938 (true), 0.02041 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 0]: 0.3351 (true), 0.3673 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 0, 1]: 0.3622 (true), 0.398 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 0]: 0.04693 (true), 0.03061 (empirical)&lt;/span&gt;
&lt;span class="err"&gt;[1, 1, 1]: 0.05715 (true), 0.03061 (empirical)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Close, ish enough.&lt;/p&gt;
&lt;h2&gt;Scaling up, and hitting the bottleneck&lt;/h2&gt;
&lt;p&gt;With data of vary dimensionality &lt;code&gt;n_units&lt;/code&gt;, the following plot gives the time in seconds that it takes to train this model for 10 epochs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/thorough-introduction-to-boltzmann-machines/output_15_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;To reduce computational burden, and/or to fit a Boltzmann machine to data of non-trivial dimensionality (e.g. a 28x28 grey-scale image, which implies a random variable with 28x28=784 dimensions), we need to compute the positive and/or negative phase of our gradient faster than we currently are.&lt;/p&gt;
&lt;p&gt;To compute the former more quickly, we could employ mini-batches as in canonical stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;In this post, we'll instead focus on ways to speed up the latter. Revisiting its expression, &lt;span class="math"&gt;\(\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j]\)&lt;/span&gt;, we readily see that we can create an unbiased estimator for this value by drawing Monte Carlo samples from our model, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{\mathbb{E}}_{x \sim p_{\text{model}}} [x_i  x_j] \approx \frac{1}{N}\sum\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\quad\text{where}\quad x^{(k)} \sim p_{\text{model}}
$$&lt;/div&gt;
&lt;p&gt;So, now we just need a way to draw these samples. Luckily, we have a Gibbs sampler to tap!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instead of computing the true negative phase, i.e. summing &lt;span class="math"&gt;\(x_i  x_j\)&lt;/span&gt; over all permissible configurations &lt;span class="math"&gt;\(X\)&lt;/span&gt; under our model, we can approximate it by evaluating this expression for a few model samples, then taking the mean.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We define this update mechanism here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                          &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_configs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;var_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# update weights&lt;/span&gt;
        &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# positive phase&lt;/span&gt;
        &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# negative phase&lt;/span&gt;
        &lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_samples&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# update biases&lt;/span&gt;
        &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we'll define a function that we can parameterize by an optimization algorithm (computing the true negative phase, or approximating it via Gibbs sampling, in the above case) which will train a model for &lt;span class="math"&gt;\(n\)&lt;/span&gt; epochs and return data requisite for plotting.&lt;/p&gt;
&lt;h2&gt;How does training progress for varying data dimensionalities?&lt;/h2&gt;
&lt;p&gt;Finally, for data of &lt;code&gt;n_units&lt;/code&gt; 3, 4, 5, etc., let’s train models for 100 epochs and plot likelihood curves.&lt;/p&gt;
&lt;p&gt;When training with the approximate negative phase, we’ll:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Derive model samples from a &lt;strong&gt;1000-sample Gibbs chain. Of course, this is a parameter we can tune, which will affect both model accuracy and training runtime. However, we don’t explore that in this post;&lt;/strong&gt; instead, we just pick something reasonable and hold this value constant throughout our experiments.&lt;/li&gt;
&lt;li&gt;Train several models for a given &lt;code&gt;n_units&lt;/code&gt;; Seaborn will average results for us then plot a single line (I think).&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_epochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_epochs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/thorough-introduction-to-boltzmann-machines/output_23_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When we let each algorithm run for 100 epochs, the true negative phase gives a model which assigns higher likelihood to the observed data in all of the above training runs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Notwithstanding, the central point is that 100 epochs of the true negative phase takes a long time to run.&lt;/p&gt;
&lt;p&gt;As such, let’s run each for an equal amount of time, and plot results. Below, we define a function to train models for &lt;span class="math"&gt;\(n\)&lt;/span&gt; seconds (or 1 epoch—whichever comes first).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;n_seconds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_units&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_gibbs_sampling&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_model_for_n_seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;update_parameters_with_true_negative_phase&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;run_num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;all_updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_units&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;How many epochs do we actually get through?&lt;/h2&gt;
&lt;p&gt;Before plotting results, let’s examine how many epochs each algorithm completes in its allotted time. In fact, for some values of &lt;code&gt;n_units&lt;/code&gt;, we couldn’t even complete a single epoch (when computing the true negative phase) in &lt;span class="math"&gt;\(\leq 1\)&lt;/span&gt; second.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/thorough-introduction-to-boltzmann-machines/output_28_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Finally, we look at performance. With &lt;code&gt;n_units &amp;lt;= 7&lt;/code&gt;, we see that 1 second of training with the true negative phase yields a better model. Conversely, &lt;strong&gt;using 7 or more units, the added performance given by using the true negative phase is overshadowed by the amount of time it takes the model to train.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/thorough-introduction-to-boltzmann-machines/output_31_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Of course, we re-stress that the exact ablation results are conditional (amongst other things) on &lt;strong&gt;the number of Gibbs samples we chose to draw. Changing this will change the results, but not that about which we care the most: the overall trend.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Throughout this post, we've given a thorough introduction to a Boltzmann machine: what it does, how it trains, and some of the computational burdens and considerations inherent.&lt;/p&gt;
&lt;p&gt;In the next post, we'll look at cheaper, more inventive algorithms for avoiding the computation of the negative phase, and describe how they're used in common machine learning models and training routines.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/boltzmann-machines"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-1.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec19.pdf"&gt;CSC321 Lecture 19: Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/"&gt;Derivation: Maximum Likelihood for Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf"&gt;Boltzmann Machines&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>From Gaussian Algebra to Gaussian Processes, Part 2</title><link href="https://cavaunpeu.github.io/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/" rel="alternate"></link><published>2018-06-12T08:00:00-04:00</published><updated>2018-06-12T08:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-06-12:/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/</id><summary type="html">&lt;p&gt;Introducing the RBF kernel, and motivating its ubiquitous use in Gaussian processes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, we covered the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions (i.e. function evaluations). √&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian (just pluck out the pieces of interest). √&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset gives another Gaussian (a simple algebraic formula). √&lt;/li&gt;
&lt;li&gt;Posterior over functions (the linear map of the posterior over weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X_{*})^T\)&lt;/span&gt;) √&lt;/li&gt;
&lt;li&gt;Covariances (the second thing we need in order to specify a multivariate Gaussian) √&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;If any of the above is still not clear, please look no further, and re-visit the &lt;a href=""&gt;previous post&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Conversely, we did not directly cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we'll explain these two.&lt;/p&gt;
&lt;h2&gt;The more features we use, the more expressive our model&lt;/h2&gt;
&lt;p&gt;We concluded the previous post by plotting posteriors over function evaluations given various &lt;code&gt;phi_func&lt;/code&gt;s, i.e. a function that creates "features" &lt;span class="math"&gt;\(\phi(X)\)&lt;/span&gt; given an input &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 inputs&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 corresponding outputs, which we'll use later on&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# makes D=2 features for each input&lt;/span&gt;


&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One common such set of features are those given by "radial basis functions", a.k.a. the "squared exponential" function, defined as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;

&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, the choice of which features to use is ultimately arbitrary, i.e. a choice left to the modeler.&lt;/p&gt;
&lt;p&gt;Throughout the exercise, we saw that the larger the dimensionality &lt;span class="math"&gt;\(d\)&lt;/span&gt; of our feature function &lt;code&gt;phi_func&lt;/code&gt;, the more expressive, i.e. less endemically prone to overfitting, our model became.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, how far can we take this?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Computing features is expensive&lt;/h2&gt;
&lt;p&gt;Ideally, we'd compute as many features as possible for each input element, i.e. employ &lt;code&gt;phi_func(x, D=some_huge_number)&lt;/code&gt;. Unfortunately, the cost of doing so adds up, and ultimately becomes intractable past meaningfully-large values of &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perhaps there's a better way?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How are these things used?&lt;/h2&gt;
&lt;p&gt;Let's bring back our GP equations, and prepare ourselves to &lt;em&gt;squint&lt;/em&gt;! In the previous post, we outlined the following modeling process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define prior distribution over weights and function evaluations, &lt;span class="math"&gt;\(P(w, y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Marginalizing &lt;span class="math"&gt;\(P(w, y)\)&lt;/span&gt; over &lt;span class="math"&gt;\(y\)&lt;/span&gt;, i.e. &lt;span class="math"&gt;\(\int P(w, y)dy\)&lt;/span&gt;, and given some observed function evaluations &lt;span class="math"&gt;\(y\)&lt;/span&gt;, compute the posterior distribution over weights, &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Linear-mapping &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; onto some new, transformed test input &lt;span class="math"&gt;\(\phi(X_*)^T\)&lt;/span&gt;, compute the posterior distribution over function evaluations, &lt;span class="math"&gt;\(P(y_*\ \vert\ y) = P(\phi(X_{*})^Tw\ \vert\ y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, let's unpack #2 and #3.&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First, the mathematical equation:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(w\vert y)
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}\Sigma_y^{-1}(y - \mu_y), \Sigma_w - \Sigma_{wy}\Sigma_y^{-1}\Sigma_{wy}^T)\\
    \\
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}(y - \mu_w^T \phi(X)), \Sigma_w - \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}\Sigma_{wy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Next, this equation in code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define initial parameters&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="c1"&gt;# dimensionality of `phi_func`&lt;/span&gt;

&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# often a vector of zeros, though it doesn't have to be&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# often the identity matrix, though it doesn't have to be&lt;/span&gt;

&lt;span class="c1"&gt;# Featurize `X_train`&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Params of prior distribution over function evals&lt;/span&gt;
&lt;span class="n"&gt;mu_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt;
     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cov_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;

&lt;span class="c1"&gt;# Params of posterior distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
          &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;
&lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;
           &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(P(y_*\ \vert\ y) = P(\phi(X_{*})^Tw\ \vert\ y)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(X_*\)&lt;/span&gt; is a set of test points, e.g. &lt;code&gt;np.linspace(-10, 10, 200)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition, let's call &lt;span class="math"&gt;\(X_* \rightarrow\)&lt;/span&gt; &lt;code&gt;X_test&lt;/code&gt; and &lt;span class="math"&gt;\(y_* \rightarrow\)&lt;/span&gt; &lt;code&gt;y_test&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mathematical equations in code:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Featurize `X_test`&lt;/span&gt;
&lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# The following two equations were defined above&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;
&lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;

&lt;span class="c1"&gt;# The mean of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

&lt;span class="c1"&gt;# The covariance of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Never alone&lt;/h2&gt;
&lt;p&gt;Squinting at the equations for &lt;code&gt;mu_y_test_post&lt;/code&gt; and &lt;code&gt;cov_y_test_post&lt;/code&gt;, we see that &lt;code&gt;phi_x&lt;/code&gt; and &lt;code&gt;phi_x_test&lt;/code&gt; appear &lt;strong&gt;only in the presence of another &lt;code&gt;phi_x&lt;/code&gt;, or &lt;code&gt;phi_x_test&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These four distinct such terms are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In mathematical notation, they are (respectively):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Simplifying further&lt;/h2&gt;
&lt;p&gt;These are nothing more than &lt;em&gt;scaled&lt;/em&gt; (via the &lt;span class="math"&gt;\(\Sigma_w\)&lt;/span&gt; term) dot products in some expanded feature space &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Until now, we've explicitly chosen what this &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; function is.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If the scaling matrix &lt;span class="math"&gt;\(\Sigma_w\)&lt;/span&gt; is &lt;a href="https://en.wikipedia.org/wiki/Positive-definite_matrix"&gt;positive definite&lt;/a&gt;, we can state the following, using &lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X)\)&lt;/span&gt;, i.e. &lt;code&gt;phi_x.T @ cov_w @ phi_x&lt;/code&gt;, as an example:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\Sigma_w = (\sqrt{\Sigma_w})^2
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\phi(X)^T \Sigma_w \phi(X)
    &amp;amp;= \big(\sqrt{\Sigma_w}\phi(X)\big)^T\big(\sqrt{\Sigma_w}\phi(X)\big)\\
    &amp;amp;= \varphi(X)^T\varphi(X)\\
    &amp;amp;= \varphi(X) \cdot \varphi(X)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As such, our four distinct scaled-dot-product terms can be rewritten as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*) = \varphi(X_*) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X) = \varphi(X_*) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X) = \varphi(X) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*) = \varphi(X) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;In other words, these terms can be equivalently written as dot-products in some space &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;We have &lt;strong&gt;not&lt;/strong&gt; explicitly chosen what this &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; function is.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Kernels&lt;/h2&gt;
&lt;p&gt;A "kernel" is a function which gives the similarity between individual elements in two sets, i.e. a Gram matrix.&lt;/p&gt;
&lt;p&gt;For instance, imagine we have two sets of countries, &lt;span class="math"&gt;\(\{\text{France}, \text{Germany}, \text{Iceland}\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\{\text{Morocco}, \text{Denmark}\}\)&lt;/span&gt;, and that similarity is given by an integer value in &lt;span class="math"&gt;\([1, 5]\)&lt;/span&gt;, where 1 is the least similar, and 5 is the most. Applying a kernel to these sets might give a Gram matrix such as:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped=""&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;France&lt;/th&gt;
&lt;th&gt;Germany&lt;/th&gt;
&lt;th&gt;Iceland&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;Morocco&lt;/th&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Denmark&lt;/th&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;When you hear the term "kernel" in the context of machine learning, think "similarity between things in lists." That's it.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NB: A "list" could be a list of vectors, i.e. a matrix. A vector, or a matrix, are the canonical inputs to a kernel.&lt;/p&gt;
&lt;h2&gt;Mercer's Theorem&lt;/h2&gt;
&lt;p&gt;Mercer's Theorem has as a key result that any kernel function can be expressed as a dot product, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
K(X, X') = \varphi(X) \cdot \varphi (X')
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; is some function that creates &lt;span class="math"&gt;\(d\)&lt;/span&gt; features out of &lt;span class="math"&gt;\(X\)&lt;/span&gt; (in the same vein as &lt;code&gt;phi_func&lt;/code&gt; from above).&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;To illustrate, I'll borrow an example from &lt;a href="https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is"&gt;CrossValidated&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;"For example, consider a simple polynomial kernel &lt;span class="math"&gt;\(K(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2\)&lt;/span&gt; with &lt;span class="math"&gt;\(\mathbf x, \mathbf y \in \mathbb R^2\)&lt;/span&gt;. This doesn't seem to correspond to any mapping function &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt;, it's just a function that returns a real number. Assuming that &lt;span class="math"&gt;\(\mathbf x = (x_1, x_2)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf y = (y_1, y_2)\)&lt;/span&gt;, let's expand this expression:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
K(\mathbf x, \mathbf y)
    &amp;amp;= (1 + \mathbf x^T \mathbf y)^2\\
    &amp;amp;= (1 + x_1 \, y_1  + x_2 \, y_2)^2\\
    &amp;amp;= 1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 + 2 x_2 y_2 + 2 x_1 x_2 y_1 y_2
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Note that this is nothing else but a dot product between two vectors &lt;span class="math"&gt;\((1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)\)&lt;/span&gt; and &lt;span class="math"&gt;\((1, y_1^2, y_2^2, \sqrt{2} y_1, \sqrt{2} y_2, \sqrt{2} y_1 y_2)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\varphi(\mathbf x) = \varphi(x_1, x_2) = (1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)\)&lt;/span&gt;. So the kernel &lt;span class="math"&gt;\(K(\mathbf x, \mathbf y) = (1 + \mathbf x^T \mathbf y)^2 = \varphi(\mathbf x) \cdot \varphi(\mathbf y)\)&lt;/span&gt; computes a dot product in 6-dimensional space without explicitly visiting this space."&lt;/p&gt;
&lt;h3&gt;What this means&lt;/h3&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-2/kernels-for-gaussian-processes.svg"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We start with inputs &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Our goal is to compute the similarity between then, &lt;span class="math"&gt;\(\text{Sim}(X, Y)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Bottom path&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Lifting these inputs into some feature space, then computing their dot-product in that space, i.e. &lt;span class="math"&gt;\(\varphi(X) \cdot \varphi (Y)\)&lt;/span&gt; (where &lt;span class="math"&gt;\(F = \varphi\)&lt;/span&gt;, since I couldn't figure out how to draw a &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; in &lt;a href="http://draw.io"&gt;draw.io&lt;/a&gt;), is one strategy for computing this similarity.&lt;/li&gt;
&lt;li&gt;Unfortunately, this robustness comes at a cost: &lt;strong&gt;the computation is extremely expensive.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Top Path&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A valid kernel computes similarity between inputs. The function it employs might be extremely simple, e.g. &lt;span class="math"&gt;\((X - Y)^{123}\)&lt;/span&gt;; &lt;strong&gt;the computation is extremely cheap.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Mercer!&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mercer's Theorem tells us that every valid kernel, i.e. the top path, is &lt;em&gt;implicitly traversing the bottom path.&lt;/em&gt; &lt;strong&gt;In other words, kernels allow us to directly compute the result of an extremely expensive computation, extremely cheaply.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How does this help?&lt;/h2&gt;
&lt;p&gt;Once more, the Gaussian process equations are littered with the following terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X_*) = \varphi(X_*) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X_*)^T\Sigma_w \phi(X) = \varphi(X_*) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X) = \varphi(X) \cdot \varphi(X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi(X)^T\Sigma_w \phi(X_*) = \varphi(X) \cdot \varphi(X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, we previously established that the more we increase the dimensionality &lt;span class="math"&gt;\(d\)&lt;/span&gt; of our given feature function, the more flexible our model becomes.&lt;/p&gt;
&lt;p&gt;Finally, past any meaningfully large value of &lt;span class="math"&gt;\(d\)&lt;/span&gt;, and irrespective of what &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; actually is, &lt;strong&gt;this computation becomes intractably expensive.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Kernels!&lt;/h3&gt;
&lt;p&gt;You know where this is going.&lt;/p&gt;
&lt;p&gt;Given Mercer's theorem, we can state the following equalities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X_*) \cdot \varphi(X_*) = K(X_*, X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X_*) \cdot \varphi(X) = K(X_*, X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X) \cdot \varphi(X) = K(X, X)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\varphi(X) \cdot \varphi(X_*) = K(X, X_*)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Which kernels to choose?&lt;/h2&gt;
&lt;p&gt;At the outset, we stated that our primary goal was to increase &lt;span class="math"&gt;\(d\)&lt;/span&gt;. As such, &lt;strong&gt;let's pick the kernel whose implicit &lt;span class="math"&gt;\(\varphi\)&lt;/span&gt; has the largest dimensionality possible.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we saw that the kernel &lt;span class="math"&gt;\(k(\mathbf x, \mathbf y)\)&lt;/span&gt; was implicitly computing a &lt;span class="math"&gt;\(d=6\)&lt;/span&gt;-dimensional dot-product. Which kernels compute a &lt;span class="math"&gt;\(d=100\)&lt;/span&gt;-dimensional dot-product? &lt;span class="math"&gt;\(d=1000\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How about &lt;span class="math"&gt;\(d=\infty\)&lt;/span&gt;?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Radial basis function, a.k.a. the "squared-exponential"&lt;/h2&gt;
&lt;p&gt;This kernel is implicitly computing a &lt;span class="math"&gt;\(d=\infty\)&lt;/span&gt;-dimensional dot-product. That's it. &lt;strong&gt;That's why it's so ubiquitous in Gaussian processes.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Rewriting our equations&lt;/h2&gt;
&lt;p&gt;With all of the above in mind, let's rewrite the equations for the parameters of our posterior distribution over function evaluations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The mean of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

               &lt;span class="c1"&gt;# Now, substituting in our kernels&lt;/span&gt;
               &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;

&lt;span class="c1"&gt;# The covariance of the posterior distribution over function evaluations&lt;/span&gt;
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; \
                  &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;

                &lt;span class="c1"&gt;# Now, substituting in our kernels&lt;/span&gt;
                &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; \
                  &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Defining the kernel in code&lt;/h2&gt;
&lt;p&gt;Mathematically, the RBF kernel is defined as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
K(X, Y) = \exp(-\frac{1}{2}\vert X - Y \vert ^2)
$$&lt;/div&gt;
&lt;p&gt;To conclude, let's define a Python function for the parameters of our posterior over function evaluations, using this RBF kernel as &lt;code&gt;k&lt;/code&gt;, then plot the resulting distribution.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 inputs&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# 5 corresponding outputs, which we'll use below&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# vector of test inputs&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rbf_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (len(x), 1)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (1, len(y))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# shape: (len(x), len(y))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rbf_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# The following quantity is used in both `mu_y_test_post` and `cov_y_test_post`;&lt;/span&gt;
&lt;span class="c1"&gt;# we extract it into a separate variable for readability&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mu_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;            
&lt;span class="n"&gt;cov_y_test_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Visualizing results&lt;/h2&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_17_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;And for good measure, with some samples from the posterior:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_19_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;p&gt;In this post, we've unpacked the notion of a kernel, and its ubiquitous use in Gaussian Processes.&lt;/p&gt;
&lt;p&gt;In addition, we've introduced the RBF kernel, i.e. "squared exponential" kernel, and motivated its widespread application in these models.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/gaussian-processes"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/gaussian-processes/blob/master/gaussian-processes-part-2.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://gaussianprocess.org/gpml/?"&gt;Gaussian Processes for Machine Learning&lt;/a&gt;. Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Mercers-Theorem"&gt;What is an intuitive explanation of Mercer's Theorem?&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>From Gaussian Algebra to Gaussian Processes, Part 1</title><link href="https://cavaunpeu.github.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/" rel="alternate"></link><published>2018-03-31T19:00:00-04:00</published><updated>2018-03-31T19:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-03-31:/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/</id><summary type="html">&lt;p&gt;A thorough, straightforward, un-intimidating introduction to Gaussian processes in NumPy.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Most &lt;strong&gt;introductory tutorials&lt;/strong&gt; on Gaussian processes start with a nose-punch of &lt;strong&gt;fancy statements&lt;/strong&gt;, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions.&lt;/li&gt;
&lt;li&gt;A Gaussian process is non-parametric, i.e. it has an infinite number of parameters (duh?).&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian.&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset of its elements gives another Gaussian.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They continue with &lt;strong&gt;fancy terms&lt;/strong&gt;, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Posterior over functions&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;li&gt;Covariances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Is this really supposed to make sense to the GP beginner?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following is the introductory tutorial on GPs that I wish I'd had myself. The goal is pedagogy — not the waving of &lt;strong&gt;fancy words&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;By the end of this tutorial, you should understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What a Gaussian process is and how to build one in NumPy&lt;/strong&gt; — including those cool, swirly error blobs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The motivations behind their functional form&lt;/strong&gt;, i.e. how the GP comes to be.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;fancy statements&lt;/strong&gt; and &lt;strong&gt;fancy terms&lt;/strong&gt; above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;
&lt;h2&gt;Playing with Gaussians&lt;/h2&gt;
&lt;p&gt;Before moving within 500 nautical miles of the Gaussian process, we're going to start with something far easier: vanilla Gaussians themselves. This will help us to build intuition. &lt;strong&gt;We'll arrive at the GP before you realize.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Gaussian distribution, a.k.a. the Normal distribution, can be thought of as a Python object which:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is instantiated with characteristic parameters &lt;code&gt;mu&lt;/code&gt; (the mean) and &lt;code&gt;var&lt;/code&gt; (the variance).&lt;/li&gt;
&lt;li&gt;Has a single public method, &lt;code&gt;density&lt;/code&gt;, which accepts a &lt;code&gt;float&lt;/code&gt; value &lt;code&gt;x&lt;/code&gt;, and returns a &lt;code&gt;float&lt;/code&gt; proportional to the probability of this &lt;code&gt;Gaussian&lt;/code&gt; having produced &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# the standard deviation is the square-root of the variance&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        NB: Understanding the two bullet points above is more important than understanding the following line.&lt;/span&gt;

&lt;span class="sd"&gt;        That said, it's just the second bullet in code, via SciPy.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, how do we make those cool bell-shaped plots? &lt;strong&gt;A 2D plot is just a list of tuples — each with an &lt;code&gt;x&lt;/code&gt;, and a corresponding &lt;code&gt;y&lt;/code&gt; — shown visually.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As such, we lay out our &lt;code&gt;x&lt;/code&gt;-axis, then compute the corresponding &lt;code&gt;y&lt;/code&gt; — the &lt;code&gt;density&lt;/code&gt; — for each. We'll choose an arbitrary &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;variance&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'`Gaussian(mu=.123, var=.456)` Density'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_8_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;If we increase the variance &lt;code&gt;var&lt;/code&gt;, what happens?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;bigger_number&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;3.45&lt;/span&gt;

&lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bigger_number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'`Gaussian(mu=.123, var=bigger_number)` Density'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_10_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The density gets fatter. This should be familiar to you.&lt;/p&gt;
&lt;p&gt;Similarly, we can draw &lt;em&gt;samples&lt;/em&gt; from a &lt;code&gt;Gaussian&lt;/code&gt; distribution, e.g. from the initial &lt;code&gt;Gaussian(mu=.123, var=.456)&lt;/code&gt; above. Its corresponding density plot (also above) governs this procedure, where &lt;code&gt;(x, y)&lt;/code&gt; tuples give the (unnormalized) probability &lt;code&gt;y&lt;/code&gt; that a given sample will take the value &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;x&lt;/code&gt;-values with large corresponding &lt;code&gt;y&lt;/code&gt;-values are more likely to be sampled. Here, values near .123 are most likely to be sampled.&lt;/p&gt;
&lt;p&gt;Let's add a method to our class, draw 500 samples, then plot their histogram.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rvs&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="c1"&gt;# Add method to class&lt;/span&gt;
&lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;

&lt;span class="c1"&gt;# Instantiate new Gaussian&lt;/span&gt;
&lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Gaussian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Draw samples&lt;/span&gt;
&lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# Plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Histogram of 500 samples from `Gaussian(mu=.123, var=.456)`'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_13_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;This looks similar to the true &lt;code&gt;Gaussian(mu=.123, var=.456)&lt;/code&gt; density we plotted above. The more random samples we draw (then plot), the closer this histogram will approximate (look similar to) the true density.&lt;/p&gt;
&lt;p&gt;Now, we'll start to move a bit faster.&lt;/p&gt;
&lt;h2&gt;2D Gaussians&lt;/h2&gt;
&lt;p&gt;We just drew samples from a 1-dimensional Gaussian, i.e. the &lt;code&gt;sample&lt;/code&gt; itself was a single float. The parameter &lt;code&gt;mu&lt;/code&gt; dictated the most-likely value for the &lt;code&gt;sample&lt;/code&gt; to assume, and the variance &lt;code&gt;var&lt;/code&gt; dictated how much these sample-values vary (hence the name variance).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5743030051553177&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="mf"&gt;0.06160509014194515&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="mf"&gt;1.050830033400354&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In 2D, each sample will be a list of two numbers. &lt;code&gt;mu&lt;/code&gt; will dictate the most-likely pair of values for the &lt;code&gt;sample&lt;/code&gt; to assume, and the second parameter (yet unnamed) will dictate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How much the values for the first element of the pair vary&lt;/li&gt;
&lt;li&gt;How much the values for the second element of the pair vary&lt;/li&gt;
&lt;li&gt;How much the first and second elements vary with each other, e.g. if the first element is larger than expected (i.e. larger than its corresponding mean), to what extent does the second element "follow suit" (and assume a value larger than expected as well)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second parameter is the &lt;strong&gt;covariance matrix&lt;/strong&gt;, &lt;code&gt;cov&lt;/code&gt;. The elements on the diagonal give Items 1 and 2. The elements off the diagonal give Item 3. The covariance matrix is always square, and the values along its diagonal are always non-negative.&lt;/p&gt;
&lt;p&gt;Given a 2D &lt;code&gt;mu&lt;/code&gt; and 2x2 &lt;code&gt;cov&lt;/code&gt;, we can draw samples from the 2D Gaussian. Here, we'll use NumPy. Inline, we comment on the expected shape of the samples.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_draws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_draws&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;The purple dots should center around `(x, y) = (0, 0)`. `np.diag([1, 1])` gives the covariance matrix `[[1, 0], [0, 1]]`:&lt;/span&gt;
&lt;span class="sd"&gt;`x`-values have a variance of `var=1`; `y`-values have `var=1`; these values do not covary with one another&lt;/span&gt;
&lt;span class="sd"&gt;(e.g. if `x` is larger than its mean, the corresponding `y` has 0 tendency to "follow suit," i.e. trend larger than its&lt;/span&gt;
&lt;span class="sd"&gt;mean as well).&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'purple'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;The blue dots should center around `(x, y) = (1, 3)`. Same story with the covariance.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'orange'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Here, the values along the diagonal of the covariance matrix are much larger: the cloud of green point should be much more&lt;/span&gt;
&lt;span class="sd"&gt;disperse. There is no off-diagonal covariance (`x` and `y` values do not vary — above or below their respective means — *together*).&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;The covariance matrix has off-diagonal values of -2. This means that if `x` trends above its mean, `y` will tend to vary *twice as much, below its mean.*&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;Covariances of 4.&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;

&lt;span class="n"&gt;plot_2d_draws&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'blue'&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Draws from 2D-Gaussians with Varying (mu, cov)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_17_1.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Gaussians are closed under linear maps&lt;/h2&gt;
&lt;p&gt;Each cloud of Gaussian dots tells us the following:&lt;/p&gt;
&lt;div class="math"&gt;$$
(x, y) \sim \text{Normal}(\mu, \Sigma)
$$&lt;/div&gt;
&lt;p&gt;In other words, the draws &lt;span class="math"&gt;\((x, y)\)&lt;/span&gt; are distributed normally with 2D-mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and 2x2 covariance &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's assume &lt;span class="math"&gt;\((x, y)\)&lt;/span&gt; is a vector named &lt;span class="math"&gt;\(w\)&lt;/span&gt;. Giving subscripts to the parameters of our Gaussian, we can rewrite the above as:&lt;/p&gt;
&lt;div class="math"&gt;$$
w \sim \text{Normal}(\mu_w, \Sigma_w)
$$&lt;/div&gt;
&lt;p&gt;Next, imagine we have some matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt; of size 200x2. If &lt;span class="math"&gt;\(w\)&lt;/span&gt; is distributed as above, how is &lt;span class="math"&gt;\(Aw\)&lt;/span&gt; distributed? Gaussian algebra tells us the following:&lt;/p&gt;
&lt;div class="math"&gt;$$
Aw \sim \text{Normal}(A\mu_w,\ A^T\Sigma_w A)
$$&lt;/div&gt;
&lt;p&gt;In other words, &lt;span class="math"&gt;\(Aw\)&lt;/span&gt;, the "linear map" of &lt;span class="math"&gt;\(w\)&lt;/span&gt; onto &lt;span class="math"&gt;\(A\)&lt;/span&gt;, is (incidentally) Gaussian-distributed as well.&lt;/p&gt;
&lt;p&gt;Let's plot some draws from this distribution. Let's assume each row of &lt;span class="math"&gt;\(A\)&lt;/span&gt; (of which there are 200, each containing 2 elements) is computed via the (arbitrary) function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, how do we get &lt;span class="math"&gt;\(A\)&lt;/span&gt;? Well, we could simply make such a matrix ourselves — &lt;code&gt;np.random.randn(200, 2)&lt;/code&gt; for instance. Separately, imagine we start with a 200D vector &lt;span class="math"&gt;\(X\)&lt;/span&gt; of arbitrary floats, use the above function to make 2 "features" for each, then take the transpose. This gives us our 200x2 matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, and still with the goal of obtaining samples &lt;span class="math"&gt;\(Aw\)&lt;/span&gt;, we'll multiply this matrix by our 2D mean-vector of weights, &lt;span class="math"&gt;\(\mu_w\)&lt;/span&gt;. You can think of the latter as passing a batch of data through a linear model (where our data have features &lt;span class="math"&gt;\(x = [x_1, x_2]\)&lt;/span&gt;, and our parameters are &lt;span class="math"&gt;\(\mu_w = [w_1, w_2]\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Finally, we'll take draws from this &lt;span class="math"&gt;\(\text{Normal}(A\mu_w,\ A\Sigma_w A^T)\)&lt;/span&gt;. This will give us tuples of the form &lt;code&gt;(x, Aw)&lt;/code&gt;. For simplicity, we'll hereafter refer to this tuple as &lt;code&gt;(x, y)&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the original &lt;code&gt;x&lt;/code&gt;-value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the value obtained after: making features out of &lt;span class="math"&gt;\(X\)&lt;/span&gt; and taking the transpose, giving &lt;span class="math"&gt;\(A\)&lt;/span&gt;; taking the linear combination of &lt;span class="math"&gt;\(A\)&lt;/span&gt; with the mean-vector of weights; taking a draw from the multivariate-Gaussian we just defined, then plucking out the sample-element corresponding to &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Each draw from our Gaussian will yield 200 &lt;code&gt;y&lt;/code&gt;-values, each corresponding to its original &lt;code&gt;x&lt;/code&gt;. In other words, it will yield 200 &lt;code&gt;(x, y)&lt;/code&gt; tuples — which we can plot.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To make it clear that &lt;span class="math"&gt;\(A\)&lt;/span&gt; was computed as a function of &lt;span class="math"&gt;\(X\)&lt;/span&gt;, let's rename it to &lt;span class="math"&gt;\(A = \phi(X)^T\)&lt;/span&gt;, and rewrite our distribution as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(X)^Tw \sim \text{Normal}(\phi(X)^T\mu_w,\ \phi(X)^T\Sigma_w \phi(X))
$$&lt;/div&gt;
&lt;p&gt;In addition, let's set &lt;span class="math"&gt;\(\mu_w =\)&lt;/span&gt; &lt;code&gt;np.array([0, 0])&lt;/code&gt; and &lt;span class="math"&gt;\(\Sigma_w =\)&lt;/span&gt; &lt;code&gt;np.diag([1, 2])&lt;/code&gt;. Finally, we'll take draws, then plot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# x-values&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Make features, as before&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (2, len(x))&lt;/span&gt;

&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Params of distribution over linear map (lm)&lt;/span&gt;
&lt;span class="n"&gt;mu_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt;
&lt;span class="n"&gt;cov_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Random Draws from a Distribution over Linear Maps'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Plot draws. `lm` is a vector of 200 `y` values, each corresponding to the original `x`-values&lt;/span&gt;
    &lt;span class="n"&gt;lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_lm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# lm.shape: (200,)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_20_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This distribution over linear maps gives a distribution over functions&lt;/strong&gt;, where the "mean function" is &lt;span class="math"&gt;\(\phi(X)^T\mu_w\)&lt;/span&gt; (which reads directly from the &lt;code&gt;mu_lm&lt;/code&gt; variable above).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notwithstanding, I find this phrasing to be confusing&lt;/strong&gt;; to me, a "distribution over functions" sounds like some opaque object that spits out algebraic symbols via logic miles above my cognitive ceiling. As such, I instead think of this in more intuitive terms as a &lt;strong&gt;distribution over function evaluations&lt;/strong&gt;, where a single function evaluation is a list of &lt;code&gt;(x, y)&lt;/code&gt; tuples and nothing more.&lt;/p&gt;
&lt;p&gt;For example, given a vector &lt;code&gt;x = np.array([1, 2, 3])&lt;/code&gt; and a function &lt;code&gt;lambda x: x**2&lt;/code&gt;, an evaluation of this function gives &lt;code&gt;y = np.array([1, 4, 9])&lt;/code&gt;. We now have tuples &lt;code&gt;[(1, 1), (2, 4), (3, 9)]&lt;/code&gt; from which we can create a line plot. This gives one "function evaluation."&lt;/p&gt;
&lt;p&gt;Above, we sampled 17 function evaluations, then plotted the 200 resulting &lt;code&gt;(x, y)&lt;/code&gt; tuples (as our input was a 200D vector &lt;span class="math"&gt;\(X\)&lt;/span&gt;) for each. The evaluations are similar because of the given mean function &lt;code&gt;mu_lm&lt;/code&gt;; they are different because of the given covariance matrix &lt;code&gt;cov_lm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's try some different "features" for our &lt;code&gt;x&lt;/code&gt;-values then plot the same thing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Make different, though still arbitrary, features&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_23_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;"The features we choose give a 'language' with which we can express a relationship between &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; Some features are more expressive than others; some restrict us entirely from expressing certain relationships.&lt;/p&gt;
&lt;p&gt;For further illustration, let's employ step functions as features and see what happens.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Make features, as before&lt;/span&gt;
&lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_25_0.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Gaussians are closed under conditioning and marginalization&lt;/h2&gt;
&lt;p&gt;Let's revisit the 2D Gaussians plotted above. They took the form (where &lt;span class="math"&gt;\(\mathcal{N}\)&lt;/span&gt; denotes the Normal, i.e. Gaussian distribution):&lt;/p&gt;
&lt;div class="math"&gt;$$
(x, y) \sim \mathcal{N}(\mu, \Sigma)
$$&lt;/div&gt;
&lt;p&gt;Said differently:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(x, y) = \mathcal{N}(\mu, \Sigma)
$$&lt;/div&gt;
&lt;p&gt;And now a bit more rigorously:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(x, y) = \mathcal{N}\bigg([\mu_x, \mu_y],
    \begin{bmatrix}
    \Sigma_x &amp;amp; \Sigma_{xy}\\
    \Sigma_{xy}^T &amp;amp; \Sigma_y\\
    \end{bmatrix}\bigg)
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;NB: In this case, all 4 "Sigmas" in the 2x2 covariance matrix are floats. If our covariance were bigger, say 31x31, then these 4 Sigmas would be &lt;strong&gt;matrices&lt;/strong&gt; (with an aggregate size totaling 31x31).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What if we wanted to know the distribution over &lt;span class="math"&gt;\(y\)&lt;/span&gt; conditional on &lt;span class="math"&gt;\(x\)&lt;/span&gt; taking on a certain value, e.g. &lt;span class="math"&gt;\(P(y\vert x &amp;gt; 1)\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y\)&lt;/span&gt; is a single element, so the resulting conditional will be a univariate distribution. To gain intuition, let's do this in a very crude manner:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;345&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# some random sample size&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;y_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Histogram of y-values, when x &amp;gt; 1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_28_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Cool! Looks kind of Gaussian as well.&lt;/p&gt;
&lt;p&gt;Instead, what if we wanted to know the functional form of the real density, &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;, instead of this empirical distribution of its samples? One of the axioms of conditional probability tells us that:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x, y)}{P(x)} = \frac{P(x, y)}{\int P(x, y)dy}
$$&lt;/div&gt;
&lt;p&gt;The right-most denominator can be written as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\int P(x, y)dy
    &amp;amp;= \int \mathcal{N}\bigg([\mu_x, \mu_y],
        \begin{bmatrix}
        \Sigma_x &amp;amp; \Sigma_{xy}\\
        \Sigma_{xy}^T &amp;amp; \Sigma_y\\
        \end{bmatrix}\bigg)
   dy\\
   \\
   &amp;amp;= \mathcal{N}(\mu_x, \Sigma_x)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Marginalizing a &amp;gt; 1D Gaussian over one of its elements yields another Gaussian&lt;/strong&gt;: you just "pluck out" the elements you'd like to examine. &lt;strong&gt;In other words, Gaussians are closed under marginalization.&lt;/strong&gt; "It's almost too easy to warrant a formula."&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As an example, imagine we had the following Gaussian &lt;span class="math"&gt;\(P(a, b, c)\)&lt;/span&gt;, and wanted to compute the marginal over the first 2 elements, i.e. &lt;span class="math"&gt;\(P(a, b) = \int P(a, b, c)dc\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# P(a, b, c)&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;66&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;77&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# P(a, b)&lt;/span&gt;
&lt;span class="n"&gt;mu_marginal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cov_marginal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# That's it.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we compute the conditional Gaussian of interest — a result well-documented by mathematicians long ago:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert x)
    &amp;amp;= \frac{
            \mathcal{N}\bigg(
                [\mu_x, \mu_y],
                \begin{bmatrix}
                \Sigma_x &amp;amp; \Sigma_{xy}\\
                \Sigma_{xy}^T &amp;amp; \Sigma_y\\
                \end{bmatrix}
            \bigg)
            }
            {\mathcal{N}(\mu_x, \Sigma_x)}\\
   \\
   &amp;amp;= \mathcal{N}(\mu_y + \Sigma_{xy}\Sigma_x^{-1}(x - \mu_x), \Sigma_y - \Sigma_{xy}\Sigma_x^{-1}\Sigma_{xy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt; can be a matrix. From there, just plug stuff in.&lt;/p&gt;
&lt;p&gt;Conditioning a &amp;gt; 1D Gaussian on one (or more) of its elements yields another Gaussian. &lt;strong&gt;In other words, Gaussians are closed under conditioning.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Inferring the weights&lt;/h2&gt;
&lt;p&gt;We previously posited a distribution over some vector of weights, &lt;span class="math"&gt;\(w \sim \text{Normal}(\mu_w, \Sigma_w)\)&lt;/span&gt;. In addition, we posited a distribution over the linear map of these weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X)^T\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \phi(X)^Tw \sim \text{Normal}(\phi(X)^T\mu_w,\ \phi(X)^T\Sigma_w \phi(X))
$$&lt;/div&gt;
&lt;p&gt;Given some ground-truth samples from this distribution &lt;span class="math"&gt;\(y = \phi(X)^Tw\)&lt;/span&gt;, i.e. ground-truth "function evaluations," we'd like to infer the weights &lt;span class="math"&gt;\(w\)&lt;/span&gt; most consistent with &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In machine learning, we equivalently say that given a model and some observed data &lt;code&gt;(X_train, y_train)&lt;/code&gt;, we'd like to compute/train/optimize the weights of said model (often via backpropagation).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Most precisely, our goal is to infer &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; (where &lt;span class="math"&gt;\(y\)&lt;/span&gt; are our observed function evaluations). To do this, we simply posit a joint distribution over both quantities:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(w, y) =
    \mathcal{N}\bigg(
        [\mu_w, \phi(X)^T\mu_w],
        \begin{bmatrix}
        \Sigma_w &amp;amp; \Sigma_{wy}\\
        \Sigma_{wy}^T &amp;amp; \phi(X)^T\Sigma_w \phi(X)\\
        \end{bmatrix}
    \bigg)
$$&lt;/div&gt;
&lt;p&gt;Then compute the conditional via the formula above:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(w\vert y)
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}\Sigma_y^{-1}(y - \mu_y), \Sigma_w - \Sigma_{wy}\Sigma_y^{-1}\Sigma_{wy}^T)\\
    \\
    &amp;amp;= \mathcal{N}(\mu_w + \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}(y - \mu_w^T \phi(X)), \Sigma_w - \Sigma_{wy}(\phi(X)^T\Sigma_w \phi(X))^{-1}\Sigma_{wy}^T)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This formula gives the posterior distribution over our weights &lt;span class="math"&gt;\(P(w\vert y)\)&lt;/span&gt; given the model and observed data tuples &lt;code&gt;(x, y)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Until now, we've assumed a 2D &lt;span class="math"&gt;\(w\)&lt;/span&gt;, and therefore a &lt;span class="math"&gt;\(\phi(X)\)&lt;/span&gt; in &lt;span class="math"&gt;\(\mathbb{R}^2\)&lt;/span&gt; as well. Moving forward, we'll work with weights and features in a higher-dimensional space, &lt;span class="math"&gt;\(\mathbb{R}^{20}\)&lt;/span&gt;; this will give us a more expressive language with which to capture the true relationship between some quantity &lt;span class="math"&gt;\(x\)&lt;/span&gt; and its corresponding &lt;span class="math"&gt;\(y\)&lt;/span&gt;. &lt;span class="math"&gt;\(\mathbb{R}^{20}\)&lt;/span&gt; is an arbitrary choice; it could have been &lt;span class="math"&gt;\(\mathbb{R}^{17}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\mathbb{R}^{31}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\mathbb{R}^{500}\)&lt;/span&gt; as well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The true function that maps `x` to `y`. This is what we are trying to recover with our mathematical model.&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;true_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;


&lt;span class="c1"&gt;# x-values&lt;/span&gt;
&lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="c1"&gt;# y-train&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;true_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="c1"&gt;# A function to make some arbitrary features&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape(D, len(x))&lt;/span&gt;


&lt;span class="c1"&gt;# A function that computes the parameters of the linear map distribution&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_linear_map_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;mu_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
    &lt;span class="n"&gt;cov_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;map_matrix&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu_lm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_lm&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_weights_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    NB: "Computing a posterior," and given that that posterior is Gaussian, implies nothing more than&lt;/span&gt;
&lt;span class="sd"&gt;    computing the mean-vector and covariance matrix of this Gaussian.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="c1"&gt;# Featurize x_train&lt;/span&gt;
    &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Params of prior distribution over function evals&lt;/span&gt;
    &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_linear_map_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Params of posterior distribution over weights&lt;/span&gt;
    &lt;span class="n"&gt;mu_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;phi_x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt;


&lt;span class="c1"&gt;# Compute weights posterior&lt;/span&gt;
&lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_weights_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As with our prior over our weights, we can equivalently draw samples from the posterior over our weights, then plot. These samples will be 20D vectors; we reduce them to 2D for ease of visualization.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Draw samples&lt;/span&gt;
&lt;span class="n"&gt;samples_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;samples_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Reduce to 2D for ease of visualization&lt;/span&gt;
&lt;span class="n"&gt;first_dim_prior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;second_dim_prior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples_prior&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;first_dim_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;second_dim_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples_post&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_34_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;Samples from the prior are plotted in orange; samples from the posterior are plotted in blue. As this is a stochastic dimensionality-reduction algorithm, the results will be slightly different each time.&lt;/p&gt;
&lt;p&gt;At best, we can see that the posterior has slightly larger values in its covariance matrix, evidenced by the fact that the blue cloud is more disperse than the orange, and has probably maintained a similar mean. The magnitude of change (read: a small one) is expected, as we've only conditioned on 6 ground-truth tuples.&lt;/p&gt;
&lt;h1&gt;Predicting on new data&lt;/h1&gt;
&lt;p&gt;Previously, we sampled function evaluations by centering a multivariate Gaussian on &lt;span class="math"&gt;\(\phi(X)^T\mu_{w}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\mu_w\)&lt;/span&gt; was the mean of the prior distribution over weights. We'd now like to do the same thing, but use our posterior over weights instead. How does this work?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Well, Gaussians are closed under linear maps.&lt;/strong&gt; So, we just follow the formula we had used above.&lt;/p&gt;
&lt;p&gt;This time, instead of input vector &lt;span class="math"&gt;\(X\)&lt;/span&gt;, we'll use a new input vector called &lt;span class="math"&gt;\(X_{*}\)&lt;/span&gt;, i.e. the "new data" on which we'd like to predict.&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi(X_{*})^Tw \sim \text{Normal}(\phi(X_{*})^T\mu_{w, \text{post}},\ \phi(X_{*})^T \Sigma_{w, \text{post}}\phi(X_{*}))
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;This gives us a posterior distribution over function evaluations.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In machine learning parlance, this is akin to: given some test data &lt;code&gt;X_test&lt;/code&gt;, and a model whose weights were trained/optimized with respect to/conditioned on some observed ground-truth tuples &lt;code&gt;(X_train, y_train)&lt;/code&gt;, we'd like to generate predictions &lt;code&gt;y_test&lt;/code&gt;, i.e. samples from the posterior over function evaluations.&lt;/p&gt;
&lt;p&gt;The function to compute this posterior, i.e. compute the mean-vector and covariance matrix of this Gaussian, will appear both short and familiar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_weights_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;phi_x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_linear_map_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To plot, we typically just plot the error bars, i.e. the space within &lt;code&gt;(mu_y_post - var_y_post, mu_y_post + var_y_post)&lt;/code&gt; for each &lt;code&gt;x&lt;/code&gt;, as well as the ground-truth tuples as big red dots. &lt;strong&gt;This gives nothing more than a picture of the mean-vector and covariance of our posterior.&lt;/strong&gt; Optionally, we can plot samples from this posterior as well, as we did with our prior.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Posterior Distribution over Function Evaluations'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Extract the variances, i.e. the diagonal, of our covariance matrix&lt;/span&gt;
    &lt;span class="n"&gt;var_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Plot the error bars.&lt;/span&gt;
    &lt;span class="c1"&gt;# To do this, we fill the space between `(mu_y_post - var_y_post, mu_y_post + var_y_post)` for each `x`&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill_between&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_y_post&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;var_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_y_post&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;var_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'#23AEDB'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Scatter-plot our original 6 `(x, y)` tuples&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'ro'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;markersize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Optionally plot actual samples (function evaluations) from this posterior&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_40_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;The posterior distribution is nothing more than a distribution over function evaluations (from which we've sampled 25 function evaluations above) &lt;em&gt;most consistent with our model and observed data tuples.&lt;/em&gt; As such, and to give further intuition, a crude way of computing this distribution might be continuously &lt;em&gt;drawing samples from our prior over function evaluations, and keeping only the ones that pass through, i.e. are "most consistent with," all of the red points above.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, we stated before that &lt;strong&gt;the features we choose (i.e. our &lt;code&gt;phi_func&lt;/code&gt;) give a "language" with which we can express the relationship between &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt; Here, we've chosen a language with 20 words. What if we chose a different 20?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="c1"&gt;# Still arbitrary, i.e. a modeling choice!&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_42_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Not great. As a brief aside, how do we read the plot above? It's simply a function, a transformation, a lookup: given an &lt;span class="math"&gt;\(x\)&lt;/span&gt;, it tells us the corresponding expected value &lt;span class="math"&gt;\(y\)&lt;/span&gt;, and the variance around this estimate.&lt;/p&gt;
&lt;p&gt;For instance, right around &lt;span class="math"&gt;\(x = -3\)&lt;/span&gt;, we can see that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is somewhere in &lt;span class="math"&gt;\([-1, 1]\)&lt;/span&gt;; given that we've only conditioned on 6 "training points," we're still quite unsure as to what the true answer is. To this effect, a GP (and other fully-Bayesian models) allows us to quantify this uncertainty judiciously.&lt;/p&gt;
&lt;p&gt;Now, let's try some more features and examine the model we're able to build.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Still arbitrary, i.e. a modeling choice!&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_44_1.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_45_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;That last one might look familiar. Therein, the features we chose (still arbitrarily, really) are called "radial basis functions" (among other names).&lt;/p&gt;
&lt;p&gt;We've loosely examined what happens when we change the language through which we articulate our model. Next, what if we changed the size of its vocabulary?&lt;/p&gt;
&lt;p&gt;First, let's backtrack, and try 8 of these radial basis functions instead of 20.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_47_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;Very different! Holy overfit. What about 250?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;


&lt;span class="c1"&gt;# Params of distribution over weights&lt;/span&gt;
&lt;span class="n"&gt;mu_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# mu_w.shape: (D,)&lt;/span&gt;
&lt;span class="n"&gt;cov_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# cov_w.shape: (D, D)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;  &lt;span class="c1"&gt;# phi_x.shape: (D, len(x))&lt;/span&gt;


&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_gp_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov_y_post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="png" class="img-responsive" src="https://cavaunpeu.github.io/figures/gaussian-algebra-to-gaussian-processes-part-1/output_49_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;It appears that the more features we use, the more expressive, and/or less endemically prone to overfitting, our model becomes.&lt;/p&gt;
&lt;p&gt;So, how far do we take this? &lt;code&gt;D = 1000&lt;/code&gt;? &lt;code&gt;D = 50000&lt;/code&gt;? How high can we go? &lt;strong&gt;We'll pick up here in the next post.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this tutorial, we've arrived at the mechanical notion of a Gaussian process via simple Gaussian algebra.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thus far, we've elucidated the following ideas:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Gaussian process (GP) defines a distribution over functions (i.e. function evaluations). √&lt;/li&gt;
&lt;li&gt;Marginalizing a Gaussian over a subset of its elements gives another Gaussian (just pluck out the pieces of interest). √&lt;/li&gt;
&lt;li&gt;Conditioning a subset of the elements of a Gaussian on another subset gives another Gaussian (a simple algebraic formula). √&lt;/li&gt;
&lt;li&gt;Posterior over functions (the linear map of the posterior over weights onto some matrix &lt;span class="math"&gt;\(A = \phi(X_{*})^T\)&lt;/span&gt;) √&lt;/li&gt;
&lt;li&gt;Covariances (the second thing we need in order to specify a multivariate Gaussian) √&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conversely, we did not yet cover (directly):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Squared-exponentials&lt;/li&gt;
&lt;li&gt;A Gaussian process is non-parametric, i.e. it has an infinite number of parameters (duh?).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These will be the subject of the following post.&lt;/p&gt;
&lt;p&gt;Thanks for reading, and &lt;strong&gt;don't let arcane pedagogy discourage you:&lt;/strong&gt; there's almost always a clearer explanation (or at least its attempt) at bay.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/gaussian-processes"&gt;repository&lt;/a&gt; and &lt;a href="https://nbviewer.jupyter.org/github/cavaunpeu/gaussian-processes/blob/master/gaussian-processes-part-1.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=50Vgw11qn0o"&gt;Gaussian Processes 1 - Philipp Hennig - MLSS 2013 Tübingen
&lt;/a&gt; (from which this post takes heavy inspiration) &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="http://gaussianprocess.org/gpml/?"&gt;Gaussian Processes for Machine Learning&lt;/a&gt;. Carl Edward Rasmussen and Christopher K. I. Williams
The MIT Press, 2006. ISBN 0-262-18253-X. &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"&gt;Fitting Gaussian Process Models in Python&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;a href="http://sashagusev.github.io/2016-01/GP.html"&gt;Gaussian process regression
&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>A Practical Guide to the "Open-Source Machine Learning Masters"</title><link href="https://cavaunpeu.github.io/2018/02/03/practical-guide-open-source-ml-masters/" rel="alternate"></link><published>2018-02-03T18:30:00-05:00</published><updated>2018-02-03T18:30:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2018-02-03:/2018/02/03/practical-guide-open-source-ml-masters/</id><summary type="html">&lt;p&gt;The higher education paradigm is changing. Motivation, logistics and strategic insight re: designing the "Open-Source Masters" for yourself.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As an "in" professional discipline, Machine Learning exhibits a curious behavior: though talent is frustratingly scarce, it's immensely &lt;em&gt;easy&lt;/em&gt; for the individual to obtain. Why? &lt;strong&gt;It's all available online, (largely) free for the taking.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I did not get a bachelor's degree in this field. Self-teaching at my kitchen table in 2014, I landed a job as a software developer. Self-teaching still, I joined &lt;a href="https://www.shopkeep.com/"&gt;ShopKeep&lt;/a&gt;'s Data Science team. Eighteen months later and wanting further technical expertise, I quit my job, left NYC, and jumped full-time into machine learning self-study — the best, if only, way I knew how.&lt;/p&gt;
&lt;p&gt;In September of 2016, I moved to Casablanca, Morocco to pursue the &lt;a href="https://cavaunpeu.github.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;"Open-Source Machine Learning Masters"&lt;/a&gt; — a full-time, self-curated schedule of textbooks, MOOCs, publishing code to my &lt;a href="https://github.com/cavaunpeu"&gt;GitHub&lt;/a&gt; and writing to this blog. Now back living and working in NYC, &lt;a href="https://cavaunpeu.github.io/2017/09/09/joining-asapp/"&gt;decidedly successful&lt;/a&gt; in what I set out to do, I'd like to inform others as to how to plan and execute a similar program for yourself.&lt;/p&gt;
&lt;p&gt;The following is a practical guide to the "Open-Source Machine Learning Masters" (OSMLM). It is &lt;em&gt;not&lt;/em&gt; a technical guide, which would provide concrete guidance on what to study. Instead, my aim is to provide motivation, logistical details, and strategic insight — to take you from "that seems cool, but crazy" to "woah, this is not only feasible, but really makes a lot of sense." I personally find the latter much more valuable.&lt;/p&gt;
&lt;p&gt;The content is organized as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Why this might be right for you&lt;/li&gt;
&lt;li&gt;Building a curriculum&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Logistics&lt;/p&gt;
&lt;p&gt;a. Educational resources&lt;/p&gt;
&lt;p&gt;b. Branding&lt;/p&gt;
&lt;p&gt;c. Workspace&lt;/p&gt;
&lt;p&gt;d. Where to live&lt;/p&gt;
&lt;p&gt;e. Costs&lt;/p&gt;
&lt;p&gt;f. Visas&lt;/p&gt;
&lt;p&gt;g. Community&lt;/p&gt;
&lt;p&gt;h. Insurance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Keeping a schedule&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;A sense of urgency&lt;/li&gt;
&lt;li&gt;Personal development objectives, in parallel&lt;/li&gt;
&lt;li&gt;Applying for the next job&lt;/li&gt;
&lt;li&gt;Why there's very little risk&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Why this might be right for you&lt;/h2&gt;
&lt;p&gt;You should pursue the OSMLM for the same reasons that you'd pursue a traditional master's program: because you want further technical expertise in this field. The motivation behind this ambition may vary: perhaps you've a bachelor's in Economics and want to break into data science, or perhaps you're dexterous with &lt;a href="http://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt;, are effective in your role, yet need to understand more deeply how your tools work in order to obtain a promotion. The OSMLM, like a formal master's, is a semi-broad deep-dive into select topics in machine learning and not much more.&lt;/p&gt;
&lt;p&gt;The OSMLM confers a number of obvious benefits in cost, expediency and flexibility. These are enumerated in detail in the &lt;a href="https://cavaunpeu.github.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;motivating post&lt;/a&gt;. Notwithstanding, what are the downsides? To this effect, what type of person is required to pull this off?&lt;/p&gt;
&lt;h3&gt;Downsides&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;There is no one to push you but yourself.&lt;/strong&gt; A priori, there are no professors. There are no study groups. There will be no tangible social pressure: it will probably just be you.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;This might be viewed as a "radical" choice&lt;/strong&gt;. In ten years, I'm certain this will change. However, in 2018, you'll find few people with the line "Open-Source Masters" on their résumé.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The meritocracy comes to bear.&lt;/strong&gt; A master's degree from a prominent University, along with a passable GPA, holds professional weight (for better or for worse). In contrast, the OSMLM attacks on one singular vector: that &lt;em&gt;talent&lt;/em&gt; is weight, and that success in this program is developing this talent. There will be no "diploma-crutch."&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Who this is for&lt;/h3&gt;
&lt;p&gt;Addressing the above in order, the OSMLM might be right for you if:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;You're convicted about your passion for the field&lt;/strong&gt;. The thought of studying machine learning all day has you smiling from ear to ear. No one will push you but yourself, and that's the only person you need.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;You don't mind being different.&lt;/strong&gt; You're not phased by the notion of explaining — crisply and with conviction — why you made the choice to pursue this program.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;You feel the meritocracy in your bones.&lt;/strong&gt; You're confident in your ability to master material, and communicate it back to others. You know what the bar is for employment in your desired role, you know that you'll surpass that bar after finishing this program, and you trust that &lt;em&gt;because the meritocracy is real, this program could do nothing but bear delicious, abundant fruit.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Logistics&lt;/h2&gt;
&lt;h3&gt;Educational resources&lt;/h3&gt;
&lt;p&gt;Personally, I derived my educational resources from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MOOCs (massive open online courses)&lt;/li&gt;
&lt;li&gt;Textbooks (often obtainable in electronic format)&lt;/li&gt;
&lt;li&gt;Blog posts&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=yDLKJtOVx5c&amp;amp;list=PLD0F06AA0D2E8FFBA"&gt;mathematicalmonk videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udacity.com/nanodegree"&gt;Udacity "Nanodegrees"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yours may be different. However, the promise is: &lt;strong&gt;all of these materials are available online, and for a trivial or zero cost.&lt;/strong&gt; If you've been in the field for more than a month, this is probably already clear to you.&lt;/p&gt;
&lt;h3&gt;Branding&lt;/h3&gt;
&lt;p&gt;Again, this is a radical choice! If only because you won't know many others doing the same.&lt;/p&gt;
&lt;p&gt;In NYC, everyone is asked for their story — in interviews, at parties, on the subway. Preparing to quit my job, leave the city and move to Morocco — what was mine? How would I present this to others before leaving, during the experience, and after returning to the city? Believe it or not, I started composing, if mentally rehearsing, this story long before I ever left.&lt;/p&gt;
&lt;h4&gt;Before&lt;/h4&gt;
&lt;p&gt;Be excited. "Here's what I'm doing! Here's why I'm doing it! I'm scared, yet convicted. It's awesome, and I will be back, more learned than ever. Promise."&lt;/p&gt;
&lt;h4&gt;During&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Maintain a blog.&lt;/strong&gt; This is an extremely important point of strategy. &lt;strong&gt;Arguably, this was my most crucial insight throughout this whole experience.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Keeping a blog serves three principal ends:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;It helps you to concretize knowledge.&lt;/strong&gt; If you're going to share something with others, it is &lt;em&gt;easy&lt;/em&gt; to motivate yourself to make sure you know what you're talking about. This stems from the basic human tendencies of fearing both rejection, and the perception of inconsistency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It helps you to stay relevant in other people's eyes.&lt;/strong&gt; I was diligent about sharing my posts on &lt;a href="https://twitter.com/willwolf_"&gt;Twitter&lt;/a&gt; and &lt;a href="http://www.datatau.com/"&gt;DataTau&lt;/a&gt;. Fortunately, they were shared by others. In fact, I received 7 blind interview requests from managers that had read my blog (and continue to receive them to this day). This is a point of expediency, not braggadocio: do it well once, share it, and reap the rewards for a long time to come.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It gives you something — nay, everything — to talk about in your interviews upon your return.&lt;/strong&gt; I was keenly aware of the importance, for better or for worse, of maintaining a public sharing space of personal projects before I left for Morocco. Throughout the OSMLM, I could cynically be characterized as a "professional blog-writer/portfolio-builder." &lt;strong&gt;Upon return, over half of my interviews began with: "Hey, I read that blog post which I really enjoyed. Mind walking me through it?" Uh, yep. No problem.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In addition to a blog, I found that &lt;strong&gt;maintaining a presence on Twitter proved valuable as well.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;The machine learning community is small: build yourself a community within,&lt;/strong&gt; for all the same reasons that community-building is more generally valuable to your work, and quality of life.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;People will likely find your idea enviable and admirable.&lt;/strong&gt; Everyone likes a "radical," and the idea of living in a "far-off land." In addition, if they're passionate about machine learning (and cognizant of the speed at which it moves), they probably relish the thought of full-time study as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;After&lt;/h4&gt;
&lt;p&gt;See "Applying for the next job" below.&lt;/p&gt;
&lt;h3&gt;Workspace&lt;/h3&gt;
&lt;p&gt;I lived in an apartment in Casablanca. However, I would not have been happy working out of this apartment as well.&lt;/p&gt;
&lt;p&gt;For all those with ambition to work and travel: co-working spaces are a thing. Even in Morocco. They have comparatively fast internet (to the median speed found in the city at large), desks, coffee, patios, food, and most importantly, other motivated individuals like yourself.&lt;/p&gt;
&lt;p&gt;Your choice of workspace is up to you. For me, finding a space outside of my apartment in which to work was important for my social and emotional health.&lt;/p&gt;
&lt;h3&gt;Where to live&lt;/h3&gt;
&lt;p&gt;The world is huge! And in most of the world, $1,000/month gets you pretty far.&lt;/p&gt;
&lt;p&gt;I chose to live in Morocco so as to improve my French, and live in an Arab country more generally. Your interests are probably different.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://nomadlist.com/"&gt;NomadList&lt;/a&gt; is a terrific resource for choosing where to "digital nomad." It indexes countries based on things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost of living&lt;/li&gt;
&lt;li&gt;Internet speed&lt;/li&gt;
&lt;li&gt;Weather&lt;/li&gt;
&lt;li&gt;Safety&lt;/li&gt;
&lt;li&gt;Female-friendliness&lt;/li&gt;
&lt;li&gt;LGBTQ-friendliness&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you're unsure about where you might want to go, I would start here. &lt;strong&gt;In addition, moving abroad is of course in no way a requirement for this program&lt;/strong&gt;. However, the assumption is that you won't be generating income, so pursuing the OSMLM in your home country might prove expensive.&lt;/p&gt;
&lt;h3&gt;Visas&lt;/h3&gt;
&lt;p&gt;Please, do not overlook visa requirements. Follow the rules. In Morocco, I was granted 3-month tourist visas, which I could renew by spending a few hours in neighboring Spain. For an in-depth look at visa regulations, please visit my personal favorite, &lt;a href="https://www.visahq.com/"&gt;VisaHQ&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Costs&lt;/h3&gt;
&lt;p&gt;Personally, my costs had the following dimensions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Monthly rent: $600/month&lt;/strong&gt; for a beautiful, central, sunlit, and furnished 2-bedroom apartment in the center of Casablanca, which I often split with a friend.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coworking space: $300/month&lt;/strong&gt;. Really expensive, in my opinion! (I found the same in Mexico City as well, that a monthly co-working fee ran roughly half of what I'd pay in rent.) All the same, &lt;strong&gt;I only spent 2 months here before moving to friends' offices.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Educational resources: roughly $1,100.&lt;/strong&gt; This said, I spent $800 on the first half of the &lt;a href="https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889"&gt;Udacity Artifical Intelligence Nanodegree&lt;/a&gt; program. Thereafter, I bought a textbook, subscribed to some Coursera courses at $50/per, etc. &lt;strong&gt;Overall, if I wanted this cost to be $0, I could have made that happen.&lt;/strong&gt; In any event, I do find the aforementioned costs trivial considering the value I received in return, and even moreso as compared to tuition in a formal master's setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In 10 months, I spent roughly $12,000. I ate out, traveled some, and afforded a terrific quality of life built around the thing I wanted to do most: full days of bright-eyed, and well-rested, machine learning study.&lt;/p&gt;
&lt;h3&gt;Community&lt;/h3&gt;
&lt;p&gt;I knew 2 people in Casablanca before I arrived. Finding a community was important to me.&lt;/p&gt;
&lt;p&gt;Technology/entrepreneurship communities can be found worldwide (&lt;a href="https://www.linkedin.com/in/adrianavendano/"&gt;Adrian Avendano&lt;/a&gt; is an unbelievable testament to this fact). Casablanca was no different. I attended Meetups, pitch events, happy hours, and even &lt;a href="https://www.meetup.com/PyData-Casablanca/"&gt;PyData Casablanca&lt;/a&gt;! After a while, I was able to surround myself with people that kept me happy and helped me grow. Choosing to work from a co-working space can often give you this "for free" as well.&lt;/p&gt;
&lt;h3&gt;Insurance&lt;/h3&gt;
&lt;p&gt;Like any extended international trip, ensure that you've premeditated your options in the event of theft, dental, or medical emergencies. This is standard practice.&lt;/p&gt;
&lt;p&gt;For the OSMLM, give special thought to &lt;em&gt;computer&lt;/em&gt; insurance as well: without your computer, there's not a whole lot of work that you can do.&lt;/p&gt;
&lt;p&gt;If your computer breaks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Where's the computer store?&lt;/li&gt;
&lt;li&gt;How will i get a new one?&lt;/li&gt;
&lt;li&gt;How long will this take?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will vary by country. Investigate beforehand.&lt;/p&gt;
&lt;h2&gt;Building a curriculum&lt;/h2&gt;
&lt;p&gt;I know the landscape of machine learning educational resources exceedingly well, if only because the field is dear to my heart. I know what I know, and I know what textbooks, courses and papers I'm dying to have the time and energy to properly consume.&lt;/p&gt;
&lt;p&gt;For me, the OSMLM was 10 months of navigating, with discipline yet flexibility, this dreamscape.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unfortunately, prescribing you a concrete academic curriculum is outside the scope of this post&lt;/strong&gt; for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Everyone will have a different initial base of knowledge.&lt;/li&gt;
&lt;li&gt;On some level, your "curriculum" should already be in motion: if "you’re convicted about your passion for the field" (as noted above), you probably already have some idea of what you'd like to learn.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This said, if you're looking for resources on specific topics, I likely &lt;em&gt;am&lt;/em&gt; a great person with whom to speak, and I'm more than happy to help.&lt;/p&gt;
&lt;h2&gt;Keeping a schedule&lt;/h2&gt;
&lt;p&gt;Like building a curriculum, this came easy to me: keeping a disciplined, focused schedule around machine-learning self-study was all I wanted to do.&lt;/p&gt;
&lt;p&gt;If this comes less easily to you: &lt;strong&gt;treat this like a job.&lt;/strong&gt; Get rest. Take weekends off. Exercise before you go to work (if that's your thing). Leave your workspace at a reasonable hour, and pursue hobbies, relationships and culture at night. Hold yourself accountable. Do whatever worked for you in a previous job, or period of your life.&lt;/p&gt;
&lt;h2&gt;A sense of urgency&lt;/h2&gt;
&lt;p&gt;In a salaried job, you trade the myriad potential upsides of entrepreneurship for stability: assuming you do your job well, you &lt;em&gt;will&lt;/em&gt; receive a paycheck twice a month, (largely) irrespective of the exact hours you put in. "Hustling" bears few fruit. Ultimately, your goal is longevity: to continue to do great work, and be excited about this work; to not burn out; to be there in a few years' time.&lt;/p&gt;
&lt;p&gt;In the OSMLM, things are different. Here, you've "ripped the carpet out from under you." You have no paycheck, no immediate professional support, no structural "crutch." Furthermore, once finished, you will have something to &lt;em&gt;prove&lt;/em&gt; to your previous professional world (in interviews, etc.).&lt;/p&gt;
&lt;p&gt;My [machine learning] goal for the OSMLM was to learn as much as possible. My resource-bottleneck was time. As such, it became &lt;em&gt;easy&lt;/em&gt; to keep a robust schedule, and to work slightly-longer-than-normal hours. It was, stated &lt;em&gt;dramatically&lt;/em&gt;, "fight or flight." Looking back, I am impressed, though not entirely surprised, with how much work I produced within this context.&lt;/p&gt;
&lt;p&gt;In the OSMLM, &lt;strong&gt;a sense of urgency is a feature, not a bug.&lt;/strong&gt; Use it to your advantage — as a biologically-principled infusion of raw motivation.&lt;/p&gt;
&lt;h2&gt;Personal development objectives, in parallel&lt;/h2&gt;
&lt;p&gt;In 2016, in addition to further technical expertise in ML, I wanted to both ~perfect my French, and live in an Arab country. Pursuing the OSMLM in Casablanca allowed me to chase all three goals simultaneously.&lt;/p&gt;
&lt;p&gt;Besides the technical, what are &lt;em&gt;your&lt;/em&gt; goals? Is there a language you'd like to learn? A sport you'd like to try? A list of books you'd like to read?&lt;/p&gt;
&lt;p&gt;There is no mandate to have a single focus: &lt;strong&gt;you're in control of your days.&lt;/strong&gt; If you want to study machine learning from Monday-Thursday, then write fantasy novels on Fridays, you should do that. If you'd like to teach English abroad, move into a school in Japan and study machine learning at night.&lt;/p&gt;
&lt;p&gt;Of course, the OSMLM isn't about machine learning at all. Instead, it's about:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Writing down your learning goals, and acknowledging that most of the knowledge they require is freely available online.&lt;/li&gt;
&lt;li&gt;Figuring out the most expedient way of accomplishing these goals, ideally simultaneously.&lt;/li&gt;
&lt;li&gt;Getting after it, now.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Applying for the next job&lt;/h2&gt;
&lt;p&gt;While this might seem daunting, it is &lt;em&gt;really&lt;/em&gt; the easiest part.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Have a narrative&lt;/strong&gt;. Clearly communicate why you chose to pursue this program, and why it was the best choice for you, personally. To almost any employer for whom I wanted to work, it was a &lt;em&gt;very&lt;/em&gt; easy case to make.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If you kept a blog, things will come to you.&lt;/strong&gt; As noted above, I got many interviews from people having read my blog; furthermore, the majority of my remaining interviews were structured around a walk-through of a post I had written. This puts you in the driver's seat from the start.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If it's clear that you worked hard, you really can't lose.&lt;/strong&gt; The OSMLM aligns cleanly with three key things a startup looks for in a prospective technical hire: a passion for your work, a demonstrated ability to learn quickly and autonomously, and a portfolio showcasing what you can actually do, today. If you worked hard throughout your OSMLM, you'll bring all three in spades.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When I returned from my &lt;a href="http://willtravellife.com/"&gt;two-year trip&lt;/a&gt;, and sat down to face my interviewer, that was all they wanted to discuss! It was seen as "different, courageous, enviable." Empirically, the OSMLM was received in largely the same manner.&lt;/p&gt;
&lt;p&gt;Finally, to the extent you've kept a blog, simply sharing with your readers that you're looking for "what's next" is a great way to get started. Fortunately, I had many fantastic individuals following my journey, and they were nothing but helpful in connecting me with their network when I began to plot next steps.&lt;/p&gt;
&lt;p&gt;Interviewing upon return is the fun part. You'll be surprised how well this experience is received, and just how far it takes you.&lt;/p&gt;
&lt;h2&gt;Why there's really little risk&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Assuming you put in the work,&lt;/em&gt; I see two possible outcomes for this experience:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;You realize you don't like machine learning all that much.&lt;/strong&gt; If you're disenchanted with this field after dedicating a meager 6-12 months to its mastery, then it's probably not for you. &lt;strong&gt;Realizing this would be a win, not a loss.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;You become a pseudo-expert in machine learning&lt;/strong&gt;. Perhaps with another spoken language under your belt, or having lived abroad for the first time. You're energized, refreshed, and ready to return to a job (or join the field itself). You have stories and a portfolio of work that raise other's eyes. &lt;strong&gt;In my view, this is at once your worst-case, and best-case, scenario.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Simply put, dedicating an extended period of time to one of the hottest professional fields of the present can only bring opportunity.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being an effective machine learning practitioner requires fluency in a large body of material. Fortunately, this material exists entirely online, largely for free, &lt;strong&gt;and is yours for the taking.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To the extent that you're an effective autodidact, the OSMLM is a cost-effective, flexible, and overall highly-expedient way in which to acquire this knowledge. It is not a sabbatical, but instead a &lt;em&gt;principled and strategic&lt;/em&gt; period of focused study. Furthermore, it allows you to pursue other pressing personal goals in parallel, and brings energy, new perspective, and a point of differentiation as you return to the working world.&lt;/p&gt;
&lt;p&gt;As i was told by a VP of Research during an interview: "it's clear from your work that you learned far more in this year than you would have in a traditional master's." Personally, I didn't know how to do it any other way.&lt;/p&gt;</content><category term="life"></category></entry><entry><title>Joining ASAPP</title><link href="https://cavaunpeu.github.io/2017/09/09/joining-asapp/" rel="alternate"></link><published>2017-09-09T12:00:00-04:00</published><updated>2017-09-09T12:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-09-09:/2017/09/09/joining-asapp/</id><summary type="html">&lt;p&gt;I'm joining &lt;a href="https://www.asapp.com/"&gt;ASAPP, Inc.&lt;/a&gt; as a Machine Learning Engineer.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The open-source &lt;a href="http://willwolf.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;"masters"&lt;/a&gt; has come to a close. I'm now joining &lt;a href="https://www.asapp.com/"&gt;ASAPP, Inc.&lt;/a&gt; as a Machine Learning Engineer.&lt;/p&gt;
&lt;p&gt;ASAPP builds artificial-intelligence tools for enterprise. My work involves building backend infrastructure for machine-learning products — with a focus on NLP. We're rather stealth, so that's all I can say for now.&lt;/p&gt;
&lt;p&gt;Of course, we are hiring — for Research Scientist, Machine Learning Engineer, Data Scientist, Software Engineer, Automation Engineer, DevOps, iOS/Android Engineer, Security Engineer and Product Designer roles. If you'd like to work with a truly fantastic &lt;a href="https://www.asapp.com/team/"&gt;team&lt;/a&gt;, feel free to &lt;a href="mailto:will@asapp.com"&gt;get in touch&lt;/a&gt;. We're on the 83rd floor of the World Trade Center as well — with a view.&lt;/p&gt;
&lt;p&gt;&lt;img alt="view from world trade center" class="img-responsive" src="https://cavaunpeu.github.io/images/world_trade_center.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;In this next chapter of life, my primary professional and personal focuses will be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Keep studying.&lt;/strong&gt; Bayesian probabilistic models and deep-learning-for-NLP, mostly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Public speaking.&lt;/strong&gt; I'd like to continue speaking about machine learning and software engineering at conferences and Meetups — in the United States and abroad. (Con el segundo, me refiero principalmente a América Latina; contácteme si quiere colaborar!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weight lifting.&lt;/strong&gt; I want to get some muscles. I feel like a stronger core, back and upper body will help with programmer-neck-aches as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My time in Morocco was extremely productive and rewarding. Now back in NYC, I'm beyond excited to stay put, and keep working. If you live here as well — and enjoy mathematics, data, language, cycling or food — let's try our best to meet.&lt;/p&gt;</content><category term="life"></category></entry><entry><title>My Next Role</title><link href="https://cavaunpeu.github.io/2017/06/20/my-next-role/" rel="alternate"></link><published>2017-06-20T11:00:00-04:00</published><updated>2017-06-20T11:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-06-20:/2017/06/20/my-next-role/</id><summary type="html">&lt;p&gt;Beginning the search for an impossibly awesome next role.&lt;/p&gt;</summary><content type="html">&lt;p&gt;My nine-month open-source &lt;a href="https://cavaunpeu.github.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;"masters"&lt;/a&gt; in machine learning and statistics — a full-time, self-curated schedule of textbooks, MOOCs, publishing code to my &lt;a href="https://github.com/cavaunpeu"&gt;GitHub&lt;/a&gt; and writing to this blog — is soon ending. I'm now starting my search for an impossibly awesome "what's next."&lt;/p&gt;
&lt;h2&gt;What I bring&lt;/h2&gt;
&lt;p&gt;I bring abundant experience in: &lt;strong&gt;machine learning, Bayesian statistics and probabilistic programming, backend engineering, visualization and making complex ideas easy to understand.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As such, within your organization, I could immediately bring value to: ETL infrastructure, experimental design and execution, internal and external data products, i.e. algorithm design and deployment, and data evangelism efforts.&lt;/p&gt;
&lt;h2&gt;I'm looking for&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A highly technical data science role — equal parts prototyping mathematical models and deploying them to production.&lt;/li&gt;
&lt;li&gt;Excellent, direct technical mentorship, and the opportunity to mentor others.&lt;/li&gt;
&lt;li&gt;A medium-to-large-sized company with a well-established data science team. Alternatively, a smaller company with machine learning at the core of its business that, necessarily, has made significant investments in data science infrastructure and personnel.&lt;/li&gt;
&lt;li&gt;Large quantities of unstructured data that do not fit in RAM, ideally.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Work samples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cavaunpeu.github.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;Minimizing the Negative Log-Likelihood, in English&lt;/a&gt;: Statistical underpinnings of the machine learning models we know and love. A walk through random variables, entropy, exponential family distributions, generalized linear models, maximum likelihood estimation, cross entropy, KL-divergence, maximum a posteriori estimation and going "fully Bayesian."&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cavaunpeu.github.io/2017/06/19/neurally-embedded-emojis/"&gt;Neurally Embedded Emojis&lt;/a&gt;: Convolutional variational autoencoders for emoji generation and Siamese text-question-emoji-answer models. Keras, bidirectional LSTMs and snarky tweets &lt;a href="https://twitter.com/united"&gt;@united&lt;/a&gt; within.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cavaunpeu.github.io/2017/06/15/random-effects-neural-networks/"&gt;Random Effects Neural Networks in Edward and Keras&lt;/a&gt;: Coupling nimble probabilistic models with neural architectures in Edward and Keras: "what worked and what didn't," a conceptual overview of random effects, and directions for further exploration.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cavaunpeu.github.io/2017/07/06/further-exploring-common-probabilistic-models/"&gt;Further Exploring Common Probabilistic Models&lt;/a&gt;: Exploring generative vs. discriminative models, and sampling and variational methods for approximate inference through the lens of Bayes' theorem.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cavaunpeu/dotify"&gt;Dotify&lt;/a&gt;: A well-tested web application that recommends songs via "country arithmetic" and hand-rolled Implicit Matrix Factorization. Built with Flask, React, Webpack, PostgreSQL, Heroku and Docker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;In three years&lt;/h2&gt;
&lt;p&gt;Management, I think. I'm a strong communicator, and enjoy teaching things to anyone that will listen.&lt;/p&gt;
&lt;h2&gt;Location&lt;/h2&gt;
&lt;p&gt;NYC/SF, primarily.&lt;/p&gt;
&lt;h2&gt;About me&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I once rode a bicycle 7,614.5 kilometers from Istanbul, Turkey to Bishkek, Kyrgyzstan.&lt;/li&gt;
&lt;li&gt;I once taught a university Spanish course, in French, in Guinea-Conakry.&lt;/li&gt;
&lt;li&gt;Mathematics and code keep me smiling from ear to ear.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more, please see my full &lt;a href="https://cavaunpeu.github.io/pages/about.md"&gt;bio&lt;/a&gt; and &lt;a href="https://cavaunpeu.github.io/pages/cv.md"&gt;résumé&lt;/a&gt;. In addition, social links can be found below.&lt;/p&gt;</content><category term="life"></category></entry><entry><title>Neurally Embedded Emojis</title><link href="https://cavaunpeu.github.io/2017/06/19/neurally-embedded-emojis/" rel="alternate"></link><published>2017-06-19T13:00:00-04:00</published><updated>2017-06-19T13:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-06-19:/2017/06/19/neurally-embedded-emojis/</id><summary type="html">&lt;p&gt;Convolutional variational autoencoders for emoji generation and Siamese text-question-emoji-answer models. Keras, bidirectional LSTMs and snarky tweets &lt;a href="https://twitter.com/united"&gt;@united&lt;/a&gt; within.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As I move through my 20's I'm consistently delighted by the subtle ways in which I've changed.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Will at 22: Reggaeton is a miserable, criminal assault to my ears.&lt;/p&gt;
&lt;p&gt;Will at 28: &lt;a href="https://www.youtube.com/watch?v=72UO0v5ESUo"&gt;Despacito (Remix)&lt;/a&gt; for breakfast, lunch, dinner.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Will at 22: Western Europe is boring. No — I've seen a lot of it! Everything is too clean, too nice, too perfect for my taste.&lt;/p&gt;
&lt;p&gt;Will at 28, in Barcelona, after 9 months in &lt;a href="https://cavaunpeu.github.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/"&gt;Casablanca&lt;/a&gt;: Wait a second: &lt;em&gt;I get it now&lt;/em&gt;. What &lt;em&gt;is&lt;/em&gt; this summertime paradise of crosswalks, vehicle civility and apple-green parks and where has it been all my life?&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Will at 22: Emojis are weird.&lt;/p&gt;
&lt;p&gt;Will at 28: 🚀 🤘 💃🏿 🚴🏻 🙃.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Emojis are an increasingly-pervasive sub-lingua-franca of the internet. They capture meaning in a rich, concise manner — alternative to the 13 seconds of mobile thumb-fumbling required to capture the same meaning with text. Furthermore, they bring two levels of semantic information: their context within raw text and the pixels of the emoji itself.&lt;/p&gt;
&lt;h2&gt;Question-answer models&lt;/h2&gt;
&lt;p&gt;The original aim of this post was to explore Siamese question-answer models of the type typically applied to the &lt;a href="https://github.com/shuzi/insuranceQA"&gt;InsuranceQA Corpus&lt;/a&gt; as introduced in "Applying Deep Learning To Answer Selection: A Study And An Open Task" (&lt;a href="https://arxiv.org/pdf/1508.01585v2.pdf"&gt;Feng, Xiang, Glass, Wang, &amp;amp; Zhou, 2015&lt;/a&gt;). We'll call them SQAM for clarity. The basic architecture looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="qa model architecture" class="img-responsive" src="https://cavaunpeu.github.io/figures/qa_model_architecture.png"/&gt;&lt;/p&gt;
&lt;p&gt;By layer and in general terms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An input — typically a sequence of token ids — for both question (Q) and answer (A).&lt;/li&gt;
&lt;li&gt;An embedding layer.&lt;/li&gt;
&lt;li&gt;Convolutional layer(s), or any layers that extract features from the matrix of embeddings. (A matrix, because the respective inputs are sequences of token ids; each id is embedded into its own vector.)&lt;/li&gt;
&lt;li&gt;A max-pooling layer.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;tanh&lt;/code&gt; non-linearity.&lt;/li&gt;
&lt;li&gt;The cosine of the angle between the resulting, respective embeddings.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;As canonical recommendation&lt;/h3&gt;
&lt;p&gt;Question answering can be viewed as canonical recommendation: embed entities into Euclidean space in a meaningful way, then compute dot products between these entities and sort the list. In this vein, the above network is (thus far) quite similar to classic matrix factorization yet with the following subtle tweaks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Instead of factorizing our matrix via &lt;a href="https://en.wikipedia.org/wiki/Singular_value_decomposition"&gt;SVD&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Ordinary_least_squares"&gt;OLS&lt;/a&gt; we build a neural network that accepts &lt;code&gt;(question, answer)&lt;/code&gt;, i.e. &lt;code&gt;(user, item)&lt;/code&gt;, pairs and outputs their similarity. The second-to-last layer gives the respective embeddings. We train this network in a supervised fashion, optimizing its parameters via stochastic gradient descent.&lt;/li&gt;
&lt;li&gt;Instead of jumping directly from input-index (or sequence thereof) to embedding, we first compute convolutional features.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In contrast, the network above boasts one key difference: both question and answer, i.e. user and item, are transformed via a single set of parameters — an initial embedding layer, then convolutional layers — en route to their final embedding.&lt;/p&gt;
&lt;p&gt;Furthermore, and not unique to SQAMs, our network inputs can be &lt;em&gt;any&lt;/em&gt; two sequences of (tokenized, max-padded, etc.) text: we are not restricted to only those observed in the training set.&lt;/p&gt;
&lt;h2&gt;Question-emoji models&lt;/h2&gt;
&lt;p&gt;Given my accelerating proclivity for the internet's new alphabet, I decided to build text-question-&lt;em&gt;emoji&lt;/em&gt;-answer models instead. In fact, this setup gives an additional avenue for prediction: if we make a model of the answers (emojis) themselves, we can now predict on, i.e. compute similarity with, each of&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Emojis we saw in the training set.&lt;/li&gt;
&lt;li&gt;New emojis, i.e. either not in the training set or new (like, released months from now) altogether.&lt;/li&gt;
&lt;li&gt;Novel emojis &lt;em&gt;generated&lt;/em&gt; from the model of our data. In this way, we could conceivably answer a question with: "we suggest this new emoji we've algorithmically created ourselves that no one's ever seen before."&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;
&lt;h2&gt;Convolutional variational autoencoders&lt;/h2&gt;
&lt;p&gt;Variational autoencoders are comprised of two models: an encoder and a decoder. The encoder embeds our 872 &lt;a href="https://github.com/twitter/twemoji"&gt;emojis&lt;/a&gt; of size &lt;span class="math"&gt;\((36, 36, 4)\)&lt;/span&gt; into a low-dimensional latent code, &lt;span class="math"&gt;\(z_e \in \mathbb{R}^{16}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(z_e\)&lt;/span&gt; is a sample from an emoji-specific Gaussian. The decoder takes as input &lt;span class="math"&gt;\(z_e\)&lt;/span&gt; and produces a reconstruction of the original emoji. As each individual &lt;span class="math"&gt;\(z_e\)&lt;/span&gt; is normally distributed, &lt;span class="math"&gt;\(z\)&lt;/span&gt; should be distributed normally as well. We can verify this with a quick simulation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;z_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z_samples&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;samples&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;z_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="aggregate gaussian" class="img-responsive" src="https://cavaunpeu.github.io/figures/aggregate_gaussian.png"/&gt;&lt;/p&gt;
&lt;p&gt;Training a variational autoencoder to learn low-dimensional emoji embeddings serves two principal ends:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can feed these low-dimensional embeddings as input to our SQAM.&lt;/li&gt;
&lt;li&gt;We can generate novel emojis with which to answer questions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the embeddings in #1 are multivariate Gaussian, we can perform #2 by passing Gaussian samples into our decoder. We can do this by sampling evenly-spaced percentiles from the inverse CDF of the aggregate embedding distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;percentiles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;percentiles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ppf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;generated_emoji&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;NB: &lt;code&gt;norm.ppf&lt;/code&gt; does &lt;em&gt;not&lt;/em&gt; accept a &lt;code&gt;size&lt;/code&gt; parameter; I believe sampling from the inverse CDF of a &lt;em&gt;multivariate&lt;/em&gt; Gaussian is non-trivial in Python.&lt;/p&gt;
&lt;p&gt;Similarly, we could simply iterate over &lt;code&gt;(mu, sd)&lt;/code&gt; pairs outright:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rvs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;generated_emoji&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The ability to generate new emojis via samples from a well-studied distribution, the Gaussian, is a key reason for choosing a variational autoencoder.&lt;/p&gt;
&lt;p&gt;Finally, as we are working with images, I employ convolutional intermediary layers.&lt;/p&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EMOJIS_DIR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'data/emojis'&lt;/span&gt;
&lt;span class="n"&gt;N_CHANNELS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_CHANNELS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;emojis_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;slug&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;listdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJIS_DIR&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJIS_DIR&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;emoji&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;emoji&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emoji&lt;/span&gt;

&lt;span class="n"&gt;emojis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Split data into train, validation sets&lt;/h3&gt;
&lt;p&gt;Additionally, scale pixel values to &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;255.&lt;/span&gt;
&lt;span class="n"&gt;X_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;255.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    X_train:  (685, 36, 36, 4)&lt;/span&gt;
&lt;span class="err"&gt;    X_val:    (182, 36, 36, 4)&lt;/span&gt;
&lt;span class="err"&gt;    y_train:  (685, 36, 36, 4)&lt;/span&gt;
&lt;span class="err"&gt;    y_val:    (182, 36, 36, 4)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before we begin, let's examine some emojis.&lt;/p&gt;
&lt;p&gt;&lt;img alt="emojis" class="img-responsive" src="https://cavaunpeu.github.io/images/emojis.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Model emojis&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;
&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Variational layer&lt;/h3&gt;
&lt;p&gt;This is taken from a previous post of mine, &lt;a href="https://cavaunpeu.github.io/2017/05/08/transfer-learning-flight-delay-prediction/"&gt;Transfer Learning for Flight Delay Prediction via Variational Autoencoders&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;KerasLayer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''A custom "variational" Keras layer that completes the&lt;/span&gt;
&lt;span class="sd"&gt;        variational autoencoder.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            embedding_dim : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_mean_weights'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_mean_bias'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_log_var_weights'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'z_log_var_bias'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt;
        &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;binary_crossentropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;base_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_output_shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Autoencoder&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# encoder&lt;/span&gt;
&lt;span class="n"&gt;original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'original'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;variational_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;variational_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;variational_params&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'encoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# decoder&lt;/span&gt;
&lt;span class="n"&gt;encoded&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;

&lt;span class="n"&gt;upsample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;encoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;reshape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMOJI_SHAPE&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;upsample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;deconv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;encoded&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deconv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;deconv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deconv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FILTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;deconv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;deconv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;reconstructed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2DTranspose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_CHANNELS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'same'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;encoded&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reconstructed&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'decoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# end-to-end&lt;/span&gt;
&lt;span class="n"&gt;encoder_decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;original&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The full model &lt;code&gt;encoder_decoder&lt;/code&gt; is composed of separate models &lt;code&gt;encoder&lt;/code&gt; and &lt;code&gt;decoder&lt;/code&gt;. Training the former will implicitly train the latter two; they are available for our use thereafter.&lt;/p&gt;
&lt;p&gt;The above architecture takes inspiration from &lt;a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py"&gt;Keras&lt;/a&gt;, &lt;a href="https://github.com/blei-lab/edward/blob/master/examples/vae_convolutional.py"&gt;Edward&lt;/a&gt; and the GDGS (gradient descent by grad student) method by as discussed by &lt;a href="https://www.reddit.com/r/MachineLearning/comments/6hso7g/d_how_do_people_come_up_with_all_these_crazy_deep/"&gt;Brudaks on Reddit&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A popular method for designing deep learning architectures is GDGS (gradient descent by grad student).
This is an iterative approach, where you start with a straightforward baseline architecture (or possibly an earlier SOTA), measure its effectiveness; apply various modifications (e.g. add a highway connection here or there), see what works and what does not (i.e. where the gradient is pointing) and iterate further on from there in that direction until you reach a (local?) optimum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm not a grad student but I think it still plays.&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;encoder_decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;003&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;encoder_decoder_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder_decoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Generate emojis&lt;/h2&gt;
&lt;p&gt;As promised we'll generate emojis. Again, latent codes are distributed as a (16-dimensional) Gaussian; to generate, we'll simply take samples thereof and feed them to our &lt;code&gt;decoder&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While scanning a 16-dimensional hypercube, i.e. taking (evenly-spaced, usually) samples from our latent space, is a few lines of Numpy, visualizing a 16-dimensional grid is impractical. In solution, we'll work on a 2-dimensional grid while treating subsets of our latent space as homogenous.&lt;/p&gt;
&lt;p&gt;For example, if our 2-D sample were &lt;code&gt;(0, 1)&lt;/code&gt;, we could posit 16-D samples as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;A. `(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1)`&lt;/span&gt;
&lt;span class="err"&gt;B. `(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1)`&lt;/span&gt;
&lt;span class="err"&gt;C. `(0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1)`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, if another sample were &lt;code&gt;(2, 3.5)&lt;/code&gt;, we could posit 16-D samples as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;A. `(2, 2, 2, 2, 2, 2, 2, 2, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5)`&lt;/span&gt;
&lt;span class="err"&gt;B. `(2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5, 2, 3.5)`&lt;/span&gt;
&lt;span class="err"&gt;C. `(2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5, 2, 2, 3.5, 3.5)`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is no math here: I'm just creating 16-element lists in different ways. We'll then plot "A-lists," "B-lists," etc. separately.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compose_code_A&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compose_code_B&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compose_code_C&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coord_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


&lt;span class="n"&gt;ticks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ticks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_generated_emojis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compose_code_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="generated emojis A" class="img-responsive" src="https://cavaunpeu.github.io/figures/generated_emojis_A.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_generated_emojis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compose_code_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="generated emojis B" class="img-responsive" src="https://cavaunpeu.github.io/figures/generated_emojis_B.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_generated_emojis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compose_code_C&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="generated emojis C" class="img-responsive" src="https://cavaunpeu.github.io/figures/generated_emojis_C.png"/&gt;&lt;/p&gt;
&lt;p&gt;As our emojis live in a continuous latent space we can observe the smoothness of the transition from one to the next.&lt;/p&gt;
&lt;p&gt;The generated emojis have the makings of maybe some devils, maybe some bubbles, maybe some hearts, maybe some fish. I doubt they'll be featured on your cell phone's keyboard anytime soon.&lt;/p&gt;
&lt;h2&gt;Text-question, emoji-answer&lt;/h2&gt;
&lt;p&gt;I spent a while looking for an adequate dataset to no avail. (Most Twitter datasets are not open-source, I requested my own tweets days ago and continue to wait, etc.) As such, I'm working with the &lt;a href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment"&gt;Twitter US Airline Sentiment&lt;/a&gt; dataset: tweets are labeled as &lt;code&gt;positive&lt;/code&gt;, &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;negative&lt;/code&gt; which I've mapped to 🎉, 😈 and 😡.&lt;/p&gt;
&lt;h3&gt;Contrastive loss&lt;/h3&gt;
&lt;p&gt;We've thus far discussed the SQAM. Our final model will make use of two SQAM's in parallel, as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Receive &lt;code&gt;(question, correct_answer, incorrect_answer)&lt;/code&gt; triplets as input.&lt;/li&gt;
&lt;li&gt;Compute the cosine similarity between &lt;code&gt;question&lt;/code&gt;, &lt;code&gt;correct_answer&lt;/code&gt; via &lt;code&gt;SQAM_1&lt;/code&gt; — &lt;code&gt;correct_cos_sim&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Compute the cosine similarity between &lt;code&gt;question&lt;/code&gt;, &lt;code&gt;incorrect_answer&lt;/code&gt; via &lt;code&gt;SQAM_2&lt;/code&gt; — &lt;code&gt;incorrect_cos_sim&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model is trained to minimize the following: &lt;code&gt;max(0, margin - correct_cos_sim + incorrect_cos_sim)&lt;/code&gt;, a variant of the &lt;a href="https://en.wikipedia.org/wiki/Hinge_loss"&gt;hinge loss&lt;/a&gt;. This ensures that &lt;code&gt;(question, correct_answer)&lt;/code&gt; pairs have a higher cosine similarity than &lt;code&gt;(question, incorrect_answer)&lt;/code&gt; pairs, mediated by &lt;code&gt;margin&lt;/code&gt;. Note that this function is differentiable: it is simply a &lt;a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"&gt;ReLU&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Architecture&lt;/h3&gt;
&lt;p&gt;A single SQAM receives two inputs: a &lt;code&gt;question&lt;/code&gt; — a max-padded sequence of token ids — and an &lt;code&gt;answer&lt;/code&gt; — an emoji's 16-D latent code.&lt;/p&gt;
&lt;p&gt;To process the &lt;code&gt;question&lt;/code&gt; we employ the following steps, i.e. network layers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;pre-trained-with-Glove&lt;/a&gt; 100-D embedding for each token id. This gives a matrix of size &lt;code&gt;(MAX_QUESTION_LEN, GLOVE_EMBEDDING_DIM)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pass the result through a bidirectional LSTM — (apparently) key to current &lt;a href="https://explosion.ai/blog/deep-learning-formula-nlp"&gt;state&lt;/a&gt;-of-the-&lt;a href="https://www.youtube.com/watch?v=nFCxTtBqF5U"&gt;art&lt;/a&gt; results in a variety of NLP tasks. This can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize two matrices of size &lt;code&gt;(MAX_QUESTION_LEN, LSTM_HIDDEN_STATE_SIZE)&lt;/code&gt;: &lt;code&gt;forward_matrix&lt;/code&gt; and &lt;code&gt;backward_matrix&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Pass the sequence of token ids through an LSTM and return all hidden states. The first hidden state is a function of, i.e. is computed using, the first token id's embedding; place it in the first row of &lt;code&gt;forward_matrix&lt;/code&gt;. The second hidden state is a function of the first and second token-id embeddings; place it in the second row of &lt;code&gt;forward_matrix&lt;/code&gt;. The third hidden state is a function of the first and second and third token-id embeddings, and so forth.&lt;/li&gt;
&lt;li&gt;Do the same thing but pass the sequence to the LSTM in reverse order. Place the first hidden state in the &lt;em&gt;last&lt;/em&gt; row of &lt;code&gt;backward_matrix&lt;/code&gt;, the second hidden state in the &lt;em&gt;second-to-last&lt;/em&gt; row of &lt;code&gt;backward_matrix&lt;/code&gt;, etc.&lt;/li&gt;
&lt;li&gt;Concatenate &lt;code&gt;forward_matrix&lt;/code&gt; and &lt;code&gt;backward_matrix&lt;/code&gt; into a single matrix of size &lt;code&gt;(MAX_QUESTION_LEN, 2 * LSTM_HIDDEN_STATE_SIZE)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://keras.io/layers/pooling/#maxpooling1d"&gt;Max-pool&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Flatten.&lt;/li&gt;
&lt;li&gt;Dense layer with ReLU activations, down to 10 dimensions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To process the &lt;code&gt;answer&lt;/code&gt; we employ the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Dense layer with ReLU activations.&lt;/li&gt;
&lt;li&gt;Dense layer with ReLU activations, down to 10 dimensions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now of equal size, we further process our &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; with a &lt;em&gt;single&lt;/em&gt; set of dense layers — the key difference between a SQAM and (the neural-network formulation of) other canonical &lt;code&gt;(user, item)&lt;/code&gt; recommendation algorithms. The last of these layers employs &lt;code&gt;tanh&lt;/code&gt; activations as suggested in Feng et al. (2015).&lt;/p&gt;
&lt;p&gt;Finally, we compute the cosine similarity between the resulting embeddings.&lt;/p&gt;
&lt;h2&gt;Prepare questions, answers&lt;/h2&gt;
&lt;h3&gt;Import tweets&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tweets_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data/tweets.csv'&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="s1"&gt;'text'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'airline_sentiment'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tweets_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;index&lt;/th&gt;
&lt;th&gt;text&lt;/th&gt;
&lt;th&gt;airline_sentiment&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;2076&lt;/td&gt;
&lt;td&gt;@united that's not an apology. Say it.&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;7534&lt;/td&gt;
&lt;td&gt;@JetBlue letting me down in San Fran. No Media...&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;14441&lt;/td&gt;
&lt;td&gt;@AmericanAir where do I look for cabin crew va...&lt;/td&gt;
&lt;td&gt;neutral&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;13130&lt;/td&gt;
&lt;td&gt;@AmericanAir just sad that even after spending...&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;3764&lt;/td&gt;
&lt;td&gt;@united What's up with the reduction in E+ on ...&lt;/td&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3&gt;Embed answers into 16-D latent space&lt;/h3&gt;
&lt;p&gt;Additionally, scale the latent codes; these will be fed to our network as input.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# embed&lt;/span&gt;
&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'1f389.png'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'1f608.png'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;emojis_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'1f621.png'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# scale&lt;/span&gt;
&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build vectors of correct, incorrect answers&lt;/span&gt;
&lt;span class="n"&gt;embedding_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'positive'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'neutral'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;'negative'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;sentiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;embedding_map&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tweets_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'airline_sentiment'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;embedding_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;incorrect_sentiment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiments&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;embedding_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;incorrect_sentiment&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;questions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweets_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'text'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;correct_answers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We've now built (only) one &lt;code&gt;(question, correct_answer, incorrect_answer)&lt;/code&gt; training triplet for each ground-truth &lt;code&gt;(question, correct_answer)&lt;/code&gt;. In practice, we should likely have many more, i.e. &lt;code&gt;(question, correct_answer, incorrect_answer_1), (question, correct_answer, incorrect_answer_2), ..., (question, correct_answer, incorrect_answer_n)&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Construct sequences of token ids&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;VOCAB_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2000&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCAB_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;questions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;question_seqs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;questions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;question_seqs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Split data into train, validation sets&lt;/h3&gt;
&lt;p&gt;NB: We don't actually have &lt;code&gt;y&lt;/code&gt; values: we pass &lt;code&gt;(question, correct_answer, incorrect_answer)&lt;/code&gt; triplets to our network and try to minimize &lt;code&gt;max(0, margin - correct_cos_sim + incorrect_cos_sim)&lt;/code&gt;. Notwithstanding, Keras requires that we pass both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; (as Numpy arrays); we pass the latter as a vector of 0's.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;

&lt;span class="n"&gt;questions_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;correct_answers_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answers_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;questions_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;question_seqs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;correct_answers_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correct_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answers_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;y_train_dummy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;questions_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y_val_dummy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;questions_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    questions_train:         (4079, 20)&lt;/span&gt;
&lt;span class="err"&gt;    correct_answers_train:   (4079, 16)&lt;/span&gt;
&lt;span class="err"&gt;    incorrect_answers_train: (4079, 16)&lt;/span&gt;
&lt;span class="err"&gt;    questions_val:           (921, 20)&lt;/span&gt;
&lt;span class="err"&gt;    correct_answers_val:     (921, 16)&lt;/span&gt;
&lt;span class="err"&gt;    incorrect_answers_val:   (921, 16)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Build embedding layer from Glove vectors&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;GLOVE_EMBEDDING_DIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="c1"&gt;# 1. Load Glove embeddings&lt;/span&gt;

&lt;span class="c1"&gt;# 2. Build embeddings matrix&lt;/span&gt;

&lt;span class="c1"&gt;# 3. Build Keras embedding layer&lt;/span&gt;
&lt;span class="n"&gt;embedding_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GLOVE_EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Build Siamese question-answer model (SQAM)&lt;/h3&gt;
&lt;p&gt;GDGS architecture, ✌️.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;LSTM_HIDDEN_STATE_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

&lt;span class="c1"&gt;# question&lt;/span&gt;
&lt;span class="n"&gt;question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int32'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;question_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;biLSTM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Bidirectional&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LSTM_HIDDEN_STATE_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_sequences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;question_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;max_pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MaxPool1D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;biLSTM&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# answer&lt;/span&gt;
&lt;span class="n"&gt;answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# combine&lt;/span&gt;
&lt;span class="n"&gt;shared_dense_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;shared_dense_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;shared_dense_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dense_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shared_dense_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# compute cosine sim, a normalized dot product&lt;/span&gt;
&lt;span class="n"&gt;cosine_sim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;dense_question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;qa_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cosine_sim&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'qa_model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="qa model" class="img-responsive" src="https://cavaunpeu.github.io/figures/qa_model.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Build contrastive model&lt;/h3&gt;
&lt;p&gt;Two Siamese networks, trained jointly so as to minimize the hinge loss of their respective outputs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# contrastive model&lt;/span&gt;
&lt;span class="n"&gt;correct_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;incorrect_answer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_SIZE&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;correct_cos_sim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qa_model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answer&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;incorrect_cos_sim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qa_model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answer&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;hinge_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cos_sims&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;margin&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;correct&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos_sims&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;margin&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;incorrect&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;contrastive_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Lambda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hinge_loss&lt;/span&gt;&lt;span class="p"&gt;)([&lt;/span&gt;&lt;span class="n"&gt;correct_cos_sim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_cos_sim&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;contrastive_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;contrastive_loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'contrastive_model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Build prediction model&lt;/h3&gt;
&lt;p&gt;This is what we'll use to compute the cosine similarity of novel &lt;code&gt;(question, answer)&lt;/code&gt; pairs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;prediction_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;qa_model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'prediction_model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Fit contrastive model&lt;/h3&gt;
&lt;p&gt;Fitting &lt;code&gt;contrastive_model&lt;/code&gt; will implicitly fit &lt;code&gt;prediction_model&lt;/code&gt; as well, so long as the latter has been compiled.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# compile&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clipnorm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;contrastive_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;prediction_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# fit&lt;/span&gt;
&lt;span class="n"&gt;contrastive_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;questions_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answers_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers_train&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_train_dummy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;questions_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct_answers_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;incorrect_answers_val&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_val_dummy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Train on 4089 samples, validate on 911 samples&lt;/span&gt;
&lt;span class="err"&gt;Epoch 1/3&lt;/span&gt;
&lt;span class="err"&gt;4089/4089 [==============================] - 18s - loss: 0.1069 - val_loss: 0.0929&lt;/span&gt;
&lt;span class="err"&gt;Epoch 2/3&lt;/span&gt;
&lt;span class="err"&gt;4089/4089 [==============================] - 14s - loss: 0.0796 - val_loss: 0.0822&lt;/span&gt;
&lt;span class="err"&gt;Epoch 3/3&lt;/span&gt;
&lt;span class="err"&gt;4089/4089 [==============================] - 14s - loss: 0.0675 - val_loss: 0.0828&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Predict on new tweets&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous"&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;"@usairways I've called for 3 days and can't get thru. is there some secret method i can use that doesn't result in you hanging up on me?"&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;"@AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!"&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;new_questions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"@usairways I've called for 3 days and can't get thru. is there some secret method i can use that doesn't result in you hanging up on me?"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"@AmericanAir Let's all have a extraordinary week and make it a year to remember #GoingForGreat 2015 thanks so much American Airlines!!!"&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;new_questions_seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;new_questions_seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_QUESTION_LEN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_sentiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeats&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentiment_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prediction_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #1&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predictions:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    🎉 (Positive): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😈 (Neutral) : &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😡 (Negative): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;negative_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;Predictions:&lt;/span&gt;
&lt;span class="c"&gt;    🎉 (Positive): 0.51141&lt;/span&gt;
&lt;span class="c"&gt;    😈 (Neutral) : 0.56273&lt;/span&gt;
&lt;span class="c"&gt;    😡 (Negative): 0.9728&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #2&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predictions:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    🎉 (Positive): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😈 (Neutral) : &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😡 (Negative): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;negative_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;Predictions:&lt;/span&gt;
&lt;span class="c"&gt;    🎉 (Positive): 0.41422&lt;/span&gt;
&lt;span class="c"&gt;    😈 (Neutral) : 0.61587&lt;/span&gt;
&lt;span class="c"&gt;    😡 (Negative): 0.99161&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #3&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predictions:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    🎉 (Positive): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;positive_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😈 (Neutral) : &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;neutral_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'    😡 (Negative): &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;negative_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;0.5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;Predictions:&lt;/span&gt;
&lt;span class="c"&gt;    🎉 (Positive): 0.87107&lt;/span&gt;
&lt;span class="c"&gt;    😈 (Neutral) : 0.46741&lt;/span&gt;
&lt;span class="c"&gt;    😡 (Negative): 0.73435&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Additionally, we can predict on the full set of emojis&lt;/h3&gt;
&lt;p&gt;Some emoji embeddings contain &lt;code&gt;np.inf&lt;/code&gt; values, unfortunately. We could likely mitigate this by further tweaking the hyperparameters of our autoencoder.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emojis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inf_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isinf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;inf_mask&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.3&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;% of values are `np.inf`.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;all_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;inf_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;4.15% of values are `np.inf`.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_sentiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_questions_seq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;repeats&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_questions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prediction_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Tweet #1&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;preds_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;top_5_matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_top_5_argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds_1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;display_top_5_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_5_matches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="predicted tweets 1" class="img-responsive" src="https://cavaunpeu.github.io/images/predicted_tweets_1.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Tweet #2&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;preds_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;top_5_matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_top_5_argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;display_top_5_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_5_matches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="predicted tweets 2" class="img-responsive" src="https://cavaunpeu.github.io/images/predicted_tweets_2.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Tweet #3&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;preds_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_sentiments&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;top_5_matches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_top_5_argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds_3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;display_top_5_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_5_matches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="predicted tweets 3" class="img-responsive" src="https://cavaunpeu.github.io/images/predicted_tweets_3.png"/&gt;&lt;/p&gt;
&lt;p&gt;Not particularly useful. These emojis have 0 notion of sentiment, though: the model is simply predicting on their (pixel-based) latent codes.&lt;/p&gt;
&lt;h2&gt;Future work&lt;/h2&gt;
&lt;p&gt;In this work, we trained a convolutional variational autoencoder to model the distribution of emojis. Next, we trained a Siamese question-answer model to answer text questions with emoji answers. Finally, we were able to use the latter to predict on novel emojis from the former.&lt;/p&gt;
&lt;p&gt;Moving forward, I see a few logical steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use emoji embeddings that are conscious of sentiment — likely trained via a different network altogether. This way, we could make more meaningful (sentiment-based) predictions on novel emojis.&lt;/li&gt;
&lt;li&gt;Predict on emojis generated from the autoencoder.&lt;/li&gt;
&lt;li&gt;Add 1-D convolutions to the text side of the SQAM.&lt;/li&gt;
&lt;li&gt;Add an &lt;a href="https://www.quora.com/What-is-attention-in-the-context-of-deep-learning"&gt;"attention"&lt;/a&gt; mechanism — the one component missing from the &lt;a href="https://explosion.ai/blog/deep-learning-formula-nlp"&gt;"embed, encode, attend, predict"&lt;/a&gt; dynamic quartet of modern NLP.&lt;/li&gt;
&lt;li&gt;Improve the stability of our autoencoder so as to not produce embeddings containing &lt;code&gt;np.inf&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sincere thanks for reading, and emojis 🤘.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/neurally-embedded-emojis"&gt;repository&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/neurally-embedded-emojis/blob/master/neurally-embedded-emojis.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ben.bolte.cc/blog/2016/language.html"&gt;Deep Language Modeling for Question Answering using Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1508.01585v2.pdf"&gt;Applying Deep Learning To Answer Selection: A Study And An Open Task&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1511.04108.pdf"&gt;LSTM Based Deep Learning Models For Non-Factoid Answer Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://explosion.ai/blog/deep-learning-formula-nlp"&gt;Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py"&gt;Keras Examples - Convolutional Variational Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html"&gt;Introducing Variational Autoencoders (in Prose and Code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html"&gt;Under the Hood of the Variational Autoencoder (in Prose and Code)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Random Effects Neural Networks in Edward and Keras</title><link href="https://cavaunpeu.github.io/2017/06/15/random-effects-neural-networks/" rel="alternate"></link><published>2017-06-15T18:00:00-04:00</published><updated>2017-06-15T18:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-06-15:/2017/06/15/random-effects-neural-networks/</id><summary type="html">&lt;p&gt;Coupling nimble probabilistic models with neural architectures in Edward and Keras: "what worked and what didn't," a conceptual overview of random effects, and directions for further exploration.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Bayesian probabilistic models provide a nimble and expressive framework for modeling "small-world" data. In contrast, deep learning offers a more rigid yet much more powerful framework for modeling data of massive size. &lt;a href="http://edwardlib.org/"&gt;Edward&lt;/a&gt; is a probabilistic programming library that bridges this gap: "black-box" variational inference enables us to fit extremely flexible Bayesian models to large-scale data. Furthermore, these models themselves may take advantage of classic deep-learning architectures of arbitrary complexity.&lt;/p&gt;
&lt;p&gt;Edward uses &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; for symbolic gradients and data flow graphs. As such, it interfaces cleanly with other libraries that do the same, namely &lt;a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html"&gt;TF-Slim&lt;/a&gt;, &lt;a href="https://github.com/google/prettytensor"&gt;PrettyTensor&lt;/a&gt; and &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt;. Personally, I've been working often with the latter, and am consistently delighted by the ease with which it allows me to specify complex neural architectures.&lt;/p&gt;
&lt;p&gt;The aim of this post is to lay a practical foundation for Bayesian modeling in Edward, then explore how, and how easily, we can extend these models in the direction of classical deep learning via Keras. It will give both a conceptual overview of the models below, as well as notes on the practical considerations of their implementation —  what worked and what didn't. Finally, this post will conclude with concrete ways in which to extend these models further, of which there are many.&lt;/p&gt;
&lt;p&gt;If you're just getting started with Edward or Keras, I recommend first perusing the &lt;a href="http://edwardlib.org/tutorials"&gt;Edward tutorials&lt;/a&gt; and &lt;a href="https://keras.io/"&gt;Keras documentation&lt;/a&gt; respectively.&lt;/p&gt;
&lt;p&gt;To "pull us down the path," we build three models in additive fashion: a Bayesian linear regression model, a Bayesian linear regression model with random effects, and a neural network with random effects. We fit them on the &lt;a href="https://www.kaggle.com/c/zillow-prize-1"&gt;Zillow Prize&lt;/a&gt; dataset, which asks us to predict &lt;code&gt;logerror&lt;/code&gt; (in house-price estimate, i.e. the "Zestimate") given metadata for a list of homes. These models are intended to be demonstrative, not performant: they will not win you the prize in their current form.&lt;/p&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;h3&gt;Build training DataFrame&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transactions_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;properties_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'left'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left_on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'id_parcel'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right_on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'id_parcel'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Drop columns containing too many nulls&lt;/h3&gt;
&lt;p&gt;Bayesian probabilistic models allow us to flexibly model &lt;em&gt;missing&lt;/em&gt; data itself. To this end, we conceive of a given predictor as a vector of both:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Observed values.&lt;/li&gt;
&lt;li&gt;Parameters in place of missing values, which will form a posterior distribution for what this value might have been.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In a (partially-specified, for brevity) linear model, this might look as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
y_i \sim \mathcal{N}(\mu_i, \sigma)\\
\mu_i = \alpha + \beta_N N_i\\
N_i \sim \mathcal{N}(\nu, \sigma_N)\\
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; is our sometimes-missing predictor. When &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; is observed, &lt;span class="math"&gt;\(\mathcal{N}(\nu, \sigma_N)\)&lt;/span&gt; serves as a likelihood: given this data-point, we tweak retrodictive distributions on the parameters &lt;span class="math"&gt;\((\nu, \sigma_N)\)&lt;/span&gt; by which it was produced. Conversely, when &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; is missing it serves as a prior: after learning distributions of &lt;span class="math"&gt;\((\nu, \sigma_N)\)&lt;/span&gt; we can generate a likely value of &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; itself. Finally, inference will give us (presumably-wide) distributions on the model's belief in what was the true value of each missing &lt;span class="math"&gt;\(N_i\)&lt;/span&gt; conditional on the data observed.&lt;/p&gt;
&lt;p&gt;I tried this in Edward, albeit briefly, to no avail. Dustin Tran gives an &lt;a href="https://discourse.edwardlib.org/t/how-to-handle-missing-values-in-gaussian-matrix-factorization/95/2"&gt;example&lt;/a&gt; of how one might accomplish this task in the case of Gaussian Matrix Factorization. In my case, I wasn't able to apply a 2-D missing-data-mask placeholder to a 2-D data placeholder via &lt;a href="https://www.tensorflow.org/api_docs/python/tf/gather"&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt; nor &lt;a href="https://www.tensorflow.org/api_docs/python/tf/gather_nd"&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;. With more effort, I'm sure I could make this work. Help appreciated.&lt;/p&gt;
&lt;p&gt;For now, we'll first drop columns containing too many null values, then, after choosing a few of the predictors most correlated with the target, drop the remaining rows containing nulls.&lt;/p&gt;
&lt;h3&gt;Select three fixed-effect predictors&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;fixed_effect_predictors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;'area_live_finished'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'num_bathroom'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'build_year'&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Select one random-effect predictor&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;zip_codes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'region_zip'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'category'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Split data into train, validation sets&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    X_train:  (36986, 3)&lt;/span&gt;
&lt;span class="err"&gt;    X_val:    (36986, 3)&lt;/span&gt;
&lt;span class="err"&gt;    y_train:  (36986,)&lt;/span&gt;
&lt;span class="err"&gt;    y_val:    (36986,)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;Bayesian linear regression&lt;/h1&gt;
&lt;p&gt;Using three fixed-effect predictors we'll fit a model of the following form:&lt;/p&gt;
&lt;div class="math"&gt;$$
y_i \sim \mathcal{N}(\mu_i, 1)\\
\mu_i = \alpha + \beta x_i\\
\alpha \sim \mathcal{N}(0, 1)\\
\beta \sim \mathcal{N}(0, 1)\\
$$&lt;/div&gt;
&lt;p&gt;Having normalized our data to have mean 0 and unit-variance, we place our priors on a similar scale.&lt;/p&gt;
&lt;p&gt;To infer posterior distributions of the model's parameters conditional on the data observed we employ variational inference — one of three inference classes supported in Edward. This approach posits posterior inference as posterior &lt;em&gt;approximation&lt;/em&gt; via &lt;em&gt;optimization&lt;/em&gt;, where optimization is done via stochastic, gradient-based methods. This is what enables us to scale complex probabilistic functional forms to large-scale data.&lt;/p&gt;
&lt;p&gt;For an introduction to variational inference and Edward's API thereof, please reference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/inference"&gt;Edward: Inference of Probabilistic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/variational-inference"&gt;Edward: Variational Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/klqp"&gt;Edward: KL(q||p) Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/api/inference"&gt;Edward: API and Documentation - Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, I provide an introduction to the basic math behind variational inference and the &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;ELBO&lt;/a&gt; in the previous post on this blog: &lt;a href="https://cavaunpeu.github.io/2017/07/06/further-exploring-common-probabilistic-models/"&gt;Further Exploring Common Probabilistic Models&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;p&gt;For the approximate q-distributions, we apply the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/nn/softplus"&gt;softplus function&lt;/a&gt; — &lt;code&gt;log(exp(z) + 1)&lt;/code&gt; — to the scale parameter values at the suggestion of the Edward docs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

&lt;span class="c1"&gt;# fixed-effects placeholders&lt;/span&gt;
&lt;span class="n"&gt;fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# fixed-effects parameters&lt;/span&gt;
&lt;span class="n"&gt;β_fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# approximate fixed-effects distributions&lt;/span&gt;
&lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softplus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;qα&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softplus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Infer parameters&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INIT_OP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KLqp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;250/250 [100%] ██████████████████████████████ Elapsed: 4s | Loss: 35405.105&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Criticize model&lt;/h3&gt;
&lt;h4&gt;Visualize data fit given parameter priors&lt;/h4&gt;
&lt;p&gt;&lt;img alt="data fit given parameter priors" class="img-responsive" src="https://cavaunpeu.github.io/figures/data_fit_given_parameter_priors.png"/&gt;&lt;/p&gt;
&lt;h4&gt;Visualize data fit given parameter posteriors&lt;/h4&gt;
&lt;p&gt;&lt;img alt="data fit given parameter posteriors" class="img-responsive" src="https://cavaunpeu.github.io/figures/data_fit_given_parameter_posteriors.png"/&gt;&lt;/p&gt;
&lt;p&gt;It appears as if our model fits the data along the first two dimensions. This said, we could improve this fit considerably. This will become apparent when we compute the MAE on our validation set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;param_posteriors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
  &lt;span class="err"&gt;β&lt;/span&gt;&lt;span class="n"&gt;_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
  &lt;span class="err"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;X_val_feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;y_posterior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_posteriors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Mean validation `logerror`: {y_val.mean()}'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;compute_mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_posterior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val_feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Mean&lt;/span&gt; &lt;span class="n"&gt;validation&lt;/span&gt; &lt;span class="ss"&gt;`logerror`&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;012986094738549725&lt;/span&gt;
&lt;span class="n"&gt;Mean&lt;/span&gt; &lt;span class="n"&gt;absolute&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;validation&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;089943&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Inspect residuals&lt;/h4&gt;
&lt;p&gt;&lt;img alt="bayesian linear regression residuals" class="img-responsive" src="https://cavaunpeu.github.io/figures/bayesian_linear_regression_residuals.png"/&gt;&lt;/p&gt;
&lt;p&gt;"The residuals appear normally distributed with mean 0: this is a good sanity check for the model."&lt;sup&gt;1&lt;/sup&gt; However, with respect to the magnitude of the mean of the validation &lt;code&gt;logerror&lt;/code&gt;, our validation score is terrible. This is likely due to the fact that three predictors are not nearly sufficient for capturing the variation in the response. (Additionally, because the response itself is an &lt;em&gt;error&lt;/em&gt;, it should be fundamentally harder to capture than the thing actually being predicted — the house price. This is because Zillow's team has already built models to capture this signal, then effectively threw the remaining "uncaptured" signal into this competition, i.e. "figure out how to get right the little that we got wrong.")&lt;/p&gt;
&lt;h4&gt;Inspect parameter posteriors&lt;/h4&gt;
&lt;p&gt;&lt;img alt="bayesian linear regression posteriors" class="img-responsive" src="https://cavaunpeu.github.io/figures/bayesian_linear_regression_posteriors.png"/&gt;&lt;/p&gt;
&lt;p&gt;In keeping with the definition of multivariate linear regression itself, the above parameter posteriors tell us: "conditional on the assumption that the log-error and fixed effects can be related by a straight line, what is the predictive value of one variable once I already know the values of all other variables?"&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;Bayesian linear regression with random effects&lt;/h2&gt;
&lt;p&gt;Random effects models — also known as hierarchical models — allow us to ascribe distinct behaviors to different "clusters" of observations, i.e. groups that may each act in a materially unique way. Furthermore, these models allow us to infer these tendencies in a &lt;em&gt;collaborative&lt;/em&gt; fashion: while each cluster is assumed to behave differently, it can learn its parameters by heeding to the behavior of the population at large. In this example, we assume that houses in different zipcodes — holding all other predictors constant — should be priced in different ways.&lt;/p&gt;
&lt;p&gt;For clarity, let's consider the two surrounding extremes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate a single set of parameters for the population, i.e. the vanilla, &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"&gt;scikit-learn linear regression&lt;/a&gt;, Bayesian or not. This confers no distinct behaviors to houses in different zipcodes.&lt;/li&gt;
&lt;li&gt;Estimate a set of parameters for each individual zipcode, i.e. split the data into its cluster groups and estimate a single model for each. This confers maximally distinct behaviors to houses in different zip codes: the behavior of one cluster knows nothing about that of the others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Random-effects models "walk the line" between these two approaches — between maximally &lt;em&gt;underfitting&lt;/em&gt; and maximally &lt;em&gt;overfitting&lt;/em&gt; the behavior of each cluster. To this effect, its parameter estimates exhibit the canonical "shrinkage" phenomenon: the estimate for a given parameter is balanced between the within-cluster expectation and the global expectation. Smaller clusters exhibit larger shrinkage; larger clusters, i.e. those for which we've observed more data, are more bullheaded (in typical Bayesian fashion). A later plot illustrates this point.&lt;/p&gt;
&lt;p&gt;We specify our random-effects functional form as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With respect to the previous model, we've simply added &lt;code&gt;α_random_effects&lt;/code&gt; to the mean of our response. As such, this is a &lt;em&gt;varying-intercepts&lt;/em&gt; model: the intercept term will be different for each cluster. To this end, we learn the &lt;em&gt;global&lt;/em&gt; intercept &lt;code&gt;α&lt;/code&gt; as well as the &lt;em&gt;offsets&lt;/em&gt; from this intercept &lt;code&gt;α_random_effects&lt;/code&gt; — a random variable with as many dimensions as there are zipcodes. In keeping with the notion of "offset," we ascribe it a prior of &lt;code&gt;(0, σ_zc)&lt;/code&gt;. This approach allows us to flexibly extend the model to include more random effects, e.g. city, architecture style, etc. With only one, however, we could have equivalently included the global intercept &lt;em&gt;inside&lt;/em&gt; of our prior, i.e. &lt;code&gt;α_random_effects ~ Normal(α, σ_zc)&lt;/code&gt;, with priors on both &lt;code&gt;α&lt;/code&gt; and &lt;code&gt;σ_zc&lt;/code&gt; as per usual. This way, our random effect would no longer be a zip-code-specific &lt;em&gt;offset&lt;/em&gt; from the global intercept, but a vector of zip-code-specific intercepts outright.&lt;/p&gt;
&lt;p&gt;Finally, as Richard McElreath notes, "we can think of the &lt;code&gt;σ_zc&lt;/code&gt; parameter for each cluster as a crude measure of that cluster's "relevance" in explaining variation in the response variable."&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zip_codes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# random-effect placeholder&lt;/span&gt;
&lt;span class="n"&gt;zip_codes_ph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# random-effect parameter&lt;/span&gt;
&lt;span class="n"&gt;σ_zip_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([]))))&lt;/span&gt;
&lt;span class="n"&gt;α_zip_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;σ_zip_code&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gather&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;α_zip_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zip_codes_ph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# approximate random-effect distribution&lt;/span&gt;
&lt;span class="n"&gt;qα_zip_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softplus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_zip_codes&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Infer parameters&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α_zip_code&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα_zip_code&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INIT_OP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KLqp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zip_codes_ph&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;zip_codes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;250/250 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 34898.613&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Criticize model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean absolute error on validation data: 0.084635&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Inspect residuals&lt;/h4&gt;
&lt;p&gt;&lt;img alt="bayesian linear regression with random effects residuals" class="img-responsive" src="https://cavaunpeu.github.io/figures/bayesian_linear_regression_with_random_effects_residuals.png"/&gt;&lt;/p&gt;
&lt;h3&gt;Plot shrinkage&lt;/h3&gt;
&lt;p&gt;To illustrate shrinkage we'll pare our model down to intercepts only (removing the fixed effects entirely). We'll first fit a random-effects model on the full dataset then compute the cluster-specific-intercept posterior means. Next, we'll fit a separate model to each individual cluster and compute the intercept posterior mean of each. The plot below shows how estimates from the former can be viewed as "estimates from the latter — shrunk towards the global-intercept posterior mean."&lt;/p&gt;
&lt;p&gt;Finally, &lt;span style="color: #377eb8"&gt;blue&lt;/span&gt;, &lt;span style="color: #4daf4a"&gt;green&lt;/span&gt; and &lt;span style="color: #ff7f00"&gt;orange&lt;/span&gt; points represent small, medium and large clusters respectively. As mentioned before, the larger the cluster size, i.e. the more data points we've observed belonging to a given cluster, the &lt;em&gt;less&lt;/em&gt; prone it is to shrinkage towards the mean.&lt;/p&gt;
&lt;p&gt;&lt;img alt="shrinkage plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/shrinkage_plot.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Neural network with random effects&lt;/h2&gt;
&lt;p&gt;Neural networks are powerful function approximators. Keras is a library that lets us flexibly define complex neural architectures. Thus far, we've been approximating the relationship between our fixed effects and response variable with a simple dot product; can we leverage Keras to make this relationship more expressive? Is it painless? Finally, how does it integrate with Edward's existing APIs and constructs? Can we couple nimble generative models with deep neural networks?&lt;/p&gt;
&lt;p&gt;While my experimentation was brief, all answers point delightfully towards "yes" for two simple reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edward and Keras both run on TensorFlow.&lt;/li&gt;
&lt;li&gt;"Black-box" variational inference makes everything scale.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This said, we must be nonetheless explicit about what's "Bayesian" and what's not, i.e. for which parameters do we infer full (approximate) posterior distributions, and for which do we infer point estimates of the posterior distribution.&lt;/p&gt;
&lt;p&gt;Below, we drop a &lt;code&gt;neural_network&lt;/code&gt; in place of our dot product. Our latent variables remain &lt;code&gt;β_fixed_effects&lt;/code&gt;, &lt;code&gt;α&lt;/code&gt; and &lt;code&gt;α_zip_code&lt;/code&gt;: while we will infer their full (approximate) posterior distributions as before, we'll only compute &lt;em&gt;point estimates&lt;/em&gt; for the parameters of the neural network as in the typical case. Conversely, to the best of my knowledge, to infer full distributions for the latter, we'll need to specify our network manually in raw TensorFlow, i.e. ditch Keras entirely. We then treat our weights and biases as standard latent variables and infer their approximate posteriors via variational inference.  Edward's documentation contains a straightforward &lt;a href="http://edwardlib.org/tutorials/bayesian-neural-network"&gt;tutorial&lt;/a&gt; to this end.&lt;/p&gt;
&lt;h3&gt;Fit model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;neural_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'output'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# model&lt;/span&gt;
&lt;span class="n"&gt;fixed_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;μ_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;α&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;α_random_effects&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;neural_network&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;μ_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;β_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qβ_fixed_effects&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;α_zip_code&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;qα_zip_code&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INIT_OP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KLqp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fixed_effects&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zip_codes_ph&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;zip_codes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RMSPropOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inference&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;1000/1000 [100%] ██████████████████████████████ Elapsed: 18s | Loss: 34446.191&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Criticize model&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean absolute error on validation data: 0.081484&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Inspect residuals&lt;/h4&gt;
&lt;p&gt;&lt;img alt="neural network with random effects residuals" class="img-responsive" src="https://cavaunpeu.github.io/figures/neural_network_with_random_effects_residuals.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Future work&lt;/h1&gt;
&lt;p&gt;We've now laid a stable, if trivially simple foundation for building models with Edward and Keras. From here, I see two distinct paths to building more expressive probabilistic models using these tools:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build probabilistic models in Edward, and abstract deep-network-like subgraphs into Keras layers. This allows us to flexibly define complex neural architectures, e.g. a &lt;a href="https://keras.io/getting-started/functional-api-guide/#video-question-answering-model"&gt;video question answering model&lt;/a&gt;, with a nominal amount of code, yet restricts us from, or at least makes it awkward to, infer full posterior distributions for the subgraph parameters.&lt;/li&gt;
&lt;li&gt;Build probabilistic models in Edward, and specify deep-network-like subgraphs with raw TensorFlow — ditching Keras entirely. Defining deep-network-like subgraphs becomes more cumbersome, while inferring full posterior distributions for the subgraph parameters becomes more natural and consistent with the flow of Edward code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This work has shown a few basic variants of (generalized) Bayesian linear regression models. From here, there's tons more to explore — varying-slopes models, Gaussian process regression, mixture models and probabilistic matrix factorizations to name a random few.&lt;/p&gt;
&lt;p&gt;Edward and Keras have proven a flexible, expressive and powerful duo for performing inference in deep probabilistic models. The models we built were simple; the only direction to go, and to go rather painlessly, is more.&lt;/p&gt;
&lt;p&gt;Many thanks for reading.&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/random-effects-neural-networks"&gt;repository&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/random-effects-neural-networks/blob/master/zillow.ipynb"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/linear-mixed-effects-models"&gt;Edward - Linear Mixed Effects Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;McElreath, Richard. "Chapter 5." Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton, FL: CRC, Taylor &amp;amp; Francis Group, 2016. N. pag. Print.&lt;/li&gt;
&lt;li&gt;McElreath, Richard. "Chapter 12." Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton, FL: CRC, Taylor &amp;amp; Francis Group, 2016. N. pag. Print.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/"&gt;The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"&gt;Keras as a simplified interface to TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html"&gt;Mixture Density Networks with Edward, Keras and TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sl8r000.github.io/ab_testing_statistics/use_a_hierarchical_model/"&gt;Use a Hierarchical Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;McElreath, Richard. "Chapter 14." Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Boca Raton, FL: CRC, Taylor &amp;amp; Francis Group, 2016. N. pag. Print.&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Further Exploring Common Probabilistic Models</title><link href="https://cavaunpeu.github.io/2017/07/06/further-exploring-common-probabilistic-models/" rel="alternate"></link><published>2017-06-06T10:00:00-04:00</published><updated>2017-06-06T10:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-06-06:/2017/07/06/further-exploring-common-probabilistic-models/</id><summary type="html">&lt;p&gt;Exploring generative vs. discriminative models, and sampling and variational methods for approximate inference through the lens of Bayes' theorem.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="https://cavaunpeu.github.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;previous post&lt;/a&gt; on this blog sought to expose the statistical underpinnings of several machine learning models you know and love. Therein, we made the analogy of a swimming pool: you start on the surface — you know what these models do and how to use them for fun and profit — dive to the bottom — you deconstruct these models into their elementary assumptions and intentions — then finally, work your way back to the surface — reconstructing their functional forms, optimization exigencies and loss functions one step at a time.&lt;/p&gt;
&lt;p&gt;In this post, we're going to stay on the surface: instead of deconstructing common models, we're going to further explore the relationships between them — swimming to different corners of the pool itself. Keeping us afloat will be Bayes' theorem — a balanced, dependable yet at times fragile pool ring, so to speak — which we'll take with us wherever we go.&lt;/p&gt;
&lt;p&gt;&lt;img alt="pool ring" class="img-responsive" src="https://c1.staticflickr.com/7/6151/6137348439_199f5119be_b.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;While there are many potential themes of probabilistic models we might explore, we'll herein focus on two: &lt;strong&gt;generative vs. discriminative models&lt;/strong&gt;, and &lt;strong&gt;"fully Bayesian" vs. "lowly point estimate" learning&lt;/strong&gt;. We will stick to the supervised setting as well.&lt;/p&gt;
&lt;p&gt;Finally, our pool ring is not a godhead — we are not nautical missionaries brandishing a divine statistical truth, demanding that each model we encounter implement this truth in a rigid, bottom-up fashion. Instead, we'll explore the unique goals, formulations and shortcomings of each, and fall back on Bayes' theorem to bridge the gaps between. Without it, we'd quickly start sinking.&lt;/p&gt;
&lt;h1&gt;Discriminative vs. generative models&lt;/h1&gt;
&lt;p&gt;The goal of a supervised model is to compute the distribution over outcomes &lt;span class="math"&gt;\(y\)&lt;/span&gt; given an input &lt;span class="math"&gt;\(x\)&lt;/span&gt;, written &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;. If &lt;span class="math"&gt;\(y\)&lt;/span&gt; is discrete, this distribution is a probability mass function, e.g. a multinomial or binomial distribution. If continuous, it is a probability density function, e.g. a Gaussian distribution.&lt;/p&gt;
&lt;h2&gt;Discriminative models&lt;/h2&gt;
&lt;p&gt;In discriminative models, we immediately direct our focus to this output distribution. Taking an example from the &lt;a href="https://cavaunpeu.github.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;previous post&lt;/a&gt;, let's assume a softmax regression which receives some data &lt;span class="math"&gt;\(x\)&lt;/span&gt; and predicts a multi-class label &lt;code&gt;red or green or blue&lt;/code&gt;. The model's output distribution is therefore multinomial; a multinomial distribution requires as a parameter a vector &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; of respective outcome probabilities, e.g. &lt;code&gt;{red: .27, green: .11, blue: .62}&lt;/code&gt;. We can compute these individual probabilities via the softmax function, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\pi_k = \frac{e^{\eta_k}}{\sum\limits_{k=1}^K e^{\eta_k}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta_k = \theta_k^Tx\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a matrix of weights which we must infer, and &lt;span class="math"&gt;\(x\)&lt;/span&gt; is our input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;p&gt;Typically, we perform inference by taking the &lt;em&gt;maximum likelihood estimate&lt;/em&gt;: "which parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; most likely gave rise to the observed data pairs &lt;span class="math"&gt;\(D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)}))\)&lt;/span&gt; via the relationships described above?" We compute this estimate by maximizing the log-likelihood function with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, or equivalently minimizing the negative log-likelihood in identical fashion — the latter better known as a "loss function" in machine learning parlance.&lt;/p&gt;
&lt;p&gt;Unfortunately, the maximum likelihood estimate includes no information about the plausibility of the chosen parameter value itself. As such, we often place a &lt;em&gt;prior&lt;/em&gt; on our parameter and take the &lt;a href="https://en.wikipedia.org/wiki/Arg_max"&gt;"argmax"&lt;/a&gt; over their product. This gives the &lt;em&gt;maximum a posteriori&lt;/em&gt; estimate, or MAP.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\theta_{MAP}
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(y^{(i)}\vert x^{(i)}; \theta)P(\theta)\\
&amp;amp;= \underset{\theta}{\arg\max}\ \sum\limits_{i=1}^{m} \log{P(y^{(i)}\vert x^{(i)}; \theta)} + \log{P(\theta)}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\log{P(\theta)}\)&lt;/span&gt; term can be easily rearranged into what is better known as a &lt;em&gt;regularization term&lt;/em&gt; in machine learning, where the type of prior distribution we place on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; gives the type of regularization term.&lt;/p&gt;
&lt;p&gt;The argmax finds the point(s) &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; at which the given function attains its maximum value. As such, the typical discriminative model — softmax regression, logistic regression, linear regression, etc. — returns a single, lowly point estimate for the parameter in question.&lt;/p&gt;
&lt;h3&gt;How do we compute this value?&lt;/h3&gt;
&lt;p&gt;In the trivial case where &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is 1-dimensional, we can take the derivative of the function in question with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, set it equal to 0, then solve for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. (Additionally, in order to verify that we have indeed obtained a maximum, we should compute a second derivative and assert that its value is negative.)&lt;/p&gt;
&lt;p&gt;In the more realistic case where &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a high-dimensional vector or matrix, we can compute the argmax by way of an optimization routine like stochastic gradient ascent or, as is more common, the argmin by way of stochastic gradient descent.&lt;/p&gt;
&lt;h3&gt;What if we're uncertain about our parameter estimates?&lt;/h3&gt;
&lt;p&gt;Consider the following three scenarios — taken from Daphne Koller's &lt;a href="https://www.coursera.org/learn/probabilistic-graphical-models-3-learning/home/welcome"&gt;Learning in Probabilistic Graphical Models&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Two teams play 10 times, and the first wins 7 of the 10 matches.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt; &lt;em&gt;Infer that the probability of the first team winning is 0.7.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Seems reasonable, right?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A coin is tossed 10 times, and comes out &lt;code&gt;heads&lt;/code&gt; on 7 of the 10 tosses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt; &lt;em&gt;Infer that the probability of observing &lt;code&gt;heads&lt;/code&gt; is 0.7.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Changing only the analogy, this now seems wholly unreasonable — right?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A coin is tossed 10000 times, and comes out &lt;code&gt;heads&lt;/code&gt; on 7000 of the 10000 tosses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt; &lt;em&gt;Infer that the probability of observing &lt;code&gt;heads&lt;/code&gt; is 0.7.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, increasing the observed counts, the previous scenario now seems plausible.&lt;/p&gt;
&lt;p&gt;I find this a terrific succession of examples with which to convey the notion of &lt;em&gt;uncertainty&lt;/em&gt; — that the more data we have, the less uncertain we are about what's really going on. This notion is at the heart of Bayesian statistics and is extremely intuitive to us as humans. Unfortunately, when we compute "lowly point estimates," i.e. the argmin of the loss function with respect to our parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we are discarding this uncertainty entirely. Should our model be fit with &lt;span class="math"&gt;\(n\)&lt;/span&gt; observations where &lt;span class="math"&gt;\(n\)&lt;/span&gt; is not a large number, our estimate would amount to that of Example #2: &lt;em&gt;a coin is tossed &lt;span class="math"&gt;\(n\)&lt;/span&gt; times, and comes out &lt;code&gt;heads&lt;/code&gt; on &lt;code&gt;int(.7n)&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; tosses — infer that the probability of observing &lt;code&gt;heads&lt;/code&gt; is squarely, unflinchingly, &lt;code&gt;0.7&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;What does including uncertainty look like?&lt;/h3&gt;
&lt;p&gt;It looks like a &lt;em&gt;distribution&lt;/em&gt; — a range of possible values for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Further, these values are of varying plausibility as dictated by the data we've observed. In Example #2, while we'd still say that &lt;span class="math"&gt;\(\Pr(\text{heads}) = .7\)&lt;/span&gt; is the parameter value &lt;em&gt;most likely&lt;/em&gt; to have generated our data, we'd additionally maintain that other values in &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt; are plausible, albeit less so, as well. Again, this logic should be simple to grasp: it comes easy to us as humans.&lt;/p&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;With the parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; in hand prediction is simple: just plug back into our original function &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;. With a point estimate for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, we compute but a single value for &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Generative models&lt;/h2&gt;
&lt;p&gt;In generative models, we instead compute &lt;em&gt;component parts&lt;/em&gt; of the desired output distribution &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; instead of directly computing &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; itself. To examine these parts, we'll turn to Bayes' theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x\vert y)P(y)}{P(x)}
$$&lt;/div&gt;
&lt;p&gt;The numerator posits a generative mechanism for the observed data pairs &lt;span class="math"&gt;\(D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)}))\)&lt;/span&gt; in idiomatic terms; it states that each pair was generated by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Selecting a label &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt; from &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt;. If our model is predicting &lt;code&gt;red or green or blue&lt;/code&gt;, &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt; is likely a multinomial distribution.&lt;ul&gt;
&lt;li&gt;If our observed label counts are &lt;code&gt;{'red': 20, 'green': 50, 'blue': 30}&lt;/code&gt;, we would retrodictively believe this multinomial distribution to have had a parameter vector near &lt;span class="math"&gt;\(\pi = [.2, .5, .3]\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given a label &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt;, select a value &lt;span class="math"&gt;\(x^{(i)}\)&lt;/span&gt; from &lt;span class="math"&gt;\(P(x\vert y)\)&lt;/span&gt;. Trivially, this means that we are positing &lt;em&gt;three distinct distributions&lt;/em&gt; of this form: &lt;span class="math"&gt;\(P(x\vert y=\text{red}), P(x\vert y=\text{green}), P(x\vert y=\text{blue})\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;For example, if &lt;span class="math"&gt;\(y^{(i)} = \text{red}\)&lt;/span&gt;, draw &lt;span class="math"&gt;\(x^{(i)}\)&lt;/span&gt; from &lt;span class="math"&gt;\(P(x\vert y=\text{red})\)&lt;/span&gt;, and so forth.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;p&gt;The inference task is to compute &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt; and each distinct &lt;span class="math"&gt;\(P(x\vert y_k)\)&lt;/span&gt;. In a classification setting, the former is likely a multinomial distribution. The latter might be a multinomial distribution or a set of binomial distributions in the case of discrete-feature data, or a set of Gaussian distributions in the case of continuous-feature data. In fact, these distributions can be whatever you'd like, dictated by the idiosyncrasies of the problem at hand.&lt;/p&gt;
&lt;p&gt;Finally, we can compute these distributions as per normal: via a maximum likelihood estimate, a MAP estimate, etc.&lt;/p&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;To compute &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; we return to Bayes' theorem:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x\vert y)P(y)}{P(x)}
$$&lt;/div&gt;
&lt;p&gt;We have the numerator &lt;span class="math"&gt;\(P(y)\)&lt;/span&gt; and three distinct conditional distributions &lt;span class="math"&gt;\(P(x\vert y=\text{red}), P(x\vert y=\text{green})\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x\vert y=\text{blue})\)&lt;/span&gt; in hand. What about the denominator?&lt;/p&gt;
&lt;h3&gt;Conditional probability and marginalization&lt;/h3&gt;
&lt;p&gt;The axiom of conditional probability allows us to write &lt;span class="math"&gt;\(P(B\vert A)P(A) = P(B, A)\)&lt;/span&gt;, i.e. the &lt;em&gt;joint probability&lt;/em&gt; of &lt;span class="math"&gt;\(B\)&lt;/span&gt; and &lt;span class="math"&gt;\(A\)&lt;/span&gt;. This is a simple algebraic manipulation. As such, we can rewrite Bayes' theorem in its more compact form.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{P(x, y)}{P(x)}
$$&lt;/div&gt;
&lt;p&gt;Another manipulation of probability distributions is the &lt;em&gt;marginalization&lt;/em&gt; operator, which allows us to write:&lt;/p&gt;
&lt;div class="math"&gt;$$
\int P(x, y)dy = P(x)
$$&lt;/div&gt;
&lt;p&gt;As such, we can &lt;em&gt;marginalize &lt;span class="math"&gt;\(y\)&lt;/span&gt; out of the numerator&lt;/em&gt; so as to obtain the denominator we require. This denominator is often called the "evidence."&lt;/p&gt;
&lt;h3&gt;Marginalization example&lt;/h3&gt;
&lt;p&gt;Marginalization took me a while to understand. Imagine we have the following joint probability distribution out of which we'd like to marginalize &lt;span class="math"&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(A\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.03\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.14\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.09\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.34\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.23\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.17\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The result of this marginalization is &lt;span class="math"&gt;\(P(B)\)&lt;/span&gt;, i.e. "what is the probability of observing each of the distinct values of &lt;span class="math"&gt;\(B\)&lt;/span&gt;?" In this example there are two — &lt;span class="math"&gt;\(b^7\)&lt;/span&gt; and &lt;span class="math"&gt;\(b^8\)&lt;/span&gt;. To marginalize over &lt;span class="math"&gt;\(A\)&lt;/span&gt;, we simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Delete the &lt;span class="math"&gt;\(A\)&lt;/span&gt; column.&lt;/li&gt;
&lt;li&gt;"Collapse" the remaining columns — in this case, &lt;span class="math"&gt;\(B\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Step 1 gives:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.03\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.14\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.09\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.34\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.23\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.17\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Step 2 gives:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(B\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^7\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.03 + .09 = .12\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^8\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(.14 + .34 + .23 + .17 = .88\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;The denominator&lt;/h3&gt;
&lt;p&gt;In the context of our generative model with a given input &lt;span class="math"&gt;\(x\)&lt;/span&gt;, the result of this marginalization is a &lt;em&gt;scalar&lt;/em&gt; — not a distribution. To see why, let's construct the joint distribution — the numerator — then marginalize:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(x, y)\)&lt;/span&gt;:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(y\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(P(y, X)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\text{red}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{red}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\text{green}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{green}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\text{blue}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{blue}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\int P(x, y)dy = P(x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(P(y, X)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(P(y = \text{red}, x) + P(y = \text{green}, x) + P(y = \text{blue}, x)\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The resulting probability distribution is over a single value: it is a scalar. This scalar &lt;em&gt;normalizes&lt;/em&gt; the respective numerator terms such that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(y = \text{red}, x)}{P(x)} +
\frac{P(y = \text{green}, x)}{P(x)} +
\frac{P(y = \text{blue}, x)}{P(x)}
= 1
$$&lt;/div&gt;
&lt;p&gt;This gives &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;: a valid probability distribution over the class labels &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Partition function&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; often takes another name and even another variable: &lt;span class="math"&gt;\(Z\)&lt;/span&gt;, the &lt;em&gt;partition function&lt;/em&gt;. The stated purpose of this function is to normalize the numerator such that the above summation-to-1 holds. This normalization is necessary because the numerators typically will not sum to 1 themselves, which follows logically from the fact that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\sum\limits_{k = 1}^K P(y = k) = 1
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(x\vert y = k) \neq 1
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\((1)\)&lt;/span&gt; is always true, the "&lt;span class="math"&gt;\(\neq\)&lt;/span&gt;" in &lt;span class="math"&gt;\((2)\)&lt;/span&gt; would need to become an "&lt;span class="math"&gt;\(=\)&lt;/span&gt;" such that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum\limits_{k = 1}^K P(y = k)P(x\vert y = k) = 1
$$&lt;/div&gt;
&lt;p&gt;Unfortunately, &lt;span class="math"&gt;\(P(x\vert y = k) = 1\)&lt;/span&gt; is rarely if ever the case.&lt;/p&gt;
&lt;p&gt;As you'll now note, the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-specific partition function gives a result equivalent to that of the marginalized-over-&lt;span class="math"&gt;\(y\)&lt;/span&gt; joint distribution: a scalar value &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; with which to normalize the numerator. However, crucially, please keep in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The partition function is a specific component of a probabilistic model. It always yields a scalar&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Marginalization is a &lt;strong&gt;much more general&lt;/strong&gt; operation performed on a probability distribution, which yields a scalar only when the remaining variable(s) are homogeneous, i.e. each remaining column contains a single distinct value.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;In the majority of cases, marginalization will simply yield a reduced probability distribution over many value configurations, similar to the &lt;span class="math"&gt;\(P(B)\)&lt;/span&gt; example above.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;In practice, this is superfluous&lt;/h3&gt;
&lt;p&gt;If we neglect to compute &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;, i.e. if we don't normalize our joint distributions &lt;span class="math"&gt;\(P(x, y = k)\)&lt;/span&gt;, we'll be left with an invalid probability distribution &lt;span class="math"&gt;\(\tilde{P}(y\vert x)\)&lt;/span&gt; whose values do not sum to 1. This distribution might look like &lt;code&gt;P(y|x) = {'red': .00047, 'green': .0011, 'blue': .0000853}&lt;/code&gt;. &lt;em&gt;If our goal is to simply compute the most likely label, taking the argmax of this unnormalized distribution works just fine.&lt;/em&gt; This follows trivially from our Bayesian pool ring:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{y}{\arg\max}\ \frac{P(x, y)}{P(x)} = \underset{y}{\arg\max}\ P(x, y)
$$&lt;/div&gt;
&lt;h1&gt;"Fully Bayesian learning"&lt;/h1&gt;
&lt;p&gt;We previously lamented the shortcomings of "lowly point estimates" and sang the praises of inferring the full distribution instead. Unfortunately, this is often a computationally-hard thing to do.&lt;/p&gt;
&lt;p&gt;To see why, let's revisit Bayes' theorem. Assume we are estimating the parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; of a softmax regression model and have placed a prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. In concrete terms, this estimate can be written as &lt;span class="math"&gt;\(P(\theta\vert D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)})))\)&lt;/span&gt;: the distribution over our belief in the true value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given the data we've observed. Bayes' theorem allows us to expand this quantity into:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\theta\vert D) = \frac{P(D\vert\theta)P(\theta)}{P(D)}
$$&lt;/div&gt;
&lt;p&gt;Previously, we computed a "lowly point estimate" for this distribution — the MAP — as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\theta_{MAP}
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(y^{(i)}\vert x^{(i)}; \theta)P(\theta)\\
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(\theta\vert (y^{(i)}, x^{(i)}))\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;While &lt;span class="math"&gt;\(P(y^{(i)}\vert x^{(i)}; \theta)P(\theta) \neq P(\theta\vert (y^{(i)}, x^{(i)}))\)&lt;/span&gt;, the argmaxes of the respective products &lt;em&gt;are&lt;/em&gt; equal. For this reason, we were able to compute a point estimate for &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;, i.e. a "summarization" of &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; in a single value, without ever computing the denominator &lt;span class="math"&gt;\(P(D)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(As a brief aside, please note that we could summarize &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; with &lt;em&gt;any&lt;/em&gt; single value from this distribution. We often select the maximum likelihood estimate — the single value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that most likely gave rise to our data, or the MAP — the single value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that both most likely gave rise to our data and most plausibly occurred itself.)&lt;/p&gt;
&lt;p&gt;To compute &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; — trivially, a full distribution as the term suggests — we will need to compute &lt;span class="math"&gt;\(P(D)\)&lt;/span&gt; after all. As before, this can be accomplished via marginalization:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(\theta\vert D)
&amp;amp;= \frac{P(D\vert\theta)P(\theta)}{P(D)}\\
&amp;amp;= \frac{P(D, \theta)}{P(D)}\\
&amp;amp;= \frac{P(D, \theta)}{\int P(D, \theta)d\theta}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; takes continuous values, we can no longer employ the "delete and collapse" method of marginalization in discrete distributions. Furthermore, in all but trivial cases, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a high-dimensional vector or matrix, leaving us to compute a "high-dimensional integral that lacks an analytic (closed-form) solution — the central computational challenge in inference."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;As such, computing the full distribution &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; is &lt;em&gt;approximating&lt;/em&gt; the full distribution &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;. To this end, we'll introduce two new families of algorithms.&lt;/p&gt;
&lt;h2&gt;Markov chain monte carlo&lt;/h2&gt;
&lt;p&gt;In small to medium-sized models, we often take an alternative ideological approach to approximating &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;: instead of computing a distribution, i.e. the canonical parameters of a gory algebraic expression which control its shape — we produce &lt;em&gt;samples&lt;/em&gt; from this distribution. Roughly speaking, the aggregate of these samples then gives, retrodictively, the distribution itself. The general family of these methods is known as Markov chain monte carlo, or MCMC.&lt;/p&gt;
&lt;p&gt;In simple terms, MCMC inference for a given parameter &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; to some value &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Compute the prior probability of &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt; and the probability of having observed our data under &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt; — &lt;span class="math"&gt;\(P(\phi_{\text{current}})\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(D\vert \phi_{\text{current}})\)&lt;/span&gt;, respectively. Their product gives &lt;span class="math"&gt;\(P(D, \phi_{\text{current}})\)&lt;/span&gt; — the joint probability of having observed the proposed parameter value and our observed data given this value.&lt;/li&gt;
&lt;li&gt;Add &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt; to a big green plastic bucket of "accepted values."&lt;/li&gt;
&lt;li&gt;Propose moving to a new, nearby value &lt;span class="math"&gt;\(\phi_{\text{proposal}}\)&lt;/span&gt;. This value is drawn from an entirely separate &lt;em&gt;sampling distribution&lt;/em&gt; which bears no influence on our prior &lt;span class="math"&gt;\(P(\phi)\)&lt;/span&gt; nor likelihood function &lt;span class="math"&gt;\(P(D\vert \phi)\)&lt;/span&gt;. Repeat Step 2 using &lt;span class="math"&gt;\(\phi_{\text{proposal}}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(\phi_{\text{current}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Walk the following tree:&lt;ul&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(P(D, \phi_{\text{proposal}}) \gt P(D, \phi_{\text{current}})\)&lt;/span&gt;:&lt;ul&gt;
&lt;li&gt;Set &lt;span class="math"&gt;\(\phi_{\text{current}} = \phi_{\text{proposal}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Move to Step 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Else:&lt;ul&gt;
&lt;li&gt;With some small probability:&lt;ul&gt;
&lt;li&gt;Set &lt;span class="math"&gt;\(\phi_{\text{current}} = \phi_{\text{proposal}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Move to Step 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Else:&lt;ul&gt;
&lt;li&gt;Move to Step 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After collecting a few thousand samples — and discarding the first few hundred, in which we drunkenly amble towards the region of high joint probability (a quantity &lt;em&gt;proportional&lt;/em&gt; to the posterior probability) — we now have a bucket of samples from our desired posterior distribution. Nota bene: we never had to touch the high-dimensional integral &lt;span class="math"&gt;\(\int P(D, \theta)d\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Variational inference&lt;/h2&gt;
&lt;p&gt;In large-scale models, MCMC methods are often too slow. Conversely, variational inference provides a framework for casting the problem of posterior approximation as one of &lt;em&gt;optimization&lt;/em&gt; — far faster than a sampling-based approach. This yields an &lt;em&gt;analytical&lt;/em&gt; approximation to &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;. The following explanation of variational inference is taken largely from a previous post of mine: &lt;a href="https://cavaunpeu.github.io/2017/05/08/transfer-learning-flight-delay-prediction/"&gt;Transfer Learning for Flight Delay Prediction&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our approximating distribution we'll choose one that is simple, parametric and familiar: the normal (Gaussian) distribution, parameterized by some set of parameters &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}(\theta\vert D)$$&lt;/div&gt;
&lt;p&gt;Our goal is to force this distribution to closely resemble the original; the &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL divergence&lt;/a&gt; quantifies their difference:&lt;/p&gt;
&lt;div class="math"&gt;$$KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D)) = \int{q_{\lambda}(\theta\vert D)\log\frac{q_{\lambda}(\theta\vert D)}{P(\theta\vert D)}d\theta}$$&lt;/div&gt;
&lt;p&gt;To this end, we compute its argmin with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}^{*}(\theta\vert D) = \underset{\lambda}{\arg\min}\ \text{KL}(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D))$$&lt;/div&gt;
&lt;p&gt;Expanding the divergence, we obtain:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D))
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\log\frac{q_{\lambda}(\theta\vert D)}{P(\theta\vert D)}d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\log\frac{q_{\lambda}(\theta\vert D)P(D)}{P(\theta, D)}d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D) -\log{P(\theta, D)} + \log{P(D)}}\bigg)d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta + \log{P(D)}\int{q_{\lambda}(\theta\vert D)d\theta}\\
&amp;amp;= \int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta + \log{P(D)} \cdot 1
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Since only the integral depends on &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, minimizing the entire expression with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; amounts to minimizing this term. Incidentally, the opposite (negative) of this term is called the &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;ELBO&lt;/a&gt;, or the "evidence lower bound."&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO(\lambda) = -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta
$$&lt;/div&gt;
&lt;p&gt;To see why, let's plug the ELBO into the equation above and solve for &lt;span class="math"&gt;\(\log{P(D)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\log{P(D)} = ELBO(\lambda) + KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D))$$&lt;/div&gt;
&lt;p&gt;In English: "the log of the evidence is at least the lower bound of the evidence plus the divergence from our (variational) approximation of the posterior &lt;span class="math"&gt;\(q_{\lambda}(\theta\vert D)\)&lt;/span&gt; to our true posterior &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;."&lt;/p&gt;
&lt;p&gt;As such, minimizing this divergence is equivalent to &lt;em&gt;maximizing&lt;/em&gt; the ELBO, as:&lt;/p&gt;
&lt;div class="math"&gt;$$
KL(q_{\lambda}(\theta\vert D)\Vert P(\theta\vert D)) = -ELBO(\lambda) + \log{P(D)}
$$&lt;/div&gt;
&lt;h3&gt;Optimization&lt;/h3&gt;
&lt;p&gt;Let's restate the equation for the ELBO and rearrange further:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
ELBO(\lambda)
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(\theta, D)}}\bigg)d\theta\\
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} -\log{P(D\vert \theta)} - \log{P(\theta)}}\bigg)d\theta\\
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\bigg(\log{q_{\lambda}(\theta\vert D)} - \log{P(\theta)}}\bigg)d\theta + \log{P(D\vert \theta)}\int{q_{\lambda}(\theta\vert D)d\theta}\\
&amp;amp;= -\int{q_{\lambda}(\theta\vert D)\log{\frac{q_{\lambda}(\theta\vert D)}{P(\theta)}}d\theta} + \log{P(D\vert \theta)} \cdot 1\\
&amp;amp;= \log{P(D\vert \theta)} -KL(q_{\lambda}(\theta\vert D)\Vert P(\theta))\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Again, our goal is to maximize this expression or minimize its opposite:&lt;/p&gt;
&lt;div class="math"&gt;$$
-\log{P(D\vert \theta)} + KL(q_{\lambda}(\theta\vert D)\Vert P(\theta))
$$&lt;/div&gt;
&lt;p&gt;One step further, we obtain:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;= -\log{P(D\vert \theta)} + q_{\lambda}(\theta\vert D)\log{q_{\lambda}(\theta\vert D)} - q_{\lambda}(\theta\vert D)\log{P(\theta)}\\
&amp;amp;= \mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[-\log{P(D\vert \theta)} +\log{q_{\lambda}(\theta\vert D)} - \log{P(\theta)}]\\
&amp;amp;= \mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[-\big(\log{P(D,  \theta)} -\log{q_{\lambda}(\theta\vert D)}\big)]\\
&amp;amp;= -\mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[\log{P(D,  \theta)}] + \mathop{\mathbb{E}}_{q_{\lambda}(\theta\vert D)}[\log{q_{\lambda}(\theta\vert D)}]\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In machine learning parlance: "minimize the negative log joint probability of our data and parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; — a MAP estimate — plus the entropy of our variational approximation." As a &lt;em&gt;higher&lt;/em&gt; entropy is desirable — an approximation which distributes its mass in a &lt;em&gt;conservative&lt;/em&gt; fashion — this minimization is a balancing act between the two terms.&lt;/p&gt;
&lt;p&gt;For a more in-depth discussion of both entropy and KL-divergence please see &lt;a href="https://cavaunpeu.github.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/"&gt;Minimizing the Negative Log-Likelihood, in English&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Posterior predictive distribution&lt;/h1&gt;
&lt;p&gt;With our estimate for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; as a full distribution, we can now make a new prediction as a full distribution as well.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert x, D)
&amp;amp;= \int P(y\vert x, D, \theta)P(\theta\vert x, D)d\theta\\
&amp;amp;= \int P(y\vert x, \theta)P(\theta\vert D)d\theta\\
\end{align*}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The right term under the integral is the posterior distribution of our parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given the "training" data, &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt;. Since it does not depend on a new input &lt;span class="math"&gt;\(x\)&lt;/span&gt; we have removed &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The left term under the integral is our likelihood function: given an &lt;span class="math"&gt;\(x\)&lt;/span&gt; and a &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, it produces a &lt;span class="math"&gt;\(y\)&lt;/span&gt;. While this function does depend on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; — whose values are pulled from our posterior &lt;span class="math"&gt;\(P(\theta\vert D)\)&lt;/span&gt; — it does not depend on &lt;span class="math"&gt;\(D\)&lt;/span&gt; itself. As such, we have removed &lt;span class="math"&gt;\(D\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Integrating over &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; yields a distribution over &lt;span class="math"&gt;\(y\)&lt;/span&gt;: we've now captured not just the uncertainty in &lt;em&gt;inference&lt;/em&gt;, but also the corresponding uncertain in our &lt;em&gt;predictions&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;What do these distributions actually do for me?&lt;/h1&gt;
&lt;p&gt;Said differently, "why is it important to quantify uncertainty?"&lt;/p&gt;
&lt;p&gt;I think we, as humans, are exceptionally qualified to answer this question: we need to look no further than ourselves, our choices, our environment.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The cross-walk says "go." Do I:&lt;ul&gt;
&lt;li&gt;Close my eyes, lie down for a 15-second nap in the middle of the road, then walk backwards the rest of the way?&lt;/li&gt;
&lt;li&gt;Quickly look both ways then walk leisurely across the road, keeping an eye out for cyclists at the same time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A company emails to say "we'd like to discuss the possibility of a full-time role." Do I:&lt;ul&gt;
&lt;li&gt;Respond saying "Great! Let's chat further" while continuing to speak with other companies.&lt;/li&gt;
&lt;li&gt;Respond saying "Great! Let's chat further" and promptly sever all contact with other companies.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An extremely reliable lifelong friend calls to say they've found me a beautiful studio in Manhattan for $600/month, and would need to confirm in the next 24 hours if I'd like to take it. Do I:&lt;ul&gt;
&lt;li&gt;Take it.&lt;/li&gt;
&lt;li&gt;Call three friends to ask if they think that this makes sense.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An extremely sketchy real estate broker calls to say they've found me a beautiful studio in Manhattan for $600/month, and would need to confirm in the next 24 hours if I'd like to take it. Do I:&lt;ul&gt;
&lt;li&gt;Take it.&lt;/li&gt;
&lt;li&gt;Call three friends to ask if they think that this makes sense.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The notion is the same in probabilistic modeling. Furthermore, we often build models with "not big data," and therefore have a substantially non-zero amount of uncertainty in our parameter estimates and subsequent predictions.&lt;/p&gt;
&lt;p&gt;Finally, with distributional estimates in hand, we can begin to make more robust, measured and logical decisions. We can do this because, while point estimates give a quick summary of the dynamics of our system, distributions tell the full, thorough story: where the peaks are, their width and height, their distance from one another, etc. For an excellent exploration of what we can do with posterior distributions, check out Rasmus Bååth's &lt;a href="http://www.sumsar.net/blog/2015/01/probable-points-and-credible-intervals-part-two/"&gt;Probable Points and Credible Intervals, Part 2: Decision Theory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Many thanks for reading, and to our pool ring Bayes'.&lt;/p&gt;
&lt;p&gt;&lt;img alt="girls having drinks on pool rings" class="img-responsive" src="https://ak8.picdn.net/shutterstock/videos/19157749/thumb/2.jpg"/&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://edwardlib.org/tutorials/inference"&gt;Edward — Inference of Probabilistic Models&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Minimizing the Negative Log-Likelihood, in English</title><link href="https://cavaunpeu.github.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/" rel="alternate"></link><published>2017-05-18T12:24:00-04:00</published><updated>2017-05-18T12:24:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-05-18:/2017/05/18/minimizing_the_negative_log_likelihood_in_english/</id><summary type="html">&lt;p&gt;Statistical underpinnings of the machine learning models we know and love. A walk through random variables, entropy, exponential family distributions, generalized linear models, maximum likelihood estimation, cross entropy, KL-divergence, maximum a posteriori estimation and going "fully Bayesian."&lt;/p&gt;</summary><content type="html">&lt;p&gt;Roughly speaking, my machine learning journey began on &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt;. "There's data, a model (i.e. estimator) and a loss function to optimize," I learned. "Regression models predict continuous-valued real numbers; classification models predict 'red,' 'green,' 'blue.' Typically, the former employs the mean squared error or mean absolute error; the latter, the cross-entropy loss. Stochastic gradient descent updates the model's parameters to drive these losses down." Furthermore, to fit these models, just &lt;code&gt;import sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A dexterity with the above is often sufficient for — at least from a technical stance — both employment and impact as a data scientist. In industry, commonplace prediction and inference problems — binary churn, credit scoring, product recommendation and A/B testing, for example — are easily matched with an off-the-shelf algorithm plus proficient data scientist for a measurable boost to the company's bottom line. In a vacuum I think this is fine: the winning driver does not &lt;em&gt;need&lt;/em&gt; to know how to build the car. Surely, I've been this person before.&lt;/p&gt;
&lt;p&gt;Once fluid with "scikit-learn fit and predict," I turned to statistics. I was always aware that the two were related, yet figured them ultimately parallel sub-fields of my job. With the former, I build classification models; with the latter, I infer signup counts with the Poisson distribution and MCMC — right?&lt;/p&gt;
&lt;p&gt;Before long, I dove deeper into machine learning — reading textbooks, papers and source code and writing this blog. Therein, I began to come across &lt;em&gt;terms I didn't understand used to describe the things that I did.&lt;/em&gt; "I understand what the categorical cross-entropy loss is, what it does and how it's defined," for example: &lt;em&gt;&lt;strong&gt;"why are you calling it the negative log-likelihood?"&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Marginally wiser, I now know two truths about the above:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Techniques we anoint as "machine learning" — classification and regression models, notably — have their underpinnings almost entirely in statistics. For this reason, terminology often flows between the two.&lt;/li&gt;
&lt;li&gt;None of this stuff is new.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The goal of this post is to take three models we know, love, and know how to use and explain what's really going on underneath the hood. I will assume the reader is familiar with concepts in both machine learning and statistics, and comes in search of a deeper understanding of the connections therein. There will be math — but only as much as necessary. Most of the derivations can be skipped without consequence.&lt;/p&gt;
&lt;p&gt;When deploying a predictive model in a production setting, it is generally in our best interest to &lt;code&gt;import sklearn&lt;/code&gt;, i.e. use a model that someone else has built. This is something we already know how to do. As such, this post will start and end here: your head is currently above water; we're going to dive into the pool, touch the bottom, then work our way back to the surface. Lemmas will be written in &lt;em&gt;&lt;strong&gt;bold&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bottom of pool" class="img-responsive" src="http://img2.hungertv.com/wp-content/uploads/2014/09/SP_Kanawaza-616x957.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;First, let's meet our three protagonists. We'll define them in &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; for the illustrative purpose of a unified and idiomatic API.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/"&gt;Linear regression&lt;/a&gt; with mean squared error&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/"&gt;Logistic regression&lt;/a&gt; with binary cross-entropy loss&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/"&gt;Softmax regression&lt;/a&gt; with categorical cross-entropy loss&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we'll select four components key to each: its response variable, functional form, loss function and loss function plus regularization term. For each model, we'll describe the statistical underpinnings of each component — the steps on the ladder towards the surface of the pool.&lt;/p&gt;
&lt;p&gt;Before diving in, we'll need to define a few important concepts.&lt;/p&gt;
&lt;h2&gt;Random variable&lt;/h2&gt;
&lt;p&gt;I define a random variable as "a thing that can take on a bunch of different values."&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"The tenure of despotic rulers in Central Africa" is a random variable. It could take on values of 25.73 years, 14.12 years, 8.99 years, ad infinitum; it could not take on values of 1.12 million years, nor -5 years.&lt;/li&gt;
&lt;li&gt;"The height of the next person to leave the supermarket" is a random variable.&lt;/li&gt;
&lt;li&gt;"The color of shirt I wear on Mondays" is a random variable. (Incidentally, this one only has ~3 distinct values.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Probability distribution&lt;/h2&gt;
&lt;p&gt;A probability distribution is a lookup table for the likelihood of observing each unique value of a random variable. Assuming a given variable can take on values in &lt;span class="math"&gt;\(\{\text{rain, snow, sleet, hail}\}\)&lt;/span&gt;, the following is a valid probability distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Trivially, these values must sum to 1.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;probability mass function&lt;/em&gt; is a probability distribution for a discrete-valued random variable.&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;probability density function&lt;/em&gt; &lt;em&gt;&lt;strong&gt;gives&lt;/strong&gt;&lt;/em&gt; a probability distribution for a continuous-valued random variable.&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Gives&lt;/em&gt;, because this function itself is not a lookup table. Given a random variable that takes on values in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;, we do not and cannot define &lt;span class="math"&gt;\(\Pr(X = 0.01)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Pr(X = 0.001)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\Pr(X = 0.0001)\)&lt;/span&gt;, etc.&lt;/li&gt;
&lt;li&gt;Instead, we define a function that tells us the probability of observing a value within a certain &lt;em&gt;range&lt;/em&gt;, i.e. &lt;span class="math"&gt;\(\Pr(0.01 &amp;lt; X &amp;lt; .4)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;This is the probability density function, where &lt;span class="math"&gt;\(\Pr(0 \leq X \leq 1) = 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Entropy&lt;/h2&gt;
&lt;p&gt;Entropy quantifies the number of ways we can reach a given outcome. Imagine 8 friends are splitting into 2 taxis en route to a Broadway show. Consider the following two scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Four friends climb into each taxi.&lt;/em&gt; We could accomplish this with the following assignments:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# fill the first, then the second&lt;/span&gt;
&lt;span class="n"&gt;assignment_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# alternate assignments&lt;/span&gt;
&lt;span class="n"&gt;assignment_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# alternate assignments in batches of two&lt;/span&gt;
&lt;span class="n"&gt;assignment_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# etc.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;All friends climb into the first taxi.&lt;/em&gt; There is only one possible assignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;assignment_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(In this case, the Broadway show is probably in &lt;a href="http://willtravellife.com/2013/04/how-does-a-west-african-bush-taxi-work/"&gt;West Africa&lt;/a&gt; or a similar part of the world.)&lt;/p&gt;
&lt;p&gt;Since there are more ways to reach the first outcome than there are the second, the first outcome has a higher entropy.&lt;/p&gt;
&lt;h3&gt;More explicitly&lt;/h3&gt;
&lt;p&gt;We compute entropy for probability distributions. This computation is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p) = -\sum\limits_{i=1}^{n} p_i \log{p_i}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are &lt;span class="math"&gt;\(n\)&lt;/span&gt; distinct events.&lt;/li&gt;
&lt;li&gt;Each event &lt;span class="math"&gt;\(i\)&lt;/span&gt; has probability &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Entropy is the &lt;em&gt;weighted-average log probability&lt;/em&gt; over possible events — this much reads directly from the equation — which measures the &lt;em&gt;uncertainty inherent in their probability distribution.&lt;/em&gt; The higher the entropy, the less certain we are about the value we're going to get.&lt;/p&gt;
&lt;p&gt;Let's calculate the entropy of our distribution above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob_dist&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prob_dist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mf"&gt;1.1055291211185652&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For comparison, let's assume two more distributions and calculate their respective entropies.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;p_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mf"&gt;0.8304250977453105&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="mf"&gt;0.2460287703075343&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the first distribution, we are least certain as to what tomorrow's weather will bring. As such, this has the highest entropy. In the third distribution, we are almost certain it's going to hail. As such, this has the lowest entropy.&lt;/p&gt;
&lt;p&gt;Finally, it is a probability distribution that dictates the different taxi assignments just above. A distribution for a random variable that has many possible outcomes has a higher entropy than a distribution that gives only one.&lt;/p&gt;
&lt;p&gt;Now, let's dive into the pool. We'll start at the bottom and work our way back to the top.&lt;/p&gt;
&lt;h1&gt;Response variable&lt;/h1&gt;
&lt;p&gt;Roughly speaking, each model looks as follows. It is a diamond that receives an input and produces an output.&lt;/p&gt;
&lt;p&gt;&lt;img alt="simple input/output model" class="img-responsive" src="https://cavaunpeu.github.io/images/simple_input_output_model.png"/&gt;&lt;/p&gt;
&lt;p&gt;The models differ in the type of response variable they predict, i.e. the &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression predicts a continuous-valued real number. Let's call it &lt;code&gt;temperature&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Logistic regression predicts a binary label. Let's call it &lt;code&gt;cat or dog&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Softmax regression predicts a multi-class label. Let's call it &lt;code&gt;red or green or blue&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each model, the response variable can take on a bunch of different values. In other words, they are &lt;em&gt;random variables.&lt;/em&gt; What probability distribution is associated with each?&lt;/p&gt;
&lt;p&gt;Unfortunately, we don't know. All we do know, in fact, is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;temperature&lt;/code&gt; has an underlying true mean &lt;span class="math"&gt;\(\mu \in (-\infty, \infty)\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2 \in (0, \infty)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cat or dog&lt;/code&gt; takes on the value &lt;code&gt;cat&lt;/code&gt; or &lt;code&gt;dog&lt;/code&gt;. The likelihood of observing each outcome does not change over time, in the same way that &lt;span class="math"&gt;\(\Pr(\text{heads})\)&lt;/span&gt; for a fair coin is always &lt;span class="math"&gt;\(0.5\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;red or green or blue&lt;/code&gt; takes on the value &lt;code&gt;red&lt;/code&gt; or &lt;code&gt;green&lt;/code&gt; or &lt;code&gt;blue&lt;/code&gt;. The likelihood of observing each outcome does not change over time, in the same way that the probability of rolling a given number on a fair die is always &lt;span class="math"&gt;\(\frac{1}{6}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For clarity, each one of these assumptions is utterly banal — "so lacking in originality as to be obvious and boring." &lt;em&gt;Can we use them nonetheless to select probability distributions for our random variables?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Maximum entropy distributions&lt;/h2&gt;
&lt;p&gt;Consider another continuous-valued random variable: "Uber's yearly profit." Like &lt;code&gt;temperature&lt;/code&gt;, it also has an underlying true mean &lt;span class="math"&gt;\(\mu \in (-\infty, \infty)\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(\sigma^2 \in (-\infty, \infty)\)&lt;/span&gt;. Trivially, the respective means and variances will be different. Assume we observe 10 (fictional) values of each that look as follows:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;uber&lt;/th&gt;
&lt;th&gt;temperature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-100&lt;/td&gt;
&lt;td&gt;-50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-80&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-20&lt;/td&gt;
&lt;td&gt;56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-10&lt;/td&gt;
&lt;td&gt;63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;-43&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plotting, we get:&lt;/p&gt;
&lt;p&gt;&lt;img alt="temperature random variable" class="img-responsive" src="https://cavaunpeu.github.io/figures/temperature_random_variable.png"/&gt;
&lt;img alt="uber random variable" class="img-responsive" src="https://cavaunpeu.github.io/figures/uber_random_variable.png"/&gt;&lt;/p&gt;
&lt;p&gt;We are not given the true underlying probability distribution associated with each random variable — not its general "shape," nor the parameters that control this shape. We will &lt;em&gt;never&lt;/em&gt; be given these things, in fact: the point of statistics is to infer what they are.&lt;/p&gt;
&lt;p&gt;To make an initial choice we keep two things in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;We'd like to be conservative&lt;/em&gt;. We've only seen ten values of "Uber's yearly profit;" we don't want to discount the fact that the next twenty could fall into &lt;span class="math"&gt;\([-60, -50]\)&lt;/span&gt; just because they haven't yet been observed.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;We need to choose the same probability distribution "shape" for both random variables, as we've made identical assumptions for each&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As such, we'd like the most conservative distribution that obeys the "utterly banal" constraints stated above. This is the &lt;a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution"&gt;&lt;em&gt;maximum entropy distribution&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;temperature&lt;/code&gt;, the maximum entropy distribution is the &lt;a href="https://en.wikipedia.org/wiki/Normal_distribution"&gt;Gaussian distribution&lt;/a&gt;. Its probability density function is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\bigg(-\frac{(y - \mu)^2}{2\sigma^2}\bigg)}
$$&lt;/div&gt;
&lt;p&gt;For &lt;code&gt;cat or dog&lt;/code&gt;, it is the &lt;a href="https://en.wikipedia.org/wiki/Binomial_distribution"&gt;binomial distribution&lt;/a&gt;. Its probability mass function  (for a single observation) is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;(I've written the probability of the positive event as &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;, e.g. &lt;span class="math"&gt;\(\phi = .5\)&lt;/span&gt; for a fair coin.)&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;red or green or blue&lt;/code&gt;, it is the &lt;a href="https://en.wikipedia.org/wiki/Multinomial_distribution"&gt;multinomial distribution&lt;/a&gt;. Its probability mass function (for a single observation) is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
\phi_{\text{red}} &amp;amp; \text{outcome = red}\\
\phi_{\text{green}} &amp;amp; \text{outcome = green}\\
1 - \phi_{\text{red}} - \phi_{\text{green}} &amp;amp; \text{outcome = blue}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;While it may seem like we've "waved our hands" over the connection between the stated equality constraints for the response variable of each model and the respective distributions we've selected, it is &lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange multipliers&lt;/a&gt; that succinctly and algebraically bridge this gap. This &lt;a href="https://www.dsprelated.com/freebooks/sasp/Maximum_Entropy_Property_Gaussian.html"&gt;post&lt;/a&gt; gives a terrific example of this derivation. I've chosen to omit it as I did not feel it would contribute to the clarity nor direction of this post.&lt;/p&gt;
&lt;p&gt;Finally, while we do assume that a Gaussian dictates the true distribution of values of both "Uber's yearly profit" and &lt;code&gt;temperature&lt;/code&gt;, it is, trivially, a different Gaussian for each. This is because each random variable has its own true underlying mean and variance. These values make the respective Gaussians taller or wider — shifted left or shifted right.&lt;/p&gt;
&lt;h1&gt;Functional form&lt;/h1&gt;
&lt;p&gt;Our three protagonists generate predictions via distinct functions: the &lt;a href="https://en.wikipedia.org/wiki/Identity_function"&gt;identity function&lt;/a&gt; (i.e. a no-op), the &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid function&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax function&lt;/a&gt;, respectively. The Keras output layers make this clear:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this section, I'd like to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Show how each of the Gaussian, binomial and multinomial distributions can be reduced to the same functional form.&lt;/li&gt;
&lt;li&gt;Show how this common functional form allows us to naturally derive the output functions for our three protagonist models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graphically, this looks as follows, with three distributions going in and three output functions coming out.&lt;/p&gt;
&lt;p&gt;&lt;img alt="bottleneck" class="img-responsive" src="https://electric-cloud.com/wp-content/uploads/use-case-graphic_bottleneck.png"/&gt;&lt;/p&gt;
&lt;p&gt;The conceptual bottleneck is the &lt;a href="https://en.wikipedia.org/wiki/Exponential_family"&gt;"exponential family"&lt;/a&gt; of probability distributions.&lt;/p&gt;
&lt;h2&gt;Exponential family distributions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In probability and statistics, an exponential family is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, on account of some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Wikipedia&lt;/p&gt;
&lt;p&gt;I don't relish quoting this paragraph — and especially one so deliriously ambiguous. This said, the reality is that exponential functions provide, at a minimum, a unifying framework for deriving the canonical activation and loss functions we've come to know and love. To move forward, we simply have to cede that the "mathematical conveniences, on account of some useful algebraic properties, etc." that motivate this "certain form" are not totally heinous nor misguided.&lt;/p&gt;
&lt;p&gt;A distribution belongs to the exponential family if it can be written in the following form:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y; \eta) = b(y)\exp(\eta^T T(y) - a(\eta))
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta\)&lt;/span&gt; is the &lt;em&gt;canonical parameter&lt;/em&gt; of the distribution. (We will hereby work with the single-canonical-parameter exponential family form.)&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y)\)&lt;/span&gt; is the &lt;em&gt;sufficient statistic&lt;/em&gt;. It is often the case that &lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; is the &lt;em&gt;log partition function&lt;/em&gt;, which normalizes the distribution. (A more in-depth discussion of this normalizing constant can be found in a previous post of mine: &lt;a href="https://cavaunpeu.github.io/machine-learning/deriving-the-softmax-from-first-principles.md"&gt;Deriving the Softmax from First Principles&lt;/a&gt;.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;"A fixed choice of &lt;span class="math"&gt;\(T\)&lt;/span&gt;, &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; defines a family (or set) of distributions that is parameterized by &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;; as we vary &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;, we then get different distributions within this family."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; This simply means that a coin with &lt;span class="math"&gt;\(\Pr(\text{heads}) = .6\)&lt;/span&gt; gives a different distribution over outcomes than one with &lt;span class="math"&gt;\(\Pr(\text{heads}) = .7\)&lt;/span&gt;. Easy.&lt;/p&gt;
&lt;h3&gt;Gaussian distribution&lt;/h3&gt;
&lt;p&gt;Since we're working with the single-parameter form, we'll assume that &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; is known and equals &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert \mu, \sigma^2)
&amp;amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\bigg(-\frac{(y - \mu)^2}{2\sigma^2}\bigg)}\\
&amp;amp;= \frac{1}{\sqrt{2\pi}}\exp{\bigg(-\frac{(y - \mu)^2}{2}\bigg)}\\
&amp;amp;= \frac{1}{\sqrt{2\pi}}\exp{\bigg(-\frac{1}{2}(y^2 - 2\mu y + \mu^2)\bigg)}\\
&amp;amp;= \frac{1}{\sqrt{2\pi}}\exp{\bigg(-\frac{1}{2}y^2\bigg)} \cdot \exp{\bigg(\mu y - \frac{1}{2}\mu^2\bigg)}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta = \mu\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta) = \frac{1}{2}\mu^2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(b(y) = \frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}y^2)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we'll express &lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; itself:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
a(\eta)
&amp;amp;= \frac{1}{2}\mu^2\\
&amp;amp;= \frac{1}{2}\eta^2
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Binomial distribution&lt;/h3&gt;
&lt;p&gt;We previously defined the binomial distribution (for a single observation) in a crude, piecewise form. We'll now define it in a more compact form which will make it easier to show that it is a member of the exponential family. Again, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; gives the probability of observing the true class, i.e. &lt;span class="math"&gt;\(\Pr(\text{cat}) = .7 \implies \phi = .3\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert \phi)
&amp;amp;= \phi^y(1-\phi)^{1-y}\\
&amp;amp;= \exp\bigg(\log\bigg(\phi^y(1-\phi)^{1-y}\bigg)\bigg)\\
&amp;amp;= \exp\bigg(y\log{\phi} + \log(1-\phi) - y\log(1-\phi)\bigg)\\
&amp;amp;= \exp\bigg(\log\bigg(\frac{\phi}{1-\phi}\bigg)y + \log(1-\phi)\bigg) \\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta = \log\bigg(\frac{\phi}{1-\phi}\bigg)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta) = -\log(1-\phi)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(b(y) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we'll express &lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;, i.e. the parameter that this distribution accepts:&lt;/p&gt;
&lt;div class="math"&gt;$$
\eta = \log\bigg(\frac{\phi}{1-\phi}\bigg) \implies \phi = \frac{1}{1 + e^{-\eta}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
a(\eta)
&amp;amp;= -\log(1-\phi)\\
&amp;amp;= -\log\bigg(1-\frac{1}{1 + e^{-\eta}}\bigg)\\
&amp;amp;= -\log\bigg(\frac{1}{1 + e^{\eta}}\bigg)\\
&amp;amp;= \log(1 + e^{\eta})\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;You will recognize our expression for &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; — the probability of observing the true class — as the sigmoid function.&lt;/p&gt;
&lt;h3&gt;Multinomial distribution&lt;/h3&gt;
&lt;p&gt;Like the binomial distribution, we'll first rewrite the multinomial (for a single observation) in a more compact form. &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; gives a vector of class probabilities for the &lt;span class="math"&gt;\(K\)&lt;/span&gt; classes; &lt;span class="math"&gt;\(k\)&lt;/span&gt; denotes one of these classes.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \pi) = \prod\limits_{k=1}^{K}\pi_k^{y_k}
$$&lt;/div&gt;
&lt;p&gt;This is almost pedantic: it says that &lt;span class="math"&gt;\(\Pr(y=k)\)&lt;/span&gt; equals the probability of observing class &lt;span class="math"&gt;\(k\)&lt;/span&gt;. For example, given&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;we would compute:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\Pr(y = \text{snow} = [0, 1, 0, 0])
&amp;amp;= (.14^0 * .37^1 * .03^0 * .46^0)\\
&amp;amp;= .37\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Expanding into the exponential family form gives:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y\vert \pi)
&amp;amp;= \prod\limits_{k=1}^{K}\pi_k^{y_k}\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K}y_k\log{\pi_k}\bigg)\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K-1}y_k\log{\pi_k} + \bigg(1 - \sum\limits_{k=1}^{K-1}y_k\bigg)\log\bigg(1 - \sum\limits_{k=1}^{K-1}\pi_k\bigg)\bigg)\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K-1}y_k\log{\pi_k} - \bigg(\sum\limits_{k=1}^{K-1}y_k\bigg) \log(\pi_K) + \log(\pi_K)), \quad \text{where}\ \pi_K = 1 - \sum\limits_{k=1}^{K-1}\pi_k\\
&amp;amp;= \exp\bigg(\sum\limits_{k=1}^{K-1}\log\bigg(\frac{\pi_k}{\pi_K}\bigg) y_k + \log(\pi_K)\bigg)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta_k = \log\bigg(\frac{\pi_k}{\pi_K}\bigg)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y) = y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(a(\eta) = -\log(\pi_K)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(b(y) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we'll express &lt;span class="math"&gt;\(a(\eta)\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;, i.e. the parameter that this distribution accepts:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\eta_k
  &amp;amp;= \log\bigg(\frac{\pi_k}{\pi_K}\bigg) \implies\\
\frac{\pi_k}{\pi_K}
  &amp;amp;= e^{\eta_k} \implies\\
\sum\limits_{k=1}^K \frac{\pi_k}{\pi_K}
  &amp;amp;= \sum\limits_{k=1}^K e^{\eta_k} \implies\\
\frac{1}{\pi_K}\sum\limits_{k=1}^K \pi_k
  &amp;amp;= \sum\limits_{k=1}^K e^{\eta_k} \implies\\
\frac{1}{\pi_K} \cdot 1
  &amp;amp;= \sum\limits_{k=1}^K e^{\eta_k} \implies\\
\pi_K
  &amp;amp;= \frac{1}{\sum\limits_{k=1}^K e^{\eta_k}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Plugging back into the second line we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\pi_k}{\frac{1}{\sum\limits_{k=1}^K e^{\eta_k}}}
  &amp;amp;= e^{\eta_k}\ \implies\\
\pi_k
  &amp;amp;= \frac{e^{\eta_k}}{\sum\limits_{k=1}^K e^{\eta_k}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This you will recognize as the softmax function. (For a probabilistically-motivated derivation, please see a previous &lt;a href="https://cavaunpeu.github.io/machine-learning/deriving-the-softmax-from-first-principles.md"&gt;post&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Finally:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\pi_k}{\pi_K}
  &amp;amp;= e^{\eta_k} \implies\\
\frac{\pi_K}{\pi_K}
  &amp;amp;= e^{\eta_K} \implies\\
\eta_K &amp;amp;= 0\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
a(\eta)
&amp;amp;= -\log(\pi_K)\\
&amp;amp;= \log(\pi_K^{-1})\\
&amp;amp;= \log\Bigg(\frac{\sum\limits_{k=1}^K e^{\eta_k}}{e^{\eta_K}}\Bigg)\\
&amp;amp;= \log\Bigg(\sum\limits_{k=1}^K e^{\eta_k}\Bigg)\\
\end{align*}
$$&lt;/div&gt;
&lt;h2&gt;Generalized linear models&lt;/h2&gt;
&lt;p&gt;Each protagonist model outputs a response variable that is distributed according to some (exponential family) distribution. However, the &lt;em&gt;canonical parameter&lt;/em&gt; of this distribution, i.e. the thing we pass in, will &lt;em&gt;vary per observation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Consider the logistic regression model that's predicting &lt;code&gt;cat or dog&lt;/code&gt;. If we input a picture of a cat, we'll output "cat" according to the stated distribution.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;If we input a picture of a dog, we'll output "dog" according the same distribution.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Trivially, the &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; value must be different in each case. In the former, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; should be small, such that we output "cat" with probability &lt;span class="math"&gt;\(1 - \phi \approx 1\)&lt;/span&gt;. In the latter, &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; should be large, such that we output "dog" with probability &lt;span class="math"&gt;\(\phi \approx 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, what dictates the following?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; in the case of linear regression, in which &lt;span class="math"&gt;\(y_i \sim \mathcal{N}(\mu_i, \sigma^2)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; in the case of logistic regression, in which &lt;span class="math"&gt;\(y_i \sim \text{Binomial}(\phi_i, 1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\pi_i\)&lt;/span&gt; in the case of softmax regression, in which &lt;span class="math"&gt;\(y_i \sim \text{Multinomial}(\pi_i, 1)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, I've introduced the subscript &lt;span class="math"&gt;\(i\)&lt;/span&gt;. This makes explicit the &lt;code&gt;cat or dog&lt;/code&gt; dynamic from above: each input to a given model will result in its &lt;em&gt;own&lt;/em&gt; canonical parameter being passed to the distribution on the response variable. That logistic regression better make &lt;span class="math"&gt;\(\phi_i \approx 0\)&lt;/span&gt; when looking at a picture of a cat.&lt;/p&gt;
&lt;p&gt;Finally, how do we go from a 10-feature input &lt;span class="math"&gt;\(x\)&lt;/span&gt; to this canonical parameter? We take a linear combination:&lt;/p&gt;
&lt;div class="math"&gt;$$
\eta = \theta^Tx
$$&lt;/div&gt;
&lt;h3&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\eta = \theta^Tx = \mu_i\)&lt;/span&gt;. This is what we need for the normal distribution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; The identity function (i.e a no-op) gives us the mean of the response variable. This mean is required by the normal distribution, which dictates the outcomes of the continuous-valued target &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Logistic regression&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\eta = \theta^Tx = \log\bigg(\frac{\phi_i}{1-\phi_i}\bigg)\)&lt;/span&gt;. To solve for &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;, we solve for &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As you'll remember we did this above: &lt;span class="math"&gt;\(\phi_i = \frac{1}{1 + e^{-\eta}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; The sigmoid function gives us the probability that the response variable takes on the positive class. This probability is required by the binomial distribution, which dictates the outcomes of the binary target &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It does this in the same way that our weather distribution dictates tomorrow's forecast.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'rain'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'snow'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'sleet'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'hail'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Softmax regression&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\eta = \theta^Tx = \log\bigg(\frac{\pi_k}{\pi_K}\bigg)\)&lt;/span&gt;. To solve for &lt;span class="math"&gt;\(\pi_i\)&lt;/span&gt;, i.e. the full vector of probabilities for observation &lt;span class="math"&gt;\(i\)&lt;/span&gt;, we solve for each individual probability &lt;span class="math"&gt;\(\pi_{k, i}\)&lt;/span&gt; then put them in a list.&lt;/p&gt;
&lt;p&gt;We did this above as well: &lt;span class="math"&gt;\(\pi_{k, i} = \frac{e^{\eta_k}}{\sum\limits_{k=1}^K e^{\eta_k}}\)&lt;/span&gt;. This is the softmax function.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; The softmax function gives us the probability that the response variable takes on each of the possible classes. This probability mass function is required by the multinomial distribution, which dictates the outcomes of the multi-class target &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, why a linear model, i.e. why &lt;span class="math"&gt;\(\eta = \theta^Tx\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Andrew Ng calls it a "design choice."&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; I've motivated this formulation a bit in the &lt;a href="https://cavaunpeu.github.io/machine-learning/deriving-the-softmax-from-first-principles.md"&gt;softmax post&lt;/a&gt;. mathematicalmonk&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; would probably have a more principled explanation than us both. For now, we'll make do with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A linear combination is perhaps the simplest way to consider the impact of each feature on the canonical parameter.&lt;/li&gt;
&lt;li&gt;A linear combination commands that either &lt;span class="math"&gt;\(x\)&lt;/span&gt;, or a &lt;em&gt;function of &lt;span class="math"&gt;\(x\)&lt;/span&gt;&lt;/em&gt;, vary linearly with &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;. As such, we could write our model as &lt;span class="math"&gt;\(\eta = \theta^T\Phi(x)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; applies some complex transformation to our features. This makes the "simplicity" of the linear combination less simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Loss function&lt;/h1&gt;
&lt;p&gt;We've now discussed how each response variable is generated, and how we compute the parameters for those distributions on a per-observation basis. Now, how do we quantify how good these parameters are?&lt;/p&gt;
&lt;p&gt;To get us started, let's go back to predicting &lt;code&gt;cat or dog&lt;/code&gt;. If we input a picture of a cat, we should compute &lt;span class="math"&gt;\(\phi \approx 0\)&lt;/span&gt; given our binomial distribution.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(\text{outcome}) =
\begin{cases}
1 - \phi &amp;amp; \text{outcome = cat}\\
\phi &amp;amp; \text{outcome = dog}\\
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;A perfect computation gives &lt;span class="math"&gt;\(\phi = 0\)&lt;/span&gt;. The loss function quantifies how close we got.&lt;/p&gt;
&lt;h2&gt;Maximum likelihood estimation&lt;/h2&gt;
&lt;p&gt;Each of our three distributions receives a parameter — &lt;span class="math"&gt;\(\mu, \phi\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; respectively. We then pass in a &lt;span class="math"&gt;\(y\)&lt;/span&gt; and the distribution tells us the probability of observing that value. (In the case of continuous-valued random variables, i.e. our distribution is a probability density function, it tells us a value &lt;em&gt;proportional&lt;/em&gt; to this probability.)&lt;/p&gt;
&lt;p&gt;If we instead &lt;em&gt;fix&lt;/em&gt; &lt;span class="math"&gt;\(y\)&lt;/span&gt; and pass in varying &lt;em&gt;parameter values&lt;/em&gt;, this same function becomes a &lt;em&gt;likelihood function&lt;/em&gt;. It will tell us the likelihood of a given parameter having produced the now-fixed &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If this is not clear, consider the following example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Moroccan walks into a bar. He's wearing a football jersey that's missing a sleeve. He has a black eye, and blood on his jeans. How did he most likely spend his day?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;At home, reading a book.&lt;/li&gt;
&lt;li&gt;Training for a bicycle race.&lt;/li&gt;
&lt;li&gt;At the soccer game drinking beers with his friends — all of whom are MMA fighters that despise the other team.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;We'd like to pick the parameter that most likely gave rise to our data. This is the &lt;em&gt;maximum likelihood estimate&lt;/em&gt;. Mathematically, we define it as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{\text{parameter}}{\arg\max}\ P(y\vert \text{parameter})
$$&lt;/div&gt;
&lt;p&gt;As we've now seen (ad nauseum), &lt;span class="math"&gt;\(y\)&lt;/span&gt; depends on the parameter its distribution receives. Additionally, this parameter — &lt;span class="math"&gt;\(\mu, \phi\)&lt;/span&gt; or &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; — is defined in terms of &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;. Further, &lt;span class="math"&gt;\(\eta = \theta^T x\)&lt;/span&gt;. As such, &lt;span class="math"&gt;\(y\)&lt;/span&gt; is a function of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; and the observed data &lt;span class="math"&gt;\(x\)&lt;/span&gt;. This is perhaps &lt;em&gt;the&lt;/em&gt; elementary truism of machine learning — you've known this since Day 1.&lt;/p&gt;
&lt;p&gt;Since our observed data are fixed, &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is the only thing that we can vary. Let's rewrite our argmax in these terms:&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\max}\ P(y\vert x; \theta)
$$&lt;/div&gt;
&lt;p&gt;Finally, this expression gives the argmax over a single data point, i.e. training observation, &lt;span class="math"&gt;\((x^{(i)}, y^{(i)})\)&lt;/span&gt;. To give the likelihood over all observations (assuming they are independent of one another, i.e. the outcome of the first observation should not be expected to impact that of the third), we take the product.&lt;/p&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\max} \prod\limits_{i=1}^{m}P(y^{(i)}\vert x^{(i)}; \theta)
$$&lt;/div&gt;
&lt;p&gt;The product of numbers in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt; gets very small, very quickly. Let's maximize the log-likelihood instead so we can work with sums.&lt;/p&gt;
&lt;h3&gt;Linear regression&lt;/h3&gt;
&lt;p&gt;Maximize the log-likelihood of the Gaussian distribution. Remember, &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; assemble to give &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\theta^Tx = \mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{P(y\vert x; \theta)}
&amp;amp;= \log{\prod\limits_{i=1}^{m}P(y^{(i)}\vert x^{(i)}; \theta)}\\
&amp;amp;= \sum\limits_{i=1}^{m}\log{P(y^{(i)}\vert x^{(i)}; \theta)}\\
&amp;amp;= \sum\limits_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}\exp{\bigg(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\bigg)}}\\
&amp;amp;= \sum\limits_{i=1}^{m}\log{\frac{1}{\sqrt{2\pi}\sigma}} + \sum\limits_{i=1}^{m}\log\Bigg(\exp{\bigg(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\bigg)}\Bigg)\\
&amp;amp;= m\log{\frac{1}{\sqrt{2\pi}\sigma}} - \frac{1}{2\sigma^2}\sum\limits_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2\\
&amp;amp;= C_1 - C_2\sum\limits_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Maximizing the log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to maximizing the negative mean squared error between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction thereof.&lt;/p&gt;
&lt;p&gt;Notwithstanding, most optimization routines &lt;em&gt;minimize&lt;/em&gt;. So, for practical purposes, we go the other way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the mean squared error between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction thereof.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Logistic regression&lt;/h3&gt;
&lt;p&gt;Same thing.&lt;/p&gt;
&lt;p&gt;Negative log-likelihood:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
-\log{P(y\vert x; \theta)}
&amp;amp;= -\log{\prod\limits_{i = 1}^m(\phi^{(i)})^{y^{(i)}}(1 - \phi^{(i)})^{1 - y^{(i)}}}\\
&amp;amp;= -\sum\limits_{i = 1}^m\log{\bigg((\phi^{(i)})^{y^{(i)}}(1 - \phi^{(i)})^{1 - y^{(i)}}\bigg)}\\
&amp;amp;= -\sum\limits_{i = 1}^my^{(i)}\log{(\phi^{(i)})} + (1 - y^{(i)})\log{(1 - \phi^{(i)})}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the binary cross-entropy (i.e. binary log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability thereof.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Multinomial distribution&lt;/h3&gt;
&lt;p&gt;Negative log-likelihood:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
-\log{P(y\vert x; \theta)}
&amp;amp;= -\log\prod\limits_{i=1}^{m}\prod\limits_{k=1}^{K}\pi_k^{y_k}\\
&amp;amp;= -\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_k\log\pi_k\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the categorical cross-entropy (i.e. multi-class log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability distribution thereof.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a href="https://en.wikipedia.org/wiki/Cross_entropy"&gt;Cross entropy&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;We previously defined entropy as a way to quantify the uncertainty inherent in a probability distribution. Next, we'll use this same notion to quantify the uncertainty inherent in using the probabilities &lt;em&gt;in one distribution to predict the events of another.&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'red'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'blue'&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'red'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'blue'&lt;/span&gt;&lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="math"&gt;$$
H(p, q) = -\sum_i p_i\log(q_i)
$$&lt;/div&gt;
&lt;p&gt;This is the definition of cross entropy.&lt;/p&gt;
&lt;h4&gt;&lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL-Divergence&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Similarly, the Kullback-Leibler Divergence quantifies the &lt;em&gt;additional uncertainty in &lt;span class="math"&gt;\(p\)&lt;/span&gt;&lt;/em&gt; introduced by using &lt;span class="math"&gt;\(q\)&lt;/span&gt; to approximate &lt;span class="math"&gt;\(p\)&lt;/span&gt;. It is given, almost trivially, as:&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p, q) = H(p, q) - H(p)
$$&lt;/div&gt;
&lt;p&gt;Why don't we use this in machine learning models instead of the cross entropy? The KL-Divergence between distributions requires us to know the true, underlying probabilities of both the actual distribution &lt;span class="math"&gt;\(p\)&lt;/span&gt; and our prediction thereof. Unfortunately, we never have the former: that's why we build the model.&lt;/p&gt;
&lt;h1&gt;Maximum a posteriori estimation&lt;/h1&gt;
&lt;p&gt;When estimating &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; via the MLE, we put no constraints on the permissible values thereof. More explicitly, we allow &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to be &lt;em&gt;equally likely to assume any real number&lt;/em&gt; — be it &lt;span class="math"&gt;\(0\)&lt;/span&gt;, or &lt;span class="math"&gt;\(10\)&lt;/span&gt;, or &lt;span class="math"&gt;\(-20\)&lt;/span&gt;, or &lt;span class="math"&gt;\(2.37 \times 10^{36}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In practice, this assumption is both unrealistic and superfluous: typically, we do wish to constrain &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; (our weights) to a non-infinite range of values. We do this by putting a &lt;em&gt;prior&lt;/em&gt; on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Whereas the MLE computes &lt;span class="math"&gt;\(\underset{\theta}{\arg\max}\ P(y\vert x; \theta)\)&lt;/span&gt;, the maximum a posteriori estimate, or MAP, computes &lt;span class="math"&gt;\(\underset{\theta}{\arg\max}\ P(y\vert x; \theta)P(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As before, we start by taking the log. Our joint likelihood with prior now reads:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\theta_{MAP}
&amp;amp;= \underset{\theta}{\arg\max}\ \log \prod\limits_{i=1}^{m} P(y^{(i)}\vert x^{(i)}; \theta)P(\theta)\\
&amp;amp;= \underset{\theta}{\arg\max}\ \sum\limits_{i=1}^{m} \log{P(y^{(i)}\vert x^{(i)}; \theta)} + \log{P(\theta)}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;We dealt with the left term in the previous section. Now, we'll simply tack on the log-prior to the respective log-likelihoods.&lt;/p&gt;
&lt;p&gt;As every element of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is a continuous-valued real number, let's assign it a Gaussian distribution with mean 0 and variance &lt;span class="math"&gt;\(V\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta \sim \mathcal{N}(0, V)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{P(\theta\vert 0, V)}
&amp;amp;= \log\Bigg(\frac{1}{\sqrt{2\pi}V}\exp{\bigg(-\frac{(\theta - 0)^2}{2V^2}\bigg)}\Bigg)\\
&amp;amp;= \log{C_1} -\frac{\theta^2}{2V^2}\\
&amp;amp;= \log{C_1} - C_2\theta^2\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our goal is to maximize this term plus the log-likelihood — or equivalently, minimize their opposite — with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. For a final step, let's discard the parts that don't include &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\log{C_1} - C_2\theta^2
&amp;amp;\propto - C_2\theta^2\\
&amp;amp;\propto C\Vert \theta\Vert_{2}^{2}\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;This is L2 regularization. Furthermore, placing different prior distributions on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; yields different regularization terms; most notably, a &lt;a href="https://en.wikipedia.org/wiki/Laplace_distribution"&gt;Laplace prior&lt;/a&gt; gives the L1.&lt;/p&gt;
&lt;h2&gt;Linear regression&lt;/h2&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\min} \sum\limits_{i=1}^{m}(y^{(i)} - \theta^Tx^{(i)})^2 + C\Vert \theta\Vert_{2}^{2}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given a Gaussian prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the mean squared error between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction thereof, plus the squared sum of (the elements of) &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;div class="math"&gt;$$
\underset{\theta}{\arg\min}
-\sum\limits_{i = 1}^my^{(i)}\log{(\phi^{(i)})} + (1 - y^{(i)})\log{(1 - \phi^{(i)})} + C\Vert \theta\Vert_{2}^{2}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given a Gaussian prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the binary cross-entropy (i.e. binary log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability thereof, plus the squared sum of (the elements of) &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;div class="math"&gt;$$
-\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_k\log\pi_k + C\Vert \theta\Vert_{2}^{2}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Minimizing the negative log-likelihood of our data with respect to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; given a Gaussian prior on &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is equivalent to minimizing the categorical cross-entropy (i.e. multi-class log loss) between the observed &lt;span class="math"&gt;\(y\)&lt;/span&gt; and our prediction of the probability distribution thereof, plus the squared sum of (the elements of) &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; itself.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, in machine learning, we say that regularizing our weights ensures that "no weight becomes too large," i.e. too "influential" in predicting &lt;span class="math"&gt;\(y\)&lt;/span&gt;. In statistical terms, we can equivalently say that this term &lt;em&gt;restricts the permissible values of these weights to a given interval. Furthermore, this interval is dictated by the scaling constant &lt;span class="math"&gt;\(C\)&lt;/span&gt;, which intrinsically parameterizes the prior distribution itself.&lt;/em&gt; In L2 regularization, this scaling constant gives the variance of the Gaussian.&lt;/p&gt;
&lt;h1&gt;Going fully Bayesian&lt;/h1&gt;
&lt;p&gt;The key goal of a predictive model is to compute the following distribution:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x, D) = \int P(y\vert x, D, \theta)P(\theta\vert x, D)d\theta
$$&lt;/div&gt;
&lt;p&gt;By term, this reads:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(y\vert x, D)\)&lt;/span&gt;: given historical data &lt;span class="math"&gt;\(D = ((x^{(i)}, y^{(i)}), ..., (x^{(m)}, y^{(m)}))\)&lt;/span&gt;, i.e. some training data, and a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, compute the distribution of the possible values of the response &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;In machine learning, we typically select the &lt;em&gt;expected&lt;/em&gt; value of that distribution, i.e. a single value, or point estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(y\vert x, D, \theta)\)&lt;/span&gt;: given historical data &lt;span class="math"&gt;\(D\)&lt;/span&gt;, a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;em&gt;any plausible value of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/em&gt;, i.e. perhaps not the optimal value, compute &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;This is given by the functional form of the model in question, i.e. &lt;span class="math"&gt;\(y = \theta^Tx\)&lt;/span&gt; in the case of linear regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(\theta\vert x, D)\)&lt;/span&gt;: given historical data &lt;span class="math"&gt;\(D\)&lt;/span&gt; and a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, compute the distribution of the values of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; that plausibly gave rise to our data.&lt;ul&gt;
&lt;li&gt;The &lt;span class="math"&gt;\(x\)&lt;/span&gt; plays no part; it's simply there such that the expression under the integral factors correctly.&lt;/li&gt;
&lt;li&gt;In machine learning, we typically select the MLE or MAP estimate of that distribution, i.e. a single value, or point estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a perfect world, we'd do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute the &lt;em&gt;full distribution&lt;/em&gt; over &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;With each value in this distribution and a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt;, compute &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;ul&gt;
&lt;li&gt;NB: &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is an object which contains all of our weights. In 10-feature linear regression, it will have 10 elements. In a neural network, it could have millions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We now have a &lt;em&gt;full distribution&lt;/em&gt; over the possible values of the response &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&amp;gt; Instead of a point estimate for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, and a point estimate for &lt;span class="math"&gt;\(y\)&lt;/span&gt; given a new observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; (which makes use of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;), we have distributions for each&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, in complex systems with a non-trivial functional form and number of weights, this computation becomes intractably large. As such, in fully Bayesian modeling, we approximate these distributions. In classic machine learning, we assign them a single value (point estimate). It's a bit lazy, really.&lt;/p&gt;
&lt;p&gt;&lt;img alt="@betanalpha bayesian tweet" class="img-responsive" src="https://cavaunpeu.github.io/images/going_fully_bayesian.png"/&gt;&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;I hope this post serves as useful context for the machine learning models we know and love. A deeper understanding of these algorithms offers humility — the knowledge that none of these concepts are particularly new — as well as a vision for how to extend these algorithms in the direction of robustness and increased expressivity.&lt;/p&gt;
&lt;p&gt;Thanks so much for reading this far. Now, climb out of the pool, grab a towel and &lt;code&gt;import sklearn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="drink and towel" class="img-responsive" src="https://www.washingtonian.com/wp-content/uploads/2015/05/Pool520-994x664.jpg"/&gt;&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;p&gt;I recently gave a talk on this topic at &lt;a href="https://www.facebook.com/groups/265793323822652"&gt;Facebook Developer Circle: Casablanca&lt;/a&gt;. Voilà the:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/WilliamWolfDataScien/youve-been-doing-statistics-all-along"&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/aboullaite.mohammed/videos/1959648697600819/"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://cs229.stanford.edu/materials.html"&gt;CS229 Machine Learning Course Materials, Lecture Notes 1&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA"&gt;mathematical monk - Machine Learning&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Transfer Learning for Flight Delay Prediction via Variational Autoencoders</title><link href="https://cavaunpeu.github.io/2017/05/08/transfer-learning-flight-delay-prediction/" rel="alternate"></link><published>2017-05-08T12:45:00-04:00</published><updated>2017-05-08T12:45:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-05-08:/2017/05/08/transfer-learning-flight-delay-prediction/</id><summary type="html">&lt;p&gt;Autoencoding airports via variational autoencoders to improve flight delay prediction. Additionally, a principled look at variational inference itself and its connections to machine learning.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this work, we explore improving a vanilla regression model with knowledge learned elsewhere. As a motivating example, consider the task of predicting the number of checkins a given user will make at a given location. Our training data consist of checkins from 4 users across 4 locations in the week of May 1st, 2017 and looks as follows:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;user_id&lt;/th&gt;
&lt;th&gt;location&lt;/th&gt;
&lt;th&gt;checkins&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;c&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We'd like to predict how many checkins user 3 will make at location &lt;code&gt;b&lt;/code&gt; in the coming week. How well will our model do?&lt;/p&gt;
&lt;p&gt;While each &lt;code&gt;user_id&lt;/code&gt; might represent some unique behavior - e.g. user &lt;code&gt;3&lt;/code&gt; sleeps late yet likes going out for dinner - and each location might represent its basic characteristics - e.g. location &lt;code&gt;b&lt;/code&gt; is an open-late sushi bar - this is currently unbeknownst to our model. To this end, gathering this metadata and joining it to our training set is a clear option. If quality, thorough, explicit metadata are available, affordable and practical to acquire, this is likely the path to pursue. If not, we'll need to explore a more creative approach. How far can we get with &lt;em&gt;implicit&lt;/em&gt; metadata learned from an external task?&lt;/p&gt;
&lt;h2&gt;Transfer learning&lt;/h2&gt;
&lt;p&gt;Transfer learning allows us to use knowledge acquired in one task to improve performance in another. Suppose, for example, that we've been tasked with translating Portuguese to English and are given a basic phrasebook from which to learn. After a week, we take a lengthy test. A friend of ours - a fluent Spanish speaker who knows nothing of Portuguese - is tasked the same. Who gets a better score?&lt;/p&gt;
&lt;h2&gt;Predicting flight delays&lt;/h2&gt;
&lt;p&gt;The goal of this work is to predict flight delays - a basic regression task. The data comprise 6,872,294 flights from 2008 via the &lt;a href="https://www.transportation.gov/"&gt;United States Department of Transportation's&lt;/a&gt; &lt;a href="https://www.bts.gov/"&gt;Bureau of Transportation Statistics&lt;/a&gt;. I downloaded them from &lt;a href="http://stat-computing.org/dataexpo/2009/the-data.html"&gt;stat-computing.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Each row consists of, among other things: &lt;code&gt;DayOfWeek&lt;/code&gt;, &lt;code&gt;DayofMonth&lt;/code&gt;, &lt;code&gt;Month&lt;/code&gt;, &lt;code&gt;ScheduledDepTimestamp&lt;/code&gt; (munged from &lt;code&gt;CRSDepTime&lt;/code&gt;), &lt;code&gt;Origin&lt;/code&gt;, &lt;code&gt;Dest&lt;/code&gt; and &lt;code&gt;UniqueCarrier&lt;/code&gt; (airline), and well as &lt;code&gt;CarrierDelay&lt;/code&gt;, &lt;code&gt;WeatherDelay&lt;/code&gt;, &lt;code&gt;NASDelay&lt;/code&gt;, &lt;code&gt;SecurityDelay&lt;/code&gt;, &lt;code&gt;LateAircraftDelay&lt;/code&gt; - all in minutes - which we will sum to create &lt;code&gt;total_delay&lt;/code&gt;. We'll consider a random sample of 50,000 flights to make things easier. (For a more in-depth exploration of these data, please see this project's &lt;a href="http://github.com/cavaunpeu/transfer-learning-for-flight-prediction/explore.R"&gt;repository&lt;/a&gt;.)&lt;/p&gt;
&lt;h2&gt;Routes, airports&lt;/h2&gt;
&lt;p&gt;While we can expect &lt;code&gt;DayOfWeek&lt;/code&gt;, &lt;code&gt;DayofMonth&lt;/code&gt; and &lt;code&gt;Month&lt;/code&gt; to give some seasonal delay trends - delays are likely higher on Sundays or Christmas, for example - the &lt;code&gt;Origin&lt;/code&gt; and &lt;code&gt;Dest&lt;/code&gt; columns might suffer from the same pathology as &lt;code&gt;user_id&lt;/code&gt; and &lt;code&gt;location&lt;/code&gt; above: a rich behavioral indicator represented in a crude, "isolated" way. (A token in a bag-of-words model, as opposed to its respective word2vec representation, gives a clear analogy.) How can we infuse this behavioral knowledge into our original task?&lt;/p&gt;
&lt;h2&gt;An auxiliary task&lt;/h2&gt;
&lt;p&gt;In 2015, I read a particularly-memorable blog post entitled &lt;a href="http://allentran.github.io/graph2vec"&gt;Towards Anything2Vec&lt;/a&gt; by Allen Tran. Therein, Allen states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Like pretty much everyone, I'm obsessed with word embeddings word2vec or GloVe. Although most of machine learning in general is based on turning things into vectors, it got me thinking that we should probably be learning more fundamental representations for objects, rather than hand tuning features. Here is my attempt at turning random things into vectors, starting with graphs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this post, Allen seeks to embed nodes - U.S. patents, incidentally - in a directed graph into vector space by predicting the inverse of the path-length to nodes nearby. To me, this (thus-far) epitomizes the "data describe the individual better than they describe themself:" while we could ask the nodes to self-classify into patents on "computing," "pharma," "materials," etc., the connections between these nodes - formal citations, incidentally - will capture their "true" subject matters (and similarities therein) better than the authors ever could. Formal language, necessarily, generalizes.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://openflights.org/data.html"&gt;OpenFlights&lt;/a&gt; contains data for over "10,000 airports, train stations and ferry terminals spanning the globe" and the routes between. My goal is to train a neural network that, given an origin airport and its latitude and longitude, predicts the destination airport, latitude and longitude. This network will thereby "encode" each airport into a vector of arbitrary size containing rich information about, presumably, the diversity and geography of the destinations it services: its "place" in the global air network. Surely, a global hub like Heathrow - a fact presumably known to our neural network, yet unknown to our initial dataset with one-hot airport indices - has longer delays on Christmas than than a two-plane airstrip in Alaska.&lt;/p&gt;
&lt;p&gt;Crucially, we note that while our original (down-sampled) dataset contains delays amongst 298 unique airports, our auxiliary &lt;code&gt;routes&lt;/code&gt; dataset comprises flights amongst 3186 unique airports. Notwithstanding, information about &lt;em&gt;all&lt;/em&gt; airports in the latter is &lt;em&gt;distilled&lt;/em&gt; into vector representations then injected into the former; even though we might not know about delays to/from Casablanca Mohammed V Airport (CMN), latent information about this airport will still be &lt;em&gt;intrinsically considered&lt;/em&gt; when predicting delays between other airports to/from which CMN flies.&lt;/p&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;p&gt;Our flight-delay design matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; will include the following columns: &lt;code&gt;DayOfWeek&lt;/code&gt;, &lt;code&gt;DayofMonth&lt;/code&gt;, &lt;code&gt;Month&lt;/code&gt;, &lt;code&gt;ScheduledDepTimestamp&lt;/code&gt;, &lt;code&gt;Origin&lt;/code&gt;, &lt;code&gt;Dest&lt;/code&gt; and &lt;code&gt;UniqueCarrier&lt;/code&gt;. All columns will be one-hotted for simplicity. (Alternatively, I explored mapping each column to its respective &lt;code&gt;value_counts()&lt;/code&gt;, i.e. &lt;code&gt;X.loc[:, col] = X[col].map(col_val_counts)&lt;/code&gt;, which led to less agreeable convergence.)&lt;/p&gt;
&lt;p&gt;Let's get started.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dataset sizes:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Train:      &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Test:       &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (30000, 657)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (10000, 657)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (10000, 657)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Flight-delay models&lt;/h2&gt;
&lt;p&gt;Let's build two baseline models with the data we have. Both models have a single ReLU output and are trained to minimize the mean squared error of the predicted delay via stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;ReLU was chosen as an output activation because delays are both bounded below at 0 and bi-modal. I considered three separate strategies for predicting this distribution.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Train a network with two outputs: &lt;code&gt;total_delay&lt;/code&gt; and &lt;code&gt;total_delay == 0&lt;/code&gt; (Boolean). Optimize this network with a composite loss function: mean squared error and binary cross-entropy, respectively.&lt;/li&gt;
&lt;li&gt;Train a "poor-man's" hierarchical model: a logistic regression to predict &lt;code&gt;total_delay == 0&lt;/code&gt; and a standard regression to predict &lt;code&gt;total_delay&lt;/code&gt;. Then, compute the final prediction as a thresholded ternary, e.g. &lt;code&gt;y_pred = np.where(y_pred_lr &amp;gt; threshhold, 0, y_pred_reg)&lt;/code&gt;. Train the regression model with both all observations, and just those where &lt;code&gt;total_delay &amp;gt; 0&lt;/code&gt;, and see which works best.&lt;/li&gt;
&lt;li&gt;Train a single network with a ReLU activation. This gives a reasonably elegant way to clip our outputs below at 0, and mean-squared-error still tries to place our observations into the correct mode (of the bimodal output distribution; this said, mean-squared-error may try to "play it safe" and predict between the modes).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I chose Option #3 because it performed best in brief experimentation and was the simplest to both fit and explain.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metaclass&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ABCMeta&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            input_dim : The number of columns in our design matrix.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'float32'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;144&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'regression_output'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DeeperRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            input_dim : The number of columns in our design matrix.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;            dropout_p : The percentage of units to drop in the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dropout layer.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout_p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'float32'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'input'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;144&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;144&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;72&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'regression_output'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Simple regression&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;


&lt;span class="n"&gt;simple_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="simple regression initial" class="img-responsive" src="https://cavaunpeu.github.io/figures/transfer_learning_simple_regression_initial.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Deeper regression&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeeperRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="deeper regression initial" class="img-responsive" src="https://cavaunpeu.github.io/figures/transfer_learning_deeper_regression_initial.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Test set predictions&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_pred_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y_pred_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;mse_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_simple&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mse_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_deeper&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, simple regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_simple&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, deeper regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_deeper&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean squared error, simple regression: 2.331459019628268&lt;/span&gt;
&lt;span class="err"&gt;Mean squared error, deeper regression: 2.3186310632259204&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Learning airport embeddings&lt;/h2&gt;
&lt;p&gt;We propose two networks through which to learn airport embeddings: a dot product siamese network, and a &lt;a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"&gt;variational autoencoder&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Dot product siamese network&lt;/h2&gt;
&lt;p&gt;This network takes as input origin and destination IDs, latitudes and longitudes. It gives as output a binary value indicating whether or not a flight-route between these airports exists. The &lt;code&gt;airports&lt;/code&gt; DataFrame gives the geographic metadata. The &lt;code&gt;routes&lt;/code&gt; DataFrame gives &lt;em&gt;positive&lt;/em&gt; training examples for our network. To build negative samples, we employ, delightfully, "negative sampling."&lt;/p&gt;
&lt;h3&gt;Negative sampling&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;routes&lt;/code&gt; gives exlusively &lt;code&gt;(origin, dest, exists = 1)&lt;/code&gt; triplets. To create triplets where &lt;code&gt;exists = 0&lt;/code&gt;, we simply build them ourself: &lt;code&gt;(origin, fake_dest, exists = 0)&lt;/code&gt;. It's that simple.&lt;/p&gt;
&lt;p&gt;Inspired by &lt;a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"&gt;word2vec's approach&lt;/a&gt; to an almost identical problem, I pick &lt;code&gt;fake_dest&lt;/code&gt;'s based on the frequency with which they occur in the dataset - more frequent samples being more likely to be selected - via:&lt;/p&gt;
&lt;div class="math"&gt;$$P(a_i) = \frac{  {f(a_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(a_j)}^{3/4} \right) }$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is an airport. To choose a &lt;code&gt;fake_dest&lt;/code&gt; for a given &lt;code&gt;origin&lt;/code&gt;, we first remove all of the real &lt;code&gt;dest&lt;/code&gt;'s, re-normalize &lt;span class="math"&gt;\(P(a)\)&lt;/span&gt;, then take a multinomial draw.&lt;/p&gt;
&lt;p&gt;For a more complete yet equally approachable explanation, please see &lt;a href="https://arxiv.org/pdf/1402.3722.pdf"&gt;Goldberg and Levy&lt;/a&gt;. For an &lt;em&gt;extremely thorough&lt;/em&gt; review of related methods, see Sebastian Ruder's &lt;a href="http://sebastianruder.com/word-embeddings-softmax/"&gt;On word embeddings - Part 2: Approximating the Softmax&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Variational autoencoder&lt;/h2&gt;
&lt;h3&gt;Discriminative models&lt;/h3&gt;
&lt;p&gt;The previous network is a &lt;em&gt;discriminative&lt;/em&gt; model: given two inputs &lt;code&gt;origin&lt;/code&gt; and &lt;code&gt;dest&lt;/code&gt;, it outputs the conditional probability that &lt;code&gt;exists = 1&lt;/code&gt;. While discriminative models are effective in distinguishing &lt;em&gt;between&lt;/em&gt; output classes, they don't offer an idea of what data look like within each class itself. To see why, let's restate Bayes' rule for a given input &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y\vert x) = \frac{P(x\vert Y)P(Y)}{P(x)} = \frac{P(x, Y)}{P(x)}$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Discriminative classifiers jump directly to estimating &lt;span class="math"&gt;\(P(Y\vert x)\)&lt;/span&gt; without modeling its component parts &lt;span class="math"&gt;\(P(x, Y)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Instead, as the intermediate step, they simply compute an &lt;em&gt;unnormalized&lt;/em&gt; joint distribution &lt;span class="math"&gt;\(\tilde{P}(x, Y)\)&lt;/span&gt; and a normalizing "partition function." The following then gives the model's predictions for the same reason that &lt;span class="math"&gt;\(\frac{.2}{1} = \frac{3}{15}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y\vert x) = \frac{P(x, Y)}{P(x)} = \frac{\tilde{P}(x, Y)}{\text{partition function}}$$&lt;/div&gt;
&lt;p&gt;This is explained much more thoroughly in a previous blog post: &lt;a href="https://cavaunpeu.github.io/machine-learning/deriving-the-softmax-from-first-principles.md"&gt;Deriving the Softmax from First Principles&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Generative models&lt;/h3&gt;
&lt;p&gt;Conversely, a variational autoencoder is a &lt;em&gt;generative&lt;/em&gt; model: instead of jumping &lt;em&gt;directly&lt;/em&gt; to the conditional probability of all possible outputs given a specific input, they first compute the true component parts: the joint probability distribution over data and inputs alike, &lt;span class="math"&gt;\(P(X, Y)\)&lt;/span&gt;, and the distribution over our data, &lt;span class="math"&gt;\(P(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The joint probability can be rewritten as &lt;span class="math"&gt;\(P(X, Y) = P(Y)P(X\vert Y)\)&lt;/span&gt;: as such, generative models tell us the distribution over classes in our dataset, as well as the distribution of inputs within each class. Suppose we are trying to predict t-shirt colors with a 3-feature input; generative models would tell us: "30% of your t-shirts are green - typically produced by inputs near &lt;code&gt;x = [1, 2, 3]&lt;/code&gt;; 40% are red - typically produced by inputs near &lt;code&gt;x = [10, 20, 30]&lt;/code&gt;; 30% are blue - typically produced by inputs near &lt;code&gt;x = [100, 200, 300]&lt;/code&gt;. This is in contrast to a discriminative model which would simply compute: given an input &lt;span class="math"&gt;\(x\)&lt;/span&gt;, your output probabilities are: &lt;span class="math"&gt;\(\{\text{red}: .2, \text{green}: .3, \text{blue}: .5\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;To generate new data with a generative model, we draw from &lt;span class="math"&gt;\(P(Y)\)&lt;/span&gt;, then &lt;span class="math"&gt;\(P(X\vert Y)\)&lt;/span&gt;. To make predictions, we solicit &lt;span class="math"&gt;\(P(Y), P(x\vert Y)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; and employ Bayes' rule outright.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Manifold assumption&lt;/h3&gt;
&lt;p&gt;The goal of both autoencoders is to discover underlying "structure" in our data: while each airport can be one-hot encoded into a 3186-dimensional vector, we wish to learn a, or even the, reduced space in which our data both live and vary. This concept is well understood through the "manifold assumption," explained succinctly in this &lt;a href="https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning"&gt;CrossValidated thread&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Imagine that you have a bunch of seeds fastened on a glass plate, which is resting horizontally on a table. Because of the way we typically think about space, it would be safe to say that these seeds live in a two-dimensional space, more or less, because each seed can be identified by the two numbers that give that seed's coordinates on the surface of the glass.&lt;/p&gt;
&lt;p&gt;Now imagine that you take the plate and tilt it diagonally upwards, so that the surface of the glass is no longer horizontal with respect to the ground. Now, if you wanted to locate one of the seeds, you have a couple of options. If you decide to ignore the glass, then each seed would appear to be floating in the three-dimensional space above the table, and so you'd need to describe each seed's location using three numbers, one for each spatial direction. But just by tilting the glass, you haven't changed the fact that the seeds still live on a two-dimensional surface. So you could describe how the surface of the glass lies in three-dimensional space, and then you could describe the locations of the seeds on the glass using your original two dimensions.&lt;/p&gt;
&lt;p&gt;In this thought experiment, the glass surface is akin to a low-dimensional manifold that exists in a higher-dimensional space : no matter how you rotate the plate in three dimensions, the seeds still live along the surface of a two-dimensional plane.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, the full spectrum of that which characterizes an airport can be described by just a few numbers. Varying one of these numbers - making it larger or smaller - would result in an airport of slightly different "character;" if one dimension were to represent "global travel hub"-ness, a value of &lt;span class="math"&gt;\(-1000\)&lt;/span&gt; along this dimension might give us that hangar in Alaska.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;In the context of autoencoders (and dimensionality reduction algorithms), "learning 'structure' in our data" means nothing more than finding that ceramic plate amidst a galaxy of stars&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Graphical models&lt;/h3&gt;
&lt;p&gt;Variational autoencoders do not have the same notion of an "output" - namely, "does a route between two airports exist?" - as our dot product siamese network. To detail this model, we'll start near first principles with probabilistic graphical models with our notion of the ceramic plate in mind:&lt;/p&gt;
&lt;p&gt;&lt;img alt="VAE pgm" class="img-responsive" src="https://cavaunpeu.github.io/images/vae_pgm.png"/&gt;&lt;/p&gt;
&lt;p&gt;Coordinates on the plate detail airport character; choosing coordinates - say, &lt;code&gt;[global_hub_ness = 500, is_in_asia = 500]&lt;/code&gt; - allows us to &lt;em&gt;generate&lt;/em&gt; an airport. In this case, it might be Seoul. In variational autoencoders, ceramic-plate coordinates are called the "latent vector," denoted &lt;span class="math"&gt;\(z\)&lt;/span&gt;. The joint probability of our graphical model is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$P(z)P(x\vert z) = P(z, x)$$&lt;/div&gt;
&lt;p&gt;Our goal is to infer the priors that likely generated these data via Bayes' rule:&lt;/p&gt;
&lt;div class="math"&gt;$$P(z\vert x) = \frac{P(z)P(x\vert z)}{P(x)}$$&lt;/div&gt;
&lt;p&gt;The denominator is called the &lt;strong&gt;evidence&lt;/strong&gt;; we obtain it by marginalizing the joint distribution over the latent variables:&lt;/p&gt;
&lt;div class="math"&gt;$$P(x) = \int P(x\vert z)P(z)dz$$&lt;/div&gt;
&lt;p&gt;Unfortunately, this asks us to consider &lt;em&gt;all possible configurations&lt;/em&gt; of the latent vector &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Should &lt;span class="math"&gt;\(z\)&lt;/span&gt; exist on the vertices of a cube in &lt;span class="math"&gt;\(\mathbb{R}^3\)&lt;/span&gt;, this would not be very difficult; should &lt;span class="math"&gt;\(z\)&lt;/span&gt; be a continuous-valued vector in &lt;span class="math"&gt;\(\mathbb{R}^{10}\)&lt;/span&gt;, this becomes a whole lot harder. Computing &lt;span class="math"&gt;\(P(x)\)&lt;/span&gt; is problematic.&lt;/p&gt;
&lt;h3&gt;Variational inference&lt;/h3&gt;
&lt;p&gt;In fact, we could attempt to use MCMC to compute &lt;span class="math"&gt;\(P(z\vert x)\)&lt;/span&gt;; however, this is slow to converge. Instead, let's compute an &lt;em&gt;approximation&lt;/em&gt; to this distribution then try to make it closely resemble the (intractable) original. In this vein, we introduce &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;variational inference&lt;/a&gt;, which "allows us to re-write statistical inference problems (i.e. infer the value of a random variable given the value of another random variable) as optimization problems (i.e. find the parameter values that minimize some objective function)."&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Let's choose our approximating distribution as simple, parametric and one we know well: the Normal (Gaussian) distribution. Were we able to compute &lt;span class="math"&gt;\(P(z\vert x) = \frac{P(x, z)}{P(x)}\)&lt;/span&gt;, it is &lt;em&gt;intrinsic&lt;/em&gt; that &lt;span class="math"&gt;\(z\)&lt;/span&gt; is contingent on &lt;span class="math"&gt;\(x\)&lt;/span&gt;; when building our own distribution to approximate &lt;span class="math"&gt;\(P(z\vert x)\)&lt;/span&gt;, we need to be &lt;em&gt;explicit&lt;/em&gt; about this contingency: different values for &lt;span class="math"&gt;\(x\)&lt;/span&gt; should be assumed to have been generated by different values of &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Let's write our approximation as follows, where &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; parameterizes the Gaussian for a given &lt;span class="math"&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}(z\vert x)$$&lt;/div&gt;
&lt;p&gt;Finally, as stated previously, we want to make this approximation closely resemble the original; the &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL divergence&lt;/a&gt; quantifies their difference:&lt;/p&gt;
&lt;div class="math"&gt;$$KL(q_{\lambda}(z\vert x)\Vert P(z\vert x)) = \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)}{P(z\vert x)}dz}$$&lt;/div&gt;
&lt;p&gt;Our goal is to obtain the argmin with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$q_{\lambda}^{*}(z\vert x) = \underset{\lambda}{\arg\min}\ \text{KL}(q_{\lambda}(z\vert x)\Vert P(z\vert x))$$&lt;/div&gt;
&lt;p&gt;Expanding the divergence, we obtain:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
KL(q_{\lambda}(z\vert x)\Vert P(z\vert x))
&amp;amp;= \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)}{P(z\vert x)}dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\log\frac{q_{\lambda}(z\vert x)P(x)}{P(z, x)}dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x) -\log{P(z, x)} + \log{P(x)}}\bigg)dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz + \log{P(x)}\int{q_{\lambda}(z\vert x)dz}\\
&amp;amp;= \int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz + \log{P(x)} \cdot 1
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As such, since only the left term depends on &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, minimizing the entire expression with respect to &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; amounts to minimizing this term. Incidentally, the opposite (negative) of this term is called the &lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;ELBO&lt;/a&gt;, or the "evidence lower bound." To see why, let's plug the ELBO into the equation above and solve for &lt;span class="math"&gt;\(\log{P(x)}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\log{P(x)} = ELBO(\lambda) + KL(q_{\lambda}(z\vert x)\Vert P(z\vert x))$$&lt;/div&gt;
&lt;p&gt;In English: "the log of the evidence is at least the lower bound of the evidence plus the divergence between our true posterior &lt;span class="math"&gt;\(P(z\vert x)\)&lt;/span&gt; and our (variational) approximation to this posterior &lt;span class="math"&gt;\(q_{\lambda}(z\vert x)\)&lt;/span&gt;."&lt;/p&gt;
&lt;p&gt;Since the left term above is the opposite of the ELBO, minimizing this term is equivalent to &lt;em&gt;maximizing&lt;/em&gt; the ELBO.&lt;/p&gt;
&lt;p&gt;Let's restate the equation and rearrange further:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
ELBO(\lambda)
&amp;amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(z, x)}}\bigg)dz\\
&amp;amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} -\log{P(x\vert z)} - \log{P(z)}}\bigg)dz\\
&amp;amp;= -\int{q_{\lambda}(z\vert x)\bigg(\log{q_{\lambda}(z\vert x)} - \log{P(z)}}\bigg)dz + \log{P(x\vert z)}\int{q_{\lambda}(z\vert x)dz}\\
&amp;amp;= -\int{q_{\lambda}(z\vert x)\log{\frac{q_{\lambda}(z\vert x)}{P(z)}}dz} + \log{P(x\vert z)} \cdot 1\\
&amp;amp;= \log{P(x\vert z)} -KL(q_{\lambda}(z\vert x)\Vert P(z))
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our goal is to maximize this expression, or minimize the opposite:&lt;/p&gt;
&lt;div class="math"&gt;$$-\log{P(x\vert z)} + KL(q_{\lambda}(z\vert x)\Vert P(z))$$&lt;/div&gt;
&lt;p&gt;In machine learning parlance: "minimize the negative log likelihood of our data (generated via &lt;span class="math"&gt;\(z\)&lt;/span&gt;) plus the divergence between the true distribution of &lt;span class="math"&gt;\(z\)&lt;/span&gt; (the ceramic plate) and our approximation thereof."&lt;/p&gt;
&lt;p&gt;See what we did?&lt;/p&gt;
&lt;h3&gt;Finally, back to neural nets&lt;/h3&gt;
&lt;p&gt;The variational autoencoder consists of an encoder network and a decoder network.&lt;/p&gt;
&lt;h4&gt;Encoder&lt;/h4&gt;
&lt;p&gt;The encoder network takes as input &lt;span class="math"&gt;\(x\)&lt;/span&gt; (an airport) and produces as output &lt;span class="math"&gt;\(z\)&lt;/span&gt; (the latent "code" of that airport, i.e. its location on the ceramic plate). As an intermediate step, it produces multivariate Gaussian parameters &lt;span class="math"&gt;\((\mu_{x_i}, \sigma_{x_i})\)&lt;/span&gt; for each airport. These parameters are then plugged into a Gaussian &lt;span class="math"&gt;\(q\)&lt;/span&gt;, from which we &lt;em&gt;sample&lt;/em&gt; a value &lt;span class="math"&gt;\(z\)&lt;/span&gt;. The encoder is parameterized by a weight matrix &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;Decoder&lt;/h4&gt;
&lt;p&gt;The decoder network takes as input &lt;span class="math"&gt;\(z\)&lt;/span&gt; and produces &lt;span class="math"&gt;\(P(x\vert z)\)&lt;/span&gt;: a reconstruction of the airport vector (hence, autoencoder). It is parameterized by a weight matrix &lt;span class="math"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;Loss function&lt;/h4&gt;
&lt;p&gt;The network's loss function is the sum of the mean squared reconstruction error of the original input &lt;span class="math"&gt;\(x\)&lt;/span&gt; and the KL divergence between the true distribution of &lt;span class="math"&gt;\(z\)&lt;/span&gt; and its approximation &lt;span class="math"&gt;\(q\)&lt;/span&gt;. Given the reparameterization trick (next section) and another healthy scoop of algebra, we write this in Python code as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;`z_mean` gives the mean of the Gaussian that generates `z`&lt;/span&gt;
&lt;span class="sd"&gt;`z_log_var` gives the log-variance of the Gaussian that generates `z`&lt;/span&gt;
&lt;span class="sd"&gt;`z` is generated via:&lt;/span&gt;

&lt;span class="sd"&gt;  z = z_mean + K.exp(z_log_var / 2) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + K.exp( log(z_std)**2 / 2 ) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + K.exp( (2 * log(z_std) / 2 ) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + K.exp( log(z_std) ) * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;    = z_mean + z_std * epsilon&lt;/span&gt;
&lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;kl_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;kl_loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Reparameterization trick&lt;/h3&gt;
&lt;p&gt;When back-propagating the network's loss to &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; , we need to go through &lt;span class="math"&gt;\(z\)&lt;/span&gt; — &lt;em&gt;a sample&lt;/em&gt; taken from &lt;span class="math"&gt;\(q_{\theta}(z\vert x)\)&lt;/span&gt;. Trivially, this sample is a scalar; intuitively, its derivative should be non-zero. In solution, we'd like the sample to depend not on the &lt;em&gt;stochasticity&lt;/em&gt; of the random variable, but on the random variable's &lt;em&gt;parameters&lt;/em&gt;. To this end, we employ the &lt;a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important"&gt;"reparametrization trick"&lt;/a&gt;, such that the sample depends on these parameters &lt;em&gt;deterministically&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As a quick example, this trick allows us to write &lt;span class="math"&gt;\(\mathcal{N}(\mu, \sigma)\)&lt;/span&gt; as &lt;span class="math"&gt;\(z = \mu + \sigma \odot \epsilon\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\epsilon \sim \mathcal{N}(0, 1)\)&lt;/span&gt;. Drawing samples this way allows us to propagate error backwards through our network.&lt;/p&gt;
&lt;h2&gt;Auxiliary data&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# build X_routes, y_routes&lt;/span&gt;
&lt;span class="n"&gt;geo_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_latitude'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'origin_longitude'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dest_latitude'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dest_longitude'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;routes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;geo_cols&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;routes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'exists'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;geo_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;geo_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# split training, test data&lt;/span&gt;
&lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;val_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test_r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;val_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dataset sizes:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Train:      &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Test:       &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (87630, 6)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (21907, 6)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (21907, 6)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Dot product embedding model&lt;/h2&gt;
&lt;p&gt;To start, let's train our model with a single latent dimension then visualize the results on the world map.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unique_airports&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DotProductEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            embedding_size : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# inputs&lt;/span&gt;
        &lt;span class="n"&gt;origin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;origin_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin_geo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest_geo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# embeddings&lt;/span&gt;
        &lt;span class="n"&gt;origin_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin_embedding'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# dot product&lt;/span&gt;
        &lt;span class="n"&gt;dot_product&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dot_product&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dot_product&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dot_product&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;dot_product&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# dense layers&lt;/span&gt;
        &lt;span class="n"&gt;tanh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dot_product&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tanh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# output&lt;/span&gt;
        &lt;span class="n"&gt;exists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dp_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DotProductEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dp_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;SVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_to_dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dp_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dot'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'svg'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="dot product embedding model" class="img-responsive" src="https://cavaunpeu.github.io/figures/dot_product_embedding_model.svg"/&gt;&lt;/p&gt;
&lt;h2&gt;Fit&lt;/h2&gt;
&lt;p&gt;&lt;img alt="dot product embedding model fit" class="img-responsive" src="https://cavaunpeu.github.io/figures/dot_product_embedding_model_fit.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Visualize embeddings&lt;/h2&gt;
&lt;p&gt;To visualize results, we'll:
1. Compose a list of unique origin airports.
2. Extract the learned (1-dimensional) embedding for each.
3. Scale the results to &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.
4. Use the scaled embedding as a percentile-index into a color gradient. Here, we've chosen the colors of the rainbow: low values are blue/purple, and high values are orange/red.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_embeddings_on_world_map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unique_origins&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{static}&lt;/span&gt;&lt;span class="s1"&gt;/figures/dp_model_map.html'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="800" src="https://willwolf-public.s3.amazonaws.com/transfer-learning-flight-delays/dp_model_map.html" width="946"&gt;&lt;/iframe&gt;
&lt;h2&gt;Variational autoencoder&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;KerasLayer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''A custom "variational" Keras layer that completes the&lt;/span&gt;
&lt;span class="sd"&gt;        variational autoencoder.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            output_dim : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'glorot_normal'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_weight&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
            &lt;span class="n"&gt;initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'zero'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_mean_bias&lt;/span&gt;
        &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z_log_var_bias&lt;/span&gt;
        &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epsilon_std&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kl_loss_numerator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z_mean&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_log_var&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_decoded&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kl_loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_output_shape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VariationalAutoEncoderEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;'''Initializes the model parameters.&lt;/span&gt;

&lt;span class="sd"&gt;        Args:&lt;/span&gt;
&lt;span class="sd"&gt;            embedding_size : The desired number of latent dimensions in our&lt;/span&gt;
&lt;span class="sd"&gt;                embedding space.&lt;/span&gt;
&lt;span class="sd"&gt;            λ : The regularization strength to apply to the model's&lt;/span&gt;
&lt;span class="sd"&gt;                dense layers.&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embedding_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dense_layer_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_unique_airports&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VariationalLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# encoder&lt;/span&gt;
        &lt;span class="n"&gt;origin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;origin_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'origin_geo'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;variational_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;variational_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'encoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# decoder&lt;/span&gt;
        &lt;span class="n"&gt;latent_vars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'tanh'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dense&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unique_airports&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;dest_geo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dest_geo'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;latent_vars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dest_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'decoder'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# end-to-end&lt;/span&gt;
        &lt;span class="n"&gt;encoder_decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;origin_geo&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;encoder_decoder&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;vae_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VariationalAutoEncoderEmbeddingModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embedding_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_layer_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;003&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;vae_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vae_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variational_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'mean_squared_logarithmic_error'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                  &lt;span class="n"&gt;loss_weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;SVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_to_dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vae_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dot'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'svg'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="vae embedding model" class="img-responsive" src="https://cavaunpeu.github.io/figures/vae_embedding_model.svg"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# build VAE training, test sets&lt;/span&gt;
&lt;span class="n"&gt;one_hot_airports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_UNIQUE_AIRPORTS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X_train_r_origin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_val_r_origin&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_test_r_origin&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'origin_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_train_r_dest&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_train_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_val_r_dest&lt;/span&gt;     &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_val_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;     &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_test_r_dest&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;one_hot_airports&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="n"&gt;X_test_r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dest_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dataset sizes:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Train:      &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train_r_origin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val_r_origin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'    Test:       &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test_r_origin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (87630, 3186)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (21907, 3186)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (21907, 3186)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Fit&lt;/h2&gt;
&lt;p&gt;&lt;img alt="vae product embedding model fit" class="img-responsive" src="https://cavaunpeu.github.io/figures/vae_product_embedding_model_fit.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Visualize&lt;/h2&gt;
&lt;p&gt;&lt;iframe allowfullscreen="" frameborder="0" height="800" src="https://willwolf-public.s3.amazonaws.com/transfer-learning-flight-delays/vae_model_map.html" width="946"&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;h2&gt;Finally, transfer the learning&lt;/h2&gt;
&lt;p&gt;Retrain both models with 20 latent dimensions, then join the embedding back to our original dataset.&lt;/p&gt;
&lt;h2&gt;Extract embeddings, construct joint dataset&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Dataset sizes:&lt;/span&gt;
&lt;span class="err"&gt;    Train:      (30000, 151)&lt;/span&gt;
&lt;span class="err"&gt;    Validation: (10000, 151)&lt;/span&gt;
&lt;span class="err"&gt;    Test:       (10000, 151)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Train original models&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;simple_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0005&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simple_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="simple regression augmented" class="img-responsive" src="https://cavaunpeu.github.io/figures/transfer_learning_simple_regression_augmented.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeeperRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;λ&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_p&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;0001&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'mean_squared_error'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_flight_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plot_model_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deeper_reg_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="deeper regression augmented" class="img-responsive" src="https://cavaunpeu.github.io/figures/transfer_learning_deeper_regression_augmented.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_pred_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simple_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y_pred_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deeper_reg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;mse_simple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_simple&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mse_deeper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error_scikit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred_deeper&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, simple regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_simple&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Mean squared error, deeper regression: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mse_deeper&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;Mean squared error, simple regression: 2.3176028493805263&lt;/span&gt;
&lt;span class="err"&gt;Mean squared error, deeper regression: 2.291221474968889&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In fitting these models to both the original and "augmented" datasets, I spent time tuning their parameters — regularization strengths, amount of dropout, number of epochs, learning rates, etc. Additionally, the respective datasets are of different dimensionality. For these reasons, comparison between the two sets of models is clearly not "apples to apples."&lt;/p&gt;
&lt;p&gt;Notwithstanding, the airport embeddings do seem to provide a nice lift over our original one-hot encodings. Of course, their use is not limited to predicting flight delays: they can be used in any task concerned with airports. Additionally, these embeddings give insight into the nature of the airports themselves: those nearby in vector space can be considered as "similar" by some latent metric. To figure out what these metrics mean, though - it's back to the map.&lt;/p&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://allentran.github.io/graph2vec"&gt;Towards Anything2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ben.bolte.cc/blog/2017/unsupervised-calcium-modeling.html"&gt;Deep Learning for Calcium Imaging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1403.6652.pdf"&gt;DeepWalk: Online Learning of Social Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/"&gt;Variational Autoencoder: Intuition and Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html"&gt;Introducing Variational Autoencoders (in Prose and Code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dohmatob.github.io/research/2016/10/22/VAE.html"&gt;Variational auto-encoder for "Frey faces" using keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/transfer-learning/"&gt;Transfer Learning - Machine Learning's Next Frontier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"&gt;Tutorial - What is a variational autoencoder?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.evjang.com/2016/08/variational-bayes.html"&gt;A Beginner's Guide to Variational Methods: Mean-Field Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/"&gt;Variational Autoencoder: Intuition and Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning"&gt;CrossValidated - What is the manifold assumption in semi-supervised learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"&gt;David Blei - Variational Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://edwardlib.org/tutorials/variational-inference"&gt;Edward - Variational Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf"&gt;On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/flight-delays"&gt;repository&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/flight-delays/blob/master/notebooks/flight-prediction.ipynb?flush_cache=true"&gt;rendered notebook&lt;/a&gt; for this project can be found at their respective links.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="http://blog.evjang.com/2016/08/variational-bayes.html"&gt;A Beginner's Guide to Variational Methods: Mean-Field Approximation&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Deriving the Softmax from First Principles</title><link href="https://cavaunpeu.github.io/2017/04/19/deriving-the-softmax-from-first-principles/" rel="alternate"></link><published>2017-04-19T17:26:00-04:00</published><updated>2017-04-19T17:26:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-04-19:/2017/04/19/deriving-the-softmax-from-first-principles/</id><summary type="html">&lt;p&gt;Deriving the softmax from first conditional probabilistic principles, and how this framework extends naturally to define the softmax regression, conditional random fields, naive Bayes and hidden Markov models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The original goal of this post was to explore the relationship between the softmax and sigmoid functions. In truth, this relationship had always seemed just out of reach: "One has an exponent in the numerator! One has a summation! One has a 1 in the denominator!" And of course, the two have different names.&lt;/p&gt;
&lt;p&gt;Once derived, I quickly realized how this relationship backed out into a more general modeling framework motivated by the conditional probability axiom itself. As such, this post first explores how the sigmoid is but a special case of the softmax, and the underpinnings of each in Gibbs distributions, factor products and probabilistic graphical models. Next, we go on to show how this framework extends naturally to define canonical model classes such as the softmax regression, conditional random fields, naive Bayes and hidden Markov models.&lt;/p&gt;
&lt;h2&gt;Our goal&lt;/h2&gt;
&lt;p&gt;This is a predictive model. It is a diamond that receives an input and produces an output.&lt;/p&gt;
&lt;p&gt;&lt;img alt="simple input/output model" class="img-responsive" src="https://cavaunpeu.github.io/images/simple_input_output_model.png"/&gt;&lt;/p&gt;
&lt;p&gt;The input is a vector &lt;span class="math"&gt;\(\mathbf{x} = [x_0, x_1, x_2, x_3]\)&lt;/span&gt;. There are 3 possible outputs: &lt;span class="math"&gt;\(a, b, c\)&lt;/span&gt;. The goal of our model is to predict the probability of producing each output conditional on the input, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})$$&lt;/div&gt;
&lt;p&gt;Of course, a probability is but a real number that lies on the closed interval &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;How does the input affect the output?&lt;/h2&gt;
&lt;p&gt;Our input is a list of 4 numbers; each one affects &lt;em&gt;each possible output&lt;/em&gt; to a &lt;em&gt;different extent&lt;/em&gt;. We'll call this effect a "weight." 4 inputs times 3 outputs equals 12 distinct weights. They might look like this:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(a\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(b\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(c\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.1&lt;/td&gt;
&lt;td&gt;.4&lt;/td&gt;
&lt;td&gt;.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.2&lt;/td&gt;
&lt;td&gt;.3&lt;/td&gt;
&lt;td&gt;.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.3&lt;/td&gt;
&lt;td&gt;.2&lt;/td&gt;
&lt;td&gt;.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(x_3\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;.4&lt;/td&gt;
&lt;td&gt;.1&lt;/td&gt;
&lt;td&gt;.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Producing an output&lt;/h2&gt;
&lt;p&gt;Given an input &lt;span class="math"&gt;\(x = [x_0, x_1, x_2, x_3]\)&lt;/span&gt;, our model will use the above weights to produce a number for each output &lt;span class="math"&gt;\(a, b, c\)&lt;/span&gt;. The effect of each input element will be additive. The reason why will be explained later on.&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{a} = \sum\limits_{i=0}^{3}w_{i, a}x_i\\
\tilde{b} = \sum\limits_{i=0}^{3}w_{i, b}x_i\\
\tilde{c} = \sum\limits_{i=0}^{3}w_{i, c}x_i\\
$$&lt;/div&gt;
&lt;p&gt;These sums will dictate what output our model produces. The biggest number wins. For example, given&lt;/p&gt;
&lt;div class="math"&gt;$$
\{\tilde{a}: 5, \tilde{b}: 7, \tilde{c}: 9\}
$$&lt;/div&gt;
&lt;p&gt;our model will have the best chance of producing a &lt;span class="math"&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Converting to probabilities&lt;/h2&gt;
&lt;p&gt;We said before that our goal is to obtain the following:&lt;/p&gt;
&lt;div class="math"&gt;$$P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; is &lt;strong&gt;bold&lt;/strong&gt; so as to represent &lt;em&gt;any&lt;/em&gt; input value. Given that we now have a &lt;em&gt;specific&lt;/em&gt; input value, namely &lt;span class="math"&gt;\(x\)&lt;/span&gt;, we can state our goal more precisely:&lt;/p&gt;
&lt;div class="math"&gt;$$P(a\vert x), P(b\vert x), P(c\vert x)$$&lt;/div&gt;
&lt;p&gt;Thus far, we just have &lt;span class="math"&gt;\(\{\tilde{a}: 5, \tilde{b}: 7, \tilde{c}: 9\}\)&lt;/span&gt;. To convert each value to a probability, i.e. an un-special number in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;, we just divide by the sum.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert x) = \frac{5}{5+7+9} = \frac{5}{21}\\
P(b\vert x) = \frac{7}{5+7+9} = \frac{7}{21}\\
P(c\vert x) = \frac{9}{5+7+9} = \frac{9}{21}\\
$$&lt;/div&gt;
&lt;p&gt;Finally, to be a valid probability distribution, all numbers must sum to 1.&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{5}{21} + \frac{7}{21} + \frac{9}{21} = 1 \checkmark
$$&lt;/div&gt;
&lt;h2&gt;What if our values are negative?&lt;/h2&gt;
&lt;p&gt;If one of our initial unnormalized probabilities were negative, i.e. &lt;span class="math"&gt;\(\{\tilde{a}: -5, \tilde{b}: 7, \tilde{c}: 9\}\)&lt;/span&gt;, this all breaks down.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert x) = \frac{-5}{-5+7+9} = \frac{-5}{11}\\
P(b\vert x) = \frac{7}{-5+7+9} = \frac{7}{11}\\
P(c\vert x) = \frac{9}{-5+7+9} = \frac{9}{11}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\frac{-5}{11}\)&lt;/span&gt; is not a valid probability as it does not fall in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To ensure that all unnormalized probabilities are positive, we must first pass them through a function that takes as input a real number and produces as output a strictly positive real number. This is simply an exponent; let's choose &lt;a href="https://en.wikipedia.org/wiki/E_(mathematical_constant)"&gt;Euler's number (&lt;span class="math"&gt;\(e\)&lt;/span&gt;)&lt;/a&gt; for now. The rationale for this choice will be explained later on (though do note that any positive exponent would serve our stated purpose).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\tilde{a} &amp;amp;= -5 \rightarrow e^{-5}\\
\tilde{b} &amp;amp;= 7 \rightarrow e^{7}\\
\tilde{c} &amp;amp;= 9 \rightarrow e^{9}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our &lt;em&gt;normalized&lt;/em&gt; probabilities, i.e. valid probabilities, now look as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert x) = \frac{e^{-5}}{e^{-5}+e^7+e^9}\\
P(b\vert x) = \frac{e^{7}}{e^{-5}+e^7+e^9}\\
P(c\vert x) = \frac{e^{9}}{e^{-5}+e^7+e^9}
$$&lt;/div&gt;
&lt;p&gt;More generally,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert x) = \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}\quad \text{for}\ y = a, b, c
$$&lt;/div&gt;
&lt;p&gt;This is the softmax function.&lt;/p&gt;
&lt;h2&gt;Relationship to the sigmoid&lt;/h2&gt;
&lt;p&gt;Whereas the softmax outputs a valid probability distribution over &lt;span class="math"&gt;\(n \gt 2\)&lt;/span&gt; distinct outputs, the sigmoid does the same for &lt;span class="math"&gt;\(n = 2\)&lt;/span&gt;. As such, the sigmoid is simply a special case of the softmax. By this definition, and assuming our model only produces two possible outputs &lt;span class="math"&gt;\(p\)&lt;/span&gt; and &lt;span class="math"&gt;\(q\)&lt;/span&gt;, we can write the sigmoid for a given input &lt;span class="math"&gt;\(x\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mathbf{x}) = \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}\quad \text{for}\ y = p, q
$$&lt;/div&gt;
&lt;p&gt;Similar so far. However, notice that we only need to compute probabilities for &lt;span class="math"&gt;\(p\)&lt;/span&gt;, as &lt;span class="math"&gt;\(P(y = q\vert \mathbf{x}) = 1 - P(y = p\vert \mathbf{x})\)&lt;/span&gt;. On this note, let's re-expand the expression for &lt;span class="math"&gt;\(P(y = p\vert \mathbf{x})\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = p\vert \mathbf{x}) = \frac{e^{\tilde{p}}}{e^{\tilde{p}} + e^{\tilde{q}}}
$$&lt;/div&gt;
&lt;p&gt;Then, dividing both the numerator and denominator by &lt;span class="math"&gt;\(e^{\tilde{p}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
P(y = p\vert \mathbf{x})
&amp;amp;= \frac{e^{\tilde{p}}}{e^{\tilde{p}} + e^{\tilde{q}}}\\
&amp;amp;= \frac{ \frac{e^{\tilde{p}}}{e^{\tilde{p}}} }{\frac{e^{\tilde{p}}}{e^{\tilde{p}}} + \frac{e^{\tilde{q}}}{e^{\tilde{p}}}}\\
&amp;amp;= \frac{1}{1 + e^{\tilde{q} - \tilde{p}}}
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Finally, we can plug this back into our original complement:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{1}{1 + e^{\tilde{q} - \tilde{p}}} = 1 - \frac{1}{1 + e^{\tilde{p} - \tilde{q}}}
$$&lt;/div&gt;
&lt;p&gt;Our equation is &lt;a href="https://en.wikipedia.org/wiki/Underdetermined_system"&gt;&lt;em&gt;underdetermined&lt;/em&gt;&lt;/a&gt; as there are more unknowns (two) than equations (one). As such, our system will have an infinite number of solutions &lt;span class="math"&gt;\((\tilde{p},\tilde{q})\)&lt;/span&gt;. For this reason, we can simply fix one of these values outright. Let's set &lt;span class="math"&gt;\(\tilde{q} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = p\vert \mathbf{x}) = \frac{1}{1 + e^{- \tilde{p}}}
$$&lt;/div&gt;
&lt;p&gt;This is the sigmoid function. Lastly,&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = q\vert \mathbf{x}) = 1 - P(y = p\vert \mathbf{x})
$$&lt;/div&gt;
&lt;h2&gt;Why is the unnormalized probability a summation?&lt;/h2&gt;
&lt;p&gt;We all take for granted the semantics of the canonical linear combination &lt;span class="math"&gt;\(\sum\limits_{i}w_ix_i\)&lt;/span&gt;. But why do we sum in the first place?&lt;/p&gt;
&lt;p&gt;To answer this question, we'll first restate our goal: to predict the probability of producing each output &lt;em&gt;conditional&lt;/em&gt; on the input, i.e. &lt;span class="math"&gt;\(P(Y = y\vert \mathbf{x})\)&lt;/span&gt;. Next, we'll revisit the &lt;a href="https://en.wikipedia.org/wiki/Conditional_probability"&gt;definition of conditional probability&lt;/a&gt; itself:&lt;/p&gt;
&lt;div class="math"&gt;$$P(B\vert A) = \frac{P(A, B)}{P(A)}$$&lt;/div&gt;
&lt;p&gt;Personally, I find this a bit difficult to explain. Let's rearrange to obtain something more intuitive.&lt;/p&gt;
&lt;div class="math"&gt;$$P(A, B) = P(A)P(B\vert A)$$&lt;/div&gt;
&lt;p&gt;This reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The probability of observing (given values of) &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(B\)&lt;/span&gt; concurrently, ie. the joint probability of &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(B\)&lt;/span&gt;, is equal to the probability of observing &lt;span class="math"&gt;\(A\)&lt;/span&gt; times the probability of observing &lt;span class="math"&gt;\(B\)&lt;/span&gt; given that &lt;span class="math"&gt;\(A\)&lt;/span&gt; has occurred.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, assume that the probability of birthing a girl is &lt;span class="math"&gt;\(.55\)&lt;/span&gt;, and the probability of a girl liking math is &lt;span class="math"&gt;\(.88\)&lt;/span&gt;. Therefore,&lt;/p&gt;
&lt;div class="math"&gt;$$P(\text{sex} = girl, \text{likes} = math) = .55 * .88 = .484$$&lt;/div&gt;
&lt;p&gt;Now, let's rewrite our original model output in terms of the definition above.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}
= \frac{e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}{\sum\limits_{y} e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;Remember, we exponentiated each unnormalized probability &lt;span class="math"&gt;\(\tilde{y}\)&lt;/span&gt; so as to convert it to a strictly positive number. Technically, this number should be called &lt;span class="math"&gt;\(\tilde{P}(y, \mathbf{x})\)&lt;/span&gt; as it may be &lt;span class="math"&gt;\(\gt 1\)&lt;/span&gt; and therefore not yet a valid a probability. As such, we need to introduce one more term to our equality chain, given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;This is the arithmetic equivalent of &lt;span class="math"&gt;\(\frac{.2}{1} = \frac{3}{15}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the left term:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The numerator is a valid joint probability distribution.&lt;/li&gt;
&lt;li&gt;The denominator, "the probability of observing any value of &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;", is 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the right term:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The numerator is a strictly positive unnormalized probability distribution.&lt;/li&gt;
&lt;li&gt;The denominator is some constant that ensures that&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\frac{\tilde{P}(a, \mathbf{x})}{\text{normalizer}} + \frac{\tilde{P}(b, \mathbf{x})}{\text{normalizer}} + \frac{\tilde{P}(c, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;sums to 1. In fact, this "normalizer" is called a &lt;strong&gt;partition function&lt;/strong&gt;; we'll come back to this below.&lt;/p&gt;
&lt;p&gt;With this in mind, let's break down the numerator of our softmax equation a bit further.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\begin{split}
e^{\tilde{y}}
&amp;amp;= e^{\big(\sum\limits_{i}w_ix_i\big)}\\
&amp;amp;= e^{(w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3)}\\
&amp;amp;= e^{(w_0x_0)} e^{(w_1x_1)} e^{(w_2x_2)} e^{(w_3x_3)}\\
&amp;amp;= \tilde{P}(a, \mathbf{x})
\end{split}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Lemma: Given that our output function&lt;sup&gt;1&lt;/sup&gt; performs exponentiation &lt;em&gt;so as to obtain a valid conditional probability distribution over possible model outputs&lt;/em&gt;, it follows that our input to this function&lt;sup&gt;2&lt;/sup&gt; should be a summation of weighted model input elements&lt;sup&gt;3&lt;/sup&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The softmax function.&lt;/li&gt;
&lt;li&gt;One of &lt;span class="math"&gt;\(\tilde{a}, \tilde{b}, \tilde{c}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Model input elements are &lt;span class="math"&gt;\([x_0, x_1, x_2, x_3]\)&lt;/span&gt;. Weighted model input elements are &lt;span class="math"&gt;\(w_0x_0, w_1x_1, w_2x_2, w_3x_3\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unfortunately, this only holds if we buy the fact that &lt;span class="math"&gt;\(\tilde{P}(a, \mathbf{x}) = \prod\limits_i e^{(w_ix_i)}\)&lt;/span&gt; in the first place. Introducing the &lt;a href="https://www.youtube.com/watch?v=kFcjl3A9QuA&amp;amp;t=314s"&gt;Gibbs distribution&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Gibbs distribution&lt;/h2&gt;
&lt;p&gt;A Gibbs distribution gives the unnormalized joint probability distribution over a set of outcomes, analogous to the &lt;span class="math"&gt;\(e^{\tilde{a}}, e^{\tilde{b}}, e^{\tilde{c}}\)&lt;/span&gt; computed above, as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
= \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)\\
\Phi = \{\phi_1(\mathbf{D_1}), ..., \phi_k(\mathbf{D_k})\}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; defines a set of &lt;strong&gt;factors.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Factors&lt;/h3&gt;
&lt;p&gt;A factor is a function that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Takes a list of random variables as input. This list is known as the &lt;strong&gt;scope&lt;/strong&gt; of the factor.&lt;/li&gt;
&lt;li&gt;Returns a value for every unique combination of values that the random variables can take, i.e. for every entry in the cross-product space of its scope.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, a factor with scope &lt;span class="math"&gt;\(\{\mathbf{A, B}\}\)&lt;/span&gt; might look like:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(20\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(25\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(15\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Probabilistic graphical models&lt;/h3&gt;
&lt;p&gt;Inferring behavior from complex systems amounts (typically) to computing the joint probability distribution over its possible outcomes. For example, imagine we have a business problem in which:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The day-of-week (&lt;span class="math"&gt;\(\mathbf{A}\)&lt;/span&gt;) and the marketing channel (&lt;span class="math"&gt;\(\mathbf{B}\)&lt;/span&gt;) impact the probability of customer signup (&lt;span class="math"&gt;\(\mathbf{C}\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Customer signup impacts our annual recurring revenue (&lt;span class="math"&gt;\(\mathbf{D}\)&lt;/span&gt;) and end-of-year hiring projections (&lt;span class="math"&gt;\(\mathbf{E}\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Our ARR and hiring projections impact how much cake we will order for the holiday party (&lt;span class="math"&gt;\(\mathbf{F}\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We might draw our system as such:&lt;/p&gt;
&lt;p&gt;&lt;img alt="simple probabilistic graphical model" class="img-responsive" src="http://i3.buimg.com/afca455be01523af.png"/&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to compute &lt;span class="math"&gt;\(P(A, B, C, D, E, F)\)&lt;/span&gt;. In most cases, we'll only have data on small subsets of our system; for example, a controlled experiment we once ran to investigate the relationships between &lt;span class="math"&gt;\(A, B, C\)&lt;/span&gt;, or a survey asking employees how much cake they like eating at Christmas. It is rare, if wholly unreasonable, to ever have access to the full joint probability distribution for a moderately complex system.&lt;/p&gt;
&lt;p&gt;To compute this distribution we break it into pieces. Each piece is a &lt;strong&gt;factor&lt;/strong&gt; which details the behavior of some subset of the system. (As one example, a factor might give the number of times you've observed &lt;span class="math"&gt;\((\mathbf{A} &amp;gt; \text{3pm}, \mathbf{B} = \text{Facebook}, \mathbf{C} &amp;gt; \text{50 signups})\)&lt;/span&gt; in a given day.) To this effect, we say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A desired, &lt;em&gt;unnormalized&lt;/em&gt; probability distribution &lt;span class="math"&gt;\(\tilde{P}\)&lt;/span&gt; &lt;em&gt;factorizes over&lt;/em&gt; a graph &lt;span class="math"&gt;\(G\)&lt;/span&gt; if there exists a set of a factors &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; such that
&lt;div class="math"&gt;$$
\tilde{P} = \tilde{P}_{\Phi} = \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)
$$&lt;/div&gt;
where &lt;span class="math"&gt;\(\Phi = \{\phi_1(\mathbf{D_1}), ..., \phi_k(\mathbf{D_k})\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(G\)&lt;/span&gt; is the &lt;em&gt;induced graph&lt;/em&gt; for &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first half of this lemma does nothing more than restate the definition of an unnormalized Gibbs distribution. Expanding on the second half, we note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The graph induced by a set of factors is a pretty picture in which we draw a circle around each variable in the factor domain superset and draw lines between those that appear concurrently in a given factor domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With two factors &lt;span class="math"&gt;\(\phi(\mathbf{A, B}), \phi(\mathbf{B, C})\)&lt;/span&gt; the "factor domain superset" is &lt;span class="math"&gt;\(\{\mathbf{A, B, C}\}\)&lt;/span&gt;. The induced graph would have three circles with lines connecting &lt;span class="math"&gt;\(\mathbf{A}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{B}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{C}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, it follows that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Given a business problem with variables &lt;span class="math"&gt;\(\mathbf{A, B, C, D, E, F}\)&lt;/span&gt; — we can draw a picture of it.&lt;/li&gt;
&lt;li&gt;We can build factors that describe the behavior of subsets of this problem. Realistically, these will only be small subsets.&lt;/li&gt;
&lt;li&gt;If the graph induced by our factors looks like the one we drew, we can represent our system as a factor product.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unfortunately, the resulting factor product &lt;span class="math"&gt;\(\tilde{P}_{\Phi}\)&lt;/span&gt; is still unnormalized just like &lt;span class="math"&gt;\(e^{\tilde{a}}, e^{\tilde{b}}, e^{\tilde{c}}\)&lt;/span&gt; in our original model.&lt;/p&gt;
&lt;h2&gt;Partition function&lt;/h2&gt;
&lt;p&gt;The partition function was the denominator, i.e. "normalizer", in our softmax function. It is used to turn an unnormalized probability distribution into a normalized (i.e. valid) probability distribution. A true Gibbs distribution is given as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
= \prod\limits_{i=1}^{k}\phi_i(\mathbf{D}_i)\\
P_{\Phi}(\mathbf{X_1, ..., X_n})
= \frac{1}{\mathbf{Z}_{\Phi}}\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\mathbf{Z}_{\Phi}\)&lt;/span&gt; is the partition function.&lt;/p&gt;
&lt;p&gt;To compute this function, we simply add up all the values in the unnormalized table. Given &lt;span class="math"&gt;\(\tilde{P}_{\Phi}(\mathbf{X_1, ..., X_n})\)&lt;/span&gt; as:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(20\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(25\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(15\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(4\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\mathbf{Z}_{\Phi}
&amp;amp;= 20 + 25 + 15 + 4\\
&amp;amp;= 64
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our valid probability distribution then becomes:&lt;/p&gt;
&lt;table class="table-hover table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{20}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{25}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^0\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{15}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(a^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(b^1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\frac{4}{64}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is our denominator in the softmax function.&lt;/p&gt;
&lt;p&gt;*I have not given an example of the actual arithmetic of a factor product (of multiple factors). It's trivial. Google.&lt;/p&gt;
&lt;h2&gt;Softmax regression&lt;/h2&gt;
&lt;p&gt;Once more, the goal of our model is to predict the probability of producing each output conditional on the given input, i.e.&lt;/p&gt;
&lt;div class="math"&gt;$$
P(a\vert \mathbf{x}), P(b\vert \mathbf{x}), P(c\vert \mathbf{x})
$$&lt;/div&gt;
&lt;p&gt;In machine learning training data we're given the building block of a &lt;em&gt;joint probability distribution&lt;/em&gt;, e.g. a ledger of observed co-occurences of inputs and outputs. We surmise that each input element affects each possible output to a different extent, i.e. we multiply it by a weight. Next, we exponentiate each product &lt;span class="math"&gt;\(w_ix_i\)&lt;/span&gt;, i.e. factor, then multiply the results (alternatively, we could exponentiate the linear combination of the factors, i.e. features in machine learning parlance): this gives us an unnormalized joint probability distribution over all (input and output) variables.&lt;/p&gt;
&lt;p&gt;What we'd like is a valid probability distribution over possible outputs &lt;em&gt;conditional&lt;/em&gt; on the input, i.e. &lt;span class="math"&gt;\(P(y\vert \mathbf{x})\)&lt;/span&gt;. Furthermore, our output is a single, "1-of-k" variable in &lt;span class="math"&gt;\(\{a, b, c\}\)&lt;/span&gt; (as opposed to a sequence of variables). This is the definition, almost verbatim, of softmax regression.&lt;/p&gt;
&lt;p&gt;*Softmax regression is also known as multinomial regression, or multi-class logistic regression. Binary logistic regression is a special case of softmax regression in the same way that the sigmoid is a special case of the softmax.&lt;/p&gt;
&lt;p&gt;To compute our conditional probability distribution, we'll revisit Equation (1):&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{e^{\tilde{y}}}{\sum\limits_{y} e^{\tilde{y}}}
= \frac{e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}{\sum\limits_{y} e^{\big(\sum\limits_{i}w_ix_i\big)_{\tilde y}}}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;In other words, the probability of producing each output conditional on the input is equivalent to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The softmax function.&lt;/li&gt;
&lt;li&gt;An exponentiated factor product of input elements normalized by a partition function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It almost speaks for itself.&lt;/p&gt;
&lt;h3&gt;Our partition function depends on &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In order to compute a distribution over &lt;span class="math"&gt;\(y\)&lt;/span&gt; &lt;em&gt;conditional&lt;/em&gt; on &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt;, our partition function becomes &lt;span class="math"&gt;\(x\)&lt;/span&gt;-dependent. In other words, for a given input &lt;span class="math"&gt;\(x = [x_0, x_1, x_2, x_3]\)&lt;/span&gt;, our model computes the conditional probabilities &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt;. While this may seem like a trivial if pedantic restatement of what the softmax function does, it is important to note that our model is effectively computing a &lt;em&gt;family&lt;/em&gt; of conditional distributions — one for each unique input &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Conditional random field&lt;/h2&gt;
&lt;p&gt;Framing our model in this way allows us to extend naturally into other classes of problems. Imagine we are trying to assign a label to each individual word in a given conversation, where possible labels include: &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;offering an olive branch&lt;/code&gt;, and &lt;code&gt;them is fighting words&lt;/code&gt;. Our problem now differs from our original model in one key way, and another possibly-key way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our outcome is now a &lt;em&gt;sequence of labels&lt;/em&gt;. It is no longer "1-of-k." A possible sequence of labels in the conversation "hey there jerk shit roar" might be: &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;neutral&lt;/code&gt;, &lt;code&gt;them is fighting words&lt;/code&gt;, &lt;code&gt;them is fighting words&lt;/code&gt;, &lt;code&gt;them is fighting words&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;There &lt;em&gt;might be&lt;/em&gt; relationships between the words that would influence the final output label sequence. For example, for each individual word, was the person both cocking their fists while enunciating that word, the one previous &lt;em&gt;and&lt;/em&gt; the one previous? In other words, we build factors (i.e. features) that speak to the spatial relationships between our input elements. We do this because we think these relationships might influence the final output (when we say our model "assumes dependencies between features," this is what we mean).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The conditional random field output function is a softmax just the same. In other words, if we build a softmax regression for our conversation-classification task where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our output is a sequence of labels&lt;/li&gt;
&lt;li&gt;Our features are a bunch of (spatially-inspired) interaction features, a la &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we've essentially just built a conditional random field.&lt;/p&gt;
&lt;p&gt;Of course, modeling the full distribution of outputs conditional on the input, where our output is again a sequence of labels, incurs combinatorial explosion really quick (for example, a 5-word speech would already have &lt;span class="math"&gt;\(3^5 = 243\)&lt;/span&gt; possible outputs). For this we use some dynamic-programming-magic to ensure that we compute &lt;span class="math"&gt;\(P(y\vert x)\)&lt;/span&gt; in a reasonable amount of time. I won't cover this topic here.&lt;/p&gt;
&lt;h2&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;Naive Bayes is identical to softmax regression with one key difference: instead of modeling the conditional distribution &lt;span class="math"&gt;\(P(y\vert \mathbf{x})\)&lt;/span&gt; we model the joint distribution &lt;span class="math"&gt;\(P(y, \mathbf{x})\)&lt;/span&gt;, given as:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y, \mathbf{x}) = P(y)\prod\limits_{i=1}^{K}P(x_i\vert y)
$$&lt;/div&gt;
&lt;p&gt;In effect, this model gives a (normalized) Gibbs distribution outright where the factors are &lt;em&gt;already valid probabilities&lt;/em&gt; expressing the relationship between each input element and the output.&lt;/p&gt;
&lt;h3&gt;The distribution of our data&lt;/h3&gt;
&lt;p&gt;Crucially, neither Naive Bayes nor softmax regression make any assumptions about the distribution of the data, &lt;span class="math"&gt;\(P(\mathbf{x})\)&lt;/span&gt;. (Were this not the case, we'd have to state information like "I think the probability of observing the &lt;em&gt;input&lt;/em&gt; &lt;span class="math"&gt;\(x = [x_0 = .12, x_1 = .34, x_2 = .56, x_3 = .78]\)&lt;/span&gt; is &lt;span class="math"&gt;\(.00047\)&lt;/span&gt;," which would imply in the most trivial sense of the word that we are making &lt;em&gt;assumptions&lt;/em&gt; about the distribution of our data.)&lt;/p&gt;
&lt;p&gt;In softmax regression, our model looks as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y\vert \mathbf{x})
= \frac{P(y, \mathbf{x})}{P(\mathbf{x})}
= \frac{\tilde{P}(y, \mathbf{x})}{\text{normalizer}}
$$&lt;/div&gt;
&lt;p&gt;&lt;em&gt;While the second term is equal to the third, we never actually have to compute its denominator in order to obtain the first.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In Naive Bayes, we simply assume that the probability of observing each input element &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; depends on &lt;span class="math"&gt;\(y\)&lt;/span&gt; and nothing else, evidenced by its functional form. As such, &lt;span class="math"&gt;\(P(\mathbf{x})\)&lt;/span&gt; is not required.&lt;/p&gt;
&lt;h2&gt;Hidden Markov models and beyond&lt;/h2&gt;
&lt;p&gt;Finally, hidden Markov models are to naive Bayes what conditional random fields are to softmax regression: the former in each pair builds upon the latter by modeling a &lt;em&gt;sequence&lt;/em&gt; of labels. This graphic&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; gives a bit more insight into these relationships:&lt;/p&gt;
&lt;p&gt;&lt;img alt="generative vs. discriminative models" class="img-responsive" src="https://cavaunpeu.github.io/images/generative_discriminative_models_flowchart.png"/&gt;&lt;/p&gt;
&lt;h2&gt;Where does &lt;span class="math"&gt;\(e\)&lt;/span&gt; come from?&lt;/h2&gt;
&lt;p&gt;Equation (2) states that the numerator of the softmax, i.e. the exponentiated linear combination of input elements, is equivalent to the unnormalized joint probability of our inputs and outputs as given by the Gibbs distribution factor product.&lt;/p&gt;
&lt;p&gt;However, this only holds if one of the following two are true:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our factors are of the form &lt;span class="math"&gt;\(e^{z}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Our factors take any arbitrary form, and we "anticipate" that this form will be exponentiated within the softmax function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Remember, the point of this exponentiation was to put our weighted input elements "on the arithmetic path to becoming valid probabilities," i.e. to make them strictly positive. This said, there is nothing (to my knowledge) that mandates that a factor produce a strictly positive number. So which came first — the chicken or the egg (the exponent or the softmax)?&lt;/p&gt;
&lt;p&gt;In truth, I'm not actually sure, but I do believe we can safely treat the softmax numerator and an unnormalized Gibbs distribution as equivalent and simply settle on: &lt;em&gt;call it what you will, we need an exponent in there somewhere to put this thing in &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This exercise has made the relationships between canonical machine learning models, activation functions and the basic axiom of conditional probability a whole lot clearer. For more information, please reference the resources below — especially Daphne Koller's material on &lt;a href="https://www.coursera.org/learn/probabilistic-graphical-models"&gt;probabilistic graphical models&lt;/a&gt;. Thanks so much for reading.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=PGBlyKtfB74&amp;amp;index=19&amp;amp;list=P6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH"&gt;Conditional random fields - linear chain CRF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/"&gt;Introduction to Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1011.4088v1.pdf"&gt;An Introduction to Conditional Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=kFcjl3A9QuA&amp;amp;t=559s"&gt;General Gibbs Distribution - Professor Daphne Koller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=2BXoj778YU8&amp;amp;t=636s"&gt;Conditional Random Fields - Professor Daphne Koller&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1011.4088v1.pdf"&gt;An Introduction to Conditional Random Fields&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Approximating Implicit Matrix Factorization with Shallow Neural Networks</title><link href="https://cavaunpeu.github.io/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/" rel="alternate"></link><published>2017-04-07T10:07:00-04:00</published><updated>2017-04-07T10:07:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-04-07:/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/</id><summary type="html">&lt;p&gt;In this post, we look to beat the performance of &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Implicit Matrix Factorization&lt;/a&gt; on a recommendation task using 5 different neural network architectures.&lt;/p&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Abstract"&gt;Abstract&lt;a class="anchor-link" href="#Abstract"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Implicit-Matrix-Factorization"&gt;Implicit Matrix Factorization&lt;a class="anchor-link" href="#Implicit-Matrix-Factorization"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The Implicit Matrix Factorization (IMF) algorithm as presented by &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Hu, Koren and Volinksy&lt;/a&gt; is a widely popular, effective method for recommending items to users. This approach was born from necessity: while explicit feedback as to our users' tastes - a questionnaire completed at user signup, for example - makes building a recommendation engine straightforward, we often have nothing more than &lt;em&gt;implicit feedback&lt;/em&gt; data - view count, click count, time spent on page, for example - which serve instead as a proxy for these preferences. Crucially, the latter feedback is asymmetric: while a high view count might indicate positive preference for a given item, a low view count cannot be said to do the opposite. Perhaps, the user simply didn't know the item was there.&lt;/p&gt;
&lt;p&gt;IMF begins with a ratings matrix $R$, where $R_{u, i}$ gives the implicit feedback value observed for user $u$ and item $i$. Next, it constructs two other matrices defined as follows:&lt;/p&gt;
$$
p_{u, i} =
\begin{cases}
1 &amp;amp; r_{u, i} \gt 0\\
0 &amp;amp; r_{u, i} = 0
\end{cases}\\
c_{u, i} = 1 + \alpha r_{u, i}
$$&lt;p&gt;$P$ gives a binary matrix indicating our belief in the existence of each user's preference for each item. $C$ gives our &lt;em&gt;confidence&lt;/em&gt; in the existence of each user's preference for each item where, trivially, larger values of $r_{u, i}$ give us higher confidence that user $u$ indeed likes item $i$.&lt;/p&gt;
&lt;p&gt;Next, IMF outlines its goal: let's embed each user and item into $\mathbb{R}^f$ such that their dot product approximates the former's true preference for the latter. Finally, and naming user vectors $x_u \in \mathbb{R}^f$ and item vectors $y_i \in \mathbb{R}^f$, we compute the argmin of the following objective:&lt;/p&gt;
$$
\underset{x_{*}, y_{*}}{\arg\min}\sum\limits_{u, i}c_{u, i}\big(p_{u, i} - x_u^Ty_i\big)^2 + \lambda\bigg(\sum\limits_u\|x_u\|^2 + \sum\limits_i\|y_u\|^2\bigg)
$$&lt;p&gt;Once sufficiently minimized, we can compute expected preferences $\hat{p}_{u, i} = x_u^Ty_i$ for unobserved $\text{(user, item)}$ pairs; recommendation then becomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For a given user $u$, compute predicted preferences $\hat{p}_{u, i} = x_u^Ty_i$ for all items $i$.&lt;/li&gt;
&lt;li&gt;Sort the list in descending order.&lt;/li&gt;
&lt;li&gt;Returning the top $n$ items.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="Shallow-neural-networks"&gt;Shallow neural networks&lt;a class="anchor-link" href="#Shallow-neural-networks"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;IMF effectively gives a function $f: u, i \rightarrow \hat{p}_{u, i}$. As before, our goal is to minimize the function above, which we can now rewrite as:&lt;/p&gt;
$$
\underset{x_{*}, y_{*}}{\arg\min}\sum\limits_{u, i}c_{u, i}\big(p_{u, i} - f(u, i)\big)^2 + \lambda\bigg(\sum\limits_u\|x_u\|^2 + \sum\limits_i\|y_u\|^2\bigg)
$$&lt;p&gt;To approximate this function, I turn to our favorite &lt;a href="http://neuralnetworksanddeeplearning.com/chap4.html"&gt;universal function approximator&lt;/a&gt;: neural networks optimized with stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;This work is built around a toy web application I authored long ago: &lt;a href="http://dotify.herokuapp.com/"&gt;dotify&lt;/a&gt;. At present, dotify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pulls data nightly from &lt;a href="https://spotifycharts.com/regional"&gt;Spotify Charts&lt;/a&gt;. These data contain the number of streams for that day's top 200 songs for each of 55 countries.&lt;/li&gt;
&lt;li&gt;Computes an implicit matrix factorization nightly, giving vectors for both countries and songs.&lt;/li&gt;
&lt;li&gt;Allows the user to input a "country-arithmetic" expression, i.e. "I want music like &lt;code&gt;Colombia x Turkey - Germany&lt;/code&gt;." It then performs this arithmetic with the chosen vectors and recommends songs to the composite.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this work, I first fit and cross-validate an implicit matrix factorization model, establishing the three requisite parameters: $f$, the dimensionality of the latent vectors; $\alpha$, the scalar multiple used in computing $C$; $\lambda$ the regularization strength used in on our loss function.&lt;/p&gt;
&lt;h3 id="Network-architectures"&gt;Network architectures&lt;a class="anchor-link" href="#Network-architectures"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Next, I explore five different shallow neural network architectures in attempt to improve upon the observed results. These architectures are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A trivially "Siamese" network which first embeds each country and song index into $\mathbb{R}^f$ in parallel then computes a dot-product of the embeddings. This is roughly equivalent to what is being done by IMF.&lt;/li&gt;
&lt;li&gt;Same as previous, but with a bias embedding for each set, in $\mathbb{R}$, added to the dot-product.&lt;/li&gt;
&lt;li&gt;Same as &lt;a href="#network_1"&gt;#1&lt;/a&gt;, but concatenate the vectors instead. Then, stack 3 fully-connected layers with ReLU activations, batch normalization after each, and dropout after the first. Finally, add a 1-unit dense layer on the end, and add bias embeddings to the result. (NB: I wanted to add the bias embeddings to the respective $\mathbb{R}^f$ embeddings at the outset, but couldn't figure out how to do this in Keras.)&lt;/li&gt;
&lt;li&gt;Same as &lt;a href="#network_2"&gt;#2&lt;/a&gt;, except feed in the song title text as well. This text is first tokenized, then padded to a maximum sequence length, then embedded into a fixed-length vector by an LSTM, then reduced to a single value by a dense layer with a ReLU activation. Finally, this scalar is concatenated to the scalar output that &lt;a href="#network_2"&gt;#2&lt;/a&gt; would produce, and the result is fed into a final dense layer with a linear activation - i.e. a linear combination of the two.&lt;/li&gt;
&lt;li&gt;Same as &lt;a href="#network_4"&gt;#4&lt;/a&gt;, except feed in the song artist index as well. This index is first embedded into a vector, then reduced to a scalar by a dense layer with a ReLU activation. Finally, this scalar is concatenated with the two scalars produced in the second-to-last layer of &lt;a href="#network_4"&gt;#4&lt;/a&gt;, then fed into a final dense layer with a linear activation. Like the previous, this is a linear combination of the three inputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="Results"&gt;Results&lt;a class="anchor-link" href="#Results"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;While the various network architectures are intruiging for both the ease with which we can incorporate multiple, disparate inputs into our function, and the ability to then train this function end-to-end with respect to our main minimization objective, we find no reason to prefer them over the time-worn Implicit Matrix Factorization algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Data-preparation"&gt;Data preparation&lt;a class="anchor-link" href="#Data-preparation"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To construct our ratings matrix, I take the sum of total streams for each $\text{(country, song)}$ pair. The data are limited to a given number of "top songs," defined as a song that appeared on Spotify Charts &lt;em&gt;on a given date&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Because the values exist on wildly different orders of magnitude, I scale the results as follows:&lt;/p&gt;
$$\tilde{r}_{u, i} = \log{\bigg(\frac{1 + r_{u, i}}{\epsilon}\bigg)}$$&lt;p&gt;To start, let's build a ratings matrix for a small sample of the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [143]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RatingsMatrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_top_songs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[143]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;5552&lt;/th&gt;
&lt;th&gt;5553&lt;/th&gt;
&lt;th&gt;5557&lt;/th&gt;
&lt;th&gt;5558&lt;/th&gt;
&lt;th&gt;5560&lt;/th&gt;
&lt;th&gt;5562&lt;/th&gt;
&lt;th&gt;5565&lt;/th&gt;
&lt;th&gt;5582&lt;/th&gt;
&lt;th&gt;5583&lt;/th&gt;
&lt;th&gt;5585&lt;/th&gt;
&lt;th&gt;...&lt;/th&gt;
&lt;th&gt;33062&lt;/th&gt;
&lt;th&gt;33064&lt;/th&gt;
&lt;th&gt;33065&lt;/th&gt;
&lt;th&gt;33066&lt;/th&gt;
&lt;th&gt;33067&lt;/th&gt;
&lt;th&gt;33068&lt;/th&gt;
&lt;th&gt;33069&lt;/th&gt;
&lt;th&gt;33070&lt;/th&gt;
&lt;th&gt;33071&lt;/th&gt;
&lt;th&gt;33072&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;country_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;3.643437&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;2.546472&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;3.597833&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;3.175760&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;1.138153&lt;/td&gt;
&lt;td&gt;0.961264&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 2110 columns&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Next, let's get an idea of the sparsity of our data and how many songs each country has streamed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [144]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sparsity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;non_zero_entries_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sparsity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Our ratings matrix has &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;% non-zero entries.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;non_zero_entries_percent&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Our ratings matrix has 7.3999999999999995% non-zero entries.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [145]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Our ratings matrix contains &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; countries and &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; unique songs.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Our ratings matrix contains 55 countries and 2110 unique songs.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [146]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Distribution of # of Unique Songs Streamed per Country'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Unique Songs Streamed'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'# of Countries'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlwAAAGDCAYAAAD+nM7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Construct-training,-validation-sets"&gt;Construct training, validation sets&lt;a class="anchor-link" href="#Construct-training,-validation-sets"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In reviewing literature on recommender engine evaluation, it seems common to create training and validation sets as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Filter your ratings matrix for users (countries) that meet some criterion, i.e. they've streamed above a certain threshold of songs.&lt;/li&gt;
&lt;li&gt;Select a random $x\%$ of their items (songs).&lt;/li&gt;
&lt;li&gt;"Move" these items into a validation matrix; set them to 0 in the training matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To construct this split, I first compute a sensible threshold for Step #1, then "move" a random 20% of songs from the training matrix to the validation matrix.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [147]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The 15th percentile of songs rated by country is &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;The 15th percentile of songs rated by country is 54.5.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's choose 50 songs as our cutoff, and move 20% of the songs in qualifying rows to a validation set.&lt;/p&gt;
&lt;p&gt;NB: The actual ratings matrix is located at &lt;code&gt;RatingsMatrix.R_ui&lt;/code&gt;. This reflects an API choice I made when first starting this project.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [12]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;split_ratings_matrix_into_training_and_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fraction_to_drop&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;training_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;
    &lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deepcopy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        
        &lt;span class="n"&gt;rated_songs_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;rated_songs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;rated_songs_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;n_songs_to_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rated_songs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;fraction_to_drop&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;songs_to_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rated_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        
        &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;songs_to_drop&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [152]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;more_than_50_ratings_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;split_ratings_matrix_into_training_and_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_50_ratings_mask&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Evaluation"&gt;Evaluation&lt;a class="anchor-link" href="#Evaluation"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Evaluating recommender systems is an inexact science because there is no "right" answer. In production, this evaluation is often done via the A/B test of a proxy metric important to the business - revenue, for example. In training, the process is less clear. To this end, the authors of the IMF paper offer the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Evaluation of implicit-feedback recommender requires appropriate measures. In the traditional setting where a user is specifying a numeric score, there are clear metrics such as mean squared error to measure success in prediction. However with implicit models we have to take into account availability of the item, competition for the item with other items, and repeat feedback. For example, if we gather data on television viewing, it is unclear how to evaluate a show that has been watched more than once, or how to compare two shows that are on at the same time, and hence cannot both be watched by the user.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Additionally, they state:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It is important to realize that we do not have a reliable feedback regarding which programs are unloved, as not watching a program can stem from multiple different reasons. In addition, we are currently unable to track user reactions to our recommendations. Thus, precision based metrics are not very appropriate, as they require knowing which programs are undesired to a user. However, watching a program is an indication of liking it, making recall-oriented measures applicable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In solution, they propose evaluating the "expected percentile ranking" defined as follows:&lt;/p&gt;
$$
\overline{\text{rank}} = \frac{\sum_{u, i}\tilde{r}_{u, i}^t\text{rank}_{u, i}}{\sum_{u, i}\tilde{r}_{u, i}^t}
$$&lt;p&gt;Here, $\text{rank}_{u, i}$ gives the percentile-ranking of the predicted preference, i.e. if $\hat{p}_{u = 17, i = 34}$ is the largest of all predicted preferences, then $\text{rank}_{u = 17, i = 34} = 0\%$. Similarly, the smallest of the predicted preferences, i.e. the last on the list, equals $100\%$.&lt;/p&gt;
&lt;p&gt;The following class accepts a training matrix, validation matrix and a matrix of predicted preferences. It then exposes the mean expected percentile ranking for both training and validation sets as properties on the instance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ExpectedPercentileRankingsEvaluator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predicted_preferences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate_train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate_validation&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_evaluate_train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_evaluate_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;expected_percentile_rankings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;preferences&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
                &lt;span class="s1"&gt;'predicted_preference'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="s1"&gt;'rank'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="s1"&gt;'percentile_rank'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preferences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;ground_truth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;numerator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ground_truth&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'percentile_rank'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;ground_truth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;denominator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ground_truth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;expected_percentile_rankings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;numerator&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;denominator&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;expected_percentile_rankings&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mean_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mean_expected_percentile_rankings_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_expected_percentile_rankings_validation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Grid-search"&gt;Grid search&lt;a class="anchor-link" href="#Grid-search"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Next, we perform a basic grid search to find reasonable values for $\alpha$ and $\lambda$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;grid_search_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Result'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'alpha lmbda'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;alpha_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;lmbda_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;alpha_values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;lmbda_values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;implicit_mf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ImplicitMF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;predicted_preferences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;country_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;song_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        
        &lt;span class="n"&gt;evaluator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ExpectedPercentileRankingsEvaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_50_ratings_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preferences&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_validation&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's visualize the results for clarity. I plot the opposite of the validation score such that the parameters corresponding to the &lt;em&gt;darkest&lt;/em&gt; square are most favorable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [159]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;grid_search_items&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
&lt;span class="n"&gt;grid_search_items&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;grid_search_items&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;validation_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;validation_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Reds'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;alpha_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha_values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lbmda_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda_values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lbmda_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\alpha$ = &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\lambda$ = &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; 
            &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'center'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'center'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'w'&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Grid Search Results'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\lambda$ Values'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;'$\alpha$ Values'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAyQAAAJSCAYAAAAyBgFsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [58]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The best parameters were found to be: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;The best parameters were found to be: Result(alpha=0.1, lmbda=10.0).
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Train-a-final-model"&gt;Train a final model&lt;a class="anchor-link" href="#Train-a-final-model"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Using these parameters, let's train a final implicit matrix factorization model using 1,000,000 top songs. Additionally, we'll choose $\epsilon = 1,000$ which seemed to work best in simple experimentation. This experimentation is not shown here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [7]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RatingsMatrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_top_songs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [163]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Our ratings matrix contains &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; countries and &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; unique songs.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Our ratings matrix contains 55 countries and 8663 unique songs.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;implicit_mf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ImplicitMF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Visualize"&gt;Visualize&lt;a class="anchor-link" href="#Visualize"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Let's plot the cosine similarities between all pairs of countries to confirm that things make sense. Intuitively, I'd think that countries in the following groups should be similar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;United States, United Kingdom, Canada, Australia, New Zealand&lt;/li&gt;
&lt;li&gt;Latin American countries&lt;/li&gt;
&lt;li&gt;Sweden, Finland, Norway, Denmark&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;First let's replace the index of our vectors such that it contains the country names.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [165]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;country_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;country_id_to_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;countries_lookup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'id'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;countries_lookup&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;country_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;
&lt;span class="n"&gt;country_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Index&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;country_id_to_name&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;country_ids&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_name'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_names&lt;/span&gt;

&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[165]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;dim_0&lt;/th&gt;
&lt;th&gt;dim_1&lt;/th&gt;
&lt;th&gt;dim_2&lt;/th&gt;
&lt;th&gt;dim_3&lt;/th&gt;
&lt;th&gt;dim_4&lt;/th&gt;
&lt;th&gt;dim_5&lt;/th&gt;
&lt;th&gt;dim_6&lt;/th&gt;
&lt;th&gt;dim_7&lt;/th&gt;
&lt;th&gt;dim_8&lt;/th&gt;
&lt;th&gt;dim_9&lt;/th&gt;
&lt;th&gt;...&lt;/th&gt;
&lt;th&gt;dim_20&lt;/th&gt;
&lt;th&gt;dim_21&lt;/th&gt;
&lt;th&gt;dim_22&lt;/th&gt;
&lt;th&gt;dim_23&lt;/th&gt;
&lt;th&gt;dim_24&lt;/th&gt;
&lt;th&gt;dim_25&lt;/th&gt;
&lt;th&gt;dim_26&lt;/th&gt;
&lt;th&gt;dim_27&lt;/th&gt;
&lt;th&gt;dim_28&lt;/th&gt;
&lt;th&gt;dim_29&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;country_name&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;United States&lt;/th&gt;
&lt;td&gt;-1.276051&lt;/td&gt;
&lt;td&gt;-1.058583&lt;/td&gt;
&lt;td&gt;-0.622779&lt;/td&gt;
&lt;td&gt;0.430231&lt;/td&gt;
&lt;td&gt;0.306830&lt;/td&gt;
&lt;td&gt;0.966786&lt;/td&gt;
&lt;td&gt;0.190530&lt;/td&gt;
&lt;td&gt;-0.549356&lt;/td&gt;
&lt;td&gt;0.538788&lt;/td&gt;
&lt;td&gt;-0.722162&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.057478&lt;/td&gt;
&lt;td&gt;0.500288&lt;/td&gt;
&lt;td&gt;0.799253&lt;/td&gt;
&lt;td&gt;0.383243&lt;/td&gt;
&lt;td&gt;0.575051&lt;/td&gt;
&lt;td&gt;0.247066&lt;/td&gt;
&lt;td&gt;0.548018&lt;/td&gt;
&lt;td&gt;-0.238643&lt;/td&gt;
&lt;td&gt;0.114481&lt;/td&gt;
&lt;td&gt;0.731294&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;United Kingdom&lt;/th&gt;
&lt;td&gt;0.106521&lt;/td&gt;
&lt;td&gt;0.346207&lt;/td&gt;
&lt;td&gt;-0.276669&lt;/td&gt;
&lt;td&gt;-0.003249&lt;/td&gt;
&lt;td&gt;-1.294024&lt;/td&gt;
&lt;td&gt;-0.090318&lt;/td&gt;
&lt;td&gt;0.666301&lt;/td&gt;
&lt;td&gt;-0.533962&lt;/td&gt;
&lt;td&gt;0.281620&lt;/td&gt;
&lt;td&gt;-0.268332&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;-0.803658&lt;/td&gt;
&lt;td&gt;-0.498773&lt;/td&gt;
&lt;td&gt;-0.041302&lt;/td&gt;
&lt;td&gt;-0.705815&lt;/td&gt;
&lt;td&gt;0.624482&lt;/td&gt;
&lt;td&gt;-0.007293&lt;/td&gt;
&lt;td&gt;1.335080&lt;/td&gt;
&lt;td&gt;-0.717667&lt;/td&gt;
&lt;td&gt;0.019507&lt;/td&gt;
&lt;td&gt;-1.700016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Argentina&lt;/th&gt;
&lt;td&gt;-0.871966&lt;/td&gt;
&lt;td&gt;0.678408&lt;/td&gt;
&lt;td&gt;0.343354&lt;/td&gt;
&lt;td&gt;-0.968614&lt;/td&gt;
&lt;td&gt;-1.099471&lt;/td&gt;
&lt;td&gt;0.014175&lt;/td&gt;
&lt;td&gt;0.031833&lt;/td&gt;
&lt;td&gt;-0.370821&lt;/td&gt;
&lt;td&gt;-0.114270&lt;/td&gt;
&lt;td&gt;-0.982766&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;0.435942&lt;/td&gt;
&lt;td&gt;-0.170878&lt;/td&gt;
&lt;td&gt;-0.172491&lt;/td&gt;
&lt;td&gt;-0.158596&lt;/td&gt;
&lt;td&gt;0.445267&lt;/td&gt;
&lt;td&gt;-0.949628&lt;/td&gt;
&lt;td&gt;0.542433&lt;/td&gt;
&lt;td&gt;-0.157643&lt;/td&gt;
&lt;td&gt;0.765119&lt;/td&gt;
&lt;td&gt;0.587947&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Australia&lt;/th&gt;
&lt;td&gt;-1.129085&lt;/td&gt;
&lt;td&gt;0.212391&lt;/td&gt;
&lt;td&gt;0.136416&lt;/td&gt;
&lt;td&gt;-0.665215&lt;/td&gt;
&lt;td&gt;0.452084&lt;/td&gt;
&lt;td&gt;-0.418566&lt;/td&gt;
&lt;td&gt;-0.187979&lt;/td&gt;
&lt;td&gt;0.807976&lt;/td&gt;
&lt;td&gt;-0.693282&lt;/td&gt;
&lt;td&gt;-0.723989&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;-0.961785&lt;/td&gt;
&lt;td&gt;-0.064047&lt;/td&gt;
&lt;td&gt;-0.161033&lt;/td&gt;
&lt;td&gt;0.211444&lt;/td&gt;
&lt;td&gt;0.180285&lt;/td&gt;
&lt;td&gt;0.597487&lt;/td&gt;
&lt;td&gt;-0.143220&lt;/td&gt;
&lt;td&gt;0.336241&lt;/td&gt;
&lt;td&gt;-0.251201&lt;/td&gt;
&lt;td&gt;-1.057264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;Austria&lt;/th&gt;
&lt;td&gt;-0.094485&lt;/td&gt;
&lt;td&gt;0.870111&lt;/td&gt;
&lt;td&gt;-0.328650&lt;/td&gt;
&lt;td&gt;-0.594735&lt;/td&gt;
&lt;td&gt;-0.429165&lt;/td&gt;
&lt;td&gt;-0.955718&lt;/td&gt;
&lt;td&gt;0.151173&lt;/td&gt;
&lt;td&gt;0.250305&lt;/td&gt;
&lt;td&gt;-0.302996&lt;/td&gt;
&lt;td&gt;0.718965&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;-0.409435&lt;/td&gt;
&lt;td&gt;-0.381884&lt;/td&gt;
&lt;td&gt;-0.144373&lt;/td&gt;
&lt;td&gt;0.340587&lt;/td&gt;
&lt;td&gt;0.006711&lt;/td&gt;
&lt;td&gt;0.870717&lt;/td&gt;
&lt;td&gt;0.067101&lt;/td&gt;
&lt;td&gt;-0.361675&lt;/td&gt;
&lt;td&gt;0.742727&lt;/td&gt;
&lt;td&gt;1.341254&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 30 columns&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Then, we'll plot the cosine similarities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [166]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"white"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_cosine_similarities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;similarities_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cosine_similarity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;similarities_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;triu_indices_from&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;

    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;cmap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diverging_palette&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;220&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;as_cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;heatmap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;similarities_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lower_triangle_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;vmax&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;xticklabels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;yticklabels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;linewidths&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;cbar_kws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"shrink"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; 
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Cosine Similarity Matrix'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [167]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_cosine_similarities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJEAAAQ6CAYAAAD0leyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I first turn to my favorite country - Colombia - to inspect its similarity with other Latin American countries. The numbers are high for: Peru, Paraguay, Panama, Nicaragua, Gautemala, Ecuador, Dominican Republic and Costa Rica.&lt;/p&gt;
&lt;p&gt;Next, I turn to the United Kingdom: it is most similar with Switzerland, New Zealand, Ireland, Hungary, Belgium and Australia.&lt;/p&gt;
&lt;p&gt;Finally, we see that Sweden is similar enough to Norway, yet not Finland (who, incidentally, seems to have nothing in common with anyone at all).&lt;/p&gt;
&lt;p&gt;Overall, this looks very good to me.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Visualize-with-TSNE"&gt;Visualize with TSNE&lt;a class="anchor-link" href="#Visualize-with-TSNE"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To further inspect similarities between countries, let's explore the 2-dimensional TSNE space.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [205]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'pca'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12345&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'dim_1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'dim_2'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [206]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_tsne_embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_embeddings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;country_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;country_embeddings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterrows&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;dim_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_embedding&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim_2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xytext&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;textcoords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'offset points'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'right'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'bottom'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Two-Dimensional TSNE Embeddings of Latent Country Vectors'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dimension 1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Dimension 2'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [207]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_tsne_embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA34AAANtCAYAAADPa6khAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This is not particularly useful. Furthermore, and perhaps most importantly, the plot varies immensely when changing the random seed. This is a worthwhile step nonetheless.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Inspect-predicted-preferences"&gt;Inspect predicted preferences&lt;a class="anchor-link" href="#Inspect-predicted-preferences"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Crucially, we'll now inspect predicted preferences for select countries and confirm that they make sense. I first normalize each of the country and song vectors, then compute predicted preferences via the dot product.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SONG_METADATA_QUERY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;span class="s2"&gt;    SELECT &lt;/span&gt;
&lt;span class="s2"&gt;        songs.title as song_title, &lt;/span&gt;
&lt;span class="s2"&gt;        songs.artist as song_artist,&lt;/span&gt;
&lt;span class="s2"&gt;        songs.id as song_id&lt;/span&gt;
&lt;span class="s2"&gt;    FROM songs&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;

&lt;span class="n"&gt;song_metadata_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_sql&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SONG_METADATA_QUERY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ENGINE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;song_vectors_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;implicit_mf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;song_vectors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'inner'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;\
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [209]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="United-States"&gt;United States&lt;a class="anchor-link" href="#United-States"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [210]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'display.max_colwidth'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [211]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'United States'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[211]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Dab of Ranch - Recorded at Spotify Studios NYC&lt;/td&gt;
&lt;td&gt;Migos&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Massage In My Room&lt;/td&gt;
&lt;td&gt;Future&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Father Stretch My Hands Pt. 1&lt;/td&gt;
&lt;td&gt;Kanye West&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Keep On&lt;/td&gt;
&lt;td&gt;Kehlani&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;First Day Out&lt;/td&gt;
&lt;td&gt;Tee Grizzley&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;When I Was Broke&lt;/td&gt;
&lt;td&gt;Future&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Star Of The Show&lt;/td&gt;
&lt;td&gt;Thomas Rhett&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Rose Golden&lt;/td&gt;
&lt;td&gt;Kid Cudi&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;oui&lt;/td&gt;
&lt;td&gt;Jeremih&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Everyday We Lit&lt;/td&gt;
&lt;td&gt;YFN Lucci&lt;/td&gt;
&lt;td&gt;0.899299&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Colombia"&gt;Colombia&lt;a class="anchor-link" href="#Colombia"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [212]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Colombia'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[212]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Ya No Me Duele Más&lt;/td&gt;
&lt;td&gt;Silvestre Dangond&lt;/td&gt;
&lt;td&gt;0.941204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Bajo el agua&lt;/td&gt;
&lt;td&gt;Manuel Medrano&lt;/td&gt;
&lt;td&gt;0.919997&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Si No Te Quiere&lt;/td&gt;
&lt;td&gt;Ozuna&lt;/td&gt;
&lt;td&gt;0.916971&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;El Ganador&lt;/td&gt;
&lt;td&gt;Nicky Jam&lt;/td&gt;
&lt;td&gt;0.898030&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Oiga, Mira, Vea&lt;/td&gt;
&lt;td&gt;Guayacán Orquesta&lt;/td&gt;
&lt;td&gt;0.897944&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Mil Lágrimas&lt;/td&gt;
&lt;td&gt;Nicky Jam&lt;/td&gt;
&lt;td&gt;0.897863&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Déjame Te Explico&lt;/td&gt;
&lt;td&gt;Reykon&lt;/td&gt;
&lt;td&gt;0.897396&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Quédate&lt;/td&gt;
&lt;td&gt;Manuel Medrano&lt;/td&gt;
&lt;td&gt;0.896307&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Afuera del planeta&lt;/td&gt;
&lt;td&gt;Manuel Medrano&lt;/td&gt;
&lt;td&gt;0.895686&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Materialista&lt;/td&gt;
&lt;td&gt;Silvestre Dangond&lt;/td&gt;
&lt;td&gt;0.895501&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Turkey"&gt;Turkey&lt;a class="anchor-link" href="#Turkey"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [213]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Turkey'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[213]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Dudak&lt;/td&gt;
&lt;td&gt;Edis&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Benim Karanlık Yanım&lt;/td&gt;
&lt;td&gt;Sezen Aksu&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Affection&lt;/td&gt;
&lt;td&gt;Cigarettes After Sex&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Ateş Böceği&lt;/td&gt;
&lt;td&gt;Mithat Can Özer&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Beni Öyle Bilme&lt;/td&gt;
&lt;td&gt;Ceylan Ertem&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Keyfim Kaçık Acık&lt;/td&gt;
&lt;td&gt;Can Bonomo&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Seni Unutmaya Ömrüm Yeter mi&lt;/td&gt;
&lt;td&gt;Ümit Besen&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Vazgeçtim&lt;/td&gt;
&lt;td&gt;Sezen Aksu&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Yanımda Kal&lt;/td&gt;
&lt;td&gt;Harun Kolçak&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Stanga&lt;/td&gt;
&lt;td&gt;Sagi Abitbul&lt;/td&gt;
&lt;td&gt;0.943574&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Germany"&gt;Germany&lt;a class="anchor-link" href="#Germany"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [214]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Germany'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[214]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Skandale&lt;/td&gt;
&lt;td&gt;Bonez MC&lt;/td&gt;
&lt;td&gt;0.908499&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Ahnma&lt;/td&gt;
&lt;td&gt;Beginner&lt;/td&gt;
&lt;td&gt;0.906591&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Am schönsten&lt;/td&gt;
&lt;td&gt;SDP&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Tage wie diese&lt;/td&gt;
&lt;td&gt;Die Toten Hosen&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Kartell (feat. Azet &amp;amp; Nash)&lt;/td&gt;
&lt;td&gt;Zuna&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Chill mal dein Leben (feat. Moe Phoenix)&lt;/td&gt;
&lt;td&gt;Kianush&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Neuanfang&lt;/td&gt;
&lt;td&gt;Clueso&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Neben der Spur&lt;/td&gt;
&lt;td&gt;Maxwell&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Hochspannung&lt;/td&gt;
&lt;td&gt;Maxwell&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Intro&lt;/td&gt;
&lt;td&gt;Capital Bra&lt;/td&gt;
&lt;td&gt;0.906250&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Taiwan"&gt;Taiwan&lt;a class="anchor-link" href="#Taiwan"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [215]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[215]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;如果的事&lt;/td&gt;
&lt;td&gt;王藍茵&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;小小的&lt;/td&gt;
&lt;td&gt;思衛&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Heathens (feat. MUTEMATH)&lt;/td&gt;
&lt;td&gt;Twenty One Pilots&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;其實都沒有&lt;/td&gt;
&lt;td&gt;楊宗緯&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;對摺 - 國語&lt;/td&gt;
&lt;td&gt;任賢齊&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;最難的是相遇&lt;/td&gt;
&lt;td&gt;Valen Hsu&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;안녕 못해 I'm Not Okay (From “MISSING 9”)&lt;/td&gt;
&lt;td&gt;CHEN&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;未接來電&lt;/td&gt;
&lt;td&gt;楊乃文&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;飛機 - feat. 林俊峰&lt;/td&gt;
&lt;td&gt;林俊傑&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;All On Me&lt;/td&gt;
&lt;td&gt;Devin Dawson&lt;/td&gt;
&lt;td&gt;0.954455&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;These look great.&lt;/p&gt;
&lt;p&gt;One interesting thing to note is the significance of the length of the vectors. To this point, &lt;a href="https://arxiv.org/abs/1508.02297"&gt;Schakel and Wilson&lt;/a&gt; offer the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A word that is consistently used in a similar context will be represented by a longer vector than a word of the same frequency that is used in different contexts.&lt;/p&gt;
&lt;p&gt;Not only the direction, but also the length of word vectors carries important information.&lt;/p&gt;
&lt;p&gt;Word vector length furnishes, in combination with term frequency, a useful measure of word significance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our case, my guess is that length indicates something like popularity - at least for songs. Let's see which songs and countries have the largest L2 norms.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [216]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[216]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Shape of You&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.687497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Starboy&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.684529&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;I Don’t Wanna Live Forever (Fifty Shades Darker) - From "Fifty Shades Darker (Original Motion Pi...&lt;/td&gt;
&lt;td&gt;ZAYN&lt;/td&gt;
&lt;td&gt;0.682996&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Despacito (Featuring Daddy Yankee)&lt;/td&gt;
&lt;td&gt;Luis Fonsi&lt;/td&gt;
&lt;td&gt;0.682941&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Rockabye (feat. Sean Paul &amp;amp; Anne-Marie)&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;0.682918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.682878&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Call On Me - Ryan Riback Extended Remix&lt;/td&gt;
&lt;td&gt;Starley&lt;/td&gt;
&lt;td&gt;0.682799&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.682442&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.682225&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Closer&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.682175&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [217]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[217]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country_name&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;4.842656&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Sweden&lt;/td&gt;
&lt;td&gt;4.602653&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Finland&lt;/td&gt;
&lt;td&gt;4.415459&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;France&lt;/td&gt;
&lt;td&gt;4.399233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;United Kingdom&lt;/td&gt;
&lt;td&gt;4.374870&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Netherlands&lt;/td&gt;
&lt;td&gt;4.330190&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Denmark&lt;/td&gt;
&lt;td&gt;4.232457&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Italy&lt;/td&gt;
&lt;td&gt;4.218088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Norway&lt;/td&gt;
&lt;td&gt;4.084380&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Germany&lt;/td&gt;
&lt;td&gt;4.076692&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;To reference Taiwan and Colombia, let's inspect predictions using unnormalized vectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [218]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[218]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Bad Things - With Camila Cabello&lt;/td&gt;
&lt;td&gt;Machine Gun Kelly&lt;/td&gt;
&lt;td&gt;0.958864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Just Hold On&lt;/td&gt;
&lt;td&gt;Steve Aoki&lt;/td&gt;
&lt;td&gt;0.958692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Shape of You&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.958325&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Side To Side&lt;/td&gt;
&lt;td&gt;Ariana Grande&lt;/td&gt;
&lt;td&gt;0.957497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Closer&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.957290&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Starboy&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.957090&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.957001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;I Don’t Wanna Live Forever (Fifty Shades Darker) - From "Fifty Shades Darker (Original Motion Pi...&lt;/td&gt;
&lt;td&gt;ZAYN&lt;/td&gt;
&lt;td&gt;0.956933&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.956803&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;The Greatest&lt;/td&gt;
&lt;td&gt;Sia&lt;/td&gt;
&lt;td&gt;0.956746&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [219]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Colombia'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[219]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;I Got You&lt;/td&gt;
&lt;td&gt;Bebe Rexha&lt;/td&gt;
&lt;td&gt;1.019838&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Party Monster&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.993647&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Hello&lt;/td&gt;
&lt;td&gt;Adele&lt;/td&gt;
&lt;td&gt;0.993421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;I Would Like&lt;/td&gt;
&lt;td&gt;Zara Larsson&lt;/td&gt;
&lt;td&gt;0.987099&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Scars To Your Beautiful&lt;/td&gt;
&lt;td&gt;Alessia Cara&lt;/td&gt;
&lt;td&gt;0.987036&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Weak&lt;/td&gt;
&lt;td&gt;AJR&lt;/td&gt;
&lt;td&gt;0.986642&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Too Good&lt;/td&gt;
&lt;td&gt;Drake&lt;/td&gt;
&lt;td&gt;0.982179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;Nancy Mulligan&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.979373&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Supermarket Flowers&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.978706&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;Hearts Don't Break Around Here&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.978494&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Respectively, there is not a song with Chinese nor Spanish letters in each set of recommendations. Additionally, it seems like the world is fond of Ed Sheeran. It is clear that the global popularity of songs dominates our recommendations when computing the dot product with unnormalized vectors.&lt;/p&gt;
&lt;p&gt;When we do normalize, our predicted preferences can be defined thus:&lt;/p&gt;
$$
\hat{x}_u = \frac{x_u}{\|x_u\|} \quad \hat{y}_i = \frac{y_i}{\|y_i\|}\\
\hat{p}_{u, i} = \hat{x}_u \cdot \hat{y}_i
$$&lt;p&gt;One idea might be to scale the song vectors by their respective lengths, leaving our predicted preferences as:&lt;/p&gt;
$$
\begin{align*}
\hat{p}_{u, i} &amp;amp;= \hat{x}_u \cdot \big(\hat{y}_i \cdot c\|y_i\|\big)\\
&amp;amp;= \hat{x}_u \cdot \Bigg(\hat{y}_i \cdot c\frac{y_i}{\hat{y_i}}\Bigg)\\
&amp;amp;= \hat{x}_u \cdot c y_i
\end{align*}
$$&lt;p&gt;Let's briefly see what this looks like.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [220]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;country_vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;country_vectors_df_norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;song_vectors_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_vec&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[220]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;Bad Things - With Camila Cabello&lt;/td&gt;
&lt;td&gt;Machine Gun Kelly&lt;/td&gt;
&lt;td&gt;0.039601&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;Just Hold On&lt;/td&gt;
&lt;td&gt;Steve Aoki&lt;/td&gt;
&lt;td&gt;0.039594&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;Shape of You&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;0.039578&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;Side To Side&lt;/td&gt;
&lt;td&gt;Ariana Grande&lt;/td&gt;
&lt;td&gt;0.039544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;Closer&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.039536&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;Starboy&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;0.039528&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;Paris&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.039524&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;I Don’t Wanna Live Forever (Fifty Shades Darker) - From "Fifty Shades Darker (Original Motion Pi...&lt;/td&gt;
&lt;td&gt;ZAYN&lt;/td&gt;
&lt;td&gt;0.039521&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;0.039516&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;The Greatest&lt;/td&gt;
&lt;td&gt;Sia&lt;/td&gt;
&lt;td&gt;0.039513&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;About the same. Nevertheless, the length of our vectors can likely offer some value when balancing local recommendations with globally popular songs. I leave this to the reader to explore further for now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Approximating-IMF-with-neural-networks"&gt;Approximating IMF with neural networks&lt;a class="anchor-link" href="#Approximating-IMF-with-neural-networks"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Again, IMF gives a function $f: u, i \rightarrow \hat{p}_{u, i}$ and the following objective function to minimize:&lt;/p&gt;
$$
\underset{x_{*}, y_{*}}{\arg\min}\sum\limits_{u, i}c_{u, i}\big(p_{u, i} - f(u, i)\big)^2 + \lambda\bigg(\sum\limits_u\|x_u\|^2 + \sum\limits_i\|y_u\|^2\bigg)
$$&lt;p&gt;Let's try to do this with neural networks. I'll use the $\alpha$ and $\lambda$ parameters found above, and keep the same dimensionality, $F = 30$, for our latent vectors. This number was chosen in separate experimentation not shown here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [32]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;backend&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Lambda&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LSTM&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers.merge&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.optimizers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Adam&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.regularizers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;l2&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plot_model&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [9]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Distribution of # of Unique Songs Streamed per Country'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Unique Songs Streamed'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'# of Countries'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAe4AAAFoCAYAAACR/hiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'The 15th percentile of songs rated by country is &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;.'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;The 15th percentile of songs rated by country is 218.20000000000002.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's set our eligibility cutoff at 200 songs and create training and validation matrices.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;more_than_200_ratings_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;songs_rated_by_country&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;split_ratings_matrix_into_training_and_validation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_200_ratings_mask&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Arrange-our-data"&gt;Arrange our data&lt;a class="anchor-link" href="#Arrange-our-data"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To interact with Keras, we'll need to change our data from wide format to long. Additionally, so as to create our embeddings, we'll need to give each country and song a unique, contiguous index. Finally, I'll do the same with the song artist, as well as tokenize the song title, for later use.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[16]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;5552&lt;/th&gt;
&lt;td&gt;El Perdón&lt;/td&gt;
&lt;td&gt;Nicky Jam&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5553&lt;/th&gt;
&lt;td&gt;Lean On (feat. MØ &amp;amp; DJ Snake)&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5555&lt;/th&gt;
&lt;td&gt;Love Me Like You Do - From "Fifty Shades Of Grey"&lt;/td&gt;
&lt;td&gt;Ellie Goulding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5556&lt;/th&gt;
&lt;td&gt;Uptown Funk&lt;/td&gt;
&lt;td&gt;Mark Ronson&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;5557&lt;/th&gt;
&lt;td&gt;Loquita&lt;/td&gt;
&lt;td&gt;Marama&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_id_to_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;c_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="n"&gt;song_id_to_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="n"&gt;song_artist_to_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;artist&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;artist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;())}&lt;/span&gt;

&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_id_to_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_artist_to_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tail&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[17]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_index&lt;/th&gt;
&lt;th&gt;song_artist_index&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;33068&lt;/th&gt;
&lt;td&gt;Affection&lt;/td&gt;
&lt;td&gt;Cigarettes After Sex&lt;/td&gt;
&lt;td&gt;8658&lt;/td&gt;
&lt;td&gt;3699&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33069&lt;/th&gt;
&lt;td&gt;Istemem Soz Sevmeni&lt;/td&gt;
&lt;td&gt;Ferman Akgül&lt;/td&gt;
&lt;td&gt;8659&lt;/td&gt;
&lt;td&gt;3106&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33070&lt;/th&gt;
&lt;td&gt;Skammekroken 2017&lt;/td&gt;
&lt;td&gt;TIX&lt;/td&gt;
&lt;td&gt;8660&lt;/td&gt;
&lt;td&gt;1677&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33071&lt;/th&gt;
&lt;td&gt;Wiosna&lt;/td&gt;
&lt;td&gt;Organek&lt;/td&gt;
&lt;td&gt;8661&lt;/td&gt;
&lt;td&gt;3716&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;33072&lt;/th&gt;
&lt;td&gt;Fuckt.&lt;/td&gt;
&lt;td&gt;Strapo&lt;/td&gt;
&lt;td&gt;8662&lt;/td&gt;
&lt;td&gt;3717&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Tokenize-song-title"&gt;Tokenize song title&lt;a class="anchor-link" href="#Tokenize-song-title"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;NUM_WORDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'title_sequence'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;texts_to_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="Convert-ratings-to-long-format"&gt;Convert ratings to long format&lt;a class="anchor-link" href="#Convert-ratings-to-long-format"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [19]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;input_tuples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;product&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;country_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;input_tuples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;ratings_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;country_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;song_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;R_ui&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="n"&gt;ratings_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'inner'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_id_to_index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-network-parameters"&gt;Define network parameters&lt;a class="anchor-link" href="#Define-network-parameters"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [22]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nunique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;n_songs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nunique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;n_artists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nunique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;
&lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;001&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-a-loss-function"&gt;Define a loss function&lt;a class="anchor-link" href="#Define-a-loss-function"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is simply the IMF objective we'd like to minimize.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [23]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-functions-to-generate,-evaluate-and-visualize-predictions"&gt;Define functions to generate, evaluate and visualize predictions&lt;a class="anchor-link" href="#Define-functions-to-generate,-evaluate-and-visualize-predictions"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [24]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
        &lt;span class="s1"&gt;'prediction'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pivot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'prediction'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [25]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;more_than_200_ratings_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                         &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;evaluator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ExpectedPercentileRankingsEvaluator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eligibility_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;predicted_preferences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Train: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Validation: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evaluator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean_expected_percentile_rankings_validation&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [74]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visualize_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predicted Preferences Histogram'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Predicted Prefence'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Count'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#1"&gt;Network #1&lt;a class="anchor-link" href="#Network-#1"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;A trivially "Siamese" network which first embeds each country and song index into $\mathbb{R}^f$ in parallel then computes a dot-product of the embeddings. This is roughly equivalent to what is being done by IMF.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [162]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [163]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_1.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 1" class="img-responsive" src="https://cavaunpeu.github.io/figures/neural_implicit_mf_network_1.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [164]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 2s - loss: 64.9000 - val_loss: 0.0185
Epoch 2/5
476465/476465 [==============================] - 8s - loss: 0.0744 - val_loss: 0.0185
Epoch 3/5
476465/476465 [==============================] - 3s - loss: 0.0744 - val_loss: 0.0185
Epoch 4/5
476465/476465 [==============================] - 10s - loss: 0.0745 - val_loss: 0.0185
Epoch 5/5
476465/476465 [==============================] - 8s - loss: 0.0745 - val_loss: 0.0186
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [165]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.5053388101677021
Validation: 0.49071882278215817
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;For reference, let's recall the results we computed when validation our model with &lt;code&gt;best_params&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [166]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;grid_search_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;best_params&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[166]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;{'train': 0.04479720392806505, 'validation': 0.18543168148626762}&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Additionally, a &lt;em&gt;random&lt;/em&gt; model should return an expected percentile ranking of ~50%, as noted in &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Hu, Koren and Volinksy&lt;/a&gt;. So, this is really bad. Let's visualize the distribution of predictions before moving on to another model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [167]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;visualize_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnIAAAGDCAYAAACvCP20AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8z/X///H7ewfENkNLXHxawuTYaEmMUmmRYxYziZRU
lEkMOdOGIqfooFRCOaWPb+ryReKDogPlLHMY8Vm0sYPY4f38/dHP+2uZWba3t6fdrpeLy8Ver+f7
+Xq8ns8X7l6v9+v1chhjjAAAAGAdL08XAAAAgCtDkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZAD
AACwFEEOuEbdf//9qlmzputXrVq1FBYWpqefflp79uwp8u317NlTQ4YMkSRt3rxZNWvW1H//+9/L
fs4Yo+XLl+uPP/4o1PZr166tZcuW5bnufD0X/qpbt65atGih+Ph4nT17tlDb3rlzp1q3bq26detq
4sSJherLJvfff79mzZqV57qWLVtqxowZkjxzPAAoGB9PFwDg0nr37q0ePXpIkpxOp06ePKlx48bp
ySef1KpVq+Tn5+eW7TZo0EAbNmxQhQoVLtv2p59+UmxsrNasWeOWWi702WefKSgoSJKUnZ2tbdu2
aejQoTp79qzGjBlzxf2+88478vHx0cqVK+Xv719U5V43rtXjAQBn5IBrWunSpRUUFKSgoCBVrFhR
derUUWxsrJKTk/Xdd9+5bbslSpRQUFCQvLwu/1fE1XymePny5V3jUalSJbVq1Urt2rXTypUrC9Vv
amqqatWqpVtuuUXlypUromqvH9fq8QCAIAdYx9vbW9Jf/7hKUs2aNTVt2jQ1b95czZs314kTJ3T6
9GkNHTpUd999txo1aqTevXvrwIEDrj6cTqemT5+u8PBwNWjQQPHx8crJyXGt//ultKysLL3xxhu6
9957FRoaqqioKG3btk1Hjx5Vt27dJEkPPPCA61Lcvn379NRTT+mOO+5Q8+bNNXLkSKWmprr6P3Xq
lAYOHKg777xT4eHh+uyzzwo1HufHYtmyZYqIiNDo0aN15513avDgwZKkH374QVFRUapfv74eeOAB
TZ48WefOnZP01+XFTZs2afny5apZs6aOHj0qp9Opt956Sy1atFBoaKg6deqkdevWubZ5Jds5P1dL
lixRt27dVK9ePd1333369NNPc+3P8uXL1bZtW9WvX18RERG5xub48eN68cUX1bBhQzVp0kQDBgxQ
UlKSa/22bdsUFRWl0NBQ3X333Ro0aJBOnTp1xWN73t+Ph2+++UYdOnRQ/fr1FR4ernHjxuncuXP5
Hg+9e/fWXXfdpUaNGmnw4MFKTk529X/y5Em98MILatiwocLDwzVnzhy1bNnSdal9yJAhiomJUffu
3XXnnXdqwYIFOnfunOLj49WiRQvVrVtXjRs31tChQ/Xnn3+65ujhhx/W/Pnzdd999+mOO+5Q//79
lZSUpIEDByo0NFT33ntvoY494FpAkAMscuTIEU2ePFlBQUFq2LCha/nixYv19ttva+bMmapQoYKe
eeYZ/f7775ozZ44WLFigypUrKzo6WikpKZKk2bNn66OPPtLw4cO1ZMkSnT59Wlu2bLnkdsePH6+l
S5dqxIgR+vzzz1WrVi09/fTTKlWqlOs7VosXL1avXr2UlJSk7t27KyQkRJ999pmmT5+u/fv3q1+/
fq7++vfvr3379mnOnDmaNWuWPv7441xBsiCysrK0fv16ff7554qIiHAtP3TokNLT07V8+XL16dNH
u3fv1lNPPaWWLVtqxYoVGj9+vNauXavRo0dLkpYsWaKwsDC1atVKGzZsUKVKlTR58mQtW7ZMY8eO
1eeff66OHTuqX79+2rx58xVv57zXX39d3bp108qVK9WyZUuNHj1av/32myRp5cqVeuWVVxQZGakV
K1bo6aef1vDhw7VhwwadOXNG3bt3V8mSJfXJJ5/ovffeU1ZWlnr06KHMzEzl5OToueee0z333KP/
+Z//0TvvvKPt27cX+Xf+kpOT1a9fP0VFRenLL7/Ua6+9ppUrV+rdd99VpUqVLjoejh49qq5du6ps
2bKaP3++Zs2apT179qhXr17KycmR0+lUnz59lJSUpA8//FAzZszQihUrdOTIkVzb/fLLL9WyZUst
WrRILVu21MSJE7V27Vq99tpr+uqrrzRy5Eh98cUXuYLx0aNHtWbNGr3zzjuaPn26Vq9erXbt2ql+
/fpatmyZmjVrppEjR+r06dNFOkbAVWUAXJNatGhh6tSpY0JDQ01oaKipU6eOqVmzpunYsaPZunWr
q11ISIh5/fXXXT9v3LjR1KpVy6SlpeXq76GHHjJvvfWWcTqdpkmTJmbmzJmudefOnTPNmjUzsbGx
xhhjvvvuOxMSEmKOHz9u0tLSTJ06dczSpUtd7bOyssyECRNMQkKC+f77701ISIg5cuSIMcaYKVOm
mEcffTTXtv/73/+akJAQ89NPP5n9+/ebkJAQ8/3337vW//rrryYkJCTXNi50vp477rjDNR41a9Y0
9erVM4MHDzbp6enGGGOWLl1qQkJCTEJCguuzAwcONC+88EKu/n744QcTEhJikpKSjDHG9OjRw7Xv
6enppm7dumbt2rW5PvPKK6+YXr16FWo7ISEhZtKkSa71qampJiQkxKxcudIYY0znzp1ddZw3d+5c
s27dOrNo0SLTpEkTk52d7Vp37tw5ExoaalasWGFSUlJMzZo1zccff2ycTqcxxpj9+/eb3bt35zmm
xlx8jF34q2bNmmb69Om5xv/48eNm586dJiQkJNf47Nixwxw4cMAYYy46HiZOnGjuu+8+k5mZ6Wp/
/hhYu3at+fbbb01ISIhJTEx0rd+7d2+u4yE2NtY0bdo0V+3Lly83P/zwQ65l3bp1M0OHDjXG/N8c
HTx40LX+0UcfNd26dbuojp9//vmSYwRc67jZAbiGdevWTdHR0ZL+uoQYGBiY5w0O//rXv1y/37Vr
l3JyctSsWbNcbc6dO6eEhASlpKTo5MmTqlu3rmtdiRIlVLt27TxrOHjwoLKyslS/fn3XMh8fH8XG
xkpSrktkkrR7927t3r1bDRo0uKivhIQElSlTRpJUp04d1/Lq1au7ludnzpw5CgoKksPhUIkSJXTj
jTfKxyf3X2MOh0NVqlTJVc/hw4dz1WP+//e4EhISdNNNN11UY2Zmpvr375/rO2FZWVm68cYbC72d
W2+91bX+/I0VWVlZkv66BNmuXbtc9fTs2VOSNGbMGCUnJyssLCzX+j///FMJCQlq06aNnnzySY0d
O1YzZsxQ06ZN1aJFi1xnK/Ny4TGW13b/rlatWmrVqpX69Omjm2++WU2bNtWDDz6oFi1a5Nn+119/
Vb169eTr6+taVq1aNZUrV0779u2Tj4+PKlSokOsYDgkJueimkwvHWpLat2+vDRs2aNKkSTp06JD2
79+vxMTEi9rdcsstrt+XLl061/qSJUtKkjIzM/OsHbABQQ64hpUtW1bBwcGXbXf+HyRJ8vX1VWBg
oBYtWnRRu9KlS7t+b/72pfTz3zP7uwv/AS4IX19fNW3aVMOHD79oXfny5bVx48Y8t1+Q7VSpUkU3
33xzvm28vLxy7Yuvr686dOig3r17X9T2/B2wFzr/2RkzZlw09hcGuyvdTl7jfH4s/h5KL+Tr66vq
1atr5syZF607H3piY2PVrVs3rVu3Ths2bNDQoUO1aNEiffTRR5fs91LH2KVqcTgcmjp1qvr16+fa
Tr9+/dS+fXvFx8df1L5UqVJ59uN0OuXr6ysvLy85nc5L1nepfl555RWtWbNGHTt21EMPPaQBAwZo
7Nixudp4e3tfdINGQW7YAGzCEQ1cZ2rUqOH6gntwcLCCg4NVpUoVTZ06Vd9//73Kly+vihUrauvW
ra7POJ1O7dq1K8/+brnlFvn4+GjHjh252kdEROiLL76Qw+HI1b569epKSEhQ5cqVXdv38vJSXFyc
jh8/rttvv12Scm3/6NGjRfKl/Lycr+d8LcHBwUpOTtbEiROVkZFxUfvg4GD5+voqKSkp12dWrFhx
yefcXcl28lKtWrVc4yxJgwcP1vjx41WjRg0dPXpUgYGBrv4rVKig+Ph47du3T4mJiRo1apSCgoLU
rVs3zZ49WxMnTtTmzZuL9Jlu27dvV3x8vKpXr66nnnpKc+fO1YABA1x3Dv/9eKhWrZq2b9/uOuso
Sfv379fp06dVrVo11axZUykpKUpMTHStP3DggNLS0i5ZQ0pKipYsWaKxY8cqNjZWHTp0UNWqVXXk
yBHumkWxQ5ADrjP33HOPQkNDFRMTox9++EEHDx7U8OHD9fXXXyskJESS1KtXL3300Udavny5Dhw4
oHHjxunYsWN59le6dGlFR0frjTfe0Lp163To0CGNHTtWp0+f1t133+26JLp7926lpaXp8ccfV2pq
qoYMGaK9e/dq+/bteumll3To0CHdeuutuvXWW/XAAw9ozJgx2rJli3bv3q3Y2Fi3nSnp3bu3fvnl
F8XHxyshIUFbtmxRbGys0tLS8jwjd8MNN6hnz56aPHmyVq5cqSNHjuijjz7Sm2++mevyX2G3k5en
n35aK1as0MKFC5WYmKhFixbpiy++0P3336+2bduqXLlyiomJ0fbt27Vv3z4NHDhQP//8s2rUqKFy
5crpyy+/1OjRo5WQkKCEhAR9+eWXRf5IFX9/f82fP19TpkxRYmKidu/erbVr17ouved1PKSlpWno
0KH69ddf9cMPP+jll1/W7bffrnvuuUeNGzdW3bp1NXjwYO3YsUO//PKL6y7gv4fC8/z8/OTn56c1
a9YoMTFRu3bt0sCBA3X8+HEuk6LYIcgB1xmHw6E333xT1atX1/PPP6+OHTvq0KFDeu+991S9enVJ
f33/6cUXX9TUqVPVsWNHZWRk6MEHH7xkn4MGDVKrVq00bNgwdejQQQkJCXrvvfd04403qnr16oqI
iNCAAQM0ffp0BQUFae7cuTp58qQ6d+6sp59+WpUqVdLcuXNdlxVff/113X333erbt6969uypFi1a
FDjs/FM1a9bU22+/rZ9++kkdOnRQTEyM7rrrrjwvUZ4XExOjrl27atKkSWrVqpUWLlyosWPH6tFH
Hy3S7fzdgw8+qJEjR+qDDz7QI488og8//FCTJk1SkyZNVKpUKc2dO1elSpVSjx491LVrV2VnZ+vD
Dz9UhQoV5O/vr3fffVdHjhxR586dFRkZqczMTL3zzjtFGpJvvfVWvfnmm9q4caPatWunJ554Qjff
fLOmTJkiSRcdDzfeeKPef/99JSUlqVOnTurbt69q1aqluXPnui6nz5w5U4GBgerWrZuef/55tWvX
Tg6H45KX2319fTV16lTt3LlTbdq00fPPP6+yZcuqV69eF53RBK53DsN5aACAhyQnJ+uXX35Rs2bN
XM9IPHHihMLDwzV//vyLbu4AkBs3OwAAPMbb21v9+/dXz549FRkZqYyMDE2bNk3BwcG64447PF0e
cM3jjBwAwKO+/fZbTZ06VXv37pWvr68aN26s2NjYix4lAuBibg1yHTt2dD3zqkqVKurSpYteffVV
eXt7Kzw8XP369ZPT6dTo0aO1d+9elShRQuPHj1dwcLC2bdtWqLYAAADXO7ddWj137pyMMZo3b55r
Wfv27TVjxgz961//0jPPPKNdu3bp6NGjyszM1Keffqpt27ZpwoQJmj17tkaNGlWotpd6uCkAAMD1
wm1Bbs+ePfrzzz/Vq1cvZWdn64UXXlBmZqbrKdvh4eHatGmTTpw44XoCfWhoqHbs2KH09PRCtyXI
AQCA653bglypUqX01FNP6bHHHtOhQ4fUu3dvBQQEuNaXKVNGR44cUXp6eq5XDnl7e1+07Era5ic7
O0c+Pt5FsZsAAAAe47YgV7VqVQUHB8vhcKhq1ary9/fP9eT2jIwMBQQE6OzZs7meeu50OuXn55dr
2ZW0zU9Kypmi2EUFBfnrxIlLP30c7sX4ex5z4FmMv2cx/p5XXOYgKMj/kuvc9kDgJUuWaMKECZKk
pKQk/fnnnypdurQSExNljNGGDRsUFhamhg0bav369ZKkbdu2KSQkRH5+fvL19S1UWwAAgOud287I
RUZGaujQoeratascDofi4uLk5eWll19+WTk5OQoPD9cdd9yhevXqaePGjYqKipIxRnFxcZKkMWPG
FKotAADA9a5YPkeuqE7DFpdTutcqxt/zmAPPYvw9i/H3vOIyBx65tAoAAAD3IsgBAABYiiAHAABg
KYIcAACApQhyAAAAliLIAQAAWIogBwAAYCmCHAAAgKUIcgAAAJYiyAEAAFjKbe9aBQDkrdeEr4u8
z/eH3F/kfQK49nFGDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZAD
AACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4A
AMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAA
AEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAA
LEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACw
FEEOAADAUgQ5AAAAS7k1yP3xxx+69957lZCQoMOHD6tr166Kjo7WqFGj5HQ6JUkzZ85UZGSkoqKi
9Msvv0hSkbQFAAC43rktyGVlZWnkyJEqVaqUJCk+Pl4xMTFasGCBjDFas2aNdu7cqS1btmjx4sWa
MmWKxowZUyRtAQAAigO3BbmJEycqKipKN910kyRp586datSokSSpefPm2rRpk3788UeFh4fL4XCo
cuXKysnJUXJycqHbAgAAFAc+7uh02bJlKl++vJo1a6Z33nlHkmSMkcPhkCSVKVNGaWlpSk9PV2Bg
oOtz55cXtu3llCtXWj4+3kWyr0FB/kXSD64M4+95zMG1gXnwDMbd84r7HLglyC1dulQOh0Pffvut
du/erdjYWCUnJ7vWZ2RkKCAgQH5+fsrIyMi13N/fX15eXoVqezkpKWcKu4uS/jp4Tpy4fHCEezD+
nsccXDuYh6uP49/zissc5BdW3XJpdf78+fr44481b9481apVSxMnTlTz5s21efNmSdL69esVFham
hg0basOGDXI6nTp27JicTqfKly+v2rVrF6otAABAceCWM3J5iY2N1YgRIzRlyhTddtttioiIkLe3
t8LCwtSlSxc5nU6NHDmySNoCAAAUBw5jjPF0EVdbUZ2GLS6ndK9VjL/nMQdXpteEr4u8z/eH3F/k
fSJ/HP+eV1zm4KpfWgUAAID7EeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRB
DgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5
AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQA
AAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMA
ALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAA
wFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAA
SxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEv5uKvjnJwcDR8+XAcPHpTD4dCYMWNUsmRJDRkyRA6H
QzVq1NCoUaPk5eWlmTNn6ptvvpGPj4+GDRum+vXr6/Dhw4VuCwAAcD1zW9pZu3atJOmTTz5RTEyM
3njjDcXHxysmJkYLFiyQMUZr1qzRzp07tWXLFi1evFhTpkzRmDFjJKnQbQEAAK53bjsj9+CDD+q+
++6TJB07dkwBAQHatGmTGjVqJElq3ry5Nm7cqKpVqyo8PFwOh0OVK1dWTk6OkpOTtXPnzkK1bdmy
pbt2DQAA4JrgtiAnST4+PoqNjdWqVas0ffp0bdy4UQ6HQ5JUpkwZpaWlKT09XYGBga7PnF9ujClU
2/yUK1daPj7eRbKPQUH+RdIPrgzj73nMwbWBefAMxt3zivscuDXISdLEiRP18ssvq3Pnzjp37pxr
eUZGhgICAuTn56eMjIxcy/39/XN9x+1K2uYnJeVMUeyagoL8deJE/qER7sP4ex5zcO1gHq4+jn/P
Ky5zkF9Yddt35JYvX663335bknTDDTfI4XCobt262rx5syRp/fr1CgsLU8OGDbVhwwY5nU4dO3ZM
TqdT5cuXV+3atQvVFgAA4HrntjNyDz30kIYOHapu3bopOztbw4YNU7Vq1TRixAhNmTJFt912myIi
IuTt7a2wsDB16dJFTqdTI0eOlCTFxsYWqi0AAMD1zmGMMZ4u4morqtOwxeWU7rWK8fc85uDK9Jrw
dZH3+f6Q+4u8T+SP49/zissceOTSKgAAANyLIAcAAGApghwAAIClCHIAAACWIsgBAABYiiAHAABg
KYIcAACApQhyAAAAliLIAQAAWIogBwAAYCmCHAAAgKUIcgAAAJYiyAEAAFiKIAcAAGCpAgW5YcOG
XbTshRdeKPJiAAAAUHA++a0cNWqUkpKS9OOPPyo5Odm1PDs7WwcOHHB7cQAAALi0fINcZGSkfv31
V+3du1cRERGu5d7e3mrQoIHbiwMAAMCl5Rvk6tWrp3r16qlJkya6+eabr1ZNAAAAKIB8g9x5iYmJ
GjRokE6fPi1jjGv5ihUr3FYYAAAA8legIDd27Fh16tRJtWvXlsPhcHdNAAAAKIACBTlfX189+eST
7q4FAAAA/0CBHj9So0YN7d271921AAAA4B8o0Bm5I0eOqFOnTqpcubJKlizpWs535AAAADynQEFu
wIAB7q4DAAAA/1CBglxISIi76wAAAMA/VKAg17hxYzkcDhljXHetBgUFaf369W4tDgAAAJdWoCC3
Z88e1++zsrL0v//7v7mWAQAA4Oor0F2rF/L19dUjjzyijRs3uqMeAAAAFFCBzsidOnXK9XtjjHbs
2KHU1FS3FQUAAIDL+8ffkZOkChUq6JVXXnFrYQAAAMjfP/6OHAAAAK4NBQpyTqdT7733ntavX6/s
7Gw1bdpUzz77rHx8CvRxAAAAuEGBbnaYPHmyvvvuO/Xo0UNPPvmktm7dqkmTJrm7NgAAAOSjQKfU
/vOf/2jp0qXy9fWVJN13331q166dhg0b5tbiAAAAcGkFOiNnjHGFOEkqUaJErp8BAABw9RUoyN1+
++2Ki4tTYmKiEhMTFRcXx2u7AAAAPKxAQW7UqFFKTU1VVFSUOnfurJSUFI0YMcLdtQEAACAf+Qa5
zMxMxcbG6rvvvtOECRO0adMm1a9fX97e3vLz87taNQIAACAP+Qa56dOnKz09XQ0aNHAtGzdunFJT
UzVjxgy3FwcAAIBLyzfIffPNN5o8ebIqVKjgWlaxYkVNmjRJq1evdntxAAAAuLR8g5yvr69KlSp1
0XI/Pz+VKFHCbUUBAADg8vINcl5eXkpPT79oeXp6urKzs91WFAAAAC4v3yDXpk0bDR8+XGfOnHEt
O3PmjIYPH66HHnrI7cUBAADg0vINcj169JC/v7+aNm2qzp07KzIyUk2bNlVAQID69u17tWoEAABA
HvJ9RZeXl5fGjRunPn36aNeuXfLy8lK9evVUsWLFq1UfAAAALqFA71qtUqWKqlSp4u5aAAAA8A8U
6M0OAAAAuPYQ5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAA
SxHkAAAALEWQAwAAsFSB3rX6T2VlZWnYsGH67bfflJmZqeeee07Vq1fXkCFD5HA4VKNGDY0aNUpe
Xl6aOXOmvvnmG/n4+GjYsGGqX7++Dh8+XOi2AAAA1zu3JJ5///vfCgwM1IIFCzRnzhyNGzdO8fHx
iomJ0YIFC2SM0Zo1a7Rz505t2bJFixcv1pQpUzRmzBhJKnRbAACA4sAtZ+QefvhhRURESJKMMfL2
9tbOnTvVqFEjSVLz5s21ceNGVa1aVeHh4XI4HKpcubJycnKUnJxc6LYtW7Z0x24BAABcU9wS5MqU
KSNJSk9P14svvqiYmBhNnDhRDofDtT4tLU3p6ekKDAzM9bm0tDQZYwrV9nLKlSstHx/vItnXoCD/
IukHV4bx9zzm4NrAPHgG4+55xX0O3BLkJOn48ePq27evoqOj1bZtW7322muudRkZGQoICJCfn58y
MjJyLff398/1HbcraXs5KSlnCrt7kv46eE6cuHxwhHsw/p7HHFw7mIerj+Pf84rLHOQXVt3yHbmT
J0+qV69eGjRokCIjIyVJtWvX1ubNmyVJ69evV1hYmBo2bKgNGzbI6XTq2LFjcjqdKl++fKHbAgAA
FAduOSP31ltvKTU1VbNmzdKsWbMkSa+88orGjx+vKVOm6LbbblNERIS8vb0VFhamLl26yOl0auTI
kZKk2NhYjRgx4orbAgAAFAcOY4zxdBFXW1Gdhi0up3SvVYy/5zEHV6bXhK+LvM/3h9xf5H0ifxz/
nldc5uCqX1oFAACA+xHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBS
BDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR
5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQ
AwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEO
AADAUgRmu08jAAAOpUlEQVQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMA
ALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAA
wFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAs5dYg9/PPP6t79+6SpMOHD6tr
166Kjo7WqFGj5HQ6JUkzZ85UZGSkoqKi9MsvvxRZWwAAgOud24Lcu+++q+HDh+vcuXOSpPj4eMXE
xGjBggUyxmjNmjXauXOntmzZosWLF2vKlCkaM2ZMkbQFAAAoDtwW5G655RbNmDHD9fPOnTvVqFEj
SVLz5s21adMm/fjjjwoPD5fD4VDlypWVk5Oj5OTkQrcFAAAoDnzc1XFERISOHj3q+tkYI4fDIUkq
U6aM0tLSlJ6ersDAQFeb88sL2/ZyypUrLR8f7yLZz6Ag/yLpB1eG8fc85uDawDx4BuPuecV9DtwW
5P7Oy+v/Tv5lZGQoICBAfn5+ysjIyLXc39+/0G0vJyXlTGF3R9JfB8+JE5cPjnAPxt/zmINrB/Nw
9XH8e15xmYP8wupVu2u1du3a2rx5syRp/fr1CgsLU8OGDbVhwwY5nU4dO3ZMTqdT5cuXL3RbAACA
4uCqnZGLjY3ViBEjNGXKFN12222KiIiQt7e3wsLC1KVLFzmdTo0cObJI2gIAABQHDmOM8XQRV1tR
nYYtLqd0r1WMv+cxB1em14Svi7zP94fcX+R9In8c/55XXObgmri0CgAAgKJFkAMAALAUQQ4AAMBS
BDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR
5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQ
AwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEO
AADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAAACxFkAMAALAUQQ4AAMBSBDkA
AABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALEWQAwAAsBRBDgAAwFIEOQAAAEsR5AAA
ACxFkAMAALAUQQ4AAMBSBDkAAABLEeQAAAAsRZADAACwFEEOAADAUgQ5AAAASxHkAAAALOXj6QKK
gtPp1OjRo7V3716VKFFC48ePV3BwsKfLAgAAcKvr4ozc6tWrlZmZqU8//VQDBw7UhAkTPF0SAACA
210XQe7HH39Us2bNJEmhoaHasWOHhysCAABwv+vi0mp6err8/PxcP3t7eys7O1s+PnnvXlCQf5Ft
uyj7wj/H+Hsec/DPrZjc3tMloIhw/HtecZ+D6+KMnJ+fnzIyMlw/O53OS4Y4AACA68V1EeQaNmyo
9evXS5K2bdumkJAQD1cEAADgfg5jjPF0EYV1/q7Vffv2yRijuLg4VatWzdNlAQAAuNV1EeQAAACK
o+vi0ioAAEBxRJADAACwFEGuEM6cOaPnnntO3bp1U8+ePZWUlOTpkoqVtLQ0Pfvss3r88cfVpUsX
bd261dMlFUurVq3SwIEDPV1GseJ0OjVy5Eh16dJF3bt31+HDhz1dUrH0888/q3v37p4uo9jJysrS
oEGDFB0drcjISK1Zs8bTJXkUQa4QFi1apDp16mj+/Plq166d3n33XU+XVKzMnTtXjRs31scff6z4
+HiNHTvW0yUVO+PHj9fkyZPldDo9XUqxwttsPO/dd9/V8OHDde7cOU+XUuz8+9//VmBgoBYsWKA5
c+Zo3Lhxni7Jo3jYWiH07NlTOTk5kqRjx44pICDAwxUVLz179lSJEiUkSTk5OSpZsqSHKyp+GjZs
qAcffFCffvqpp0spVnibjefdcsstmjFjhgYPHuzpUoqdhx9+WBEREZIkY4y8vb09XJFnEeQKaPHi
xfrwww9zLYuLi1P9+vX1xBNPaN++fZo7d66Hqrv+5Tf+J06c0KBBgzRs2DAPVXf9u9T4t27dWps3
b/ZQVcXXP32bDYpeRESEjh496ukyiqUyZcpI+uvPwYsvvqiYmBgPV+RZ/KkvoMcee0yPPfZYnus+
+ugjJSQkqE+fPlq9evVVrqx4uNT47927Vy+99JIGDx6sRo0aeaCy4iG/4x9XH2+zQXF3/Phx9e3b
V9HR0Wrbtq2ny/EoviNXCG+//baWL18u6a//IRT307tX2/79+9W/f39NnjxZ9957r6fLAa4a3maD
4uzkyZPq1auXBg0apMjISE+X43H8F64QOnXqpNjYWC1dulQ5OTmKi4vzdEnFyuTJk5WZmalXX31V
0l9nKWbPnu3hqgD3a9mypTZu3KioqCjX22yA4uKtt95SamqqZs2apVmzZkn66+aTUqVKebgyz+DN
DgAAAJbi0ioAAIClCHIAAACWIsgBAABYiiAHAABgKYIcAABAEbrS9/BmZWVp4MCBioqKUnR0tBIS
Ei77GYIcAI86evSoatWqpfbt27t+tWvXTkuWLCl033369NGyZcskSe3bt1dqauol26alpemJJ574
x9v46quv8vwLuyj36/jx42rTpo3atWunrVu3/uPPA7h6CvMe3nXr1ik7O1uffPKJ+vbtq6lTp172
MzxHDoDHlSpVSp9//rnr56SkJLVp00Z169bV7bffXiTbuLD/vJw+fVrbt28vkm2dV1T7tXnzZt14
44364IMPirQ+AEXv7+/h3bt3r8aPHy9JCgwMVFxcnPz9/fP8bNWqVZWTkyOn06n09PQCvbGFIAfg
mlOxYkUFBwfr0KFD2rVrl5YsWaI///xTfn5+mjdvnhYvXqyFCxfK6XQqMDBQI0aMULVq1ZSUlKQh
Q4bo999/V+XKlfXHH3+4+qxZs6a+/fZblS9fXm+//bY+++wz+fj4KDg4WBMmTNDQoUN19uxZtW/f
XsuWLdOhQ4f06quv6tSpU8rJyVH37t1dT5GfNm2aVqxYocDAQAUHB7t1v06cOKGpU6cqLS1N3bt3
17x58/T1119r9uzZysrKUqlSpRQbG6sGDRpoxowZ+u2333TixAn99ttvKl++vN544w1VrFhRBw8e
1MiRI5WcnCwvLy8999xzat26tZKSkjR27FgdP35cWVlZeuSRR/Tss88W+ZwCxcXf38M7YsQIxcXF
qXr16lq8eLHmzJmjO++8U5MmTcr1uZiYGNWpU0e//fabWrVqpZSUFL311luX36ABAA86cuSICQ0N
zbXsp59+MnfddZc5duyYWbp0qbnrrrtMWlqaMcaYzZs3m+joaHPmzBljjDH/+c9/TKtWrYwxxjz/
/PPmjTfeMMYYc+jQIRMaGmqWLl1qjDEmJCTE/PHHH2b16tXmoYceMqdOnTLGGBMXF2dmzZqVq46s
rCzTunVrs2PHDmOMMampqaZVq1Zm69atZtWqVaZ169YmLS3NZGVlmWeeecY8/vjjbt2vpUuXmmee
ecYYY8zBgwdNmzZtTHJysjHGmH379pmmTZuajIwMM336dPPAAw+4+uzTp4+ZNm2aMcaYDh06mI8/
/tgYY8yxY8dc7bp3727WrFljjDHm7Nmzpnv37uaLL74o+AQCuMiRI0fMY489ZowxpmHDhubxxx83
jz/+uOnSpYuJjY295Ofi4uLM66+/boz5689py5YtzdmzZ/PdFmfkAHjc+TNhkpSTk6Ny5crptdde
U6VKlST9dTbNz89PkvTNN9/o8OHDioqKcn3+9OnTOnXqlDZt2qTY2FhJUnBwsO6+++6LtvXtt9/q
4YcfVtmyZSVJQ4cOlaRc/4M+dOiQEhMTNWzYsFw17tq1SwkJCWrZsqWrnk6dOmnevHlu3a8Lbdy4
Ub///rt69uzpWuZwOJSYmChJatSokavP2rVru/rYs2ePHnvsMUlSpUqVtHr1ap05c0bff/+9Tp8+
rWnTpkmSzpw5oz179qh169Z57hOAf6Zq1aqaOHGiKleurB9//FEnTpy4ZNuAgAD5+vpKksqWLavs
7Gzl5OTk2z9BDoDH/f27ZH9XunRp1++dTqfat2+vQYMGuX7+/fffVbZsWTkcDpkL3jqY1/dLvL29
5XA4XD+npqZedBNETk6OAgICctV08uRJ+fv767XXXsu1DW9vb7fv14WcTqfuueeeXF+CPn78uG66
6SatWrUq1/smz4/H+XG4cL8PHDigoKAgGWP0ySef6IYbbpAkJScnq2TJkpesGcA/M3r0aMXGxio7
O1sOh8P1fvC89OzZU8OGDVN0dLSysrI0YMCAXH9P5IW7VgFYpWnTpvriiy/0+++/S5IWLlyoHj16
SJKaNWumTz/9VJJ07Ngxbd68+aLPN2nSRKtWrVJ6erokacaMGfrggw/k4+OjnJwcGWNUtWpVlSxZ
0hXCzt81umPHDjVr1kxfffWVUlNT5XQ6L3sTRVHs14UaN26sjRs3uh5LsG7dOrVr1y7fO+T8/PxU
p04dLV++3LU/Xbt21dmzZxUaGqq5c+dK+ivUdu3aVWvWrCmSfQKKqypVqmjRokWSpLp162revHla
uHChFixYoKpVq17yc2XKlNG0adO0YMECLV68WG3btr3stjgjB8AqzZo1U+/evdWrVy85HA75+flp
5syZcjgcGjVqlIYOHapWrVrp5ptvzvPO0HvvvVf79+9X165dJUnVq1fXuHHjdMMNN6h27dpq1aqV
Fi5cqFmzZunVV1/VnDlzlJ2drf79++vOO++U9NddaJ06dVJAQIBuv/12paSkuHW/LlSjRg2NHTtW
L730kuts2+zZsy/7v/bJkydrzJgxmjdvnuusQFBQkF5//XWNGzdObdu2VWZmpusxJwDs4DAXXiMA
AACANbi0CgAAYCmCHAAAgKUIcgAAAJYiyAEAAFiKIAcAAGApghwAAIClCHIAAACWIsgBAABY6v8B
I1yu2QjYhF4AAAAASUVORK5CYII=
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_2"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#2"&gt;Network #2&lt;a class="anchor-link" href="#Network-#2"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as previous, but with a bias embedding for each set, in $\mathbb{R}$, added to the dot-product.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [168]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [169]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_2.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 2" class="img-responsive" src="https://cavaunpeu.github.io/figures/neural_implicit_mf_network_2.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [170]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 2s - loss: 0.0734 - val_loss: 0.0204
Epoch 2/5
476465/476465 [==============================] - 2s - loss: 0.0563 - val_loss: 0.0216
Epoch 3/5
476465/476465 [==============================] - 2s - loss: 0.0534 - val_loss: 0.0236
Epoch 4/5
476465/476465 [==============================] - 2s - loss: 0.0515 - val_loss: 0.0252
Epoch 5/5
476465/476465 [==============================] - 2s - loss: 0.0502 - val_loss: 0.0269
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [171]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.10900494586591782
Validation: 0.1906236846450195
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This looks a lot better. Now let's visualize our ground-truth and predictions side-by-side.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [172]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'predictions'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;'ratings'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA7MAAAFyCAYAAAAnNY1XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This model approximates the training distribution much better than the others. Key to this task is delineating between items with 0 ratings, and those with high ratings. Remember, we first down-scaled our ratings with the following transformation to shrink the domain:&lt;/p&gt;
$$\tilde{r}_{u, i} = \log{\bigg(\frac{1 + r_{u, i}}{\epsilon}\bigg)}$$&lt;p&gt;Furthermore, the model scores worse on the training distribution, yet similarly on the validation set. The bias term was clearly helpful.&lt;/p&gt;
&lt;p&gt;Let's try a similar model, but &lt;em&gt;concatenate&lt;/em&gt; the country and song embeddings, then stack fully connected layers, then compute the final linear combination as before.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_3"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#3"&gt;Network #3&lt;a class="anchor-link" href="#Network-#3"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as &lt;a href="#network_1"&gt;#1&lt;/a&gt;, but concatenate the latent vectors instead. Then, stack 3 fully-connected layers with ReLU activations, batch normalization after each, and dropout after the first. Finally, add a 1-unit dense layer on the end, and add bias embeddings to the result. (NB: I wanted to add the bias embeddings to the respective $\mathbb{R}^f$ embeddings at the outset, but couldn't figure out how to do this in Keras.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [185]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;concatenation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;dense_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;concatenation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batch_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batch_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;batch_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BatchNormalization&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'predicted_preference'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [190]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_3.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 3" class="img-responsive" src="https://cavaunpeu.github.io/figures/neural_implicit_mf_network_3.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [191]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 5s - loss: 0.0702 - val_loss: 0.0830
Epoch 2/5
476465/476465 [==============================] - 5s - loss: 0.0472 - val_loss: 0.0601
Epoch 3/5
476465/476465 [==============================] - 5s - loss: 0.0477 - val_loss: 0.0667
Epoch 4/5
476465/476465 [==============================] - 5s - loss: 0.0483 - val_loss: 0.0508
Epoch 5/5
476465/476465 [==============================] - 5s - loss: 0.0491 - val_loss: 0.0513
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [192]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.11102242413459021
Validation: 0.1899202531334214
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;These results are roughly identical to the previous. Choosing between the models, we'd certainly go with the simpler of the two.&lt;/p&gt;
&lt;p&gt;Now, let's try some architectures that benefit from other data sources altogether: the song title, and the song artist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_4"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#4"&gt;Network #4&lt;a class="anchor-link" href="#Network-#4"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as &lt;a href="#network_2"&gt;#2&lt;/a&gt;, except feed in the song title text as well. This text is first tokenized, then padded to a maximum sequence length, then embedded into a fixed-length vector by an LSTM, then reduced to a single value by a dense layer with a ReLU activation. Finally, this scalar is concatenated to the scalar output that &lt;a href="#network_2"&gt;#2&lt;/a&gt; would produce, and the result is fed into a final dense layer with a linear activation - i.e. a linear combination of the two.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [194]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'title_sequence'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;padded_title_sequences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pad_sequences&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sequences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'title_sequence'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [205]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;title_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int32'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_title_sequence'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;title_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;MAX_SEQUENCE_LENGTH&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;title_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;title_lstm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;title_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_title_lstm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;title_lstm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_title_lstm&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;final_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [206]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;title_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;final_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_4.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 4" class="img-responsive" src="https://cavaunpeu.github.io/figures/neural_implicit_mf_network_4.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [207]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padded_title_sequences&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;network_4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 124s - loss: 65.4419 - val_loss: 0.0215
Epoch 2/5
476465/476465 [==============================] - 126s - loss: 0.0506 - val_loss: 0.0256
Epoch 3/5
476465/476465 [==============================] - 120s - loss: 0.0476 - val_loss: 0.0282
Epoch 4/5
476465/476465 [==============================] - 119s - loss: 0.0472 - val_loss: 0.0288
Epoch 5/5
476465/476465 [==============================] - 119s - loss: 0.0471 - val_loss: 0.0291
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [208]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.11471700578472085
Validation: 0.18885209182480478
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;We seem to get a pinch of lift from the song title, though we're still underfitting our training set. Perhaps we could make our model bigger, or run it for more epochs.&lt;/p&gt;
&lt;p&gt;Let's plot our ground-truth vs. prediction distributions side-by-side just to make sure we're still on the right track.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [210]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'predictions'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;'ratings'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA7MAAAFyCAYAAAAnNY1XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Finally, let's add the artist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a id="network_5"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="Network-#5"&gt;Network #5&lt;a class="anchor-link" href="#Network-#5"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Same as &lt;a href="#network_4"&gt;#4&lt;/a&gt;, except feed in the song artist index as well. This index is first embedded into a vector, then reduced to a scalar by a dense layer with a ReLU activation. Finally, this scalar is concatenated with the two scalars produced in the second-to-last layer of &lt;a href="#network_4"&gt;#4&lt;/a&gt;, then fed into a final dense layer with a linear activation. Like the previous, this is a linear combination of the three inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [217]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;artist_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'artist'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;artist_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_artists&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;artist_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;artist_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_title_lstm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dense_artist_embedding&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;final_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'linear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference_merge&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [218]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;title_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;artist_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;final_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;plot_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'figures/network_5.png'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;img alt="network 5" class="img-responsive" src="https://cavaunpeu.github.io/figures/neural_implicit_mf_network_5.png"/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [219]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
    &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
    &lt;span class="n"&gt;padded_title_sequences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_artist_index'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 122s - loss: 28.0315 - val_loss: 0.0224
Epoch 2/5
476465/476465 [==============================] - 129s - loss: 0.0484 - val_loss: 0.0269
Epoch 3/5
476465/476465 [==============================] - 121s - loss: 0.0473 - val_loss: 0.0310
Epoch 4/5
476465/476465 [==============================] - 134s - loss: 0.0489 - val_loss: 0.0333
Epoch 5/5
476465/476465 [==============================] - 131s - loss: 0.0485 - val_loss: 0.0300
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[219]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;lt;keras.callbacks.History at 0x12ee912b0&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [220]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.11092262775206822
Validation: 0.19037930919362725
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This seems a bit worse than the last, though similar all the same. I'm a bit surprised we didn't get more lift from artists. Nevertheless, we're still underfitting. Bigger model, more epochs, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="What-went-wrong?"&gt;What went wrong?&lt;a class="anchor-link" href="#What-went-wrong?"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Our best neural network in terms of performance and complexity is network &lt;a href="#network_2"&gt;#2&lt;/a&gt;. Let's refit this model and inspect a few predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [409]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'int64'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmbda&lt;/span&gt;&lt;span class="p"&gt;))(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;country_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_countries&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'country_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;song_bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_songs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'song_bias'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country_bias&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;predicted_preference&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [410]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;country_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;song_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predicted_preference&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;implicit_cf_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [411]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country_index'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'song_index'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="n"&gt;network_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train_rating'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ratings_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation_rating'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train on 476465 samples, validate on 476465 samples
Epoch 1/5
476465/476465 [==============================] - 3s - loss: 65.2042 - val_loss: 0.0205
Epoch 2/5
476465/476465 [==============================] - 8s - loss: 0.0559 - val_loss: 0.0212
Epoch 3/5
476465/476465 [==============================] - 3s - loss: 0.0519 - val_loss: 0.0225
Epoch 4/5
476465/476465 [==============================] - 2s - loss: 0.0496 - val_loss: 0.0235
Epoch 5/5
476465/476465 [==============================] - 2s - loss: 0.0482 - val_loss: 0.0248
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [412]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Train: 0.10905118689288228
Validation: 0.19050085125816704
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [441]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;predictions_unstack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_level_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;)][&lt;/span&gt;&lt;span class="s1"&gt;'song_title'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;song_metadata_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_level_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'song_id'&lt;/span&gt;&lt;span class="p"&gt;)][&lt;/span&gt;&lt;span class="s1"&gt;'song_artist'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_level_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'country_id'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country_id_to_name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Colombia"&gt;Colombia&lt;a class="anchor-link" href="#Colombia"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [443]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'Colombia'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[443]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.715299&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.704788&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.683443&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.681572&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.680419&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.674962&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.674799&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.672363&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.671484&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;Colombia&lt;/td&gt;
&lt;td&gt;0.669374&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="United-States"&gt;United States&lt;a class="anchor-link" href="#United-States"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [444]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'United States'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[444]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.752865&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.742354&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.721009&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.719138&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.717985&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.712528&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.712365&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.709929&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.709051&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;United States&lt;/td&gt;
&lt;td&gt;0.706940&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Turkey"&gt;Turkey&lt;a class="anchor-link" href="#Turkey"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [445]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'Turkey'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[445]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.728861&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.718350&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.697005&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.695134&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.693981&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.688524&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.688361&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.685925&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.685046&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;Turkey&lt;/td&gt;
&lt;td&gt;0.682936&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Taiwan"&gt;Taiwan&lt;a class="anchor-link" href="#Taiwan"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [446]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions_unstack&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'country'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'Taiwan'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rating'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[446]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe table-striped table-hover table"&gt;
&lt;thead&gt;
&lt;tr style="text-align: right;"&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;country&lt;/th&gt;
&lt;th&gt;rating&lt;/th&gt;
&lt;th&gt;song_artist&lt;/th&gt;
&lt;th&gt;song_title&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;song_id&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;30519&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.789601&lt;/td&gt;
&lt;td&gt;Katy Perry&lt;/td&gt;
&lt;td&gt;Chained To The Rhythm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31060&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.779090&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Perfect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30137&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.757745&lt;/td&gt;
&lt;td&gt;Ed Sheeran&lt;/td&gt;
&lt;td&gt;Castle on the Hill&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31043&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.755874&lt;/td&gt;
&lt;td&gt;Zedd&lt;/td&gt;
&lt;td&gt;Stay (with Alessia Cara)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31041&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.754721&lt;/td&gt;
&lt;td&gt;The Chainsmokers&lt;/td&gt;
&lt;td&gt;Something Just Like This&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;18894&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.749264&lt;/td&gt;
&lt;td&gt;Calvin Harris&lt;/td&gt;
&lt;td&gt;My Way&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;30487&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.749101&lt;/td&gt;
&lt;td&gt;Major Lazer&lt;/td&gt;
&lt;td&gt;Run Up (feat. PARTYNEXTDOOR &amp;amp; Nicki Minaj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20131&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.746664&lt;/td&gt;
&lt;td&gt;The Weeknd&lt;/td&gt;
&lt;td&gt;I Feel It Coming&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;20222&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.745786&lt;/td&gt;
&lt;td&gt;Bruno Mars&lt;/td&gt;
&lt;td&gt;That's What I Like&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;31045&lt;/th&gt;
&lt;td&gt;Taiwan&lt;/td&gt;
&lt;td&gt;0.743676&lt;/td&gt;
&lt;td&gt;Clean Bandit&lt;/td&gt;
&lt;td&gt;Symphony (feat. Zara Larsson)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;As is abundantly clear, the model is simply converging to a popularity baseline. I tried extracting the vectors outright, normalizing as before then computing dot products, but the results made little sense.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Future-work"&gt;Future work&lt;a class="anchor-link" href="#Future-work"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;At this stage, Implicit Matrix Factorization is the clear winner. The algorithm is popular for several good reasons - it's simple, relatively scalable, intuitive (especially given the significance of the interplay between $C$ and $P$, which I did not expressedly discuss) and effective. Herein, I did not produce a neural network approximating this algorithm with superior results.&lt;/p&gt;
&lt;p&gt;My intuition tells me I'm missing something simple; why shouldn't a non-linear function approximator be able to approximate $f(u, i)$ better than a linear matrix decomposition? At this stage, I'm not sure. My best guess is that we should seek to optimize a different loss function. Here, we've mirrored that of IMF; why not optimize a &lt;em&gt;ranking&lt;/em&gt; loss function instead, since that's what's ultimately important? The IMF objective makes the actual optimization process easy for ratings matrices with billions of items; in our case, with far fewer items, we're simply using stochastic gradient descent. To this effect, I did experiment with some composite loss functions; on one side, &lt;code&gt;implicit_mf_loss&lt;/code&gt;, and on the other, a &lt;code&gt;cutoff_loss&lt;/code&gt;, which seeks to enforce that songs which received a rating $\tilde{r}_{u, i}$ above a certain cutoff should be indeed predicted at above that cutoff. It's a crude, binary absolute error. This was unsuccessful.&lt;/p&gt;
&lt;p&gt;Finally, if there is in fact some value in using neural networks for this type of problem, I'm extremely intrigued by the ease and intuitiveness with which we can incorporate multiple inputs into our function. Encoding our song title with an LSTM, and training that LSTM end-to-end with our main minimization objective, is one example of this. Furthermore, Keras makes the construction of these networks straightforward.&lt;/p&gt;
&lt;p&gt;Thanks for reading this work. Your feedback is welcome, and I do hope this serves as a point of inspiration, trial and error should this question interest you.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Ordered Categorical GLMs for Product Feedback Scores</title><link href="https://cavaunpeu.github.io/2017/03/17/ordered-categorical-glms-for-product-feedback-scores/" rel="alternate"></link><published>2017-03-17T16:55:00-04:00</published><updated>2017-03-17T16:55:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-03-17:/2017/03/17/ordered-categorical-glms-for-product-feedback-scores/</id><summary type="html">&lt;p&gt;A follow-up to Erik Bernhardsson's post &lt;a href="https://erikbern.com/2015/12/05/more-mcmc-analyzing-a-small-dataset-with-1-5-ratings/"&gt;"More MCMC – Analyzing a small dataset with 1-5 ratings"&lt;/a&gt; using ordered categorical generalized linear models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;TL;DR: there's a &lt;a href="https://willwolf.shinyapps.io/ordered-categorical-a-b-test/"&gt;Shiny app&lt;/a&gt; too.&lt;/p&gt;
&lt;p&gt;I write this post as a follow-up to Erik Bernhardsson's post &lt;a href="https://erikbern.com/2015/12/05/more-mcmc-analyzing-a-small-dataset-with-1-5-ratings/"&gt;"More MCMC – Analyzing a small dataset with 1-5 ratings."&lt;/a&gt; Therein, Erik builds a simple multinomial regression to model explicit, 1-5 feedback scores for different variants of Better's &lt;a href="https://better.com/"&gt;website&lt;/a&gt;. I like his approach for the rigor and mathematical fidelity it brings to what is a straightforward, ubiquitous use case for any product team.&lt;/p&gt;
&lt;p&gt;I recently learned about ordered categorical generalized linear models (GLMs) and thought back to the post above. Of course, while feedback scores do fall into discrete categories, there is an implicit ordinality therein: choosing integers in &lt;span class="math"&gt;\([1, 5]\)&lt;/span&gt; is different from choosing colors in &lt;span class="math"&gt;\(\{\text{red}, \text{green}, \text{blue}\}\)&lt;/span&gt;. This is because 5 is greater than 4, while green is not "greater" than "blue." (For a royalistic debate on the supremacy of the color green, please make use of the comments section below.)&lt;/p&gt;
&lt;p&gt;In a multinomial regression, we can formulate our problem thus:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*} y &amp;amp;\sim \text{Multinomial}(1, p)\\
p_j &amp;amp;= \frac{e^{\phi_j}}{\sum\limits_{k = 1}^{K} e^{\phi_k}}\\
\phi_j &amp;amp;= \alpha_j + \beta_j X_i\\
\alpha_j &amp;amp;\sim \text{Normal}(0, 10)\\
\beta_j &amp;amp;\sim \text{Normal}(0, 10)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;In a model with &lt;span class="math"&gt;\(k\)&lt;/span&gt; categorical outcomes, we typically have &lt;span class="math"&gt;\(k-1\)&lt;/span&gt; linear equations for &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt;. The link function — which you'll recognize as the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; — "squashes" these values such that they sum to 1 (with one of the values of &lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt; fixed at an arbitrary constant). The Normal priors placed on &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; were chosen arbitrarily: we can use any continuous-valued distribution we like.&lt;/p&gt;
&lt;p&gt;In situations where we don't have predictor variables, we can alternatively draw the entire vector &lt;span class="math"&gt;\(p\)&lt;/span&gt; from a &lt;a href="https://en.wikipedia.org/wiki/Dirichlet_distribution"&gt;Dirichlet&lt;/a&gt; distribution outright. As it happens, this offers us a trivially simple analytical solution to the posterior. This fact owes itself to &lt;a href="http://stats.stackexchange.com/questions/44494/why-is-the-dirichlet-distribution-the-prior-for-the-multinomial-distribution"&gt;Dirichlet-Multinomial conjugacy&lt;/a&gt;: given total observed counts &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; of each category &lt;span class="math"&gt;\(k\)&lt;/span&gt; and respective parameters &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; on our prior, our posterior distribution for our belief in &lt;span class="math"&gt;\(p\)&lt;/span&gt; is given as:&lt;/p&gt;
&lt;div class="math"&gt;$$p \sim \text{Dirichlet}(\alpha_1 + x_1, ..., \alpha_k + x_k)$$&lt;/div&gt;
&lt;p&gt;This makes both inference and posterior predictive sampling trivial: a few lines of code for each. Unfortunately, while delightfully simple, the multinomial regression makes a strong concession with respect to the data at hand: the ordinality of our feedback scores is not explicitly preserved. To this effect, let us explore the ordered categorical GLM.&lt;/p&gt;
&lt;p&gt;The ordered categorical GLM can be specified thus:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
&amp;amp;y \sim \text{Ordered}(p)\\
&amp;amp;\log{\bigg(\frac{p_k}{1 - p_k}\bigg)} = \alpha_k - \phi_i\\
&amp;amp;\phi_i = \beta X_i\\
&amp;amp;\alpha_k \sim \text{Normal}(0, 10)\\
&amp;amp;\beta \sim \text{Normal}(0, 10)
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;There's a few components to clarify:&lt;/p&gt;
&lt;h3&gt;Ordered distribution&lt;/h3&gt;
&lt;p&gt;An Ordered distribution is a vanilla categorical distribution that accepts a vector of &lt;em&gt;cumulative probabilities&lt;/em&gt; &lt;span class="math"&gt;\(p_k = \text{Pr}(y_i \leq k)\)&lt;/span&gt;, as opposed to traditional probabilities &lt;span class="math"&gt;\(p_k = \text{Pr}(y_i = k)\)&lt;/span&gt;. This preserves the ordering among variables.&lt;/p&gt;
&lt;h3&gt;Link function&lt;/h3&gt;
&lt;p&gt;In a typical logistic regression, we model the log-odds of observing a positive outcome as a linear function of the intercept plus weighted input variables. (The inverse of this function which we thereafter employ to obtain the raw probability &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the &lt;em&gt;logistic&lt;/em&gt; function, or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid&lt;/a&gt;&lt;/em&gt; function.) In the ordered categorical GLM, we instead model the &lt;em&gt;log-cumulative-odds&lt;/em&gt; of observing a particular outcome as a linear function of the intercept &lt;em&gt;minus&lt;/em&gt; weighted input variables. We'll dive into the "minus" momentarily.&lt;/p&gt;
&lt;h3&gt;Cumulative probability&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p_k\)&lt;/span&gt; in the above equation is defined as &lt;span class="math"&gt;\(p_k = \text{Pr}(y_i \leq k)\)&lt;/span&gt;. For this reason, the left-hand-side of the second line of our model gives the log-cumulative-odds, not the log-odds.&lt;/p&gt;
&lt;h3&gt;Priors&lt;/h3&gt;
&lt;p&gt;Placing Normal priors on &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; was an arbitrary choice. In fact, any prior that produces a continuous value should suffice: the constraint that &lt;span class="math"&gt;\(p_k\)&lt;/span&gt; must be a valid (cumulative) probability, i.e. &lt;span class="math"&gt;\(p_k\)&lt;/span&gt; must lie on the interval &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;, is enforced by the inverse link function.&lt;/p&gt;
&lt;h3&gt;Subtracting &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Ultimately, &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; is the linear model. Should we want to add additional predictors, we would append them here. So, why do we subtract &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; from &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; instead of add? Intuitively, it would make sense for an increase in the value of a predictor variable, given a positive coefficient, to shift probability mass towards &lt;em&gt;larger&lt;/em&gt; ordinal values. This makes for a more fluid interpretation of our model parameters. Subtracting &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; does just this: &lt;em&gt;increasing&lt;/em&gt; the value of a given predictor &lt;em&gt;decreases&lt;/em&gt; the log-cumulative-odds of every outcome value &lt;span class="math"&gt;\(k\)&lt;/span&gt; below the maximum (&lt;em&gt;every&lt;/em&gt; outcome value below the maximum, because we have one linear model &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt; which we must subtract, separately, from each intercept &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; in order to compute &lt;span class="math"&gt;\(\log{\big(\frac{p_k}{1 - p_k}\big)}\)&lt;/span&gt;) which shifts probability mass upwards. This way, the desired dynamic — "a bigger predictor should lead to a bigger outcome given a positive coefficient" — holds.&lt;/p&gt;
&lt;p&gt;Let's move to R to fit and compare these models. I'm enjoying R more and more for the ease of plotting with &lt;a href="http://docs.ggplot2.org/current/"&gt;ggplot2&lt;/a&gt;, as well as the functional "chains" offered by &lt;a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html"&gt;magrittr&lt;/a&gt; ("to be pronounced with a sophisticated french accent," apparently) and Hadley Wickham's &lt;a href="https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html"&gt;dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, let's simulate some scores then plot the results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;50&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;feedback&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;rmultinom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="n"&gt;max.col&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="explicit feedback scores" class="img-responsive" src="https://cavaunpeu.github.io/figures/empirical_distribution_explicit_feedback_scores.png"/&gt;&lt;/p&gt;
&lt;p&gt;Next, let's fit an ordered categorical GLM in the Stan modeling language. Note that we don't have any predictor variables &lt;span class="math"&gt;\(x_i\)&lt;/span&gt;; therefore, the only variables we will be estimating are our intercepts &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;data&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kn"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;ordered&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kn"&gt;model&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;ordered_logistic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kn"&gt;generated quantities&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;vector&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;real&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dev&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;ordered_logistic_lpmf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;obs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our model estimates four values: &lt;span class="math"&gt;\(\alpha_1, \alpha_2, \alpha_3\)&lt;/span&gt; and &lt;span class="math"&gt;\(\alpha_4\)&lt;/span&gt;. Why four and not five? The cumulative probability of the final outcome, &lt;span class="math"&gt;\(\text{Pr}(y_i \leq 5)\)&lt;/span&gt; is always 1.&lt;/p&gt;
&lt;p&gt;The posterior samples from our model will be vectors of cumulative probabilities, i.e. cumulative distribution functions. Let's examine the variation in these estimates, and plot them against the proportion of each class observed in our original data to see how well our model did. The following plot is constructed with 2,000 samples from our posterior distribution, where each sample is given as &lt;span class="math"&gt;\(\{\alpha_1, \alpha_2, \alpha_3, \alpha_4\}\)&lt;/span&gt;. The dotted red line gives the column-wise mean, and the error band gives the column-wise 92% interval.&lt;/p&gt;
&lt;p&gt;&lt;img alt="posterior cumulative distribution" class="img-responsive" src="https://cavaunpeu.github.io/figures/mean_posterior_cumulative_distribution.png"/&gt;&lt;/p&gt;
&lt;p&gt;Key points are as follows:&lt;/p&gt;
&lt;h3&gt;The scale of our estimates&lt;/h3&gt;
&lt;p&gt;The marginal distributions of each estimated parameter are on the &lt;em&gt;log-cumulative-odds&lt;/em&gt; scale. This is because &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; — the only set of parameters we are estimating — is set equal to &lt;span class="math"&gt;\(\log{\bigg(\frac{\text{Pr}(y_i \leq k)}{1 - \text{Pr}(y_i \leq k)}\bigg)}\)&lt;/span&gt; in our model above. So, what does a value of, say, &lt;span class="math"&gt;\(\alpha_3 = -1.5\)&lt;/span&gt; say about the probability of receiving a feedback score of 3 from a given user? I have no idea. As such, we necessarily convert these estimates from the &lt;em&gt;log-cumulative-odds&lt;/em&gt; scale to the &lt;em&gt;cumulative probability&lt;/em&gt; scale to make interpretation easier. The sigmoid function gives this conversion.&lt;/p&gt;
&lt;h3&gt;The width of the band&lt;/h3&gt;
&lt;p&gt;The width of the band quantifies the uncertainty in our estimate of the true cumulative distribution function. We used 50 samples: it should be reasonably wide. With 500,000 samples, the red band would be indistinguishable from the dotted line.&lt;/p&gt;
&lt;p&gt;Finally, let's simulate new observations from each model. Our posterior contains a distribution of &lt;em&gt;cumulative distribution functions&lt;/em&gt;: how do we translate this into multinomial samples?&lt;/p&gt;
&lt;p&gt;First, samples from this distribution can be transformed into probability mass functions with the follow code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;simulated_probabilities&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;cbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cutpoint_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nf"&gt;cbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoint_samples&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we can use each sample from &lt;code&gt;simulated_probabilities&lt;/code&gt; to roll a multinomial die. Here, I'll adopt the strategy Erik uses in the original post: with each sample, we'll simulate a large number — I've chosen 500 — of multinomial throws. This will give a histogram of empirical counts — similar to the first plot shown at the top of this post. With this distribution, we'll compute a weighted average. After repeating this for a large number of samples from our posterior, we'll have a distribution of weighted averages. (Incidentally, as Erik highlights in his post, the shape of this distribution can be assumed Normal as given by the &lt;a href="https://en.wikipedia.org/wiki/Central_limit_theorem"&gt;Central Limit Theorem&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The following plot compares the results for both the ordered categorical and multinomial models. Remember, we obtain the posterior of the latter through the Dirichlet-Multinomial conjugacy described above. Sampling from this posterior follows trivially. While vanilla histograms should do the trick, let's plot the inferred densities just to be safe.&lt;/p&gt;
&lt;p&gt;&lt;img alt="comparative posterior density plots" class="img-responsive" src="https://cavaunpeu.github.io/figures/comparative_posterior_predictive_density_plots.png"/&gt;&lt;/p&gt;
&lt;p&gt;To be frank, this has me a little disappointed! According to the posterior predictive densities of the weighted-average throws, there is no discernible difference between the multinomial and ordered categorical models. To be thorough, let's plot 100 draws from the &lt;em&gt;raw&lt;/em&gt;, respective posteriors: a distribution over cumulative distribution functions for each model.&lt;/p&gt;
&lt;p&gt;&lt;img alt="comparative posterior cumulative distributions" class="img-responsive" src="https://cavaunpeu.github.io/figures/comparative_posterior_cumulative_distributions.png"/&gt;&lt;/p&gt;
&lt;p&gt;Yep, no difference. So, why do we think this is? What are our takeaways?&lt;/p&gt;
&lt;h3&gt;We didn't use any predictor variables&lt;/h3&gt;
&lt;p&gt;In the ordered categorical case, we estimate &lt;span class="math"&gt;\(k - 1\)&lt;/span&gt; values of &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; and a &lt;em&gt;single set&lt;/em&gt; of predictor coefficients. In the multinomial case, we estimate &lt;span class="math"&gt;\(k - 1\)&lt;/span&gt; sets of &lt;span class="math"&gt;\(\{\alpha_k, \beta_{X, k}\}\)&lt;/span&gt; values (for example, should we have &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; classes and two predictor variables &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;, we'd estimate parameters &lt;span class="math"&gt;\(\alpha_1, \beta_{a, 1}, \beta_{b, 1}, \alpha_{2}, \beta_{a, 2}, \beta_{b, 2}\)&lt;/span&gt; in the simplest case). Given that we didn't use any predictor variables, we're simply estimating a set of intercepts &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; in each case. In the former, these values give the log-cumulative-odds of each outcome, while in the latter they give the log-odds outright. Given that the transformation between the two is deterministic, the ordered categorical and multinomial models should be functionally identical.&lt;/p&gt;
&lt;h3&gt;We might want predictor variables&lt;/h3&gt;
&lt;p&gt;It is easy to conceive of a situation in which we'd want predictor variables. In this case, the ordered categorical model becomes a clear choice for ordered categorical data. Revisiting its formulation above, we see that predictors are trivial to add to the model: we just tack them onto the equation for &lt;span class="math"&gt;\(\phi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;The inconvenient realities of measurement&lt;/h3&gt;
&lt;p&gt;There's a quote I like from Richard McElreath (I just finished his textbook, &lt;a href="http://xcelab.net/rm/statistical-rethinking/"&gt;Statistical Rethinking&lt;/a&gt;, which I couldn't recommend much more highly):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Both types of models help us transform our modeling to cope with the inconvenient realities of measurement, rather than transforming measurements to cope with the constraints of our models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this quote, he's describing the ordered categorical GLM (in addition to "zero-inflated" models — models that "mix a binary event with an ordinary GLM likelihood like a Poisson or binomial" — hence the plurality). And therein lies the rub: a model is but an approximation of the world, and if one needs to bend to accommodate the other, the former should be preferred.&lt;/p&gt;
&lt;p&gt;To conclude, I built a &lt;a href="https://willwolf.shinyapps.io/ordered-categorical-a-b-test/"&gt;Shiny app&lt;/a&gt; to be used as an A/B test calculator for ordered categorical data using the methodologies detailed above. Example output looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="a/b comparison plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/a_b_comparison_plot.png"/&gt;&lt;/p&gt;
&lt;p&gt;Code for this work can be found &lt;a href="https://github.com/cavaunpeu/ordered-categorical-glm"&gt;here&lt;/a&gt;. Thanks for reading.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Intercausal Reasoning in Bayesian Networks</title><link href="https://cavaunpeu.github.io/2017/03/13/intercausal-reasoning-in-bayesian-networks/" rel="alternate"></link><published>2017-03-13T15:14:00-04:00</published><updated>2017-03-13T15:14:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-03-13:/2017/03/13/intercausal-reasoning-in-bayesian-networks/</id><summary type="html">&lt;p&gt;Simple intercausal reasoning on a 3-node Bayesian network.&lt;/p&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal of this work is to perform simple intercausal reasoning on a 3-node Bayesian network.
&lt;img alt="simple bayesian network" class="img-responsive" src="https://cavaunpeu.github.io/figures/simple_bayesian_network.png"/&gt;&lt;/p&gt;
&lt;p&gt;In this network, both the "president being in town" and a "car accident on the highway" exert influence over whether a traffic jam occurs. With this relationship in mind, we will try to answer two simple questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"What is the probability of an accident having occurred given that a traffic jam occurred?"&lt;/li&gt;
&lt;li&gt;"What is the probability of an accident having occurred given that a traffic jam occurred &lt;em&gt;and&lt;/em&gt; the president is in town?"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are given point estimates for all component probabilities which we can use to answer these questions. However, in the real world, we're just given data. As such, we use the given component probabilities (which we'd never actually know) to simulate this data, then use it to try to answer the questions at hand. This way, we'll build uncertainty into our answers as well.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Problem-setup"&gt;Problem setup&lt;a class="anchor-link" href="#Problem-setup"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-component-probabilities"&gt;Define component probabilities&lt;a class="anchor-link" href="#Define-component-probabilities"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;PRESIDENT_PROBABILITY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
&lt;span class="n"&gt;ACCIDENT_PROBABILITY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;
&lt;span class="n"&gt;TRAFFIC_PROBABILITY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'JointInput'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'p'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'a'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Define-PyMC-sampling-parameters"&gt;Define PyMC sampling parameters&lt;a class="anchor-link" href="#Define-PyMC-sampling-parameters"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We'll need these later on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N_CHAINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Simulate-data"&gt;Simulate data&lt;a class="anchor-link" href="#Simulate-data"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [7]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;president&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PRESIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accident&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ACCIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;traffic_probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TRAFFIC_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accident&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;traffic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;traffic_probabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [8]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'President Mean: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Accident Mean: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accident&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Traffic Mean: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;traffic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;President Mean: 0.07
Accident Mean: 0.16
Traffic Mean: 0.19
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [9]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;observed_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;'president'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'accident'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;accident&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'traffic'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;traffic&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Compute-$P(\text{president})$-and-$P(\text{accident})$-posteriors"&gt;Compute $P(\text{president})$ and $P(\text{accident})$ posteriors&lt;a class="anchor-link" href="#Compute-$P(\text{president})$-and-$P(\text{accident})$-posteriors"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;One way to estimate the probability of the president being in town given observational data is to compute the observed proportion, i.e. if the president was seen in 4 of 200 trials, then the estimated probability is .02. Of course, this discards all &lt;em&gt;uncertainty&lt;/em&gt; in our estimate (of which we have much). Uncertainty declines as the size of our data tends towards infinity, and 200 trials is not infinite.&lt;/p&gt;
&lt;p&gt;Instead, we can express our belief - uncertainty included - in the true probability of observing &lt;code&gt;president&lt;/code&gt;, and that of observing &lt;code&gt;accident&lt;/code&gt;, as a Beta-distributed posterior. This follows trivially from the &lt;a href="https://dataorigami.net/products/bayesian-beta-binomial-model"&gt;Beta-Binomial conjugacy&lt;/a&gt;*.&lt;/p&gt;
&lt;p&gt;*Accessing this video costs $9 CAD. This said, the author Cam Davidson-Pilon's work on Bayesian statistics is excellent, and well-deserving of the fee. Alternatively, the Wikipedia page on &lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior"&gt;conjugate priors&lt;/a&gt; contains some cursory information about Beta-Binomial conjugacy under the "Discrete distributions" table.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;times_president_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;times_president_not_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;times_president_observed&lt;/span&gt;

&lt;span class="n"&gt;president_probability_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;times_president_observed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;times_president_not_observed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_CHAINS&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [11]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;times_accident_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;times_accident_not_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;times_accident_observed&lt;/span&gt;

&lt;span class="n"&gt;accident_probability_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;times_accident_observed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;times_accident_not_observed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_CHAINS&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Compute-$P(\text{Traffic}\-|\-\text{President},-\text{Accident})$-posterior"&gt;Compute $P(\text{Traffic}\ |\ \text{President}, \text{Accident})$ posterior&lt;a class="anchor-link" href="#Compute-$P(\text{Traffic}\-|\-\text{President},-\text{Accident})$-posterior"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This might look a little funky for those used to Bayesian parameter estimation for univariate systems - estimating $P(\text{heads})$ given the results of 15 coinflips, for example.&lt;/p&gt;
&lt;p&gt;So, how do we do this? Do we filter the data for all unique combinations of (&lt;code&gt;president&lt;/code&gt;, &lt;code&gt;accident&lt;/code&gt;), i.e. $(0, 0)$, $(0, 1)$, $(1, 0)$, $(1, 1)$ then estimate in the same fashion as above? That way, we could estimate $P(\text{traffic} = 1\ |\ \text{president} = 0, \text{accident} = 0)$, $P(\text{traffic} = 1\ |\ \text{president} = 1, \text{accident} = 0)$, etc. In fact, this would work. But what if we added more variables? And what if the values were continuous? This would get messy quickly.&lt;/p&gt;
&lt;p&gt;In fact, modeling $P(\text{Traffic}\ |\ \text{President}, \text{Accident})$ is none other than logistic regression. Think about it!&lt;/p&gt;
&lt;p&gt;Herein, we'll formulate our model as a vanilla Binomial (logistic) regression, taking the form:
$$
\text{traffic} \sim \text{Binomial}(1, p)\\
\log\bigg(\frac{p}{1 - p}\bigg) = \alpha + \beta_P P + \beta_A A\\
\alpha \sim \text{Normal}(0, 10)\\
\beta_P \sim \text{Normal}(0, 10)\\
\beta_A \sim \text{Normal}(0, 10)\\
$$&lt;/p&gt;
&lt;p&gt;$P$ and $A$ represent &lt;code&gt;president&lt;/code&gt; and &lt;code&gt;accident&lt;/code&gt; respectively. The priors on their respective coefficients are meant to be uniformative - a mere guard against our sampler running off to check values really big or really small.&lt;/p&gt;
&lt;p&gt;Finally, as this is one of my first times with PyMC3, I fit a baseline logistic regression model from scikit-learn on the same data just to check that our parameter estimates make sense. Note that scikit-learn's implementation specifies an L2 penalty - whose strength is controlled by an additional parameter $C$ - on the cost by default, which is analagous to placing Gaussian priors on model coefficients in the Bayesian case. I did not take care to ensure that the hyper-parameters ($\mu_P$ and $\sigma_P$, for example) of our Normal priors are comparable with the analagous parameter $C$ in the scikit-learn model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Baseline-logistic-regression"&gt;Baseline logistic regression&lt;a class="anchor-link" href="#Baseline-logistic-regression"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [12]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;observed_data&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;'president'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'accident'&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;observed_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'traffic'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Estimated intercept: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="si"&gt;}}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Estimated president coefficient: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="si"&gt;}}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Estimated accident coefficient: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;.&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="si"&gt;}}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Estimated intercept: -1.7206
Estimated president coefficient: 1.1502
Estimated accident coefficient: 1.0845
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Bayesian-logistic-regression"&gt;Bayesian logistic regression&lt;a class="anchor-link" href="#Bayesian-logistic-regression"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This way, we get uncertainty in our estimates! Uncertainty is important, as we'll see in following plots.&lt;/p&gt;
&lt;p&gt;Sometimes, the chains immediately flatline to a single value. I'm not sure if this is a Bayesian modeling thing, or a PyMC thing. I did not take the time to investigate. Should this happen when running this notebook, kindly try again. If stuck, restart the kernel and start again from there. Advice on this matter is appreciated.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [14]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;priors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;'Intercept'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="s1"&gt;'president'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="s1"&gt;'accident'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'traffic ~ president + accident'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;observed_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;family&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;families&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Binomial&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;priors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;priors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;start_MAP&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_MAP&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fmin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fmin_powell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;disp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NUTS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scaling&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;start_MAP&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;trace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;njobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_CHAINS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;progressbar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;100%|██████████| 2000/2000 [00:15&amp;lt;00:00, 127.65it/s]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [15]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;warmup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;
&lt;span class="n"&gt;variables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Intercept'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'president'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'accident'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;traceplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;warmup&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;varnames&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;variables&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1gAAAGoCAYAAABbkkSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzsvXmYXVWZ6P3b+8ynplRVKnMCBBICYZ4CotA4NEijOOCs
tHbjdFuv3dr9cdv7NF8PYl8/+z52N/dqiwMO4AAqgi2tgogoQ4BACEkqc1VqrjrzsOe91/r+2KdO
nZOqykAqqQTX71Hq5Ozp3evs4X3XO2lSSolCoVAoFAqFQqFQKI4afb4FUCgUCoVCoVAoFIpXCsrA
UigUCoVCoVAoFIo5QhlYCoVCoVAoFAqFQjFHKANLoVAoFAqFQqFQKOYIZWApFAqFQqFQKBQKxRyh
DCyFQqFQKBQKhUKhmCOUgaVQzDGvfe1reemll2ZdXqlUuPnmm4+jRDNz3333cc8998y3GAqFQqGY
B9S7SqE4digDS6E4zpRKpYO+1I4XmzZtwrbt+RZDoVAoFCcg6l2lULx8ovMtgELxSuXcc8/lIx/5
CE888QQTExPcfPPNfPCDH+Rv//ZvsW2bG2+8kZ/85Cf09/dz++23UywWCYKAD3zgA9x0001s3LiR
22+/nXQ6jWma/OhHP+LBBx/krrvuQtd1Ojs7+cIXvsDSpUt59NFH+cpXvoLneSSTSW699VYuvPBC
7rjjDnbv3k02myWXy7Fu3Tpuv/12nnrqKR599FGeeOIJkskk73vf++Z7uBQKhUIxD6h3lUJxDJAK
hWJOueaaa+SWLVvk2rVr5Xe/+10ppZQvvfSSPOecc6Rt23JwcFBecMEFUkopPc+T119/vdy6dauU
UspyuSzf+MY3yhdeeEE+/fTTct26dXJoaEhKKWVvb6/csGGDHBkZkVJKedddd8m/+7u/k319ffKG
G26Q+XxeSinlrl275JVXXikNw5D//u//Lq+66iqZyWRkEATy05/+tPxf/+t/SSmlvPXWW+XXv/71
4zo2CoVCoTgxUO8qheLYoTxYCsUx5HWvex0A69evx3VdTNNsWt7f38/AwACf/exn69/Zts327ds5
/fTTWbp0KcuXLwfgqaee4tWvfjVLly4F4IMf/CAA99xzDxMTE/V/A2iaxsDAAADXXXcdCxcuBOCm
m27i85//PLfeeusxOV+FQqFQnHyod5VCMbcoA0uhOIYkEgkgfIkASCmblgdBQHt7Ow888ED9u2w2
S1tbG5s3byadTte/j0Qi9f1A+HIbHh5GCMEVV1zBv/7rv9aXjY6OsmjRIh5++GEikUj9eyEEuq5S
LxUKhUIxhXpXKRRzi7p6FYrjTDQaJQgCpJScdtppJBKJ+ktrdHSUG264ga1bt07bbsOGDTz11FNM
TEwA8IMf/IAvfvGLXH755TzxxBPs3bsXgN/+9re8+c1vxnEcAH79619TqVQQQnDvvfdyzTXXAOFL
0Pf943HKCoVCoTjJUO8qheLlozxYCsVxpqenh7PPPps3vvGNfP/73+fLX/4yt99+O1//+tfxfZ9P
fepTXHzxxWzcuLFpuzPPPJO/+Zu/4ZZbbqnv5/Of/zyLFy/mH//xH/n0pz+NlJJoNMpXvvKV+ozi
woUL+fCHP0yhUODSSy/lYx/7GABXXXUV//RP/wTARz/60eM4AgqFQqE40VHvKoXi5aPJA/3ACoXi
FcMdd9xBoVDgtttum29RFAqFQqGYEfWuUrzSUCGCCoVCoVAoFAqFQjFHKA+WQqFQKBQKhUKhUMwR
yoOlUCgUCoVCoVAoFHOEMrAUCoVCoVAoFAqFYo446asIZjKV+RZBoVAoFHNMT0/bfIswp8zFu6qz
M02hYB56xROIk1FmODnlPhllhpNT7pNRZjg55T7RZZ7tXaU8WAqFQqFQnAREo5FDr3SCcTLKDCen
3CejzHByyn0yygwnp9wno8ygDCyFQqFQKBQKhUKhmDOUgaVQKBQKhUKhUCgUc8RJn4OlUCiaMSoO
2YkqlumhaZBKx1m4uJVUOsaIMcauwl5GjXGqbhUJdCYXsDDVxRkdp7GybTmaps33KSgUCoVCccIh
pWRkoIiOek8qDo4ysBSKExwpJe7ICHZ/H87Afrx8DmFZSNdBi8XRk0m81m4G9aUMVhKUqsGM+/Hb
DMa791FcOIzUxYzrLEx2ccmSC/mjFVfSFm89lqelUCgUCsVJhecGmIbL6GCJxSvb51scxQmMMrAU
ihMUd2yU8pNPUHl2I14mM32FSARXRunrOp/hjmVILYIuHBbaY3S3aXSsWkppcRsvjO1G5OO0lLtZ
XjmXU8bP44KrlrJu/TI0TSNvF5gwMryU6+Wl7HZ+0f9rHh38HVctv4JrT7mGdCx9/E9eoVAoFIoT
DCHkfIugOElQBpZCcYLhDA+R/88HqTz3LEiJlkjQdtkGUmvWklh1CvHFSyCeYMsLozz/xH5cN6Ct
NcpZyzWWeRmc7Ttxdw7CTkhFYO3KJO6rL+ayc17D8FaDLc8O8twvRxjfZXHVtWtZtWAFq9pWcMmS
C3EDlydHn+VX/b/hkYHfsnFsE+9c+xYu7DlXhQ4qFAqF4g+aIJg5+kP6PlpUqdRHirBtpAiIpFvm
W5Q5R10NCsUJgrBtsj+5j+JvHgUpSaw6hc7r3kjr+ReiJxL19fIZg0d/tIXMWIVkKsqVrz+D9Rcu
IxIJa9aMm3/EfU/eSfeOEc7r8ziz34b+J9DPrXD+m97CWedfyu9+tZvBvgI/+tYm3nDj2aw8rQuA
eCTOH624kiuXXsajg7/jof5H+MbWu7lw0Xm8b91NpKLJeRkbhUJxYlJxqwgp6EiocCnFKx8RTPdg
+aUS5o5eEsuXk1ixch6kOnmpvrgZgPYNl8+zJHOPMrAUihMAc0cvY9/6Bn42S3zJUha+8120nHt+
k9dICMHmjYM8+/t+RCBZu34xV77+DJKpWH2d3vwuvrH1HizdYvW113He6uvwduwk//OfYby0BeOl
LbRd8SqufdtN7B7o4Xe/2s3P793ChqtXc8GGlfXjxSIxrj31tVyw6Fzu7r2PFya2MFwZ4cPn3syy
1iXHfXwUCsWJyc78bgAuWXLhPEuiUBx7ZvJg+aUiEIb1KwNLMYkysBSKeURKSfHhX5G57wegaXRd
fwNdb3ozeizetF4+a/Cbn+9gYrRCuiXO1det5dQ1C5vWeXLkGb6/8yfoaNx81rvYsPRiAOLrz6Fl
/TmYO3rJ3PsDKk89SfX5TSy76Z3c+N5L+NVPt/H0Y/soFSyuunYNuj7VvWFxuoe/vPCjPLjvFzwy
8Fu++NwdfGj9ezmvZ/2xHxyFQqE4AvxSCRn4xLq651sUxSuU2UIEFXODUXWwTY/uRdOLbHmFAn4+
R3L16SdFyoIysBSKeUJ4HhPf+Rblp54g0tHBso9/gtQZa5rWCQLBi88c3GslpeSX+x/lZ/t+SVJP
cXXHjYzv6+Te3j0EQqJpsKA1QVd7F0s/8hl6dj5P7sf3MXHPd0mvf4Eb33kzv/rVAL0vjmKZLm94
89lEY1Od0yN6hLee8Sec2r6K72z/AXe+9B3evuZNXLPy1cdtrBQKheJQmDt6AYhtUAaW4tgga0Uu
NH0GBV/VvzhqRgZCb+CC7nQ97WESa9dOAOJLlx4yZ8tyfDJFixWLWtHnyRhTBpZCMQ8Ix2Hky3dg
bttK8rTVLP1vnyTW2dm0Tna8wm8e2kl2vEq6Jc5V167ltLXNXivDdrlz033scV5EOkmKOy/mp3YR
KM567EQswlkXvour9j8O27Zif+EfeO27P8ATyQX0787xn/du4fqbziWeaH48XLjoXLqSC/jKlrv4
0e4HKTol3nL69SfFTJJCoTixqJRspJS0L0jNtyhzRs7KY3gmq9pXzLcoimOE/AM1oibMLBkry1ld
a9E1/dAbzDO9+wu4fkAiFmFxV1gJ+XgbXcrAUiiOM4FlMfLvX8LavYuW885n6cf/oikk0PcCnn9q
gBeeHkAIybpzl/Cq151OIjnltRovmDy0cR/PGg+jdY4izFY6M1dxzvplLF3YQk9HknQyRkTXCISk
WHXIlW0GJ6r0jZZ5cdxgc+IKLujp5vW5TeS++R+c96qrSay9hH27cvznD7fwJ+88t+mYAKe0r+Rv
Lv4k//fFr/PIwG/xhMdNa958UjxwFQrFicPYcAmAto7kEU/SCNtGi8fR9Pl77gSBIDtepbM7jSGr
jBrjmJ4JwIq2ZfP2THS9AF3XiEaOz/ED08Tc0Uvy1FNPutDMsuliWB5Lu195FezmmoHyIACmZ9Ea
P/HHy/XDfqB+Q1n9mYyuY4kysBSK44hwHIa/9C/Y+/bSesllLL3lI/XSrlJK9u3M8NSje6mUHVrb
E1x93ZmsWt1V3z5ftnnwiX5+v20/0TXPE+ks0Kkt5WNXfJAV3Z2zHXYaZcPlxb1ZXtjVw7d2LOXN
I4+x6Mnf0t25G/fCtzE0UObB77/Im959flM4IkB3qpO/vOhj3PHC1/jt0JP4wufdZ75NGVkKxQmE
FAK/WCDasQAtEpm+XMpj4n0WjtNU9fRQSAlHIkZgWRhbXiTa2UlixUrc4SESp56GHosdeuM5pJA1
KRctbNMj0zpwXI99MJ7fnUHXNC47a/FxOZ5fLCI9D2v37pMuNHN7fx6AhR0pYtHDf385rsBy/GMl
1knD+EiZltY4re0nR3XhutF1nPLo5sXAevHFF/mXf/kXvvvd7zZ9/61vfYv77ruPrq5QofyHf/gH
Vq9ePR8iKhRzjvR9Rr58B/a+vbRtuIIlf/7h+gxsZqzCE4/sYXSohB7RuGDDSi5+1Sn1MD0/EPzy
mQF+9mQ/nlYlfc4LiHiFC3vO40/PfhexyJEpF+0tcV5z3jJec94yyuY6ntl8Lrsf+CFrcruIPvY1
xs98O9nxKg9+bzM3vPt80i3NRTfa42186qKP8n82f50nRp4hEUnwtjNuUOGCCsUJgjc+hj0wQGzh
QlKnn9G0bGSwiFFxOOOsRXN6zzojwziDg6TOOINY98JDbwCEiSuHL4OwbQD8QgFhWaE3KxYnsSIM
yxNSIoQkyGeRUjLsJ7HdgLNOCSeginmTltY4sfjRqT9ChEqa7594RQ/E8YxjE8FR78I3DMobnya5
ejXxnkVzINSRcaTjNTBRoaXqcMrq8JoKAslYwUdPmER9+5DtTHw/wPfEtMnLkwnH9igXLcpFizVn
HwMD6xUQinncDayvfe1rPPjgg6RS0+Out27dyhe+8AXOOeec4y2WQnFMkUIw9o07MbdtpeW881ny
oT9H03VMw+WZx/vofXEUgFPXdPOq155OR+eU+7q3P8/dD+9iNGfS2l2h5YwXsKXJ61ZdxVtOv/6o
PUft6Tivf9XpBBv+B9t+/BCJh+/n1b3f4/EV15PLLOLH332et73/Qlpam2elW2MtfOKCW/jSpq/w
6ODvaIm1cN2prz0qWRQKxdwQmGG4WlAuT1tmVBwAhJBEIgc3bmbzdEkpCUSzFuRlswD4+fxhG1hH
agto0SlvnAxqyr0UuGPhM3RgQtCyK0NPaR8AE3o3MpUGOjENl8xYhZyucfq6o1PkT+jJJN9ncKLK
soVpIocZRlmqOqSTUWLR6d7OgyFrhubRhGtao2MgJU5//7wYWEdLqeyRNyyMsolXGuCs7rUHXX//
njxCCE5bu5DoDOPtegHx2JH9DkeLaXvsGS5xxvIO0snjY/g5g4OzLpM1C0tKyUBfHqmdfBbXcY/p
WbVqFXfccceMy7Zt28add97Je97zHr761a8eZ8kUimNH9sf3Unn2GVJr1rL0o/8Noels3jjI9766
kd4XR+lcmOZN7z6PN7793LpxVag4/McDW/niDzYzljdYe3EGefpTONLiHWtu5G1n3DCnYXmRiM55
77yB0/7n36Ev6OLqoYfoqPRTLdrc/bVnyGaNadtMGlldyU5+tu8XPD701JzJo1AojoIGA0AGAZVN
z+GOjjStYlZd9u7I4NgzhztJKdk0vpk9xb5py3YMFHlu50RTjsNkZbVJpftwKdhFxoyJad9bvsXO
/B6cwG04rVmeeQ36l2iQKTa4l+hIGMIX1LxNQpx8ytrhIKVEq5aJ79nK2I69PLcjw9Pbx/D8mb1M
pYJFIWdiuz7b9+d5cU9u1n17vmA8b0739kwauTUDS0pJEAjkEVjOL45tZ6AyfCSOzDlFCok9sJ/A
nP6OmxWt+bNEIiUY3qH3Men9nKlp8Vje5PndGSaK1uHL0oDt2+wvDxIcoWexf6yC6fj0j1WO+JgF
u4gbeAddR/rTnzHOyPDU8lmuF6PqUi5YDO8vHLFcs3KcJkeOu4F17bXXEo3O7Dj7kz/5E/7+7/+e
b3/722zatInf/OY3x1k6hWLuKT7+GIVf/oLYkiUs+8SnGB0zue+u53jqN3vRdY3XvGEN7/yzS1hx
ahgaK4TkkecG+Z9fe5pneidYsUpw+lXbGYxsoj3Rxqcu/Ch/tPLKYyZv8tRTOf0f/onW887n4vHH
WFbqRTgB3/vGszz+7OC0B2FncgGfvOAW2mKt3Lvrpzw/seWYyaZQHIyhoSEee+wxgiBg8CCzo39o
BNUK0vexB5pzhSZGywghKObNGbcTMlQEi/b0qqQlI/SCNeUzTBo/UuIGHsPlMYYHCvjeQZQ9CXuL
fQxVhqct2lcaoOJWGKqMTFtm2JKK2Xjs2ZU03SjPuEwKgTMyUvf2zYbt+kdkMBxrpO/jjAxPU1ql
BL0SFg/Ri9m6F6BkTBmovfsLjNQmyyZGy2THK2zL7qTf3ol/EMN4z1CRvrEy4wdcK3UPVi3Pb3h/
kX07M+zpncBzD1PJl+ALn/mysPx8Fnd0FHP79qbvTdunb7Q8o0EuJeQmqriOP6dSZ2uGVaFsI4TA
tg5uuBzI1mwvGTPLuJmZcfnAvhx7eqdPZtTP0HNxx8fwXH/WiZdGKm6VvcU+Bme4f10vwPMF1u5d
VDY9h/DcGfYwCzWB7MFBvGr18Lc7gThhstKllPzpn/4pXV1dxONxrr76arYfcLErFCcbxvZtTNzz
XfTWVhbc8kkefWQ/D37/RQpZk7MvWMp7P7qBcy5eXm/u2z9W5p++8xzfe2Q3esLinNeMkF/yMMPW
IBf0nMvfXvaXrOk89nmJkZYWln3iU/S87SbOyj7DafkXiEl4/pE9fOnuTWRLzbNri9I9/MUFt5CI
xPnO9h+wv6yUW8Xx5aGHHuLjH/84n/vc5ygWi7z73e/mgQcemG+x5pFJte/IDQM3cOnN78L0Dz2L
Lpo8WDUvhhDsLu5jz8Ag+4ZGGRueHqY4iTyIfLJm4M20TqYkyJaCKcNH0xjMzmYgaDOGIvrFAs7g
ANbO3lllqFoem/dk2VOrejgTviObDLBjbYw5Q4M4g4PY/c2exUMdde9wiZLhMDARein8Ugm7vx/r
MLwuRk3Zdg40lqUkEIJ82SEQAsucUqLNmhF+aELJJw364410XVxfki14Tdfz9v484wWT8cIMBriU
5LMGw/sLDeGi4d9tuZ3Ynsu+kfJRFcMYHy4z2JfHrM4+joWKw3jBxB0fw9q7p/59IKcbt8PVUUYn
ChQqDu4skx7avl3Y/f30/mIj+3dNHPJanvRcFYwc+YkKXsN+n9+dYdOuCbx8WExEWPbsOzrgOJIw
39LLZLCGhsLtT6BJjsPhhDGwqtUqN9xwA4ZhIKVk48aNKhdLcVLjjIww+pX/A5qG/aZb+Mn9fezp
nWDR0jbe/qcXcfV1Z9aTXPNlm28+1Ms/3f0UQ+4uFl+0Dc76DXudLfSku/mL8/+cD5/7AVpjx688
qqbrdF1/Ays+/Tes8fo4I/scCU0jOlThc1/fyGMvDDc9fFe2LeND69+LLwK+uuVbFJ3ZFRKFYq75
2te+xve//31aW1vp7u7m/vvv584775xvseYMz/cZLo/VZvqbEY5T92Z4rk/Q4FUKhMR1D664HqhE
DVfHMFxjxtDAA7drmtyfVDSFwPJMRBAqzcHBqnYdhs6kNfoIpAzzdVwfL/Dw/SkDKwxNm/lYMyln
kzlcwvVmnV03ah6EXHlm5VAIQWnYo5o5+mIPh0tQK/QhnJkU7xkGtPZV5oCJMS+TQfo+unloQ1rT
IJB+U7jmJJmCRaZsMTTRbKjNpg+PVMd4buwFvMlrubZeX3mA4epo07p2fx/ljU8fcdjpkSCCgLG8
oGJJyg2heZMevYOFlM5U5MTyTHaNjzJRNNlxkNC2Q9kL1VqupHcQD/DOwQJ9o2Wqe/eFOZCz7NT0
LEarY/SO9ZMpWuwZOsArPXkb+VMeM+E6SAnu6Aha+eBepOD5Poae3sLI/tl7cB4OsiGcVQrR5B3M
lg4+YEIKBipDGO7Br2dZa11zrCdC5r1M+89+9jNM0+Rd73oXf/VXf8XNN99MPB7niiuu4Oqrr55v
8RSKl0VQrTJyx5dwHEH/hpsZeLZINKbzmj9ew/oLlyGkYNzM0F8Y4Xc7d7E3NwLJCskLK6BJysAp
bSu5esWruHjx+UT1+btV02edzarb/oHYV7+MntnIrp4NnOEK7vvlTp7dMcGHrl/Hwo6waM05C8/i
rWf8CT/Z85/8x5Zv8emLPk48Ej/EERSKo0fXdVpbW+v/XrRoUd0zfLIjhOTZF3eht/ksXryAlenl
AERrifDVzS+gRSK0XHQJ/XtyRCI6S2t56n0jZZwgyzopcYRLttSPlIfqPXX4ikeTkqJPhQhOEtGn
kvXzZZuy6XLK4rbDOtKk5+pAUSuWx3i5TBB1aU21kiinGcr2kTHDFZdOjENDHS15kPDBSYRpondM
PaucTAZjVz+9Wgnbb2VBdKoEuZRyWlilWxVQq8/wwsQWzu8554iru86G5VsMV8c4pX0lES1C7/48
7YHN8pYDJtwOcY6aUQE9Uiv6MTP5sk1Xe5LAFwghGqotagzYeyhV4pxJcwET1xeg6zhe0KQQb+vL
0VVx6hUcJxmpGVFVt0pnckGDgDBaHWN569KpfY+PA6E3I5I+Nr2LvNGRcKJA12fNi1rclZ61t5gf
1Mq2N9SlmDTKvIOWBJ87BT8Q4qC9zya9g76QxADTaTBkpKTsFZHygPYKtVw6e2CASHEY//wzcQKH
Fjn9dwgCDXRw3UN77NzxMcQMYbnu+Dh2fx/xRYtBdk+7nk1n9vEKKhWymsmEkaEvN0GCZbOuO5QN
jcXVyzpYdAwbnc+L1rZixQruvfdeAN70pjfVv3/LW97CW97ylvkQSaGYM6QQjH7tPxirRtmx5l3Y
GUH30jQLNwQ8FzzOA8+MMm5mmlz4ejfoRDilfSXru9dxzsKzWNm2fB7PoplYZycr//pWUj++D23j
k+xc9CrOJeCl/jz/7zef5U+vO7Ped+W1K1/DmDHOk6PP8p3tP+TPz3n/iV1xS/GKYM2aNdx99934
vk9vby/f+973WLdu3cve38MPP8wvfvEL/vf//t/Tln3uc5/j+eefp6Wm4H75y1+mra1t2npzReAH
eIEHhsAJHPp2h9X61py9uG44yCCoJ88HgUCr2QpB7TspJSPVUXyrHc1toz0xu7wzqTG+H2BW3abG
wBKJ6fi01TzxjSGCk+gNKveu2qz50q40tu8Q0fQj1jElEtsN8KVHEAgMNyzVbvsOEJaLDsoVSDVX
YTtkXYsDlLly7w6MfBHZKij4VpOBNVmF8WCYvk3HHBlYe4p9OL5DXI+xNB0qjlV7em7OoU4xNrgX
AHfdBTN6hCbcYbrLSbrak+zbFebwrDk7fK5PPsKP5OfyAlnP1Ts4k3ttfk+4gceEmaUruQB3bJTU
6tOntpASzw3qrUxeNp5bP7wej88YjuoFgsGJKqctbW+WWoKwLYbzVQzbQ49PGReO6eNaHvGDVOQ7
3GIrh3K0SCnxfEkixmF3PWh8JWesHGP2MLpI0dWwjhTBtOukr7Qf07dYFGuu9lj/BQ/jXW/39wNg
uT4RDeKxKCIQeNnwmvOrYUhxvUroIXAzE9j79uEviGMmA0ZyBqelwvvEcwPGhkv0LJn+vDMsD15p
BpZC8Uom+8D99A5L9iz/Y5Aa9ukj/LZrM9TytDURJTBbEVYLSbmAi1adxhvOXceS1p4TulmvFo3S
8673cNm6zUR/8Ajb2i/mAnx2Oxb/8cA2tu7L8943rCEZj/KuM9/KhJXlhcxL/HrwcV6/SnmjFceW
2267ja985SskEgk++9nPcvnll3Prrbe+rH197nOf4/e//z1nnXXWjMu3bdvG17/+9XrPxmPNbIqY
lJLdgwXabe/QpZU1mNSVBIcKt5KTm9QZ3l/EdXz8bJakXwW5oHHVkEkPlhDUSqs1h/fVZN5X6mek
GlasW8OSpmVFp0QymmzqJSRlWJChtT1JnDDcT9MATTtMhV87pDI7mxIbqZqQbEUzKniFBLHOToxq
GCYnHBdhGjMqtC9nTsnat4+gVKDl3PPrDegN22PXUIFFXXFEWtA44FXLI+b7FLY8R2tbJwsuu2hW
C6jRg6dVSlSe65+2jhFUyJVttIbwsXzGoKunZdopCinZM1Sic5YiDGEJ/5mFKY/4BL5EdNSuwwPs
Ky/wiEViDFaGqbpVAuGzNBMltfp09g6XsL2ABb5LbvteTrvkDNqWTW+qLIRkrGBQFGMsau1kYWrm
JshhSFxolWjR6KzjZ9dyqRrHMSiX8DIZfDP0iEgB0R37sLsWUi3GcG1BPDk1PtmSxd7hMm2+IBbV
j7hFwWyYRZvhrGBxpwc0eGFtj4F9edoXpHBxwqSg2kEbq57bQRhyagVG03UrHAdpTQ+3y9kFeqLN
BlbdAJcSIQVCht4vcZA+ccOZcNxOXdLOpp0TdE16hWVNzBkqD04uD0wDPZVG0zT8XPgskRWDUtCs
Q+VGK7SnosRmKHt/rOd9lYGlUMwhxec28fvniowtvJQgZtO/ZhN2a5k2bzmlsQ7cQhfSbmHdqk5e
e9EKLliz8KBu/ROR1vMv4FWnnELkzvvZIlazVjicmqjw+5dg91CRj964nlOXtPNn69/PF579Vx7Y
+1+c2r6KMxacNt+iK17BpNNpPvOZz/CZz3zmqPd10UUX8frXv54f/vCH05YJIdi/fz+33XYb2WyW
m266iZsfmHdaAAAgAElEQVRuuumQ++zsTM/Y8+ZwMA2XdDpBxTEwPMHSlgQRXSeWjONLKBgeSxa1
oY3tJx1JE02nWdAGtp0kXXKRrUlEJU7FbKUzEifakqA9lUKPaIhA0rEgRU/P1AxvjjRezCYWieIF
oZKT9GIk4zG00SHSbRF6OpKUfRdHj9PWHm5fKbZiOxX0WJy2dh2/4jJanWBZVw89PW20t5UwnCpV
2ySZjEGxQkdrjDY/nEVe0J1i19BOcOHVSy+l3U1h+zppEsgKVPI2a1e3Mp6KE41G0HSNVCpGe3uS
lBsnZoQqTfhdHCElqWQcIhGiiRgdLQnMcmgc9fS0YQuTSjY05Nq7WkgsDMfAdC1ynkNrW4LW0TFS
nV209PUTazXoWbuK8cEy7W0pysP70YFkK5AMx6H+e3el6UqF+5NCUO7dQXLJYhLdMyv6AJneMiQj
dLbHida8o6N7MiQSMUxP0rmghe72NtKpOL6nkas4DGcGaDFGiEoLuJCWlgSaG4d4HNmWqsnSQndn
OhwLIBlYtHcmSZXCsZAtUVK1iIqCPky2EnBpW+j99eyAnp42OsarpPw46XScnp428mUbH41M2SGd
jhPRYixYkEbWUvt7+3O0dqZJtyWbri3bs4loUSIx2Nk3yCVnpUBKUuk4xGPQnqKzOx3mG8ZtUuk4
yWj4G/f0tLF9sIQWieBlyqRSMeTEOD3nNzfSBhjLGYxVqmSCPJEWn7N6Tp22TntbCan7tEaSpCxB
tCVBZ1cL1WiR1nia9rap31MCCzpb8OyAdM1jm9QF0VQML4gR8zy0qCQVETBeQHQtI6pF0dDo6WlD
CMn2wRJtbUmCgkVXW5LurhZa25ub9HbkLPSYS0dHEk0LnxddXS1097QiRGi4RCPRpnOwcxaGE5Ap
B7S2JdAiETrbW9CrengOAfi+JN2dBGzSqRjtHeE9G4iASqSFVDqO5gak03p4P5Vd8Cz0wT31+4va
9Z0oWSRyo7S3LcDRTWLtYCSjpPQY7W0phv0BTM9G9ztwyi5aOk5bWqeju5X4gjaonXMqV7seU3Fa
W5N4ufDeBZ+FC1sRmsUoUUCG51G7rxNWESeToWX1atIrllMYjOHLJFoiRWvEI0Cr/3YaDu1tSTo6
UrQ7zR6xzgUtTdfmXKMMLIVijijsHeDnD/VTaTsdN5ln/5lbsXLLcXZfhOklWNyZ4tKLFrPh7MUs
X3j8ilUcC6ILOnnVX3+Q+D2/4rnhJAlL8J7UAD/Mr+D272zi7Vefzh9ftpIPrX8f/775Tr659W7+
x2V/SXv82D3MFH/YrFu3blp4Sk9PD48//vis29x33318+9vfbvru85//PNdffz0bN26ccRvTNHn/
+9/Phz70IYIg4Oabb+acc845ZDhiYaZKZIeJabiYpkO+alPSAzw7SXdHkuGXRnAdh7jtUi7bBAWL
XFaQOuMMioGJW7YxLRe/aiMmbDzHJz9mkvKixHwLXQ89O1KTJFumPGCFokHFtohGYvi1KmF6JZzJ
TlYdIlKngk3Br2JGXSpxi0ymgl2ycMs2YFNpTWAaPhEZpX9imNMzyylXLCaGc0T1IlHXRS9W2fX7
J6isCpWh8YkSlXJ4nEymQqlk4gYuQsSIVkLFPZeDquHg+wIXH8OQlMs2luniOeE6VtRjZNwnW5YY
lks8EWFkrExXKka5MrV/L2dg1YpXeNkKMRkqfM+NvUBbaZgOvRPH8bGrFjkzR7ncBplKfR9WzXvj
RAKkhEpDscQJrUxQC1P0cjmsvmHoG6Z9w+XTft8wz0hi12QJshUitRL0paKFabnEfEmhaJB0Kozk
8zjGKK3Sozihc4rpkvMqlEe3MlDsp9PUsJyAZE3OfD6O9Hwsu+Z5Mx3KEYlhehimj1ymMVngzSJc
pyynPBcD+3MUCiaW5RLVYGKiTHZ3P8ZwHs33EJaLo0siRROtFj5pWh7EbXxdkslM9VZ6cuBFymWb
eEzHtuDZTX34OWiTLnog8csWQ5EsewoDICVR00VEoazbTWMvDZvA8jAMjUymgrBtZBAQqRmmY1mD
StUi75h0t8TIZCoUnRJtsdZ6XuBwX45gfJyFi30sSycSdcnlKuyXYXEXvzKVCwawZccYSSExLY90
KoZpOASWh+N4eF6AJgMs08N2IpiGg2tLdC281oZGyhSGJoh7JrZII/v3kjCX0Xlm88RjqWRi79tD
pKMVvSXMdUvko/iew45nHsZa1EHPklNZ1bYi/J0qFqblEfV8HEfiFkxi8SiFwCBR1evjZfsOZswC
GTYVLpdtNu3tZcLM0JHowDRdHC/ANIPwfrJCY6RcDojoGpWyh58xiCZ0gu19ZBM2pVadauAQKZr4
tkdU86BiUYqFhT3yGQ0pNDKWjfR1/FyVqKcznM2Sjibr12OlalOOmkRMh4QX3sPZbJXtO56hPOGw
dMFCyhULy/IolwVadRgpBGbfMOlEO0bJJDBsqkkdI+5jWS5lwvOOGB5RJHpMq4/FJOmYRiZ59A2d
ZzPSlIGlUMwBY/sn+M8fbsOLdyOS/fT2mLhbL2NRWweXXrKIS9ctYuWi1ldULpKm61z6geuI/HIz
G1+AQauHv/Cf5SdtF3Dvb/awrT/Ph990Nm9efR0/3fsQd237Pp+84JYTOgxScfKyY8eO+mfP83jk
kUfYvHnzQbd5xzvewTve8Y4jOk4qleLmm28mlQqNgssvv5wdO3YcVb7XoaiHJVUtUnaWoGMRQSBx
CQ6ZFFMxwC84pKGhcvuhY5OkkNhGQCQl6w2Ej1Doqc+NRQDLZaJurp6b5Vs21UyMeFqHhdNLNc+0
38Zd58tuvbhEI2UzXMn1dOKJQxUbmPVoDYc9yPIZnuuyIQxTegfvZWT39zGcDYhFNRYt0JsSxuq7
rn3lez6lqoe0U5Auoes99fVMz8YVDgW/itBiICySeqomf6NsGkIEFCoOni9wcx6RaJQgcUCRgxpD
/QUK4xVob5BjdIhIwUC0LZhxmwaRmxjIFHEqkiVdoVxVt0LSsAjiWr2stZQSPVtAH27u1ySbT6J2
3uEAGdteQvoBbRdfgtC18Dpp2LZgF9lb7KM90cbaztDj5dk+UaNCIBo8VQf5nYMDwkwDIZsqZAoh
KRoOiUhzXk8QCIYHixh7Bkm1uBhBBx0JcMbG4MzpkR16uYCwq+irp4qJeGPjuNUKkWqFibYWelLd
pKLNx5FAEATEaqp9o74hkdT+Vxs4yUStV1bFm6oQOO1Krm1QLCcIhn26V8fRDyhA7ruzhIM27Mxy
JZn+EunlDhkzSzwSA0JjeGrIG+SVIESA64dGWFBpqGI4y2+k6RrSlwiXepEb1w0mDzONY131/ag0
nQ9/+MP813/9F94hHh4KxSuZX/9+Fz/93ot4WpIUW9jV2snVi9/A373/VfzzRy/n7VefzqrFba8o
46qRi669gMtfswon2sKL+nrePfBL/jidZVtfnr//5jOs5DzOXXg2uwp7eKjvkfkWV/EHQCwW441v
fCNPP/30nO+7v7+f97znPQRBgOd5PP/886xfv37Oj9NIpBZGHC+WiFWqSM+FWkiXkD4lP8dIvnSA
Ahg+b1wfPCf0sGiaFuZiHbT3VLjMyAVUMz5WYbphImSAGRj4rsDJCaSQTBRMxnJG445m1Ll0p7nc
ue/rOBVBZdw/iFQHPDsbNCNXuAxWhppW07XGpPvm85qkkJvyKJbdCiV7eluJgl1oOl5phn5Eludi
ONOT8aXn1wsEHE6yvheEVdKEkJimy67BImZDo1cJeJki1U3PortTpdI96cx4fkGgY2QsRDBV5GSq
ZxjsG63g1fJjEmMTtA9MbxTrl8sI10VKScHN1uSQTYUgpJT4B7bGEpLo2ACRQnbaPqe9Bz2/cSEQ
VrzTR6ca5R6YxwcQ1M6lWAko5Mx6ee+9hT5emNiCKzwC2VB0olZevmxPbzrdeNvUh6hqoBdz045r
+w4Vv4CQktGcEZa+r4nn+gG2G2Ae0PsqX7LJlWwIDq8nlh/AROkAY+OAu2MmQ1AICGaogti0Xe1v
44iG4xsuqdrejKXnD5fZjJaSIciXLHb3ZQmkJF81mzc6oDiHOzHefJ+PjzWcQ926xvMFe4ZKFKsO
GhrWuMBv+NnK1uzNjU/oHKyPfOQj3H///Xzxi1/k6quv5q1vfSvnnXfeXMmmUJzQDIxXuO8/nyed
CdClzjLraZa+933cfOZK9Jcz43sSc+GVq3E8yQtPD7K56zVc9NJ/cfra8/hmZR1f/MFm3vSayxlO
jvKL/l+zrmuNysdSzDk//elP65+llOzevZtYbG6quAHcddddrFq1ite97nXceOONvPOd7yQWi3Hj
jTeyZs2aOTvOTExWSotGQiVSjo+SzVoYpy9HEwIhHLYOF1nVkST5MqqqTaoxvf15TMcnSFlk8w5t
sRiB1zwPK6VkqDLKaMwhmGhDRiROVbCvWCZStGhP1spFH2R6WEowHI9EPMJo1iDXodHdnpyhh1W4
D8u3SYkkUT3CdKtNq+eJaZpEykllMXwG28JCBh6tnt+kUBkVh9aWUJi+3BiRDp0F7T3Y+/bBglAp
c2qz51pNA8+UrKaiDqZnMlTIY0W6SLQ1e0HcF7dgtHbTeuFFs44DgC98slYOL2ghFokxnBMUrAn0
RR3YbkAiVgt7rAZs2zzAypbUjGMbyKBJ6bbNBHFXYFddZG3Mc/44ESK008U0jRbQfB9BFCJhAQ9v
IvQgBct6ENUCLKx5wxqKD1SrEmlotLSH5cqLRYsF8bCiil7IwqrlmKZLZrRCz+LWaT+fXpr0TNSM
KyBTtgiEIHJggRTHIZIdx+9aSCZXpScRGqbZ8QqTZS6KdhFiUezh/dj7t8GKMMwvUstniu7sw8xG
Sa09E2o9n5qHM/xHZO8QmlHFbW2H6NRzZH95EFtYmH6V2Z4uAtnkLM6Xmw1zTQiQEsf32bInw5qV
naQa7tsD67EMZarYE3l0IdHrFTxDvAZjznI19ENVJpQSzXMPWmjQcn0ma87PtrdgWhXK+kxG+Nf1
0DwPqdeKbkjIlmxsN8AqOkgE6alFB3wAvzIVVjqJkIK9w1V6OlO0p2KUDBcnUyUwDYaFzaJOicbh
h/w57rHtXXdUBtall17KpZdeim3b/OIXv+C///f/TmtrKzfddBPvfe97icdV/xvFKw/PD/jRY3vp
3byPpX6SWOByduF3XPJ3f028o/PQO3iFsuHq1ZhVl51bYdup13HOzp/zVwuH+OHCq3jw8RFOX3sJ
LHiEb237Pp+97K9Ix45deVTFHx4H5kx1dnbypS996WXvb8OGDWzYsKH+7w996EP1z7fccgu33HLL
y973y0EiEZPhcTLAFBZFe4zOeE99naZmupMalBBojsVkixshp89+V8s20EHJDA2KsZyBafu0xmK4
5pQi5fmCQqWAK4EF4Es3VGcmlT4p6v14KuM+nq0RSQFBgLmjF81OgJR4gQj/bwmS6RjCsCAZpVx0
cMoBicBALpo6ru1b5Comp3asxPUE5gG9djxPIxKZ8pg1np0lDIRnY7zwHIOnnoHmaUTapvqlmY6P
YXsYWQNb7EU4DpGJfNP+Yw2V1GzPY1+xn/ZEG47f6I2bOur+sQpazqBtMud0sjqe8ELvTEOYdN4u
UnYqlN2A7lQ3fiCwfYtE0IqUcqokvhmg+T65StB8hrWPju9OU+galehACFxPw/d1WoVsUkPrxoAR
4JlR9BaQbVNj7A8OEy9VkAWXqJ+k7FQZKo0hRQrLc3G8AprXjjRdilWXas1wCQS4RYvenVnaExFy
GQNkeLRM0aYlAm2GBZPGhaZhWh7FSplI1aW7tTlksbhtO5HsBKWyjzAMSMwQ+zV5bY+PEnEc0oUi
rFhAZLLCpeMRmCbVzS8QnQiT5hptkkrJxo2L+lhWvDyalqI10l5bVxAvl9FtASSmjfMkJdMlVTMu
ppV+FwESnWzRxlvkMjhRZe3K2UMtcyWLtAjIFuLEojrtQLZkkvUDKpnm3E5xkD5UUkI0X0Qrl3GW
QO2U0Dj8kvGG6VPKGpT0JO1tcvq51cY/2ruP6Hgn7tLTmtYQnqRSTZFOH5iTOhlmWDO0PW/aRILt
BMSjobdcI81o2UC4I0SBkp8n6USBjtmFF4LIxDBB1yKIJ6aFfM41R52DtXHjRh544AGeeOIJrrrq
Kq6//nqeeOIJPv7xj/ONb3xjLmRUKE4Y9o9V+NrPtpEsllgaJEl6ZS4Y/TXrPvPJP2jjCsLQj6vf
eCam6TG4D/ouuInVm+/j3cbPefLs6/ntrhZaT1tLoWcn39/5Y/5s/ftesWGTiuPPP//zP8+3CMcM
TQPXCw0O142F/W4ATYrDSiTQjQrEUyBhJGvQ6uv01HqFBkLieAG+d2CcF3V9RwShYdc/VsHMGixo
SRCYodfEcwWeCNCkz7iznzaRYHlsUdh4t1Y1US9V8KVGdKzKgeqmBJKFEhHXJttVxRgqEg3yOC2D
yNTUSqIWErmjd6jplIXUKFYS6NrM49AYTFDcvYeWVBuptqmqcw1RcyAlvi8Iohq5QoyIFiqr6Yks
6CmkhFItF6TsVIiIYOokGtRszw/CMQ0ku7eP0665RIXHYHmYfLGPNZ1T/Zx84SPllEfA8C3GqhYL
Fup0aMvrskVtG084JPXmqnMH/mQi0AiCqYbPkZEB/DaHyrKVOE4y9GTZeRbPMFzCrSm3NsjW5vwi
0EhncyRKEZ5ydyHKJq1xH1tEkFJSEQXi9bDVsJag42kE+QrZpE/7snZsywMvPEYgJFXXo7E8QJhD
JEjt3Ylx4PUIONUwBNV3AnSgULZZ0Dlzgs1k6fFkoUh0yy7cCxcxNFFl2QHxjEIKyk4ZZAdIiesH
VPI+6dqyvDeBIEZrqr02Cjox00LzdZglZ21mm0NCJAq4aK6LTIfbRvITyNRiIDSwRCAwbZ1IzQIW
QuBl8vjVDEJqiCAcv76xMkl9quFvIH1MUaWzN4DXhPmghuUxnDFY0p2uy6WbNlXfIyi6LF6+EhyX
UiHAGJJEWqFiaSzunJRbMFTKkYrWxjgQjOxy6JGTYzf91N2xUSLVkekDccCr3jTTmJpGMiboHS1Q
1ZIslZJaLDNIQeAH+LMYQZPhrQQ+rpgeuisCgVV2kAJKTgWrWiEynidSyqNXy3hnrD/mOVhHZWBd
c801rFixgre//e3cdtttJJPhjX/ZZZcdVtlaheJkYuP2cb79815OlR5tIkHay3Dx4COseO+7SJ0+
vUzsHyKRiM61bzmbB773Iv1jsOANf073o9/iis0/5bQr38S3+08hnhrnebZwdvc6rlh6yXyLrDjJ
ee1rX3tQQ/3Xv/71cZTm2DHpdRJi6lzj5SpuV/vML3JNq4X6TCFq+ovbkGMxlC3jupIlxSkvjStD
5VRA3dMhPBfdaMhTkqECClA2PdqjoZJT9SrAommGnxu4te+0AysuNIvtugSaRlCp1BPVaydO0Sph
D45iuTpTIUla2BcLrf6VkBLPn95/62AdWL2Kzl7TInArZNIuaT+C6cRpaZnK4ajaGsGIMcPWUyeh
j0zQuXcYoXWEhTYWQC7vsJBQsS855aYtt/ZliRsuaMnaMWySto2zqr1J3slL3ArMuolqm7Ew3KwB
uxLHtwREw7HUHQuRGUdftrI+7JZn4UqN2bL/NQAhw/wioKtjKhjO9aIYhodmJ2mJizAjq5bftd/a
zQKWTQv1bPyJ/SIzImsnqblePXRv5pXqEuL4AZYTkAYqboWqa4AMQwIzRvOB8lu246Yl+YoDU7Uj
MIIyJScgQYo0MFIda9hKNl2rsqkvk0SboU/TTJfZoNlPxi2ztFa9ECnCvCopsSf2YhS2sfyUt5KQ
CbJDJXwBkUh4reSsPH6+SNWrkkzWGnkHgnj/XvQFS4AEjjAJgsrU8WvsGytjOz5V0yOeBDMfNGbP
EbgeiR19+GY7xNIE1dAodrxwLVf4lIoCCPcdGRqHmldOyIBxYxwvEtAuo/X9CstCEw0FKQ6C7Wno
eoDt+xSLZZZEorVw1rCBeK5vnKp1kIrLmoYjbYzGpKva4BdHp2TIWTnSlTItpfAe1HwPzawiU8d2
UvyoDKxvf/vbtLS00N3djW3b7N+/n1NOOYVIJML9998/VzIqFPOKkJKf/m4fDz+5nzM1SMgYSbGf
ywYep/PKK+m4+pr5FvGEIhaPcv07zuVH39rE5j6H17/3k2g/vpMlj/+Uv7n+HfzfoYsJzvgt39v+
E05tW8XS1hlKgCkUh8l3v/vd+RbhuCEBX/p1JS5ZLFHtaGcy4K3slUknwz5LwjRrjTz1hu1l/VPF
cEmkYLg6QkJPYtpTzVoDWatOODmbDLgjw+ilBuOgsTKglATCx3d1RENiii8k2aJFWyAZqozgihaQ
iWkeqPrnWip/vuywY3SESM9SAinxpSAyOM6IM0TqYCFAdXmmJ7BnvFEIUrTUfCaaFo5HthQak76j
M+44LGwB0/ZJBhpuIEg0GLSur5GcYdpbEwJvLA8ru9EzYWGMohElnXBpr5/b9O2klHiBR+D6xHWt
3pwVKdHRwmIdTS42EEwpyYEfIdFYevqAQ8RyE7iRCFKIprBQiTZztbzGHBjh4wiLmJ6YVmRC1+W0
DC7PjeKN0djn9qBEXJdkvoBomQyjAy+QtebRtckEOZVzNPlvOODAtc8ZM4eQkqrpkZZT6zpOnL4R
WNRq1HL4pnCFXTMQNQQSwzMIxNQ6sv4f0MpFyjv3E+lJ1pZJ8KZ7TgyjhdaUbNqH6U83yoUIQ1rN
oEIyGqFgF/FHmwcvZ+UREnzfDu/J2r0sbY9ydZCYMQGdl1L2i3VT2fMFuYpDZyTArtRyCDWQQuJZ
DWG3rqBvb57TvADXcGDBlDdsanJi+jUSNw1kClzhYAVOU0+uRhqdupM/m5DBtJDCshmFmAsSSl6W
Rdpiik4Jxwjw/UjT9jOR9yZo9CM23feBT6RcQCZAMyykjNeXR/r3UBJn4y7rqOe4zjVHVUXwscce
q8eh53I5Pvaxj83YmFGhOFkRQnLXQ7389sn9rNc0ElJHS/Tyqn2/oeW0U1j0vg+oMLcZSLfEue5t
64lENB5/vkrHx/+aSFsb+kP3cespgvbCRQjN5//7/TcpVKZ3ilcoDpfly5ezfPlyenp62L59O88+
+yzPPvssTz/9ND/60Y/mW7w5o24aSbBdHz+QWFaSwJ5SmnwxNfPvl5qr4hUrbpO+NFawGMqH6zjC
bsrfijouLaNjYNgYtk9xYCLMiWhi6rkXOFDN2HhGlGJZC6uQSYmZryKKVSq13K6KnwfCMKxJHG9q
P73DI5QMD4lgrBJWkds/VmFoIlRQPV/gy0k5ZNO4TP9HzWtWQwgNc4aCYlVv5tn2quVh2h75Ynwq
5I6ZK9rFKwZeoUxkaMr74QibglVskGG6luj4DuWaS8fzBBMFC6cWGhc1LSSSXdXt+NKHWUIgU+PT
K/VNoREIKBnhu+zAku8H2YyyXcIRNnZg1r8Lt52MHZ2sCDiZBCZoGRtH86cP8uSmdqmIcML9JYrN
16dhu0wULSq2j1e7jvPl5oqTosFbF0gfSxhN3rJCxWG8aGKPTnk0XDdGEIT93qK1Zl/FqkPFNrCF
hYhGCSQYnkHJbvYuIiXa5LlW8liuxBlvNJZmfve7fqORJpis2B8ac+FnxwunE2qFPcNqjP5UyK+U
AUEQXhOTMtT/1P560sEIyk2eNC8IyJdtKoUp42+8YDE8WeGztq1vRvCLOhOF6VUPsyVrhiIWIXqt
4p9g9vBkIcBx9TD0tVbJUUpJwctg+FPFKw7cvFLz0PmBTy4TNkqfHLcD0WDaLIrjNDyTpE+klEOz
DOyxHIHQyFV0KhbkvQwVz8Y2PExj9iqDR8tRGVj33nsv99xzDxC+5H7yk59w9913z4lgCsV84weC
rz64jR0vjXEmOjoSo/t5rtn+DNH2dpZ+/JPoc1il7JXGoqXtXHXtWlzH59Encyz+1P9DpL0d8/4f
8tcLO+lwV+PG8/z9f93N/rHpFYMUiiPhE5/4BN/5znf40pe+xO9+9zv+7d/+jb179863WMcESdgs
NAgixPZNj7mStdyZxhQaxw+mKTTVhhLGsiHXIV4shfkaxRIV0yPoH5mm5GgjFaJmqCz7FYmohR2a
dlj1TJOSeL5ArFRuCN0Tsyr3RcPBDpwpQ09KqJoEgWiq+C6mVRqkvlAgcRtOumiX6tvZdgoRNKs8
huGRsw4oFw14RmwqAV6CZaWwnSi+8JmwmnszAeg1BVc7sF45MFQZYdzMNJXPnjyHEXNi6ph++Hfy
uOmR/5+9N4+zrKruvr97n/HONffc0DRDN2Irg6ACigwqSgvII+AAhoBBDWrUENS8j0nMgzx5Xk3k
o69GjANIXhkFx4gKiAQVAjLT3dA00PTcXXPd6Uz7+ePc4Zw7VXVXNSXk/vgAVfeeu8/a+5xza/32
Wuu3tqNEmPoYOOAXW7trIggd2mr5Ss0JF/WFcbwGKfKGMWq/R24QL1+pd1IuU26+RiUcP8CrzKVY
NtDLcQdV27UNUYwLGCilGB4tsHXTUySHK9L3DdykVA5AhMpuZd+hVLaIZrjuyO9iS35r7f4o+FO4
gUPZrxMJ1w/vr+HJFjU5SmBW6ud2jk6yZbgicKHroFSNvClgsujiBSoWwQpfl5QiynMz2VotBYWW
Knl+UH0WBEKIkLBEoFDIzdsRLST+o49AsPtFknua5eQbyYfj+jFJ/MCVaGWHIAgVP6PnUcDusXYb
n6Lyr0KUHbQtOyhO5JnIO7X7ejJvMjllUvBLjLi7KHlF2j34isiNGzE5xu/afGeoiP+lFExNRMRj
3F24lXtDOR5ePpx73gk3afL+ePuB5wizIliu68aUAudSEreLLuYTnh/wjdseZ+v63RyEROkeOw/4
A+94bANCShZ/5DKM3v/eohYzwao1izjiqCWM7M5z38MTLP3rK9AyGUZuuJ5P9b6ahMji9j/D//7R
L3TDRbEAACAASURBVHliU4s/El10MUM899xzXHfddZx22mlccskl3Hzzzeza1ewMv2xRc4AEk95o
7eVWLkLpuU0ATEWc8kAFoYKd5yFLRYTTGB2IR7AaBw8adrT1YhlrolL3Ufkn9JUEXsOxgSsivvsM
wieEKWTasy80OY+KZoKlUJRcn4m8Q6HkVkQmPEoRwlOrXYuQiM3P7aZUrqcH6ZV+Y4bR7BoVHUEh
yFdStVojaHBqA1RYU7ZnmG3DlbohN+CBjS/wzJYxiuUyQlWl5eOf9QOYmioRBGFfH+W3duddV2dk
zGZkvI3gAtVpR9gX9TS88L3qD7W3Y/a4fj16KVCUK8zHdZpTqxQBslDfMNNGdsPUJCMTRfIlB6XA
yE8hGho/K1XtgaUoly08V8dx6v5lwS1UUih9UHX1Ol8FeEGU9NSjTlEUgzrpG/Z2MerUn6F82av1
1SqWPfJFjz3jxco5qsQ3tLdcCYO2iqpUEQSqwmjDGrVCya99JrrWStUjotWUzVJlbauHaaVS7VxV
YYcowdKc5giMmnAZKY5SjsxZGx9B27Y7chAYFdEQhcKanFndVLXHXIDC2T6M6/qMP72HkiNq83Rd
CSjcivhE0S+Hc3VbKRzGV9Lzg6bvDwUoFTDlTlEO6t9bpYKPVwq/W0olm0bNH1eFayM9HyEg70/V
I2qIlqnEc4lZEaxTTz2VD37wg1x//fVcf/31/Pmf/zknn3zyXNnWRRfzgkApvvPTpxjfOMISBIHt
8Nyh/8nZj+9CL5YZOv/9JA45dL7NfNngjaesZNHSHJs27Gb91oAln/g0wrQY/e53+UjmBKQQiAMe
5erbHuS+x7fPt7ldvEzR39+PEIIVK1awYcMGFixYgNPC+Xi5Qo42N8KdDtGohU/YyDe9fQfm6Cja
nh2xY3eMRpyxchka3NTR0jgTEWIXhVKA5yFUpANQjJzBVFHHKU9f66CActmiXEozVfQx8sVYaKZZ
FjpUWCxFwh0jkyVGJ8uUyl6T/HT1N79QYHTXiwB4noYIgpYRCbcaDVQN8uhtbIe6kzeRd3C3hE5t
Nf1s52iByWKZ3eMFNu+cqqWtNaYeThY0PL9MoVAmtWMn5lSzAyz8gJIj8AOFCgSlqTj585P1uppA
xSMEk/54W5IQZmFGCFib+QYo3CD+jPkqIHqisjeFNxqqylXXxZysp9lN5MNGxqWyRXHKhkDVyJbn
6Xi+oOSVa3YUVJ6p0edqny/6RTZPvFj73R5praIRbTochfQ9giAkVgBB2UWrNcNWtczM4q6Jps8q
FE5Qwg/i6+75CmvHVti6CbX9OVAKa2wc4TmMTMnaWkwWZe26B0rVbIBQiCZf8rAmp2pr7Hg+TotI
dBSua2A+O0J5V6GWfooK0Ap5Sq6PV7mf9UrrAdetBEbapAQ6fsBYvhxX86umGfoB+aKLU5b4nhaL
giui5BGmSh7W5GRkbev3Q5R0Dk+UmCq6MXuUCgU3nKDMuDfCuDeMrxTFnaM4U5JCIYXva5TG85Qr
fdVEECCDIEyvJdwg8pVLOSjWzuV4QSwiOdeYFcG6/PLLueCCC3juued48cUXufDCC/nkJz85V7Z1
0cVLDqUU1/9sHWPrdtOPIMgV2bDqHt69oUxy5xjZE04kd1JX1GJvoGmSt551OImUwR/u3sSE1ceS
yz4OgLr2Rs7OHIswS5grnuTbP3uKn/3++dYF2F100QGHHHII//iP/8hxxx3H9773Pa655hrcprqh
lyeEEFAstX2/1eMyVQxiKYIiImzgKpdJfwwnshtcdZrkcCTqF00ZCzo4Ikph7tqBVq7XeblBUNtx
D1TA7lEP121WQKg5PFEjgMDX2LpLhqluqk4FWqUINhKFlt8eojoPj21TOyhOjtbSskolG7Oymx8E
7Qaojy6HxxGlcpNwRdUOz6sTSaepL1EYfXmh9DRjTnvSrFBsLz5XIbvUIj5RIhY2mRZMFh0cz2f3
7vpaCj+MnlGZju/Hm9+G/1ex34kIjhTd+liqTpurLwAQtCEtUY5dDkqUgsaeR3FMVOpggiBMh4yi
UIyQcgVFPy4YIT0PMVonP9XrCNREEmofV1As2pTL9bo66YYbD1XiYe7ag13ZzPDxQ4l+4um29bkV
KQVF9hRaP5vloIDwfaTjopdKkfrB6C1WUb0rj1FwSvV3KsqMjfADReC3v0GVCuclSpE0SRVuEEQ3
IRrVJ2Xld1c5jHnDtUhxlfS5XjUKVx2zeTsioMFmQSX1sC75r0VscAKn6VFr9XxPlCdq9aWep1Hy
y+wuVqNx0bo2H3MyjzkxQXLXbqTnU6ioK/ptfIp8qX3fsNliVgQLYOXKlZx++umceuqp5HI5/uu/
/msu7Oqii3nBbXduZPiJnWQQsDDPukPu4R0vGgxs2I698uCuqMU+Ipm2OHXtaoJA8cvbn0JbcQgL
LrqYoFjkoB/ez2p7KfTsILtsB7fes4kb79rYJVld7BX+/u//ntNPP52DDz6Yj33sY+zatYsvf/nL
823WnGGqvHeOQL6Fz1d3rENVuagIRBVix1YKhUQlaFR1wFslXVWOD3w8L6gRJYVg3BtldDxO3to9
zrGUu8hB9YiDiBGeWrpUoyceReT4KtmpfmtPuXlKXomdI8PIFi6Q58dl7BthbNmJmCogd47Q6As6
Xqt5hmeeLLqMTpUb5kbtA54fUAzysfdEh5q1RhTKHoVyJJUvCBARufMgwhxFh/nV7Yr/6AWqlopV
vRtaKSO2HaQDOjW5VS0c+SgKZQ8xMVWvQYugKpIQjgOOY+L7Gq5rUCxGegBE0ldrUvbFBCNTQZiC
p1TL+7eWpkjrzYdGslD0802vaUJSLHu4fsCeXZugWE/bawWloDzZYV1biqEEMXLXduCajX5NaKQV
HFcnX6yXBEnXRaHYmd/NC8O7Y8cmRsfq6piN46jmOq8pfzxmD0DeKVL0iijCa+p5OsP5Vu0SQsJt
FIqV8esks9VdJEp50vb+K22alTbhP/zDP3D33XezbNmy2mtCCK677rpZG9ZFFy81fvXbZ9n64BZs
BNrKKR7t+y1v3pPjoPufQR8YYPFffhxpzFCDtosmLD2wj6PfeAAP/e4FfvMfG3jrWa/H2baNkZ/9
hHfem+WF42zcJU8x5A7xy/96EccL+MBbD43J9HbRRTt87GMf413veheO43DKKadwyimnzLdJLzkC
wp1mQ9eaagsUCtfzaVCqrr9fcWhGnamw8B06KyNUxk+MjFHo640dVw6KCFF3Oo1Y9G3vN05Eg1Pa
6KS2UvZrFA2oJi0ppfCVQuULFJua2XaqrImP1OrIsakypaIVUzOrXgc/CNORym6ApfkEkx5+WUc3
62f2Gp3aDsSjFbypuCNZCPKYwkJhsL20tfa6XizFeoypavpgJI0w6hBPViJM5bJVNbZyTBuiVouO
tCLMraGqBTEKorNo9Msbo1IA43kH1zMoOimSyTxuQUM6zY5zLR2uBWS1uKhyPt+XCCFCJ3mazT6l
go6KFzJCfAN8prxxUjKBUhrCM/Fw2DFSwByfpFyeQEmrJfmH8D7pFEwWDfFGX7lo05DU1mhP8FxP
Q4sMKYM61XYCh2RNOF0gPI+yY6O30qZRilKQx9BazLXh3o/+GiiJ1obUNo5fnYnnS2j4TMmdRGr7
z7+YFcG67777+MUvflFrMNxFFy9X3HPPJp7+/WYkAvPVU/wx8VteW+jjtXdvRNo2Sz72SfRsdvqB
uuiIY044gG2bx9i0YQ9PPryNV515Ns62bUw9/BB/1ruGr6/czsCqJzCD1/Obh7fiuj4XvWN1+Mev
iy464Nxzz+WnP/0pX/ziFznxxBN517vexXHHHTffZs0JgmqH4Gn87VLZY9gv0ZuxSDb0dlEqTC+K
EyyF5rj4pkHgK7yyS9Fp7TS3jWB5XpNvqRRxEYOG+p920EutU638qbj8cj6Iq442pwgGLeSgwzHG
8w5jo0WSQlBuqL9QqiIbP42tU0UXPwhIJ+IOuxKCoI0YRRT62AjWxmHKuQwqK2rzakKgWprSntio
puPKqsj2McXUi4JGGYyyKuH6ZTK2aCaMLUhFPfVx70nyqLuL5LRHdV47BbiegSZFXeWxgqrUeLls
4aEhnL3bDNVkqIwXE/+oMclqFM+JXCfV0V6fes+n1O49lAj95GqKY8krE6iAQgGETOCWHEwq8vO+
hy3bNNhVIMYqaW/+DJPQ9iEbZC5ik1UR/6mSS1JWr0d8fUMSJMN5tSV1KvY51zFwHYOUpdFK9Gam
0MZHwP8TTRFctmxZN42ni5c1lFLcdcfTPPX7zQRA4nUF/pj4LSv9Xt5y11bwfRZ9+KNYS5bMt6mv
CEgpOfXMw7ETOvfduZE9u/IsvOQvsJYtw3jgMc7aOcT2wnYOf8MuVizKct8TO7jmJ0/W8uO76KId
TjrpJL70pS/xy1/+khNPPJF/+qd/4i1veWXUS/qBwjfb775H06gCFVB2fbaND9dFFwgoBaFjJyIp
RMk9I9gjo+jFIsWJMpPbx/GCuFtQlW9uTOOajka0advUEdb4ZE3yPHoi5TUTh/jv8Y80koV8vu6s
VqNfSoGpx+fqBQGO12lmFTIUVGvL4u8attWkgFZ126PQJsI0KL1SJxNNZYtPpDnvrZOKYd3ChmFQ
uFNBPTJZQVXlTQWh8xuS4mpaaPsLONPtrn3yDlV8tRwviNfPTDPodCmFrRFJQ42MXyVTglARrxhE
6rsCr2NULprCGrWpllYYBPWU3UAgG+wuBa1T4BTAZBHfl23um4qwRKz+cgZXoilTRDE2Va7dB2W3
2uC6SizbXxOn4e+14/rkvQ4qhUphFEtNKbf1zweMTZVjIiAhAqTTLMk/UygUTDSLl8wVZkWwcrkc
73znO/n0pz/NZz/72dq/0+HRRx/lggsuaHr9rrvu4pxzzuG8887jpptumo1pXXQxLYJAcefP1rPh
4W2UUCSOLfGQ+A1Lggxn3jlMMDHJ0HvfT+qINfNt6isK6YzFyWesJvAVv/rRU3hoLL7sr9AyWQ74
zTpeNZHg3u33sfZtSQ5ZmuOBdbv4zs/WdczT76ILgI0bN/LNb36Tq6++mp6eHj7xiU/Mt0lzguk2
MqMO3JQ/zoQ7zqRTqIlYlKJiFlGRhAqZ0ZwwhUnbHVcWVIA5OYVSIlYg38KCFvZ02o1uj8bi+6q9
zc5VHXuzAVMlKL7y91qjWTXqQDfAmpgKFdAaMF5Jsatdp4gnqTyaiE8VYl82sNt4qaLDTn2xZKJU
qMCX2rELIz8V69vVfI72bxX8yc71cR0gXRcRxCO1+ZLL9j0uw+M+juvh+21yXKvYB35VSztsESQu
Fm30UhlHNc9purTHvUFQmNk9XA1kh+mazZOt2l/vcScoeBN7aWtIyZyGvm7tmg83wvMaBVkUPuH5
jXyz4Eko3Z+PKWVGr0O5KZW3eky9efR0iK5UEGkpJWatRNEes0oRPPHEEznxxBP36jPf+ta3+PGP
f0wikYi97rouV111FbfccguJRIL3vve9nHzyyQwMDMzGxC66aAnfD7jzJ+t4dv1u8igSR5V4mLsZ
UEnOu7eIv2cPfWespefkU+fb1FckDljZz2uPW8Yj97/IPXc8zalrV7Powx9ly5f/D6feM8yLpya5
YeMtfPqsj3PNbRv5w1M7MQ2ND779sK7ISBctsXbtWjRN48wzz+Taa69laGhovk2aMyhVjwg1OoAK
4gX7gOt7KHw0FE5QJohEPbQWf/al66Ht2d7SORcqwPHi6VaNO9SNpKNQSEJqbjZEqlzNnYkwQwVu
C2c4Zk2gCJTfFA0KAh+5Nx56U3Spec7RfSHHMfGLdXEI4Qd4U9COMoh9adTTZtmt8WhaZUO/robr
bk7mwTYQQrWJCHW+tsUgjyHN2GHW+PSRAuH7pLfvJEyWq5xJQalkUSpBOl2NVrQn8CjaSo63QxDI
Ws1e4yX0fY30th1M0SZlb46gnKDjJkLNniBgouCgaapCgpuKLYFQlES6DhSHKepFWsW6lJT1DY0g
qKXo+oFEa7G+rhfgBwpNdE6PbLJZ+W1ryqBO5IRbv27ljhs6FZPbhbymQ1VYRrkt6zfnCrPibmef
fTave93r6O3tZe3atRxzzDGcffbZHT+zfPlyvvrVrza9/uyzz7J8+XJyuRymaXL00Ud3FQm72C/w
/YBf3v4kz67fzSSKYHWJ9cY9pLG44H7wt24j9+a30H/mu+fb1Fc0jn3TChYszrLxqV2se2w7ycNW
MXju+TA5xfseUBSKk9zwzM184n+8mgMWZPjto9v4wZ3PdNOSu2iJL33pS9x+++1cdNFFryhyBVD2
q7UfM3MGwvoUUZPIju9eN//Zl55HUC4w4Y3G0+9qOUzx8xZKbuw5TIy06I81R4+pUGGvopbvtXGk
W0VQdFmfd2rX7lAusAFT3ljbXl9VlN32Tl2x1Gqnvb4Qnqfjj1GrG5Geh9mpwatSJIZHOtoTQ4s5
VdFurSqnARoiaYFC09qMtw/XtholnQ6OY8VEGqI2Ve3sxDl9X8MoNqvThZ/rbHhQKLOvN2456NBG
YQbPbWY4P6O/bZrmo5QiaJMKGT1XYngUOTmO1iLKozQZi5CKIMAaq8jTRyJ6UbSLJDWhlnEZaVa9
FxskM0XRn1lz5EbkJ5O4rkEmuf8UBGGWBOvnP/85H/nIR7jyyisZHx/n/PPP50c/+lHHz7ztbW9D
15t30KampshkMrXfU6kUUy0a63XRxWwQBIq7frqO558ZZgJF4cASW7P3YgaSSx5JEGx6nvQxx3bl
2F8CaJrktDMPx7J1/vNXGxnePUXPKaeROe71WFt3c9ZTOutHn+GubXfzqfNew+KBFL9+cAu33btp
vk3v4k8Qhx122HybsN8QBKp9TVOL1zvs77d19bwWUZ+qcMB0fp9olaLXIcCwN3DLstaXqBWiPacA
fKu1uEGjGulE3iHYh9rOapqUELLJ4SyVEq0+0oRSeWaOalAUiIY+PUbCxzBaE057z0hbMjodmiNV
qgORmf4qdiIcne1oP3y0lm5vEQTNyppN5+40r9lsGMzgs4Y7sxNUSWLgy9bRxYZhikG+JlvefGz7
czpBmZLq3L+szSlD7AOfanyW5wqtiGsQyL2LVu8DZkWwvvWtb/GDH/yAVCpFf38/t912G9dcc80+
jZVOp8lHdO3z+XyMcHXRxWyhlOI3P1/PxnVh5Gp4qMTEot8hvYAPPWSjnnqa5KuOYOHFH0LI/ZiY
20UNmZzNW96xCt8L+NXtT+G5AQsuvAhr2TKWPr6dY1/UuOOFu3gu/wx/ff5rGepN8NPfvcDPfv/8
fJveRRcvGQzNqOUIqmB6p8BvqGOJo/V3W1uHWLWvEeqEdoX10uscyWiMtHhT9XNPF4EAcJNttOpa
LJs7C/EciWxSsmvdK6kZwXR1RBW4U4JiMT4foSt0vX1ErxXZVcTruXxf1mTIqxuJvq83f2gWiDeQ
njk0MX1UYV/cYscxCKZ5dlrVz71UEGpm98TMMZ0ayAxG6EDAphNBmVnDgzg6yejPBn6gYjWkfrQG
a1/UeGaIWXmRUkrS6XTt96GhIeQ+OqYrV67khRdeYGxsDMdxePDBBznyyCNnY14XXcTw+7s3seGJ
nUyh2JotoVbeT1Au86EHTeTTm0i9eg2LL/s40ti/YeMu4lhx6ACvPmYJo8MF7v3VM0jLYtFHP4ZM
pnjD73ezeFRx3bobceQkl59/JP1Zi1vv2cQ9j2ydfvAuungFoLqrX3WKZwPPm5/vN6XETJTmX0aY
2XVoFudRM05zlp7frJCYsPdBdbtZjbBctoh2vyiXzcZPAArLblZp6xRRjKK5F9kcYTaPwD58dvZZ
6dOfNFAz951Nc3oiaOQ7E9xOKaOzhecHYSuDfRQB0SMRWt/sJLc/8wtT3Qwp9fXiW5XvwP2cpDQr
gnXIIYdw/fXX43ke69at43/+z//JqlWr9mqMn/zkJ9x4440YhsFnPvMZLr74Ys4//3zOOeccFixY
MBvzuuiihicf3sqjD7xICdhkFbGPeAi3NMUl92voz24m9dojQ6e+20h4XvCGt6xkcGGGDY/vYMPj
OzAHh1j0F5eC73PO74owVeDfnvg+mbTkr88/knTC4Lo7NvDIM3vm2/Qu/kSwdetWLrroIt761rey
a9cuLrzwQrZs2TLfZs0JggozsS0NfYaNMdvVfZj63OyUT+vatD1g373VfZPgnvVpW2JfLDGlRXL3
HoKZZV61rSWaS7Tl65X1MiLRsr0VBCj4k9Mf9BJARTb+5yPxfy7LhjWtUwLwSwNfeXiqPclrjOzu
LWYkK0+b1ORpEOg6sbtgP94QsyJYn//859m5cyeWZfG5z32OdDrN3/3d3037uaVLl9Zk2NeuXct5
550HwMknn8ytt97KD3/4Q97//vfPxrQuuqhh86Zh7v3lM3gCntbK9Bz1KGpilIvvDTBf2E76mNex
+MN/2Y1czSM0TfLWsw7HtDR++8unGR3OkzpiDf1nno0cn+L8B2HbxDZ+sOGHDPUm+MR71mDokn/9
0RM8u3Vmu6ldvLLx+c9/nosvvphUKsXg4CBnnHEGV1xxxXybNScIHTQVc3ANOc1mUBsfRRfT1zmY
Vr2puua0ltyerm1Cu6bBVUxr/76iDWOYC3Gc2SqO6cLcJ6cwCiVEc1rTLKKa7fnV9Ou1f8qUZ36d
THN6OXilSYoD/R3H9y0TNzGz+rl2sM32z9W00vLQJCTTDlrDFZvJpyxr33tFNaJTSp3cn5rnEZim
Q0+qsW32zBC3Xvzpqggmk0k+/elPc+utt3LbbbdxxRVXxFIGu+hivjG8a4o7bnuKQMHTuPS+7jGs
3bv44K8LWDtGyJ74JhZ96MOIFsIrXby0yPYkOOn0w/DcgDtuexKn7NH3jjNIHXkU2c17OH2d5IEd
f+TuLf/JysU5PnLmEXi+4upbHmP7cOumjF3898Ho6CgnnHACSimEEJx77rn7JJQ0OTnJhz/8YT7w
gQ9w3nnn8fDDDzcdc9NNN/Hud7+bc889l7vvvnsuzO8IQ5MIFDKS05WQKSzZpkFtB4gZOEEzdToE
oMm9c1CqjnujXPxcOevtxtkbetU2chTd+N5HgzUxuwiiFKBpAZYVaWQ7jS2tVBX3pa6uESnbJJsK
56PNgLjPBGoO0wrTCROEjHOXhqVKJQzK2Xq9f6t9g5kQpNn2aZxpdLbpsFk+N3t7H0vZ/vrM5HtD
6TrmNN9b09VF7UvdlGj6Yf9jVk/YqlWrWL16dezfN73pTXNlWxddzAqFqTI/vfkxPNfnWXyyr3uS
/s1bOO/XExj5EgPvOY8FF16E0Oa6uLSLfcXKVUO8+ugljO4pcNdP14MQLPzzD2EsXMghj+7kNVsF
P3zmpzw1vIHXHDzAhW8/jKmiyz/f+ChjU3O3S9fFyw+2bbNjx46aw/Dggw9idszfb43vfve7vP71
r+f666/nqquu4gtf+ELs/d27d/P973+fG264gW9/+9v88z//M06bKM9cIWnr9CQMzERkPsZ0O7it
PYm59C8UnXfu9wY525ibnXYlKPX3zn6cCOo1L/XV84J4ipQ2g/rzma59J6c3lw2vu67Xz28nwmhh
VBXNTbUR+5ipRdO04NINj4H+Ekk7IKmlMcTMn7VSb67te6aw9omEtnK6dVlJNotMJJp+Zls6xgyu
W6k0/UaG3MuNhkbMdFOjORA7f2rHzVHoGRAsAWbDvSJli2vXRikTwE0nYW+/d+ZBFXpW34zr16+v
/ey6Lr/+9a955JFHZm1UF13MFkEQcMftT1GYdNhCQPLoDRz8+DO8/vE80jRZdOlHSR959Hyb2UUL
vOHklQzvzvPcM3t48D+f53UnrmDxRz/O5iu/wEm/H2HnqT1858l/5/KjL+NNr1nM2GSZ2//zOb5y
06N85gNHzZnD18XLC5/5zGe49NJL2bx5M2eeeSbj4+N85Stf2etx/uzP/qxGzHzfx7LiROaxxx7j
yCOPxDRNTNNk+fLlrF+/njVr1nQct7c3ib6P9U/K97EMDaXraOkEnhtgLl2Ot91FL7goKRANO+hC
KmQg8FJJ9HxY9GNaLpZmoHud7TAMfWZpTUAyaeL4Qcc6k+i8DamhAg1T1/EidlimjtTAD7R9KzER
ApRC2RaeLnAtk8Sueo2mrsm9drKqdpumH8p8I9rWtmmaQPidxzd0Hc/X2kvuV9Bj9DHmNve/sg2N
vlyS3XvC77hyObTPtjT8sgYIdF1D6RrKNNDLna9hNfrYeO9AeAlMQ8M09do6hPNXlbkokraOEBKv
kKAclPBmcM/4CRstk0Kv9P+ytQQlv1hfa90kLdJMeuNNjaArRmBoEoXANDWCQEPXJVI6OE78/Ial
Y/ggLJu0laLkl5CaIPBl5VxaeIxpYBgauqthaRaeKNR7QXVA9L5e0JNj68hYmyM1puNxpqGju9Of
U9PD6G8QaFjSxlHladNfG5/nqN0SUWt8PRPoOogKATakjgrq10gXWkytshUCXcdQOnqERGtagO/X
F8g0NXTdZ2pKQ+oSHD9uc9LGcPS9+j6tVq6Zpo4WhOOZpkZvT5KBwf2jWD5nnohhGJx++un867/+
61wN2UUX+4z77nqWHVvGGUWRWv0Mx/z2EQ7c7qD19bHkLz+OfcCB821iF21Qrce69do/8uB9L9A3
mGLlqsUsvPhDbP/6V3nPvQW+d3LAvz72PS4/5jLWHn8gI5NlfvvoNq758VNc9u5Xz3o3sYuXH9as
WcMtt9zC888/j+/7HHTQQdNGsG6++Wauvfba2Gtf/OIXWbNmDbt37+byyy/nc5/7XOz9fe3ZODo6
Q2WDFvCnpkiJFFOmIK/blByfRNFFF1DwfHzTRPPCKJqUAUEgq3wD1/VqDWil9HB8F69DQ1oA1/Up
JRI1JxhCoYBG5bFA13HLHp4XtHXydF2LnU9IHy/wcZWP59dfLzs+nuaFx+4DwSr15ggME6nAT8Ll
8AAAIABJREFUd3yE72NEzisBEWgt0+Wms9t1PTxPw5Q2bhs5exXIWo8sAN+20Rrq0Fzl4QVBa+JQ
gS0TOK7X8hq5EgqFMk5Fcrpmn+eG9ukyfM3zcXUDMc11DsUfVFtRAik9elKSwNJx/QAhBFPF6vr5
FEsKx9FwXA83CK/rdPBdD8eJXBulgajPxVM+CI+y1/o6maaD69l4XoDr+niej0DhCw/Pi3/vu2UP
TJ3AlUjfxPPyyCBA01xcV8fTBBLQk+B6PlZg0Jsw2FV28LzO3x3V+8NL2OjFEp4bTPtcdYLT8DzE
5mw5OBWVR0GAkB6eJ9Gkj6f8aQmWRn19G59HKSSB6pyWKaWopUAq5VM1U8ogds2VkB3vbQBfarh+
fK6a5hAEei1t1XU9wMPzTFzLIhGxX0lJOZEkOVXYq/WuEizH9dFcH+n5OI7P2FgBtXt2YiyDbQja
rAjW7bffXvtZKcUzzzyD0RUK6GKe8cRj23niwa2UUGQXb+ANdz5AphBgH3EESy75MFq3TvBPHomk
yennHMFt1z/MnT9ZRyJlsvioo/HOPZ/dN93A++7T+O5Ju/i3J67no6/5cz7w1kPZPVbkkY17uOU3
z3LuyQfP9xS6eInw2c9+tuP7V111Vdv33vOe9/Ce97yn6fUNGzbwqU99ir/5m7/h2GOPjb03Hz0b
hWUh0ynIAOP1NDqrRbS2GqSp+lxuKgmqXlfUKf1M172w2adu4KasaQmWk06BX7dH1yTeNCIOVWew
sSDeHRxCjIx2/Ox0CAlDZX4CNM1HKUkQCGxLY29E+fQqWYlgbwri9y0IJ2r1KUlLx/GC+Ho2DJpM
FhBCoaSGKS1adXeVWlCL2MzMiPh5NE2AJjCQCBlNtQt/NmTd52t1j9TGERpCK+OrkChYVplyee+F
CsJUwKoddUPzi4YwN9fvn3QitMvtyUJDtplllXFdvf6wRAp0dB3Yi0xVhSCTNNHlvrnTmhQdVfcM
aWLo+RrBMg1Jvb+1wJYJin6BUl8vSgoSe5ojn+mkwVShPZGeDtXlKfX1oosy+nAh9vreQBLWEcbv
FYVllSkWm4VGnEwamUqg7RpGCIGRGSJrLQA27tV5YzOP1lLuxxKRWRGs+++/P/Z7b28v//Iv/zIr
g7roYjbY+Nwwv/n5BoRSLE6uY829DyCA1Np3snjtOd0Gwi8j9A+ledvZr+LnNz/OL259grM/cCQ9
p70Nd2SYsV//ivN/38O/n/AM/77+Fi5cfR4fPfsIrrzuIX7xwGYW9id502sWz/cUungJ0EiAZouN
GzfyiU98gq985Sst246sWbOGr3zlK5TLZRzH4dlnn+XQQw+dUxsaIQ0DfeFCcIeBMlm9B4g7OErX
EZ6HlPF0G8NIInrTUHxhZicTAi/XgyC+q6vafHcq24JK/WM27TAy3tmtqKaYZRIG+copDE1i6BKP
Jv8+YpaYZqe+shqmqjjIgkSiRLlsEgRGmArVsLuuGx6e29reVs6jKa19bqAL4bwabYifs35WTZNI
v3m+MuIPVmtXFAJbJilRIcRCEFQ2uw3dpezPnMikknny+VTNonDJm6+KIU0Wp/rxPZORPfnwWK09
wbJkEk8WZ60wLoRqujiBZaFS8YiTXr1fG7MZ5ji5IZsy0YqdFTPbwbNt3FwGY+fulu/bMoEpbXxC
4phOGCQsGC/Uo3sJLUzPbNUvyk0kahsr2bTD6ERzAGQmmwZSCAJRiVgnTYKywJwKr3kUytDB6dyj
SxeQTVmMd4r6i/gvgW2hEapwWjKBQNZSWB3XRwiJmiYK19aevrmt14yNPZsPd9oZ7KKLlxpPvzjK
T296nASw1F/PYY8/QMHWGLzkEpa89g3zbV4X+4BlK/p48+mHcffP1vPTmx7jrPcfyeC578UbGYE/
PsRZD/Vxq3yIXquHd618O3/1njX8r+se4vt3bGCwJ8HqA/bfl2cXfxo4++yzaz+vW7eOP/zhD2ia
xvHHH8/KlSv3erwvf/nLOI7DlVdeCYQRq2984xt897vfZfny5ZxyyilccMEFvO9970MpxSc/+cmm
Oq39gajPYclmAYNSLktieARQGNKspcLl9AECAkpMT7ASpk4hZ4Ze/Az9FeGF4QFN89H0vXFy6jNK
2QbTeb5SQIAgJTNM+RPth9NCG1qJsmWTBqP5SDijg7MfX+8EupaKOaOWTLQlW+Vctq28/Uzkz6G5
XMyWSTS9gKaLtiRUlzoOLv5AH742A/dOgCUsCpG0x+h5O1oqJJZmUYwsZzmbITEcj0IKBAOJDEW3
okA3C5FArz+JcjwI4oujSY290NiYHqI9oXdTSYx8Pd03qqJpmTplp1mcIWUbICBfjJMPL5lAGFrt
nC2h1ycWksboAoZksy1JigT6TFPV7I1aKIVGgOpIUBS07L/X9EpkE6ZdSnHWNtA0EbYbiI4VKUxs
1wdLAPge1SJGXZM4ro8lLEqq/caHkApVu2di21L7ddN9VgTr5JNPbpluUJXJvfPOO2czfBddzBiP
PTvMj299nH4FC4rPc9jW+9k2ZHLgR/+KJUsPn2/zupgFVr16IfnJMg/89jl+csOjnPm+17LwkkvZ
8uX/w9KNG3mn7OFn3EmvnePEJW/gL88+gi/d8Ahfv+1x/vbCY1jYNxM1rS5e7vjOd77DDTfcwCmn
nILv+3zkIx/h0ksv5Zxzztmrcb7xjW+0fP2iiy6q/Xzuuedy7rnnzsrevUWr4E30z281YiGFQBcW
Lk5N5SuuUh2Jksh43ZChS6SuUK0U5Gaw8z+T4EBVhrtpeBF3xgzDxTBcCoXI86tChzBMhwMnqOdy
JbQUpZSPFBo+Xov0r/ZKf4Yucb1mB7PqJKa1LFNCoOwETIUEop0suSYMvITdkmDNpAdZK1jSRpcG
7dQxZIXkJrU0k3aATNlQaq3CFiMBSqFpBtAQgYkwuFoAi9bXt/EejJI/U9rYMkEmEZCyfXa0U79v
Ma8owUnZBiXHo1piZWgilsWnSY0Zt2CqnMqyygjal7TkkiZj+eZcQd+2YgQrioSh0ZP22TnS2KeM
mFqhYbq4joESIiblHRgG0m0gYUOLmbRLaE/X+z1apqR6e0nRXjEyqKS/qWyKlONhmzoJW8fznViU
O61lmfInpo0CRZMzNU02M/DpLoKA3kPT8Cy4yQTWxF7UPgmBbXkkbEWRxnux/XlTqTCduxqV1XLA
zpmfdjaYFXVbu3YtZ599Nj/4wQ+4+eabufDCCznyyCP5/ve/z3XXXTdXNnbRRUfc9/h2rr/5MfoD
SDsjrN52L39cnWLBJz/FgV1y9YrAUW9YzlFvWM74aJGf3PAoJReWfOKTWMsP4OCnx3jrH0vcuP42
Htv9JIct7+WDb19FvuRx9c2PMlXsnLLQxSsDN954Iz/84Q+54oor+NznPsfNN9/Mt7/97fk26yVF
OZcFBJrQSWs5EjJ0Kmbc10ooAj1Uo5spoWrswWSZ7SW4oR7BkVKiaQGG3t4N0TRFMtnszNoyid0Q
xUtqWZb3rkSr1VQI0lo2liHWGD2q9h5KWK1q2USNPdQJQLQGqY3NmX6IqA1KqdANL9azrNTbExmn
9UgJPdH+JB2ujZNKxV+IHGuaDgmjnmOYlD3ImbiBLYmpQghIWoKkpbBNv+l80blJodGrD9bfi5Cq
TMpoSkFd3juEoUmStoGhSTIJszaelIJkJSo0HaqHBJkwm6EqpGAYHulUmSCbAk3Wj5XxVr6a0NGF
QbknR7G/F9/oHCprSRbbHKs0GVujxmepFZ1WCjIJsxY5U/1DuNlsiyMhMA0Kg/0EiwaRCGxDQwqB
bRdJJOpsd7p2uzXRqMhBKVtvmlm1x54QAquyCeKmonVVgqA/h3HgCnzLjL0eH6fFXDQNy/RJ2JXv
JsdrabOTrt//pukQeYxrp4pFy/ajfPusCNa9997LZZddxtDQEH19fXzwgx9k06ZNLFmyhCVLlsyV
jV100RJKKf7j/hf4wU+fZIVS6L7D6l2/4ZdvTHPwBz7E6sHm+okuXp4QQnDsm1aw5nVLGR0u8OMf
PELJ11n6qcuxli1j9YZJTnkwz3ee+HeeG3+BE9Ys4h2vP4Cdo0W+ftvj0xbed/HyRy6XQ480DE8m
k6Qanc1XGJrcIhF1aCXKStSOjHwoAhVrPKwMHSebRTQeVkFj/6Kc3hc7Z1JLcsDihR1trkYlBILl
iwQpqxJFiOT0KSnxLQvbiBOfTKqziNaSgUwsyqOL+PGNwhqdYBoSJQQpLYMUkLTaJ8wJIUAKPNtG
ZeNpyYbhIBuic4Fh1NbdkGbNyYtezwaXEwHYlhvWvHWCCKOY9V/rdtf7edXP0SkN0BIWQ4lBVOS6
V8UjqtFDKUOCpWlBk9UAyrRwVoYbnVqL9RcCbFMjiDy76YSJpRv0ZzL0J+vkQenx9anOLfr13hhZ
NAwNIyfRentRS+P3ZmDbqN4GchJZu1TCwBAWujBQAnJ9GaSMpwXaqerJRdPnm+cauceTCZTUZhb2
jRyTMBJIUZdab9eWxNAllqHT25tqIm5CgNTimwa13yLH2qZOwtKxTa3pu0a2MlzopLQMaa014VMi
3OSQDWQ6jJhH78Tmu1LTVWiDghXLs1iWj2FUlAUjKdqqssFiGF7T/d5s0OwbRHfCrJMPf/e739V+
vvvuu1/xf9C6+NNAoBQ33rWRH925gdf6DgjJgZO/58cn67zm1HM5ZsFr59vELuYYQgjeePJK1hyz
lNE9BW67/mHynsaST12OuXQZr3omz8n3jfCNh7/NtqkdvPvNB3HUoYOs3zzG9+/YMK2UbRcvbyxb
tozzzjuPa665hu985ztceOGFpNNpvva1r/G1r31tvs2bPVr4M4kEJG0DU2uuy/EHFuL31SMGhhE6
G512bN3B/opX0HpP27fiDVejzmxSS9Fr9aFFIlKtVA6rUApalHUAilTCJFi6kHQuQcKM9+zpBBnJ
WjKlVUslBEiYNlkrrvbYbik0zQ9T1KSszTFpKZLJCBm16xE0iUawPEuwKkc6k6HPGCKjt4/kKSkw
Er0ktBS2TKILA28gFbt+0XTGwLTI2ArRn0Glk6Rl3IEt9fZg6hFiGVnYhFafs2U0K6YFLYQ0ajYI
nWVHHgOJzo12+zN2xJlsQRK15jojv5LSaps6UqgacQNqUc2smcHWLILBHoJcGi9dX3NFeJ0MwyXb
U2dYUsgwemuENi9aabPyoB6WD6VJpqwYkUtqSbSK+l8tghZ535CSXNrElBZJmcYyJEM9CTKRht+N
InS6FCQsnXTlGNOsp4ombb22DG5l/oFS4f1GK1SuTUNhnCLsiZZJmPRkE7QiJFIIchkLUw8fCpWs
X8PRlQfW1r8R2WR9blKApWvhcydCOwxp1q5jU0NoGd4zAonVktyIyPfP3kWOlAhjwoYuWTm0gIW9
Bkal5jNIJLFkZTOpNn6Hv/dVJVMpsRP7r2/mrAjWF77wBa688kqOO+44jjvuOK655ppaYXAXXewv
eH7Av/3kKe79wzO8qTyMqyfpd9bzHydN8saj3slJy46fbxO72E8QQvDGU1ZyzPEHMDle4rbrH2as
IFl2+WewVx7MqudLnHz3Dr7x4DcZKY3yoTMO54AFGe59bDt3PPDifJvfxX7EihUrOO2003Ach0Kh
wPHHH8/RR7/ymolrkV18rSeLqUlSg7kwuiPqJESZdqwmQohQbU4m45ugzW5O1Zmqv7IkO9iSjaQz
ek3JTtTO3DoKk7B0elKR6IsVT7USQpHT+xAizNjqz9iwaDBe45OpO9i5lNVQUyWQUtR2wgUCv38o
do5Gh9A0ywjZ7IjZdol0yqsr0LXwBf10Fk2KGok0dA1DGhi2jib0GBnUmvryCby+fgyz3jJEpCWF
hQtQdjJsihyB6h0gvfJgVh1zPAf3rkRvUHRQUiD7B8C0wrobCUm7spMfIcHLs5EIjmj6ASUlYyuW
gxBoQsNM9mP09WFnKyQECDKpppqfnkyHqFoYLolaG/535VIATF0iJOhDvWT7M/SkrSYiqHQD1ZNB
2KIW6Kz4+6SSLobREF0RkoyZwTIEmh7KqLfqjZgyLWytmsqWxE9ncZeuiEVBq5/S2siwR4Ny/ck+
lucWYekaeuV8VcXP6Fj1VQDPU/hmnewkK+mqStPwEymGepulyyvaFlimFpPOj9klRIwsBosGw+bj
EpCy1hA4cvXD39s0760+h1Jo9OkLGEz2N9chxiKUzWm1UKndan654VwtdUTD/6rw+i5IDtWukxIC
o2cRgaGjKuObsjWBrH61CQGWETRF0+YSs6JuRxxxBD/72c8YGRnBsqxu9KqL/Y5i2ePrtz/Btg3P
887CZl7oWYPNHu59/SZOW3kqbz/wlPk2sYv9DCEErztxBaal87u7nuW26//Iae86nOWf/Gu2/X9f
5aB1T5L6+Yt8y/s6Hz3x43z8f6zhH6/9L26+eyML+5O89uCB+Z5CF/sBl1122Xyb8JIgkzLwK/Xu
9qI+8Eok+w3YJckziSWT9GUSTCUMHNfH8eJ9lJI9WaxxHVOTlD0fP6KFUPeD495Po8NfhZXQcWUS
9tT7grXym3TdQ1TcjXTCwHUVmeVLccjDSFhjFVg2lrRJahoQoAKa0+FMg2xGR/k+CUsn4xmMF5ww
Oi1C5y3qGvu9g7C53UqG9VGpZIGkrTFRmUK1biObAnMqC15rsQghJKmEgUAQSIGdMdkdTXOMrIRt
atSUtZclMXqA0fr7upRIE0TlmKSto6hHPnTNZnD5QhakMwSO09RzSQmB3zuASAcw/gJmSuI4QWVs
sCwHTfOA3oo9OmW3eV5ZvZeRilNuyQSY4frnBk12EaZ3Br1ZHFkmMdwgUNBAwENSWX8tSGaQk1M1
7tLfk2ByySC2ozPYm2VM6agF/Rg787FxBILeZRISBiPbi+Hc/fCeTCYNLF3HaXTiZRsnPvJ7Lmli
aDVaAULiZbNgmGiymnpmECprdIqG1IfWhIbR8KzoGAjRGM1pGE8T4MZfL/Xm0O1BcimTyK3QrCvR
YuPD0GQlxTX+el+ujGHpbIMm1RxT2EwlBf6SIRh/vjp49Ez1OWkyllpcbW5ePV8uFTCimVAoxmrr
UobNYT0rCZIaj+58utZ3T8oAIcA3DfQW92X99PFrUbUosG0CLU3R7Ee6Lpa00aTTXrAygLRt0k40
Zq4wK+q2detWLrroIs4//3wKhQIXXnghW7ZsmSvbuugihom8w//7g4fZvX4j54w8yObcEUhR5pHX
PMKbV5zA2oPeNt8mdvES4jXHLuOtZx0OCv7j1id4/LHdLP74X5F94wksGPF46483cd1dX8WwPD52
zhoMXfLNHz/Jll0d+m908bLFtddey7HHHsvq1atZvXo1q1atYvXq1fNt1pyhvpMeiYxokp4VNnZW
I2Wk6dEHsTUbXZMs6k+23LUXQpAwNExTkTC1ikR6CN2o72rH/bb2280qm6I4OFA/poXDZ9uRZsRS
0rvyQEzTQkVSvjBNgkwP2Upky0wJSNoUDloWNZ7+1YfGbK46XNXURCXiO/OtLC/299XtaaPstzi7
kISZI0jG0wqrwgtLBzNUJQoSloHemCsWa8pb/6+fsxF6/DUFLBuqR7PsSlpkdQQZjbyJMCUxpdWP
1/p6Wqb/QXg5DMOtRRqhSpgVid5cQ3le6+usVSXfI/5oWs92TPIKzxEhWULEbg2hCbRcuCm/NCL6
Uft8Xx/CNEAIpKXX7uVeu5dU0qU3VyZlGeiaqImXaFKQtix60uE9VFXRTOphelzaqN9vlqnXfpcN
96yUit6eEtkeLzJngZmW9B9Ujx6mtEx9egg0CU1ZsY1jN9ShySSI/nBMv6HdgyUT9LRSwY3xnuar
kLB1Una931r1lFHBB63h6lnSJquHz4VhuGHbhabobviZRf0tomqEEVBDB0ODUn8fTjqFl6gfmzFz
JI0kdsLkwIUrsawyPdkyuh7aGiwYIG2bbSJY4flFw+/t1qD9GCGao8pzj1kRrM9//vNcfPHFJJNJ
BgYGOOOMM7jiiivmyrYuuqhhz3iRq65/iOLzz3Pu7vtYN/BGlBA8e9jDvH7FkZxz8Nr9qgbTxZ8m
Vq4a4sz3v5Zk2uR3dz3LvXduYuDCi+g/6xwyhYCTfrSRm378ZYYGdC4543DKjs/VtzzGRL51j5ou
Xr649tpruf3221m3bh3r1q1j/fr1rFu3br7NmntEvuaijmG2z0QTGrrW7FikLJ2kITGlhTANhFD0
Zh3SKY9EfyTzpIWzndZyrUXkBCgrJHdKk1hm6wa6VXKVTNSfOdNKIITgqJVvBNskGAwdbCU1bFPR
0+NgZzUGkwOoROh0tnWXIg4w1AmTbKwPqbyv6XZN0r76um3Vd80zeo6ETIY1PJV8xXo2nSBlG5ia
xEpYsZEbowVKj5xDgLBCRznQ4mSpJxWQS5cqkQ/R0llsnIehS6IC30sW5zh0aU90mk3zjv6sS0E2
adGXizvKuTiXrKHHCmu+TN0k0a8hU5C24gSyldWC5hqlVla1ekFIibXsAOwVB8VS3RSChO2jRWwX
fl1oYiCdwaikueWsDAf1HMhQMqxFXJReUDvNAZllGBVFwMYlLwwOhKm42RRGm1SzbMqkN5VERtJW
B7KyrR+iS42BRD9pLV6bp+fA6cviDS6OpV5WI6CZXOf6N4SIBaOUlEgEqaRHzzKDRE7DytTvlaXZ
RSyxVoAm0fV4tChViRibpkMiUYqR8thdFPlCKAzV6zwDAtJ2eC0yiYUEmRwZs79iZvwuMTQTISCd
DK/VAb1DLO/vYyDZS1qP34gL+pNUv5Gioi2tyqrloIaVCPCG6qRdF3pYgyoI8/aq6ZF/yhGs0dFR
TjjhBCBcvHPPPZepTt2Zu+hiH7B1T56rrv8jYvuLvH/X3awbOB5HT7J9+TqOOHQF5x12dpdc/TfG
0KIs51x4FANDaZ56ZDs//v8fxXrTaSz8iw9jKMHxv9jEr//tC6xekeTsE1cwPFHiaz98HNdr7RB2
8fLEypUrGRh45ad/CgHp/gRmUseMFGhn+hP0DiVoE8igz+oLZcsjQhXmwYNkXjXUdGz021SKME6j
lcoIM+wrk0wWSCXzKDuJlZaYC4mJLFTRmytgVoiXXnk/baXJmaHDLqQkWDiISibqGUDAglQvB+aW
c0B2Wc0WFW3CE0H1rLKSHthvLiCppUlq6ZYfSFp9DJqLYnONKsOZ0iKj96BJSWDasTH0iGMppIa/
dEHTitVr+OvH9ll9yMFleIuW1yTP0wsT9GQFuhZGTKpDBKkMQjchHSW+dZInhGCwN1EjkFVBi5qI
j6CmmlrOZggyrcU2BAI9myNI5WqEU9cgXXGyQwIXjr04tZBeu4esmcEwJWZOokfrqhql+mUCTWgM
5tKR9ai+GxduqL5sVmqhqkqDWjIZRlw0yasGIorAoirAoqBSu2QE1bobK566JgR9dm8tahSLoCEQ
lchcYzRjwcKj8F59KOh6LAocPUrTZFgvlU3TEZrGgt6lDC05OO6nRMY9YEEGPZ3AsI26kZFDbcuI
2NjYoLc+X7NH4A6GUSiVS6MZgvSgyUE9B9bGS+oJTGkxtWghfiZO3jK5FFIXHTm+RCCrhNcww15x
leMXDw0RpDJ4i5ZjSos+YwGWTJDUMqS0bBh9lSL2rNuGxlBvgiU9Axy56hCGchopLUNCS9ak5DUZ
ph+GKoL175mgsqshhCCRtZAWCFuy8m3H4AyE6bC6MOgzhrAsh1QyjzSoCZ3EIuj7AbOqwbJtmx07
dtRumgcffBDTnMt22l38d8dz2yf4l5seJTO6jffvupsNvccwYQ8wOrCFVUct5PzDztor6d0uXplI
Z23O+sCR3HPHBp55chc3f/dBTj5jNcs/87c8+9UvsfqhHfxx5+d488f/H7YNL+D+p3byvf/YwCVn
rO6S81cILrjgAtauXctrXvOaSC8kuOqqq+bRqv0DM2FgJowmx7BdwXssMhWVis6makXh4XtU/F8J
IlSny07WUwaNfuiZ6GPcG4kPHxlTNzp/Hyf1RGs7Iy/1WDl6Ev1tJ6L39xMUCgTJXsjvAKC3stOv
SyN05ioIFenqro7fG0lnrIwpoKb6Vn2n5/BVsKV9AZesqnHEbIuwwMiamHbYnyz6mqZpyCo5ioyi
TJvE4gUURra2PXfC1LGEjS50BrImHiHBGuhN8sJkmGJYFgFeMok3tBjGNkZsjFis6ahcL4GzC81z
QAgOzR3Ks9oImqo/Q0IIDC0kM0f0r0IvjyPFE/GxIsTAECZpw8LUNXCaI2gtTGFxagFO4KL2hGJE
6Z4EqVwKL7IRduDCDEuVx/DwHpQClbIRjktGphkyFyK0eERmujQwc8EC2LITy9QYWGKyPNWHozRM
w6QMDX1zI89NwoQgACFIW2kKNDexDnrSFO0U7oKDsDJJRGEqRo36czYl3WRooYWpSTIDSbKsZHTH
riY7Fw8kGU+08q3j9+DSZUkKMsDvSeIvDwnrgdll9Nm9qOxyVKa+Pr5l4A70Y4zXGxhrjcIzlf9L
KXFyIRnJar30LR9iOFB4hgv+LkCR1nP0JLOM5SpKopH+k9Wo8uCrDgzVOYkvbnX7IJm2KFQioxmt
h6K2jZqeS/3LqQZDC1+TZkiwDEOwbCiNZWgEkXYQRIcgTMUM+pL4Axau79bu7bnGrAjWZz/7WS69
9FI2b97MmWeeyfj4OFdfffVc2dbFf3Osf2GUq299jMGJ7bxv5908n1nFjszBFFJjHP6mfs485O1d
57iLGgxT45QzVrNoaQ//+etn+PnNj3PUG5fz2r//3zzy1f/Fgud2s+kf/pazLvkou8ey/P7JHSwe
SPLONxw436Z3MQe48sorWbt27Su3B2M9hNHx7b1FgArHVAqla+AGYdRKVBwRLQOiLjxgiIY6Eb0a
eQidlIUDNo+1MEqTzf1vqtNRqtF+FXu/EUZvL/T2wo4NmKaLEAFG2giJYMWpXrWij2V6BQ+mAAAg
AElEQVTLMjz4vEk+XycL/b0pdCl4whki6HVIlQF3tHb+pUNpkpUpLupP4lIi7ZiUHCfm3gkhGEgM
4FmTuEHouK5c1Is5ZTO5p8BQb5LNuysHLzsQXoyLQohIkl89lqOiL9SOawUhBLaewV9aT4UazCVY
2J/ENCQV1QQgJIOBUi0JR3V8ywCEwNZNEoaF6wQ1AhiFqRv19Mt2N10bTtX6kPAnKSW2tKi2vx1a
0oNe6YNW9EpAWP+WqNbfKVVz0vszFmNaBi0Zd5R70+2fCtnXh57LwZadCGDBogSLchn67T6EEIyW
NMrlPbxIM9FV2QyMhcTkoNwBiEVl1MQk1ZZKQSaJymboTSuWDmWRnsJPphCarEUwhWmS0DXM2AaH
IHmgzeQmgdI0BiqbBr09CxhnR+Xk8TWMpuvVZqtJkII1g0dgVshDj52joFm1g/r0BexkO0ZEkVKL
9jkAAstElgJ6rCwjaR2KHinbYnBhhvHRIkyOgx+eWCKRRmtK4feFUfL+oTCa2qllimyXU1qZoRlp
A5FK+CwY0Cj21udQ5/mV+6rNBryywujbWHmCwWSrzZzZY1YEa3h4mFtuuYXnn38e3/c56KCDuhGs
LuYETz4/wldveYyF+R28d+ddbEsewHN9R+KYBQ4/rYfTDz1pvk3s4k8QQghedeRihhZluOO2J/nj
7zazY8sEJ1/2Dzxw29UsuW8DY1dfzXtPegvfTK/g1ns2sbAvxdGHDU4/eBd/0jBN85WtJNjse8ec
B1GJkLTbc3JWrMKdcBoGCdlNYaAPzfXAtsAtxup76ilZrQde3XcoYzuTBIcayJ4ElMbQNInvB+hS
MpVNMmlaZHDJBdXIUvNYuh49R9xDL/fkMAuFJlXB/mUSvyQYmfBYOpSlEIBuaGQGEiw7oBddqqYz
9dUkxQW+bRH4fpSLoMnQiQs8P1Q8TBoIp+a1xZC10hT1BJYK0OweDh1aiZdRGI6Pk5+sbQAGDap/
A1mbA3uypL1JdrxYQjfNignVlKfKj5ElqW0mRutfDlmF3V+KjS0Bw47u2kv6sjbF8v9l777j2yrv
xY9/ztSelrxH4uyQhCz2CLNAgTKSEAgFUkovtP21QLnQFm5bLg2UtMDtJL3poFxKgUIpLaOUPcMI
gQSyFxkkduJtSZY1z+8P2bLk7cRDDs/79YLYks7RV8c60vM9z/N8nziOjDWO2hNjkHArXqyEAAmz
rmC3JIkrSRRT17+T0v3iZT3qvO6RJumE6WgIdy4KIVst2IzWdHLVmer3w94tJPPcSOFWJMCsyRw7
tZCPt20HwGWT8Ni7b1i3r7GlKjKyLOExuWiJpS4gpBa8TQXmMbsJm2PpBKunRFeWJMz5BRjePFRT
FClYh+GwgQRj3aXkuSw01LUgKTKmMZXEqwM0WxT8isI4fwWNVGXtL1nso7WpJbV2lWFgGAaVrgp2
srbb58/SKUQ9o2emc/wO1cWEinw2Nu2C+lSvmazKWedC0u1Arm4CJOzmVI/5mKKO3uFSv42avRBx
O/FrPlRVYUqxk6ZwjD3hjp4xw2ylPN9OgceaPs49sc6YRTy2GbVmX9btMasFImFKJ4/teE0SOCwJ
Wru5cOPWfCQkE1bZQb7HyoH9XZ9Lt0pDOgLqkPb8s5/9DE3TmDBhApMnTxbJlTAo1n1axy+f+Jji
UDWXVb9CrV7ARv9xxNUoR37RxzmTTxnpEIUc5y90sPArcxg7wce+3Y088aePGHv8NexYPI8mu0zi
tVe5uupZShJN/O6Z9eyqDvS9UyGnHX/88dx9992sXLmSVatWpf87XBjdZVidf+6UXVky1ycymUHT
ujQTJUnCUFTiZjNIUGYZS2e9dUiosopZsYKiIGkaEgZ2s4bDoqMoMuHiAuI2K9GyooyCAV1bWG57
dtGIdlZzKjapJD9r3ko7XVEo9FhxOzu218wamt7blXDaenmyG54uswu5vfCB0tG/lHlYXWYXDlP2
vBtZkimxF2FSMoomSFJ6TSN/p7WMZFnC57JgNWlUFDhSVQMl0L1yxry67iedSbKMdfIUopXZFTLb
G63FjiImFI1Jvy5JklFlCUdbSfl2zrY1uFJrh6WKM2hqamioZLGlerRMXQssZM9Jyvy5+4IISWdG
hUDDwKY42haAlvCO0Sj0Zr/GKafMZMwZx3V53nbmPD/x6RMwvK6O97vRY0HuLuxmlQKPFUVOLXpr
UjvarUkje15uf9re7T2mkqqi5uXjmDMX2pLD9gSnIzeWUDQZ2SJhz1cZO7br/Een7sLrNKGpEi6b
CaNt3af255EL8/F7ilI9flr2HKLuqob2Rtfl9ILZTovcUS2yY4dtLzL1GqwmNStpsJhUCrwW/GPz
KRibGjngsOrpJDbrubTMIafZ91m1jveZpKkY5q5VCm0+J46jZmMt6ljLzWlygEnDrnbMV2xfUiLf
7sKj5aHJWtbiyVmkrkM7B9Mh9WCVlZXx/e9/nyOPPBKzueMAXXjhhd0+PplMcvvtt7N582Z0XWfp
0qVUVFSk7//Tn/7E448/jtebmqT33//931RWVh5KiMIo8/H2On795CeUt1SxsPoVavV81hadgiEb
HHdeOXMnT+57J4IAmMwaZ118BJ+s3su7r27n30+uZ+rM4znwtQL2PP1Ppm/bz5flp1npmsav/grf
u/IYfO7uy88KuW/Dhg0ArF+/Pn2bJEn83//930iFNLi6q5jV+f5OLRe1jyTDZ80jKUnYLE2EwnHM
JgU53mmb1ArFGb/23ohT8wuQ5HVk7qXIVMFEl5emvCoSLS3QuSHXab9GxhPaTCpjJ9uQDRmpqbeK
agNrXHocJhoIY1asqTlYug2TomMdPx5TPITJ74eNzW2xtcVlSOkkqotODTVZ09AUGXeBNyv5sytO
bBawqGaindeOMkno7RUODbCaI0RjXeeHqC4X6GGIt3S5z6pa8FvzyDcVosdbkTvNper4ORVvqc/G
7qgboyVGorgCCYgXVyAHm0i6PF323/nvbyouJrJvHw6rhsOqdXmbGqaOz9SkYbTVplCQ5VRyonTK
YpRuGueZzKqZib6JqJLCpurUGEwj2f8ECzLnFmW/lkSnRE2WZUyqgsWk4vJaCUtdj7duUrFYNcIt
MWxtFzRK7MXp+zPfy948K94SJ1uCjZg0udvXWmgpIGyOUuqwoigSRjL7iBqShMPqxmeRkNva3fke
K+FYNDXnjT6KN/UwDc7rNHf9iLGYSDqsmApKIVbb4+5UJXunRtuo466zplLMukqpz05yTCFJmwl/
MLtnuj3pa80o3++w6JQWZhds8VnyKK6spMXkZMtnjUDHNZgCj5Vd3UacEarUdfHxwXRQCdb+/fsp
KCjA40mdfGvXZndd9pRgvfTSS0SjUR577DHWrFnD3XffzfLly9P3r1u3jmXLljFt2rSDCUsY5dZs
q+X+v39CRUsVC6peocZUyMdFp4IsccoF4zhiUkXfOxGEDJIkMWNuKSUVbl7650Y2rKnCvTsP9zmX
88+P/8ZpqwKc0PAxU4Kf8sjvarjqmxf0fLVLyGkPPfTQSIcwLLLX/sxISrpJsMry7TTVhMhzWgjS
dWhOib2Iz8IHKJtgJRE30DSZREazSzfJWA0Nwtm79mkFGIbRbVNOMZtJlBWi7OoY+mSWLZhVEyGP
B9WT3WhPN9h7qBIIYLWrTPNOZc+2xk4Ho5v99ELqpnEpa2YqXRUETQYNQQO7z4miuLP2Z9ZlQq1g
s6gQzF401lwxhmQ8hqRmN9QkVSFRVkDCr6G39WRpJoVTp8zApMvIkpyxfmz7i+94TqtmxdANVJtG
1KxgsQ3sc0nTdZSWBBZzRzNPQsJijpNIdgx5lJBA1ZAqx4PFlDpIqkrSnT0vRXW5QFF7HIKa57JQ
rap0riOdnseVsaHNrCK1vUZJitCXzo1gp+4gmoh2dDF1el/3N9VO9z617adzgiVJEgUeBSQTSV1N
zw8rtBWgRGUS0VSSVjrGi2EY6fdMZq9Yaj9tYRqphv/uSMfrmeSdQDgeZkd96mzSZZ0ie0F6nlK3
85XaX2DbfSZVxaCHBXo7B9HtjlI/SrKCRTWn/4aKrFJQPgldNUNjR4KVeW6UO0txe20EGzv+jqme
WwmbRSMY7rzIckppvp3qZBFJQAqF07dLAIpCdPJMwuFNUJcZYsfzqk4n8eZm1G56WbsjIWUluwB2
3YFVHbqLqgeVYF133XX8/e9/5yc/+Ql//OMfufrqq/u13erVqznppJMAmDlzJuvWZVehWb9+PStW
rKCmpoZTTjmFa6+99mDCE0ahj7bWcP/f1zExtJsvVb/OfmsZ6wpORlJkzpk/jTGVYo6McPDy/Hbm
XzWb9177lI8/+Izmf0tMnXM5j57/d2avbWDm5iBnbX6W9368heNuvA57/tBMehWGzgcffMAf/vAH
WlpaMAyDZDLJvn37eOWVV0Y6tEGRbmdltIkyGwydK3NBqqFTUZhaUya/2MnO1kTbVe62XbWV2lZ0
CUXvOim8pMTK8cWlrH+9IavRKkupIW29XStPlBYQLVOgJdXw63v4UvdDzNopGWXBPXk2HC4T6z7r
Y5dZpO5GGGJYrFhKK2DLZuyWVPntTpvhsOpYTAoxu4W6YDj7bk1F0dTuL9XLbUPuJAlPcceitL3N
+0g30hUdh6WYxmSEqM/W59wnwzDa5uGlfncX2DCsKpqmpv9OEmCztjXE29rDHp+VJslIzb+h53kj
mt/fHiAASYu97fV3Tvw6EkWXVaYlcx2ngXUypumKxgTPeCxqR09HMjV2rtvHm8oroH5Pt/c5nB2L
B7e/J9vXk+s8RBBJwmJKLSCcOZtPVzQURSNMLP336j3B7/k+h27HodvZ0V7EopOuvVxSl2Qp3+qj
MdIMtNIT2WZD8+fjGFsCGxvadpXaT8JXhF7iICZLlDiK2SGZiBkRNFnFonafwHj9NpobW3HZLV3O
7TyXmQmlbpoPhNjWNherv/XI+vs48/gJJMNhVKcTKZB6M7cXcel8KprHjMVnChBCpqHtT2zRLEz0
VmZ9rgy2gxp8mJlRP/300/3eLhgMYrdnrD6uKMTjHVn3ueeey+23386DDz7I6tWrefXVVw8mPGGU
Wb05lVxNC27j/H2v8alrKusLTkVRFc5fOFMkV8KgUFWFE84Yz3mLZmC2aOxaFWJu9QVsnD2OR852
U+O1UVGzlV0/+D41zz1LMiYWIx5N/uu//oszzjiDRCLB5ZdfTkVFBWecccZIhzWIurbgO1/d7m3Z
zHy3JZVsSRLJ0gIS48uyKpC169K+kbKvHMdKKzE0nXhBR7VGZ1v1NrMpo7GiyKmFldp300PLqchW
gEk1kddtWfaMMDJ70ArsmMydhs4NcC6FZumYY6V5PGj+fBR792saSVJqWFNPhQ6gu2Pf1vCWO/7t
6Ri0r/FVWeRkYpm728f0V/szxBMGkiyl56Sk7utmTpckpZOrrB10Uljiwum2oLXNpYkXlWGuGJNO
vLp7bYWTy9KNb5s5s+ACXd5XfXGZHOhZwzONHlvjMcWMuaIC6+QpXe5T1Y5eKkmSKK/Mo2hc6iJE
58R30IoUp3uw+l7Y1gDyi1Lx+PLtWNpGVHRcYJEzAkvdqMoqPouXSd4J+Ho4jyRJwlJZiSmjBzn1
vlZAN6G3D8czUgmSxaR2O5dKbfv75/ntjJ3gI7/ImT30tO1fp03v1xy2gch8HlnTUJ2pghsum05x
no1in63b7WSLBam4gGRPK2kPkYPqwcoaK92PN0w7u91OKNRR7jWZTKK2L/hlGFx11VU4HKkDMG/e
PDZs2MCpp556MCEKo8R7G/bzu6c3MDe8lnlVn7C+4AQO2Mdjtqucv3AmvoI+FvEThAEqG+vlkq/O
5bV/bWbn1jrKao8hOGEXj5y5liO3WTh6TRMNTz5O4PVX8c9fiP2oo8VyAKOA2Wxm/vz57N27F6fT
ydKlS7n44otHOqxB010PVjJjSJNh9NzgzCRJgKpi2Hpa+6XTsKFO86UMu5OYfWrWbeNKXDQFo3id
JmLRRNb2Wc/bDbNqosRehCar6Q16SgScbgsm88CaLe29fJltPYfTjFNVaTzQcaull/neUqd/+0NC
wuxU0G29bNXpoJh1FadZp7km1Fv+kGbQ89yj9nWAzCaFmCxjGEnsM2cS3wfq1o7ZKZ2fo6ekx+Ey
42jrjZKQMDQdvbCgh89GCcnhxFxaBhtT5duK/Taa2+ZMSen/dSiv9JJI9L89aVJMOCxubNauxyAS
TaBXFnWzFRg2KzQ2odhSjXGTWWWCNpbq0AGKbAVZj5VtHe2Pg/4K6LIEAYxxlffYixmPJ3H67Tg7
zQd26DYCkSB23Ua6+7HT4XLodmyalbpI9jp1PZEgNRcKI92LhwRWXQFJp6XtNlWVcZdqREJJvP5u
kphejk1Zvj298HVnU/ImEY63olr3IKXLs3fdWbEjn0AshEPrvj0oyxLlBQ5q9qsYbZ+JBgaxMZOQ
g82pRKznzr0hc8j55UAaHrNnz+aNN94AYM2aNUycODF9XzAY5LzzziMUCmEYBu+9956Yi3WYe+vj
Klb88xPmRV5lbs0e3i3/Egfs48krsHHJkqNEciUMGYtV5+yLpzHv7IkYSQPTulLmVJ3N+rFWHrzQ
wwfFJUTr66lasZw9P/kx4a1bRzpkoQ8mk4nGxkbGjh3L2rVrkSSJlpauk9JHu8xv3LjRad5Fv8fh
9HxX54afYrejer1E/T33MKmKTJ7LjNRLmXi5x9g6bk/k5ZO02LBMnNTtIwuKnbi9mZXT+s7gkom2
YWDFhVjGjs3arCzfwazx/Rgh0Z/D2qmxK0syNp/Saxsp3rbIq5GxfXsPgd2h4eqhsmK7ZOYF7k4J
+PgSNz6nhdJ8B2X2YsodpUi6CaypJElTAUlO90gBOK16v95Csyf6OXKcL3uB6bafZaUjwc68X1Pk
jtcgd309JrOGdQDzzCRJYoxnDHa9azuht2OeLPaTGFuCVtiRgOmKTrmzFLVTOX1Z63QRop+nV9lY
b/fbtf2NfJY8vObsuYjtFSd7YlEt5Ft96KopvZ/u0tHxUwrwVKj9a5tLqQ4xJasyZIa2J9B0BUWX
sHqUrkNo6f14p8r+d38xx6ZZ8Vm82Gccie2IaW37yn6My6ph06wUWP0DGs7nsOqYnA5Kjxjf9rqy
d9zP61GH5KB6sLZu3crpp58OpApetP/cPsnv5Zdf7na7M888k7fffptLL70UwzC46667ePrpp2lp
aWHRokXceOONXHnllei6znHHHce8efMO8mUJue7VDz/j0VdWc0HLq+itY/igdB6GpDBjbinHnDIW
VR26cbGCAKkvhakziykd4+HV5zazb3cjR9SdTnX5Rt6et4O11WM4e6NByY4d7Fl2J/ZZc/BdPB+9
qLjvnQvDbsmSJdx444386le/YsGCBTz99NOH50W6jEaBSe5okKYm6A+sxdDfC6RaXh42mgjQd0GC
rP0DHj0Pol0bM7a25MHm0Ak0tWIyq+hmM9GKCajO/g3lsek2WklV5Otx+F3bv6rVgqzrJGIZCQB9
l2numFFE/1pk7dX0e3hoT7cbtlSPhaYpVE7yk2xsIBzo6xp4x5N1lMlIPYHVrDK+NFV1TclIHMqc
pbSY9+HSZOQSF2a7DgdSy1SUFzhojfZRLIHUWlKdh49JEngqNIzPWumm2F5WzFbVCrTgMbuA/vW2
dEf1eNALCtB8qSRZLy7GiMV6/TOZNSuK1T7wUQkDeLzZoiHLEslkqk3c/jfprX9ucrmHqroWivK6
H+aWjkGS6egQ6thj+7IEqqIwzT8FVeq7eZ81dyorQW+Pt389ilnHcpCTFrtFS73uAYyWAzBpCjPH
+zrCyohRcYCqS+mLGUPloBKsf//73wf1ZLIsc8cdd2TdNm7cuPTPF154YY8VCIXDg2EYPPfuTl5c
+zznNjRQazmTiNWGxapy+vlTu175EYQh5nRb+NJlR7L+o3288+p2fFsn48svZ1PxOzw+L87E3Udz
QXUdwY9WE1zzIa6TTsZ7/oVonq4ljIWRc84553D22WcjSRJPPvkkO3fuZPJhuKyDlNEoyrf62RNI
LYSqKjIoCqosI6ndf7V78qyEAhHsvlTDovNV3TGucnY3dx3OkypFraDY9T6rQGea5J5AddKgORrO
6sHSTSrF5ak5H/lFTpxuCxarxkynmUQy2W3jt7vbxrsrqbU3YFZMXe5rZxhtPStyRrdJVlutP8Mq
+35Md43R8Z5+LjNT6GuLJPU8iiJ3GfyX7KZ92f6c/Z7LJEkUWP00m1O9kU6vlWhGwqnIEg6rjt9t
oaYx3NNeety3rEgdjfbOQw8zjqHb5GRc/hjMiokAnw7seTrt0zxmbPp3xWrt8bFWm05LKMr0gikD
Tq46j84tK3ewr6Gm121Kx3iorwnh8loINKfGp/U2pUbXlHRBmj7jad9N5vIJGQfc0ktlvMyXnnlx
oa/0pcxZ2mOSk3sj6HuaEZmi2MFXbhnyof8HlWCVlJT0/SBB6CSZNPjff79DYMdaZjSUsNcxBYkk
s44pZc4JY9A6L+kuCMNEkiSmzS6hvNLb1psFRzSezp7iDWwds5P/KfRy7XGLsL36Jk1vvE7zOytx
n/EFvOd8EcXayxVHYVi8+uqrjB8/nrKyMl566SWeeOIJpkyZwsSJE1MN6wEIBALcfPPNBINBYrEY
3/ve95g1a1bWY5YuXcqHH36IrW0ex/3335+ePzzUZAXaW9+SJDHeU0ltuJ5SZx5WOYZjzInU14ZT
le060U0qlZP81Fd/lt6+nabo+Cx57KGj4dh+b8W4PGqqd1PT1HfvRiabboO2os+ZbZmi0o71bGRZ
Sg8NkySQOw0Dmpw3kdZ4pPs5KxLZZZbb2lXp4VayjGGAYdKQFbnbnoSBtLGsbYsAu2wS/SmB4za5
utzWbTLksNO1UZj9e7Kbxm16fpkkd1TK6HXKV/9erM81sATLMWcuSqwFmrbjcZiw6AqFlc4ujyv2
2Ug0yJh0pcfqdIfC67NRXxvCV9D1XCyp8GSVUj8U/dmHyaxR1FawJGspgkNk0PFekCWwO0wEA5GD
6jlS1G4uZCBBRSmJSADdpEAErHYT3l6+5wYzUeluVxKDceikYc8ERYtWGHKGYbDns3r++q+3sdWb
cDCFqJKgwhPhxEUnd5nMKQgjpb03a8Oafbzz6g6Kd07FVzuG3RWrud94lZlfmsWFodMIPPssDf96
lqbXX8N77nm4TzsduUu5YmE4/OEPf+C5555j2bJlbNq0if/8z//ktttuY9u2bSxbtozbbrttQPt7
4IEHOPbYY1myZAk7duzgpptu4u9//3vWY9avX8/vf/97vN7h63HXZA2SYDfbmFaan77dbXKlG/L5
7tRXeok9dSW/eaC9EJ11ao9o8sB6sDI3z6oA1kfJ8Ux2zYZd699FjAKvlVgiSUlbNTFJltGOLCe8
P4knz4LRvsBPRrLSZ+9PRtwmk0K5X0aWJfZ3flwfLUBNUYglEt2WWzfkVBOyt/ZfspsurPYJ/XLb
EMGeWKdkFyWxTJyUUV484462n/U+FvvtTFJVZFLzbDRFprzAht7NvBuLrmI3y93O4xkMefl28vJ7
nrt90IlA53W2DnJ0YXd/w/7yThpLoLYZWVUxm1MXIRxWOf2a+l4GoavMCpNZ54TXgxGRsagK5XZv
eh23nmSPEBzkJGaQdtd5Dmh/hz8eCpFgCUMi0hpn/74mdm+vZ/PmKqLBJDacaPEwRdHNHLvgZPJm
dC2hKggjTZIkjphVQnllHm+/vI1Pt9Qyfv1J1OXt5eOKT9igaSz85gImbWig4V/PUvv4YzS+/CJ5
F1yM87jjuy19LQydf/zjHzz22GNYLBbuueceTjvtNBYuXIhhGHzxi18c8P6WLFmCrqeS5UQigcmU
PfwsmUyya9cufvjDH1JbW8uCBQtYsGBBn/v1eKyHNLc0z+3AkTBRMdZPnq9/BYBsFp1wIIbJouL3
d1zVd7SkLmr5/Q4MayHR2haKHQX4vQ6c+4M0WYJ4LaVMrfCntzsz7wR27G2iqiZ7gk3mfiF1Qc3p
NWNyKJQUuwnEknicJkqKXDRWh9LbDMY825iepNFpzoqjuCi716giUUhzUZBJRcVU7QgSCcdxus00
K6lj4Muz4zR33/s466hyArtVpAOpRNXitREOp7aTCl20BDv6sTxeG36/A7OupRddzTw2Z3htRGMJ
rJnl5dtiD7ityHICl9ma3qbVCBNwmrHU6ZgdFnx5djzOjl4fp6MJS7IUn6+VSm85DXsjBBpbcbks
Xf4m9PJ7LJ7AWZXKPP0+R7qhnpBl7BYdv6d/F0BbYiqOqAWsOk6nGYvHit3vwOlIFfLw+S2EYiEc
pR5808o6yux3+vvlHKeZcaYk9S4bqjeOokkUOJwE7TKF3m6OdTeCjRFCagSrXT/o15mMFVFjtYME
pVYXTiOMyWbCdUQh+/Y0UVji6leFzVg8idOR+psWFbkINqTeq063GRkZk0Ul7omRbIli0y2UFvV9
EcluMxEOpBYU9vntqKpC0BwhUJ9duq8/rz2RNHDuTc0JtKCTn2+DMEyYVJCet9kTV6uFRDKJx2nD
78l+LlfMSpQ4jS2pON1u65C/50SCJRyyWDROQ10L9TUh9u9rpnpvM/U1ocxH4AvtpbhpG6VTCim9
6itiWJWQ8xwuM2dfPI09n9bz1otboa4EV0Mh1WUbeTj5D0ryilnwvW/gWbmBxpdfZP8Dv6fhhefx
zV+AbfqRorT7MJEkCYsl1WB47733WLx4cfr2vjz++OM8+OCDWbfdddddzJgxg5qaGm6++WZuvfXW
rPtbWlr48pe/zFe+8hUSiQRXXnkl06ZN63O+V0PDoVU0bA6EcTosNDWGux0u1hOHx4TJolFTE0jf
FmhOJQw1NQEkdMq0CixxMzU1AZqbwrSEY6iyRiIkZW3X3NhKcyC7Vyzz/naaOxif6ToAACAASURB
VEmSJDU1ASZXeKmpCVBbG0xvW1MTGJQEKxEIEWqb30I3cQAUSMU4tVbCzUmamsJEI3ESJAloqVjq
tBCRboqc+f0OgqEIsViM1rbnaLWGiLb97JhswubS2b4pNaRSNcnIqkQoEMl6nZ2FAh2Nzua2fTUF
VFqkGFJEo0ZObROrDxFubsXn1AnrMrHWKDWRWMe2bc9RopYRaTZobGhBkWSam8PdPm9PYvFkR7y1
gfSVfrsmQzze7321xiMEmsOo4SjNSiutlhbCNYH0voMl+cTzy9DcbpoDrdB2HJqbW3E6zQOKeTi1
/42cHo2qcCuEoT4Wwu9WaW5u7VfcTU0ttISiRGL9P56dJUnSGo3hL7DTcKCGWDhCNGFgNIWxOvXU
ce7Hrt0eW/pv0hxoxeLQMFs0ag8ECQUi6FGVmBom0BomrkGN2vdODcNI77OuLogsy7SEov36rOhO
nk3j0+pmwuEokXiUabNKqKsN0RLufWBuc3MryWSChkQIezz7ucwxG4FgDeG2fTQ2tqTPtUPVU6Im
Eiyh39oTqYbaFuprQzTUhqivbSHQlH2VQlEllLwYwfinHLFrFxW1VSTNOiWXX4Hz2ONFw1MYVVLr
Zh3Fx6s+44O3d1K6axp5+8ZxYMx6fhF8iKkTJvGlY29Ce2klzSvfYt8vf45l4iR88xdiGTd+pMM/
7CmKQnNzMy0tLWzcuJETTjgBgL1796bXWezJwoULWbhwYZfbN2/ezHe+8x1uueUWjj766Kz7LBYL
V155ZTqpO/bYY9m0aVPOFtSw9nHVF8Cq9dBL0emjWm0bOmZSFSLxAYwVHCKS2YxsMqHl+Xp8jCIr
bWsHZQxl6qE4QJ/P11a2Wza3l6Tvuq3VrmOx6ri9fff82KZNJxEKgdIAsVjWfXLb+8vmy8PfzZyi
SWUeMkeFyYoESQY8/G6wCsD1XIa/bd+yjJbXtdS/ddJknF4r3dRWyV0DHF3WXphmIOvGdqZqChXj
UscvfKDbai390nkoYW9TNPobb9bw30EYwVHgtfJpdXPH/gdhnGC+1Y/ssdHcuJFQItClJP9QEAmW
0EUymaS+JsSBqgD1NaFUUlXXQijQtUSvxaZRUuHG67NhdstsSW5gY9WbHP1JE5N2RVKlY+cez4Qv
L0axi3WthNFJUWRmHVvOhKn5vP/mTjZ/Uk3F1rmELAF2Vazn7voHOOaYOZx18n8Sf+5FQmvXsOcn
S7HPnoPvIlHafSj9x3/8BxdeeCHxeJwFCxaQn5/Pc889x//8z//wzW9+c8D727ZtG9dffz0///nP
u02adu7cyQ033MBTTz1FMpnkww8/5KKLLhqMl9Ivg3F9amreJGSp+x4kh02nilQS1bkxVuCxYBgG
DovGxt0NBx/AIE1/kDUN+8xZfT8wrbtJ/f3fRM8vAMNIlwXP1N4WlSSJ0jH9qzCq2GypBW/rGrrE
olht2I88MrV2VTc8juzbfQV2kjHoR3XuIdHRCB5Y4191uzHlOXrsgcwdB3/itSefh5JgZdILC4jV
HMBUMWbA2yqyxJQKb9baV6ngUv8c7OdLe2n6oTBYF+UVWSFPK0CWFModpYOyz96IBEsgmUxSvbeZ
3dvrqf6siZr9AeKx7MtJNoeJ0jEePHlW3HlWvD4bXr8Ns0VjX7Ca1z57i0+2v8/cT5q5fHsrsgEB
dwGV13wVz+SJPTyzIIwudqeZ086dzMxjynjz5W3s+xTGbjqWkLWJ9Y1bWOX+iOPOPIrTTz2RyNPP
E/xwNcE1H+E68STyvnQhqluUdh9sZ599NrNmzaKhoSGdENlsNpYuXcoxxxwz4P3de++9RKNR7rzz
TgDsdjvLly/ngQceoLy8nNNPP50LLriASy65BE3TuOCCC5gwYcKgvqZeDUJjw6r1XM56bJGDQHUA
m1nrcjVaVWRK/XaShoHVpPW4gGhfhn56efc8eVaq9zbh8ljY1zaKfUCNN0nCVFLa6SbpkBvOmSU3
Msnm/heAUlUFf5FjwEPQsnqwDuW91Z5EWFKJn2zqu+d0NDGrOibVhN+SB7G+H5/J6UmVavf6Bmdq
hGK14Tzm2IPe3tXHgs4H02M0FEvs+LUiCmwDvzDfUwELVZFQJJWxrnJ05eA+uwZCJFifU7Fogk+3
1rJzay17Pq0nGkkN95Ak8PhsFBQ7yS924Mu34/Zau1SRSSQTrK/bxGub3mZ31RbmbAhxxZYwahIa
zS6s51zA7HNOERP+hcOS12fjgkVHUr23iVde2Ar7wbblKFrNQdbXbuO9vNUce9EcTm2eR+s/n0uV
dn/3HTxnfAHP2eeIOYiDrKCggIKCgvTvh7JI/fLly7u9/Stf+Ur652uuuYZrrrnmoJ/jUAz1AGtF
ltOJU09XpGVJYsa4rsO92hXbi0gYXYcQen02As2t6aGGw83hMmN3mlLl7fVKmiLNva4ZBKA4M4pm
9JqAHEqSNVIp5+BVfVPayuhLY8owK0Vo/vzeNxhlZENiui9VjTHe2Digba02nfFT8nN6ekTHRQIp
/TYfjkp7PZk1wY+EH30QFwN2WHWmjvFiGaYlgUSC9TmSTBrs293I5nXV7Nhck+6lcrjMTDiigPJK
L8Vl7l5LclaHDvBu1Qe8X70ao76R2RtbOH1HK1rCoFmz0XrimRy96Fx009BfHRCEkVZY4mLxV+ay
v6qZl17YilEFpTtmEt8VZetne/ggfz2zL5vJyVUnEH3+Jeqfe4bG118l79zzcZ16mijtLgzYcLTR
2ntlBlJOPVOxvbDb2/sqoz0c2hu5meXte6NY+teLNBijv3K5Ad4XWZKZ5puKLMnD0jsw3A412Rgt
f1tJ6lhn61CS7/aLKAfbw2saxMQqk9M6fN+5IsH6HIi0xtn0cRWfrN6bLkjhcJmZeEQB46fm48mz
9nryh+OtfHhgLe/s+4BPm3ZSUBfnuM2tTNgdRjagWbNTM+dE5lz6JRzOnoeeCMLhqqDIyeVXzaG2
NsQLL2yhfk8T/qpx+Koq2buzluV52xi7aCanfaZhvPIWNX99lIaXXsRz9jm4TjjpsBtOIwyhYWio
FZe7CYeimA9yCODniSQdenKltk2cUuWhaVT2ahDfTmb1MP4cy/gbKw4HSoMZs7do5OIZQsm23mel
h3ma/aGbVMrGepEk2L2jfrBCG1VEgnUYa6xv4ZMP9rJ5XTWxaAJVlZk8o5BJ0wspKnX1mlQljSTb
Gj/lnapVfHTgE4hEmLQrwmWbY+S3JWn1Zg/xY09j5kVnYLeJxYIFweezsXjxLFojcV5+dRtb1+3H
0eTH0eQnsjPOY95qnGcdzan1rUjvfEjNX/5M3T/+jvvU03Cfegaqq+8r6oIw1Kw2HWsf8zSEwTPW
VU5VaD/F9uFvsPdV/U9ok7kQr6LgPfqonC0tfzAyLxIk2hawVg5xiofZohGNxA9pHwMxzjWGnc27
ybd0LUIzEkSCdZgxDIO9uxr4eNVedm2vA1IFKmYfV87UmcV9XpE80FLLqv0f8e6+D6hvraegLs4J
2+NM3tmCKZEgiURt0QSc807lqFOPHbIV2QVhNDObVM49ezLGWZP4cF0177+7m0R9EE9tKdTCK0qU
+Cwv0wlTtn419c88Tf2/nsMxew6uk0/BMnnKqBlSIgwv8bbILbpJpTUcQ9UO/rtQV3QqnGWDGFXu
qChwEIuPphrsPRm5+UjDSZLApJgIEepzfmKucZmcHOmfNtJhpIkE6zARiyXYsm4/n6z+jIba1IKW
BSVOZswtZexEX6+JUCAS5I3dq3m/6kNq41V4G+NM+jTCxJ0x3G2LskXMdiLHHMf4889isr/nNUcE
QeggSRJzphcxZ3oRTaEIr7y5ky1b9mCJSJjrC9gKbC4pxa4HqGzcTfTDjwmseh8tvwDXyfNwHn8i
qtM50i9DyCEi8R5B3Rz7ojIXgaZWXB4xPL47RXmHSUGfwzy/0k0K4RbQdJVCRwlW1ZKqmHiIBqky
/agkEqxRLtDUyroP97JxbRWR1jiyLDFhaj7T55ZSUJzdMEsaBvVNreyra2F3bT2bGzezL76NiL6P
goYY4/dG+MLuBP629a6SqoY0Yy6F807EPm06kjIC48MF4TDhspm46OxJGGdNZPveJl5ftY19VXtw
tJpoafWwzuzBGDsDuxSioGEHeU+/jOPJJ7BNPQLn0cdimzW73xPuhcOXyK9yi6oqeA6XJELoxeGd
KeTl29FNKg6XGUWWKbQNThVI3aSgKDIuz+fvu0skWKNQIpFk9/Y6Nn1cza7tdRgGmK0ac46v4IhZ
xZisGvvrW/hg0wH21YWoqmuhqjZEdXMDCUc1irsaj3yAstooJ++PUlEVx9pWph1FwXrkLFzHHIvt
yJli8r0gDDJJkhhf6mZ86Vziidl88ukBXvrkA4KNtdhbHBD0EHLPYId7BloyQt6BPfj++iLehx/B
M20yjqOOxnrEdJFsCYIgDDFJVTHicST18C74oigybu/g98JKkkTlpNyYEzXcRII1ShiGwYGqANs2
HGDL+v20hlMr3bnyrDjLnIR1lfdqAvz9kY+oaQinymyqEWRHIyZrLQVqNTMtTRTvi1GyJoYj3DEm
WnG5sB11JLYZR2KbOnVAixsKgnDwVEVm1vhCZo0/j1g8wVvbNvHWzlW0NDfiaM7D3uQnJo+n2jke
APuBejxPrML752coKnHgPnIGthlHohd2XxZbOByJLqzhZp06FaM1IoZnfg7Zpk0nEQyi2Ed2eQFh
9BEJVg6LRePs3d3Izq117NxaS7ilbflwRaLFqrK7NU6gLoAUqEEyhZH0EC5TgMmFDXiD9eQ1hPFX
xfA2J5Azerclux3bnMlYJk3GOnESenGJWBBYEEaYpiqcOvkITp18BK3xKC9vW8W7VR/QEgzgaPJj
a8rDwEvQ5GUPR/BxPInz7VrcL/0Vr95K0Vg/nikTsUyahJYn5kkeblweC6qioOnis3q4qQ4nOEY6
CmEkyCaTGMkjHBSRYOUIwzBobmylel8D23fUUv1ZM5GmjvKWMTlOsy1As72RoL0GVzxEeWsrnmAI
TyCO50Acb3MCczR7nHBSU5ErynCOnYClYizmynHoRUXiSpwg5DCzqnPu5BM4d/IJ1IbreL9qDe/v
W8uu1g+wBj3YmvJw1ueTlPw0WfLZBXxUA9a9tbie2YA1EUA1yyheJ1JBMWpBIXOOnYLJIhoKo1V+
kRO/33FYlYYWBEE4XIkEa4h8sH8NNS21JIwECSNJPJkgEUkSbzWIhJJEggliIYgHZaSQjha2ICc7
ikgkpQRRaxOGWouJfXhaaxhfH8ezM447kEDpVPXUkCQSeS6MgnzsJRU4KsZhrqhAyy8QvVOCMIr5
LHl8sfJ0vlh5OnXhBtbWrmPNgXXsaFoJCRlL0I2t2Y27IQ9w0qJP6Ng4DJYtzdjXraHpX6/h0BM4
HToOh47VZUN1OVFsNiTdhGxOXamVdBOypoEsI1us6PmDM9lZEITcNGuCn0Ti8C7iIAjDbVgTrGQy
ye23387mzZvRdZ2lS5dSUVGRvv+VV17hN7/5DaqqMn/+fC655JLhDC+toS7Evt2NGEaqZ8lItv1r
gIGBkUz9jGGQNCARTxCPJYnHk8RjCSLROBtqtiEnFJS41vafGanT2Hm17T+DBLIUQE82Y4804W+p
Jr+5Gi2R6BKbZDahlZdhKirGVFSMVliEqagIzZ+PpIp8WRAOZ3kWD6eVncRpZScRSUTZ0bSTrQ07
2N70KXuCawnHWjGF7VhCbuwBO/aAnbDkIqyPoaZ9JwbQDEpjDEssgClehZZoRU+E0dv+1RJRFCOO
koxRdMUVOKdMRtVkJElCkiRkmfTPUsbPgiCMPiZNgcO7hoMgDLthbZG/9NJLRKNRHnvsMdasWcPd
d9/N8uXLAYjFYvzkJz/hiSeewGKxcNlll3Haaafh8w3tXIKmUDRdFMJoK9j/0SvbaagOHtJ+7fjA
SKIkY6jJCGoygJaIoMfD2OJBzLEg5ngIc9vPckYJUMXhRKuoQPPlo/l8aH4/ms+PXlSE4nKLhowg
CJgUnSneiUzxTgRSF4HqWhv4LLiPAy011IXrqWtt4EDLTpoDLaghK3rEiqnVijlsQW+1ElccBA1v
70/0WiO89m6/YpIkkGSJo04cw+zjKvreQBAEQRAOQ8OaYK1evZqTTjoJgJkzZ7Ju3br0fdu3b6e8
vByXywXAnDlzWLVqFeecc86QxWMYBnf8aRUNbes+tdMBO9mrHowP7mFG89a2PigDyTCQMAADxUgg
J+MoRhw5mUj9aySQjQQSkFA1DE1H1k0oZjO6z4Hu9qE4K1FdLhSnE9XlQnV70Hx+ZLN5yF6zIAiH
J0mS8Fm8+CxdEybDMGhNtBKKtaT/C8fDxJJxIpEYkZYEkdY40ZYESkyn3FKe6pWPJYhF23voExhJ
0hejDIO23vzsn5H4XK55IgiCIAjthjXBCgaD2DNKXSqKQjweR1VVgsEgDkdHmR6bzUYw2Hcvkt9/
aKV9/u/2sw9pe0EQhNHB2fdDhCFzqN9Vg72f4TQaY4bRGfdojBlGZ9yjMWYYnXGPxpiHtfqB3W4n
FAqlf08mk6ht84Y63xcKhbISLkEQBEEQBEEQhFw3rAnW7NmzeeONNwBYs2YNEydOTN83btw4du3a
RWNjI9FolA8++IBZs2YNZ3iCIAiCIAiCIAiHRDLaKzsMg/Yqglu2bMEwDO666y42bNhAS0sLixYt
SlcRNAyD+fPnc/nllw9XaIIgCIIgCIIgCIdsWBMsQRAEQRAEQRCEw5lYgVYQBEEQBEEQBGGQiARL
EARBEARBEARhkIgESxAEQRAEQRAEYZActglWIBDguuuu48tf/jKLFi3io48+6vKYP/7xj1x88cXM
nz+fF198cQSiTOlPrK+//jqXXHIJCxcu5Pbbb2ckps71J05IFTO55ppreOSRR4Y5wpT+xPmnP/2J
hQsXsnDhQn7961+PQJT9i/Ovf/0rF198MZdccgmvvvrqCETZ4cUXX+Smm27q9r5cOZeg9zhz4TzK
1FusMPLnUrve4syFc+lwlkwm+eEPf8iiRYu44oor2LVr10iHlCUWi3HzzTezePFiFixYwMsvv8yu
Xbu47LLLWLx4MT/60Y9IJpNAbn2eAdTV1TFv3jy2b98+amL+3//9XxYtWsTFF1/M448/nvNxx2Ix
brrpJi699FIWL148Ko712rVrueKKKwAGFGtrayvf+ta3WLx4MV/72teor68fkZg3btzI4sWLueKK
K/jqV79KbW1tTsbcOe52Tz/9NIsWLUr/notx94txmPrFL35hPPDAA4ZhGMb27duNCy+8MOv+pqYm
Y968eUYkEjEaGxuNU045ZQSiTOkr1kAgYJx77rlGXV2dYRiGsWLFivTPw6mvONvde++9xsKFC42/
/OUvwxhdh77i3L17t3HRRRcZ8XjcSCaTxqJFi4yNGzfmXJwHDhwwzjvvPCMSiRjNzc3pn0fCj3/8
Y+Oss84ybrjhhi735dK51FucuXIetest1nYjfS4ZRu9x5sq5dDj797//bXz3u981DMMwPvroI+O6
664b4YiyPfHEE8bSpUsNwzCMhoYGY968eca1115rvPvuu4ZhGMYPfvAD44UXXsipzzPDMIxoNGp8
4xvfML7whS8Y27ZtGxUxv/vuu8a1115rJBIJIxgMGr/85S9zPu4XX3zR+Pa3v20YhmG89dZbxv/7
f/8vp2NesWKFcd555xkLFy40DMMYUKx//OMfjV/+8peGYRjGM888Y/z4xz8ekZgvv/xyY8OGDYZh
GMYjjzxi3HXXXTkXc3dxG4ZhrF+/3rjyyivTt+Vi3P112PZgLVmyhEsvvRSARCKByWTKut9isVBc
XEw4HCYcDiNJ0kiECfQd60cffcTEiRNZtmwZixcvxufz4fV6cy5OgOeffx5JkjjppJOGO7y0vuIs
LCzk97//PYqiIEkS8Xi829cy0nF+/PHHzJo1C13XcTgclJeXs2nTpmGPE1Jr2N1+++3d3pdL51Jv
cebKedSut1ghN84l6D3OXDmXDmerV69OvwdmzpzJunXrRjiibGeffTbXX389AIZhoCgK69ev5+ij
jwbg5JNPZuXKlTn1eQawbNkyLr30UvLz8wFGRcxvvfUWEydO5Jvf/CbXXXcdp5xySs7HPXbsWBKJ
BMlkkmAwiKqqOR1zeXk5v/rVr9K/DyTWzHP15JNP5p133hmRmO+77z6mTJkCdLQtci3m7uJuaGjg
vvvu49Zbb03flotx95c60gEMhscff5wHH3ww67a77rqLGTNmUFNTw80335z1B2tXVFTEueeeSyKR
4Nprr83ZWBsaGnjvvfd46qmnsFqtXH755cycOZOxY8fmVJxbtmzhmWee4Ze//CW/+c1vhiy2Q41T
0zS8Xi+GYfDTn/6UqVOnDumxPNg4g8EgDocj/bvNZiMYDI5InF/84hd57733etxuuM+lg4lzJM6j
g401l86l3uIciXPp8yYYDGK329O/K4pCPB5HVXPj69tmswGpOL/97W9zww03sGzZsvSFFpvNRiAQ
GJHPs548+eSTeL1eTjrpJFasWAGkksNcjhlSn2H79u3jt7/9LZ999hlf//rXcz5uq9XK3r17Oeec
c2hoaOC3v/0tq1atytmYzzrrLD777LP07wM5vpm3tz92JGJuv2jw4Ycf8uc//5mHH36YN998M6di
7hx3IpHgtttu4/vf/37WRbpcO9YDkRuf0Ieoffx/Z5s3b+Y73/kOt9xyS/oKRLs33niDAwcO8PLL
LwPw1a9+ldmzZzNjxoyci9XtdjN9+nT8fj8Ac+fOZePGjUPakDmYOJ966in279/PVVddxd69e9E0
jZKSEk4++eScihMgEolw6623YrPZ+NGPfjRk8R1KnHa7nVAolP49FAplfdAMZ5y9GYlz6WDiHInz
CA4u1lw6l/oy3OfS503nz4FkMpkzyVW7qqoqvvnNb7J48WLOP/98fvazn6XvC4VCOJ3OEfk868nf
/vY3JEninXfeYePGjXz3u9/NmsORizFD6jOssrISXdeprKzEZDJRXV2dFV+uxf2nP/2JE088kZtu
uomqqiquuuoqYrFYVmy5FnMmWe4Y6NVXrJm3tz92pDz33HMsX76cFStW4PV6cz7m9evXs2vXLm6/
/XYikQjbtm3jzjvv5Nhjj83puHtz2A4R3LZtG9dffz333nsv8+bN63K/y+XCbDaj6zomkwmHw0Fz
c/MIRNp3rEcccQRbtmyhvr6eeDzO2rVrGT9+fM7Fecstt/D444/z0EMPcdFFF7FkyZIhbRAebJyG
YfCNb3yDSZMmcccdd6AoyrDHCH3HOWPGDFavXk0kEiEQCLB9+3YmTpw4ApH2LpfOpd7kynnUH7ly
LvUlV86lw9ns2bN54403AFizZk3OfQbU1tZy9dVXc/PNN7NgwQIApk6dmu71fOONN5g7d25OfZ49
/PDD/PnPf+ahhx5iypQpLFu2jJNPPjmnYwaYM2cOb775JoZhsH//fsLhMMcdd1xOx+10OtOJksvl
Ih6P5/z7I9NAYp09ezavv/56+rFz5swZkZj/8Y9/pN/fZWVlQM/tiVyJecaMGTz77LM89NBD3Hff
fYwfP57bbrst5+PuTW5dBhtE9957L9FolDvvvBNIXQVcvnw5DzzwAOXl5Zx++umsXLmSSy65BFmW
mT17NieccELOxnrTTTdxzTXXAKkx7yPxwdOfOHNBX3Emk0nef/99otEob775JgDf+c53mDVrVk7F
efrpp3PFFVewePFiDMPgxhtvzKn5Lbl4LnUn186j3uTaudSTXDuXDmdnnnkmb7/9NpdeeimGYXDX
XXeNdEhZfvvb39Lc3Mz999/P/fffD8Btt93G0qVLue+++6isrOSss85CUZSc/jz77ne/yw9+8IOc
jvnUU09l1apVLFiwAMMw+OEPf0hpaWlOx71kyRJuvfVWFi9eTCwW48Ybb2TatGk5HXOmgbwvLrvs
Mr773e9y2WWXoWka995777DHm0gkuPPOOykqKuJb3/oWAEcddRTf/va3czbm3vj9/lEZN4BkGCNc
p1gQBEEQBEEQBOEwcdgOERQEQRAEQRAEQRhuIsESBEEQBEEQBEEYJCLBEgRBEARBEARBGCQiwRIE
QRAEQRAEQRgkIsESBEEQBEEQBEEYJCLBEgRBEARBEARBGCQiwRIEQRAEQRAEQRgkIsESBEEQBEEQ
BEEYJCLBEgRBEARBEARBGCQiwRIEQRAEQRAEQRgkIsESBEEQBEEQBEEYJCLBEgRBEARBEARBGCQi
wRKEUeBrX/sa27Zt63L7888/zxVXXHFI+7766qupr68/pH0IgiAIgviuEoQUdaQDEAShb7/73e+G
bN9vv/32kO1bEARB+PwQ31WCkCISLEEYJO+99x4//elPKSgoYM+ePZjNZu6++25+97vf0djYyJ49
ezjllFO4/vrrueeee1i1ahWJRIKpU6fyX//1X9jtdv7yl7/w6KOPomkaJpOJO+64g/Hjx3Paaafx
i1/8gunTp/OLX/yCp59+GrfbTUVFRfr5o9Foj/s97bTTuOiii3jnnXeoqqrinHPO4ZZbbuH73/8+
AFdddRUrVqygqKhopA6fIAiCMAzEd5UgDD0xRFAQBtGGDRu4+uqrefrpp7n44ou5+eabAWhtbeXZ
Z5/l5ptvZsWKFSiKwpNPPsk///lP8vPzueeee0gkEtx11138/ve/529/+xuXXHIJq1evztr/Sy+9
xAsvvMBTTz3Fo48+SjAYTN/X037btbS0pL8U//znP7Nnzx5+8pOfAPDggw+KLyxBEITPCfFdJQhD
S/RgCcIgmjx5MnPnzgVg/vz53HHHHeTn5zNnzpz0Y1577TUCgQArV64EIBaLkZeXh6IonH322Vx6
6aWccsopnHDCCZx//vlZ+3/nnXc488wzsdvt6ed46KGHet1vu9NPPx2AQu/QYwAAIABJREFUgoIC
8vLyaGpqoqysbIiOhCAIgpCrxHeVIAwtkWAJwiBSFCXrd8MwkGUZq9Wavi2ZTHLrrbcyb948AEKh
EJFIBIB77rmHLVu2sHLlSn73u9/xxBNPsHz58vS2kiRhGEa3z9fbfgFMJlOP+xEEQRA+P8R3lSAM
LTFEUBAG0aZNm9i0aRMAjz32GLNnz8bpdGY95sQTT+Thhx8mGo2STCb5wQ9+wH333Ud9fT3z5s3D
7XazZMkSbrjhBjZv3py17UknncTzzz9Pc3MzyWSSf/zjH33uty+KohCPxwfh1QuCIAijgfiuEoSh
JXqwBGEQ+Xw+fv7zn7N37168Xi8//elP+fWvf531mG984xssW7aMiy66iEQiwZQpU/je976H3W7n
61//OkuWLMFsNqMoCkuXLs3adt68eWzevJn58+fjdDqZPHkyDQ0Nve63L2eeeSaLFy/m/vvvZ+LE
iYN3MARBEIScJL6rBGFoSYboexWEQfHee+/x4x//mGeeeWakQxEEQRCEbonvKkEYemKIoCAIgiAI
giAIwiARPViCIAiCIAiCIAiDRPRgCYIgCIIgCIIgDBKRYAmCIAiCIAiCIAySUV9FsKYmMNIhCIIg
CIPM73eMdAiDajC+qzweKw0NLYMQzfAZjTHD6Ix7NMYMozPu0RgzjM64cz3mnr6rRA+WIAiCIByE
uro65s2bx/bt24fl+VRV6ftBOWY0xgyjM+7RGDOMzrhHY8wwOuMejTGDSLAEQRAEYcBisRg//OEP
MZvNIx2KIAiCkGNG/RBBQRiNEi0h4vX1JAIBkpEIkq4jm0yo3jxUlwtJFtc+BCGXLVu2jEsvvZQV
K1aMdCiCcFgwDIPmxlZsdh1VG529FoLQTiRYgjBMInt20/zOSlo2biCyZ3ePj0vICs1WL6G8YrQx
4xh38tEUVxQOY6SCIPTmySefxOv1ctJJJ/U7wfJ4rIMy1GU0zk0bjTHD6Ix7NMYMqbibGsK0BgMk
4wYTpxaMdEh9Gs3HerQZjTGP+nWwRJELIdeF1n1C/fPPEd60EQBJVTGPn4CRl8+2xgQ7alohHseU
jOKMh/DEAviijahGMr2PWpsf7cg5TL/4HMxu10i9FEEYNrn8hXr55ZcjSRKSJLFx40bGjBnD8uXL
8fv9PW4zGN9Vfr9j1H3njcaYYXTGPRpjho64G+tbqKlOxT8hxxOs0X6sR5Ncj7mn7yrRgyUIhyBp
JJGl7ofzxWprOPDoXwit+QgA69QjcJ92BvqkKTz/YRXPrNxFPJHEU2Li+GmFTK/Mo9hnw2ZWScbi
1Gzexr4PPyGy/hM89Z+hrHyeT995gcj4aYy/4BxskyYjSdJwvlxBEICHH344/fMVV1zB7bff3mty
JQiCIHy+iARLEPqpOnSAj2vXs6NpJ58FqgjFW4gmolhVC06TkxJbIWOcZUzyTsD20RZqHnkYIxrF
MnES+ZddjqmsnD0Hgvzh0Y/ZvT+Ix65zwfFjmDPJj9WqZyVLiq5ROH0KhdOnAJfQsL+OD594Huv6
98nb+jH77vkYuXwsRRddhHXadJFoCYIgCIOiuTHM/n3NlFd6MZm1YXte8TUmHE7EEEFB6MOGus28
uPt1tjRsS9/m0p04dTu6YiIUb6GxtYnWRCtKwuCUDwJM295K3KTBhWcx5uRzaa6N8v6He/lkwwFM
hoFNliHZcepJEpitGk6XBV+hnYrKPEoq3F0m+jYEWvnXX1/F+fFKJob2AGAeP4H8xV/GXF4xPAdE
EIZBLg8RPBhiiODIi8cSRFrj2BymPh+bS3H312DFvH3TAZJJA7fXir9w6M/D9ribGsIcqGoGxBDB
oTIa4871mMUQQUEYoIbWRh7f+k/W1qwDYKJ7HMcVH8VEzzjcpux5UIZhsL9uD/X3L0fd3Uq118Yb
0yagbU9g/2glcjKVKHkBSZZxey1YLBqariLJ0BqOEQ7FqKkOsH9fM+s/3IeqylRO9jPz6DLy8u0A
eBxmFn/1HFZvnsujT73NnH0fMGHbVnb/+HZcp5yGf/5CZFE2WhCEXiQSSaT/z96bx+lVlvf/73Oe
ZfaZbJNMEpKQhEBQlIBE1ApShC8itSguUK1LXZGq3/qly1f7c62tVmv7VWsXebUVi1UEZVNBxUpV
EEhCQsg2mcns27NvZ1/u+/fHedZ5nplMkkkG5Pm8XpCZs93Xuc99zlyf+7quz60oqOrzK2QweiyN
EIJNW1cSbWm6P882NCNYTfw24Yx+YYQQfPrTn6a/v59oNMrnPvc5Nm2qzLrv37+fL3zhC0gp6e3t
5Utf+hItLcefaWqiicXGM8lD3H7ou5iexdaes3nzua9nQ9e6OY/3czmMr91GJhMicf4bmfG7WTkV
RKicVoNCdxyjK4PZkaOjO4rs6qO1o4/l7b10RjtoCbUjpcT1PLJxi+SoSWrE4eiBGEcPxGjtE6x8
sUJXb4SOSDtr163h5puv4t8f2MCeo4d5TXo3/OLnGAf20/fu99O2bduZ6qommmjiDEMISf9YhrWr
OljWeeJ/I4f6E6iqytbtz6+6MSEC4SDPE0SbrsU8UIDndHJTE00sOc4owXr44YdxHIc777yTffv2
8YUvfIF//ud/BoIIwCc+8Qm++tWvsmnTJu666y4mJyfZsmXLmTSxiec5pJT8ePhn/HjkYSJqmLee
90Zevm7nnEIWAPpMgif/+S5GwpdiresEF1aubuec81ezZyrHrgHB+uVnc+Ul5zKkDTKUHeFA6ggH
UkfmNqQN2A6duV5WTW+BmZVMzkC6d4iZjUcQIY+wGmbbBVvw1q3mtiev4crsIS5OHGD8i3/D8muu
ZeX1b0CNnLn8+SaaaOLMIJ23yBkOuTGHndt7CaknLv9eIhvPRzQjJQvDma4gadYSN/HbhDNKsPbs
2cNll10GwI4dOzhw4EB53/DwMMuWLeOb3/wmAwMDvOpVr2qSqybOKIQU3Hn0Xn49+TgrW5fzvhe9
gw1d6+c83rE99j06yNNPjOO1vYCQInnBjnW84MK1rFzdyTceOMiTA0k2r+3m/9x4IR2tEeAVAGiO
zrQ+Q8JMYXgmlmejKAohJURIUWkJRWkJtdASbqE11EJ2yuboY1lIbGS1vpGWi/JMtBzjcPoocJTO
S1t4ZGID/cNXcWNuD5mHfoxx+BBrb76FaO/qM9OBTTTRxBmFJ132xvezom05W3rOXmpzmjiDkEJg
j40SWb2aUHvHol57MXlOTrOJZUzOOasH9TgXbvKrhUFKiZfJEO7pQQmd3gWZC45GwdFY19lci/NE
cUYJlqZpdHZ2ln8PhUJ4nkc4HCaTybB3714++clPsnHjRm6++WYuuOACXv7yl59JE5t4nkJIwR2H
7+KJmT2s71zLh3e8j65oZ+NjheTg3kl2/2oEy/KICJ8XrnbY+baraWuPIqXk9oeO8OThONvO6uFP
3nwhbbPy/TujHWyLbmXb8q0LM3AF7DxfsOexUfb+Zgz90W6uesXr2XxpD7vie3hk/FHC6wdJ9Lby
tcGLeWchyfJj+xj77Kfoe/d76bzoJafaRU000cSzBUVH1BE2rUDazCyYYD3Hda0WBUsRKXEdDxSF
SGRxHGI3mcSJxXDjcbpeeumiXPN04PBYBoCc1sbyBYiLPNshHAdhGjhhj6WSMXBjMazRESIrV9J2
zuktB+hPDwCwqm0F0VD0tLZ1qnB8l6SZoq9j9bxZR2cKZ9SCzs5OdF0v/y6EIBwOBuiyZcvYtGkT
W7duJRKJcNlll9VEuJpo4nRBSsldR+/niZk9bOrewJ9c9IE5yVVipsAPvrWHX/9sEM+02JJ6imvP
TnPZe19bJlff+8Ugv3x6mk1ruhqSq5NFKKTy0ss2c8M7LqZ7WSt7HhvliR+Oc2XfFXz2FR/jNWe/
GjXqoLxgP/++WeXoJdciPZ+pr3+N+J3fQXreotjRRBNNLC2UEsM6iToZIZoEaykiJSODKUYGkot3
wWKK53OFMC/IzudACEvb+xTGkSPknjmAr2kLOkdKief5i2aDrwft+oXTo6yXtXPsntmL5lb89TM5
yqTvU9izG3tq8oTOG8wOMaVNEzMSp8myE8MZJVgXX3wxv/zlLwHYt28f5557bnnfhg0b0HWd0dFR
AHbv3s22ZqF+E2cAPxn9Bb+cfIz1nWv50IXvpT3SXneM7wue+J8hvn/7HhIzGutljJeP3M2OF61g
3Vv/oDwj+sPfjPKTJ8dZu7Kdj964eOSqGr19XbzpXS9h49YVjA9nuPs/dlNIOrxuyzV87KV/wtr2
PsKrJ/jh2mM88vLXE+lbS/ZnP2H8i3+Dm04tuj1NNNHEmUXJDz0Zp+e54pA3sXQoj68zPFSe/fSq
FsK2F3RcfLrA8NEktuUuTsPFB+MJTpm4NfoeTBSmAIjpFaLS6NkIKXHcxSOOJfi6jvQ87PHxEzrP
8oPn4YpF6udTxBklWFdffTXRaJSbbrqJz3/+83zsYx/jgQce4M477yQajfLXf/3X3HrrrbzxjW+k
r6+PK6644kya18TzELtj+3hg6CGWtyzjlgvfTXukre6YbNrg3jv28tRvxujsauFlHGD7sQdZ+dKL
Wf2H7yiTq5/vmeCeXw6xsruVW2/cQXf76Qunt7RGeO2bXsQlrzybQt7mnjv2MnAoxrrOPv5854e4
aNWLCXVleWrVE9y5/Srad16KNTTE2Gc/jXH40Gmzq4kmmjiTOHEPuMmv4Ey78qeF1D7X2MgC8BwI
YM3Cwp5rPmsCYBqLS7DGJk2Gj558VNT3BYOH48xM5movX7yv4z2PI6MZnhpILD7JWuT3ZSxWIJkz
F/WaC8EZTSBVVZXPfvazNdu2bq3UoLz85S/n7rvvPpMmNfE8xpQ2w7cP30VrqIUP7XhP3dpWAMNH
k/z8h4dxHZ9zX9DLloGHcAefofOSnfS96z0oajBH8ZsDM3z7Z0fp7ojypzftYEX36V+LSlEUdr7y
bFb3dfGz+w/x8P2HyaQMdr7ybN7zorfxwLFefjL2c8ZWPcI/i8v50Ju3UvjBnUz8/ZdY9YY3svw1
ry3b30QTTTx3UJrUORk3xHUWf8b5uYYz7cg/96KGS8R0jvNgRvPjRENR1nY8OxYhlnMocUopG9b5
newwkL6PfvAA4WXLaN24qW48CSEZmsqzZkUbXScwsVv6FhRyFn3r6/2f4yFvOABYrk90kWoLAyze
+yKlZCoVpDqu6qmfQD+daHpXTTwvYXomtz3zLRzh8vYX3EjfrA+2lJLdvx7hoR8cQArJla89l/OG
HsI9/AwdOy5i7Xs/UFbv2dMf599+dJj2ljC33riDNSvqUwxPJzads5Ib3n4xXT2t7Hl0lJ/eewjP
E/z+Odfwhq3XoURtMqv/hy+PQ8cH/w/hZctJ/uBupv7pa/iGcUZtbaKJZyMmJiZ45JFH8H2f8RNM
S1lSnITHNjmaWVQTDN1h4FAMs+hsPRdQ6rbFIj7Hu85zru6tnCL47LI7YSSZLKavAQjLwhw4uuBU
vUVHg+eatjLsie1jSptZvGZsC2GaONPTxQ0CUfVsknmLZN7k4Ej6hK471/OtbK8mifOQ38UeJidx
vdORqniqaBKsJp53kFLynSM/IG4muXrjFezovaBmv+8JHr7/MLt+PUJXdwuvf+uL6XzkTowDz9Dx
ohez9gO3oBTFWfYfS/Ev9x0kElH56FsuZMPqxuIYpxsrejt44zsvZu2GHob6E9z37b1oBZurNr2K
N2+7HiXqoK//NV/8zRTRW/6Mtu3no+/by9jnPoM98RxyKJtoYpHx4x//mA9+8IN87nOfI5vNctNN
N3HfffcttVnzQlGC75icwxPR8haxqfyitSeEnJMkJGNBoX0mefoma6SUuIvsQB0YSvH04KnXpM5M
5hg8HJ+XjJj66agJOX1RJsNy8f1nB7lybA8tbyFkfbTIGh7CTaexRkdOqY24kSRhLGws5HSBbpX6
pr6PhrKBLVPadIOzTzaENft3SXV3yEUk8EfHs6QLFgCu5S8JyT7RNoen8zw1kMC0AyEvperdWMpR
3CRYTTzvsCu2lz3xp9ncvYnXbbmmZp9je/z47mcYPByn76xubvjDHTj3fAt9/9O0v/AC1t7yofLi
vUdGM3z9nmdQVYU/edOL2XoSIfbFRFt7lNfddCHbX9xHYkbj+7fvIT6d54oNv8PrtlyD2mLhbHyM
L/7oEPJtN7P82utw4zHG/uavyD/xmyW1vYkmlgq33XYb3/nOd+js7GTlypXcc889fOMb31hqs+aF
nrfITBbwrMakY3oiRz5rnhIpkVLiicBhOXYkzrEj8YbHiaIjroZOn8M/NZZlZCCJY5+cEqovfDJW
tibgp1kuluvh+ae24HIhFzijTkGfU6m15DAKyyL/xOO46ROLNMyLRXaAdctlIqExkViYQt6iosG9
jB5LMT2Rw3EqJNX0THzhV1L0TnLR7NJzGcuPM5ofW9A5GU2SyIk57Z2/wRM7vHJe7YlSiEbBs1OG
kJJ0wSKRNbF1QXLcQk8G35AzmjRavF/LsxjIHMMT83/HYplgcsc8ye/D6UKTYDXxvELKzHBn/720
hKK88wU3EVIrecOG7nD/d/YxMZLh7HNWct0N55P5z9vQ9z5F2/bzWffHH0GNBPnNgxM5vnL3foSQ
fOiGF3HexuVLdUs1CIVUrrj2PF5x5VZM3eHeb+9j4FCMazZdye9ueCVqm46zfhdf/N5ejN95DWtv
+TCKqjJz278S/687mlLuTTzvoKpqzfqMq1evRn2W1ybmM0Wn3qh9X1M5i5xeSdU7lVqjY7kR9sWf
wfHnj76USJyqnsaISvGeFkqwtLwVrDtVxFhhkmPZYfJOKapX8U5PJX3P8wW+L5Ceh37gGfSDjZeW
KfnHXjYLgD2xMGd+XigKluMxOJVb1AJ+u1iX43inRjznguv5xDLGCUcpvCoSdTB5hMPpo+XfpZTk
dKecNieFwMvl5m0jPp1n8HC8RoWv0fGm4aBrjVMQFzNydEKQc8WuF+nygGdJFAUc/fgtuY6HZS5O
lNYVHikzjZSCKW2GnJ0nri1Mdv3ZltL67P4r0kQTiwgpJf915G4s3+LN266nt31leV8+a3LvHXtJ
zGhsf3EfV71mC7F//H9lcrX+w3+CGg3I1cGRNH93515cT3Dz9Rfwoi0r52pySaAoChe+dAPXvulF
qKpSTnd8w9bruHDVCwn1pHH7nuaL33mK5NptbPz/Pk103Xqy//0w41/6Al52cWs0mmji2Yxt27Zx
xx134Hkehw8f5hOf+ATbt29farMWBGXWvPLAZJbDo7XRkWqicSLIWlmELxkbXZhzEz5Okbv0POyp
qdM+iWNbLtMTOSbHsuVtenE9H8urd5QbuWSGWyEAnueXCd5s7O6PMzRdKN+TsKw5rDo9jl+JTE/E
9eMcOTdsx2dkJo9VM05Ofh0A6c8dbXBcnz1HEwxP50lkT4wUzk4RtDyrPIOQylkcHk0zEQ+ibtax
YxhHDuPNsyxJLhO0b1vBfUspcfz6sTkxkmGqaizV4AQd+pMeBbPacX2XnJ0/5ejl3KcXVQTrtjRG
bDLP+HAafxFI+bHsMJOFSQpuJYJ6Sne5hJyrSbCaeN5gd2wfRzIDvGDlebxs7SXl7fmsyX3/tY9c
xuTiV2zkla9Yw9SXv4B5tJ/Ol1zC+v/9f1BbghXonzqa4Ct3PY0Q8Mc3XMBLzutdqts5LjZtrRW/
ePi+I7x121vY2HUW4d5J3JVH+bvv7mXMb2Pjxz9B10svxTo2yOhnP4XRf2SpzW+iiTOCT37yk8Ri
MVpaWvj4xz9OZ2cnn/rUp5barONgYV5DJmkwMpgqy0SfKOy8IJMLHB3TswKndrYlbuCUhkIVd2JK
m2G8ULtIqDU2hj0+hjU2elK2wML8yVJEqlotUVUC2yT1DmD1rLfvC3J2gUOpfo6lR8lnAxnsydEM
3jzplp4vmUkbOA3WJErMFIhPlxaEXVxvr2T7qUQqJxIaM2mjlqSdJL8yh45R2L0L4TQmpENVdYG2
O7cznjBTZO1a6XB/njQxo7i+VL5IOEuThL5+fOJZ6rr8pMfQkRNcoLZBXVg18oaDXT1uTrYEa9bA
nyhMkTGzmL4dvBOn8PylrLqNk7SvFMX2FoFg6a4OEtzjpAWeCuwzJIjRJFhNPC+guwZ3D9xPRI1w
47lvKEuolsiVlre59FWb2XFOKxN/+3ns8XF6XnUFaz9wC2okgpSSnz45xtfveYaQqvLRN7+Yi7Y9
e8lVCXXiF7c/zZtXv5nlLcuInDWA2znJP3xvH+NZl7733UzvTW/D13UmvvxF0j958FkXcm+iicVG
e3s7t956K9///ve55557+Iu/+IualMFnJxbmAZfqg7RCELkRto09MbFg1TUJJPQ0UymN8fw0B5KH
a/a7mQzW6AhuKlVjzJQ2TUyvrdmSdmDL3FGexUGjT1aJYIkGO0ubYlN5hvoTaGbglM9oiRqhECEk
k0md0ZlC3TXSBRvdcoml64U+sg22LUpFi0K5yxtJgi8UJWezRA5P5ZPvJhLFf+vr9aSsTTts9LfF
tjwc36VgFxjMDNXs82R9dGn2cgWudBgrTGC5LpZzgimIjgTkgv/mJbLevO+fkJJDI2n2DsxP2qSU
pHLW/LWAVTZJIcoL6UopigyptE+STRuIWTVp2r69c65/OZXUGJ7J16TKnnwCYuW8lJnhYOrInMRY
M10KcymPSrloNV+z7yWeMdg7kGj4ri42mgSriecF7h38MZqrc93mq1nVtgKoJ1fbIjHG/vqzuIk4
K153Pav/8J0oqornC7754BG++9+DdHdE+fO3XsT5Z69Y4jtaOEriFxe9bCP5rMXP7jzK/xLX06q2
0LL1GexIki/fuY/plMHyq65mw5/+BaGuLpJ33cn0v3wdYZ35BfqaaOJMYfv27Zx//vk1/11++eVL
bdb8KPOr+R2h2b6iPTmBsKyK3PMCUDBdCqZdjg5Uwy9GCrz8AhQLlbL294LbPh6OjmcZnMgcN+0w
VIpgzUOwSlE+z25sn6LAeLzAdFqvc4Y96WL6GkJI8obDgaEUmUKDdERZa0NBszk8mDxpoY3SlU6l
/K2RIHdlZ6P+qo34TYxk6lIo7YkJvMKJq1gm4xpzsZb5IlglxYekEyOuJzg6rTOTEQurr1NOTnEu
q4mA4MxpUoO+a9BCImcxMJllYCJXt6/6zPJPxbEuG+zLxTQSMwWSsSDqLERAGIVtz/mOGraPLySu
L9ALNmL2WCzfR/CvrtlMjWVr36UGHTecGcZ0TXLFukfDNWqe4YHh1IJl5TVHn/f5l1KlZdWWuZDK
B+9l4gwsPHxKBOt973sfDz74IK57OiRIm2hicTCYHeax6SdZ37mWKzdcBswiV5dtYsPYY0z/yz8B
krUfuIVV1wdRrumUzl9/aw+/2j/Npr4uPvnOnWxe2720N3QSCIVUXnbFFn7vxhfT0hrm4KMJdk68
lpATofuF+9H9PH/33b0kcyZt285l0yc+Q9u2c9H27Gb8i1/AL9TP2jbRxG8Djhw5wuHDhzl8+DD7
9+/n7//+77n22muX2qxFwizP52TIjQxOq3YYDUen4NSqzDW6tO34aKVZ6kUkWDOTOabGAjnp3J49
FPbsnvNYx/ZIjdl4lkA0SBFcsFtd7YjPOiVtp9D8AqZvcWgkjZ7N039sBillWTSi9lIKQkh27Z5g
eCjNeFHqPp81yaQWPrNesqNRBMvxHZ5OHKhLtau/SO01ZEPXPahzMi2XwcNx4tNFp1lzMA0nSKH0
fHRXx/ACx1XOipJK10EZO4ZSjGSe6DAQ8zyncgRLFCO1ZeJzgszzeBFhR8OtEn1xpqexpybnOLpB
2w2ubxWFW+aM5sw6zxwcqNuXjmvYhlNW9HTsYMw9eSTG/mP1dWi25QYEqeoh2JbL1HiWQkKvq+2s
xtRYFl2zSR3oR003js4JyyT8zFHURECgdEfnV8P7eSY+QDpvnXCKXkJPczRzrG77WGGiZvt8Eygl
lCYjTlJ48oRwSgTr/e9/P7/61a+45ppr+MxnPsP+/fsXy64mmlgU+MLnzv57UFD4g/NuIKSG0Ao2
93/nabS8zc5L19L76++Q+dlPiPatZeNffoqunS9FSMkvnprgM/+xi9FYgVe+aC3/920Xs7yrZalv
6ZSwYfMK3vLuS9iweTmZCZvzD11Ja2I5vTsOkDUMvnLXfgzLI7xsGWfd+ud0v/Iy7LFRxr/0+bIC
VhNN/LYiEolw7bXX8vjjjy+1KQvDcRzCsgNeta01WGXiuKIUNXBtoqksuB7Zg/s5+osH6E8ePe5p
ewcT7DkSD0hGmQTUGl0wLSbTcwvrWMNDmMeO1Si2FXIWumYjpUSZLUwwy6NKxjV8F7SkX1GZqzpk
dpCjep/ZoOYsOKb2pJLzX5plj4z0Ex06zERCZyyu1UT/SmdOjWXwvJKQRvBvbCpfXlesGjnNZmzW
dgWlrgbLGBtn5rFdpGbyxI0kru/y9PRRHj80gzaHyttC6rg002VgMsv+gSS245NNByQqFK64kNMT
OWJ6ghkt1vAa3vQkqpYjPDkyd0Mlm6AhFZ6NmJHA8IzyQ/Ncia2JKi5f/4LknQJDuVH8hh72/C9U
f3qAjFX7d9Cec2HyE2OQmpdvKMIy+1p1k51Soudt9HTjsWrOErnJZQzGhtLEJzLYsVi5AMt1glHs
e/UCF43gpjOE45M4hlu2rqyWmQ9sVKcCArZncDoQIpmc4ehEtiHpOx50V0dKiaE75ec6U4gTm8lU
5PoXAFVVcG0PI2+d9gXAw6dy8s6dO9m5cyeWZfHQQw/xkY98hM7OTt70pjfx1re+lWhRda2JJpYK
j00/yZQ+wyvW7mRzzyZMw+GB7z5NIWex4/wulv/wnzBzOTov2Unfu96N2trG8HSeO356lOHpPB2t
Yd77ey/gku2rl/pWFg3tnS1c95YXc3DvFL/5xTE2DO0gl5kh+uJ9hTG3AAAgAElEQVRBJvadxz/d
+wx/8uYLCYfDrHnHH6G2tpJ9+GeMf/HzbPiLjxHuWbbUt9BEE4uGe++9t/yzlJKBgQEixbXunu2Y
yz1w3MDJjM4mUVUOp0Jlpr9UozQX1EyCsGujxpKMpIoRkXnU4mZj72CCC0JFcjPLqfnx4ScRUvD6
zlfQ1sBncOLxQJbadRdUHyMBz7axReAiqqqCqigg6pXojnutGiImG/5cC6XmpFQ+cHoNy6O7I1ox
UAHTcOf0Yi3TZexYir41rXT3LuPwWEBAVy9vozUaxrQ9khmjHG0qRZ/0kRHiSYdoNEX7ecEYTmVt
1kWDtYI62+rXaiw9DkVR8E0Te+9uFLMbwm3lAVaKsLm+YCyus6LBRGNJyjybjzLpu7QvM9i4srZ2
qPhDsRsa96GUkMiapI6abG+tSuNq8Oyydo6cZhAJbwz6Le6hGQqlCspGlTxDuVE830Vxu+hu6aoj
llLWk03XE7i+V8/G58MCD5USHGERdyc5mDJ4yZoL64+Zt93afcd7RXQtIPup/hFWRm1UPYzo7Kmr
25qvlVIbtqugpU26V3XUHKuEg++OQGI5PqkxDdeD6Npgf2Nye3wkYxrZtEFvXxfLVrRjZgRWzifs
W3SsbKu1V8qGkV1VUSgkDCJhldHBFJvPXXVStiwEp0SwAJ544gnuu+8+Hn30US6//HJe+9rX8uij
j/LBD36Qf/u3f1sMG5to4qRgeiY/HPopLaEov7flNdiWyw+/u59syuC81R4rfvxP+FLS+5abWHb1
NUynDB746UGePBRDApe+YA1v+d1znvNRq0ZQFIULLl7Phs3L+e8fHYEJ6CgsJ7oxyaERhf/8ST/v
unY7iqrSe+NbUSJRMg/+iMn/9/ec9Wf/l1B7+1LfQhNNLAqeeOKJmt+XL1/OP/zDPyyRNSeKKge2
yrMaLTrE284qOtSz60wUBRTYO/gYKAp9fVtY37m27urls6RASElec+iWMiAss+Dl8mgTA7Ru2drQ
Ul+CImuFBBJZs0x6XN+njWDNq8nRDGedvZy29grhChTh5naGSg6Vr2nEDg5gR1rJnrc6IFgoSBE4
9ZbjMzhRiULMJktKlXiEdBzcgkF4+Ypy+lXQVk3LtflG1RE1CabQ8FzJKq+1tKkM1dBQZD3ZGB9O
46WSTIzm2HTJeXXt7j+WQsnphIvph7NrsKTnYZbSsIrPqlHaV8HR8IRD6Um7sRmQklAmide7oXxc
rpi+5ksfzc8RNrsbdQQAnqcymbDoXq7TmZ9fTMV1fNSQUpZKL0EIEHlqREZKUPIaaiKDv3l95TlR
e4+zIyo19hVT/BrxEEkwHoQvCRdVMR1XMBor4HQq0LpwglXk0WBb+K5PqLNzTvLjSQ9pQfKYRba9
QLQ1zIQ2ybqOtbQTwRw4WrTFYypp4HT7qHJ2Q1W/z4PSqytsG6JUxmvNecXx4AkM22MFjYMlrg+K
awMBwZKzJiHiWZPM6Eks+yJlQxKmF+saLcOFFSCK0Tbh+XjSLb/LCrDrSJzOtgjnbqidEC6t1ydF
sASDrtl0dJ4eH++UCNbv/u7vctZZZ/HGN76RT37yk7S2Bh+Ql770pbzpTW9aFAObaOJk8ZORX6C5
Oq/b8hralTYe+N5+knGNs6MZ1j92H+Hubtbe/MdkVpzFnfcfZNfhOBLYuLqTm169je2bnh2LB59O
9Cxv5/q3XsSeJ4bZ9csRVo6toaXL5NH906xe3sZ1Lz8bRVFYdcObEIZO7n8eYeofv8L6j95aXnS5
iSaey/j85z+/1CacOciAyGQ9g5BjEBqfAGCqo42+1jU1KV9lVJVO5QyHkOMRDbXg5ETJrwLAnpok
lrFQMiP4ZwtCkVqHPmmkKOTGafeXIQ73ElIipKRAuCDdonFQru/Jpgza2qMUzMDR8tIpfHM1LKua
qa5yCof6E7S0RujwdRzPR/F0TNsjrCgoilomSNNJnXbaCUdDddcIfq1s8CYnUdUIajSKlJW/B9WO
ZDgxQ4ubpuSDKloex4NIKLiW5uXBg6GZHKscB9ornapkkoSEQMpNNTYYlkd2PEZPV5TRwSRuWyeR
1oq7JqWo8akVoPDUHrraKhHLvJVHUYM0Qikl8azBxjWdZeIwOJVmcmIXzkScrnOvDEjzccIfOS+F
6et4lothzRYWqY/wNZKtr9wDjAwmEbaN4utoqyTtYjl5ozEpK7nboeGg5knJVQQx4vY07Wwus4dS
sEtKiev6TI9n6e3rqiHs1XA8QaZgs7ZdcHQsS950uOS81YRDKm5R+bBgOLS1nFhKmSscOoaP4IS7
aD377Lr9vvAZ1o6h+wIvD3TDvpEjdPaG8XxB/3SMy8O9QbRZSI5N2ygIcprLcrU0fmelqxYnMeaG
UjxOoLsWqu4jelbi+6KoLlrJK55JG5i+S3TMxlvm01IVEE9kg1rBUGIaersp0QlnZgbpOict3BL0
i2AyqeMrNl2ljY6LPTWJ0rMCCLhGiSwqKEzaw3SHg6itlBLhuuQbjOdy+iiltNLTV4x1SgTr9ttv
p6Ojg5UrV2JZFqOjo2zatIlQKMQ999yzWDY20cQJI2Nl+cXEr1nesozL1/4OD37/ALHJPGu9abYM
/pS2c7bBm9/Jtw5k2XX4iTKxuv6Vm9mxbdUpyd4+16CqCjtfvoXlG1r44T1P0Vno4kUhn4f+Z4j1
vZ3sOCfoj9Vvewe+pqHt2U3s9v+g7z3vf171UxO/XbjyyivnHb8///nPz6A1Jw5bmOh+DgiiCQtx
/VIZh5xtYeOQHOmnu+gwFWI+Q5kEm7etqq3NUurTpXwhKJhRxIzFtBjFtFXa2kMgBb6AkJRouyZY
2WXibVhXadvOgqvimzopdYqzOs7CVi28ZLDfc31oqzg84UgIQ7NJ5asiR/bcQgCe6+P7kki41mEK
R9Qa0jTbnSqRpZieoC3cykraq/YVjxbBOlfla1R3tltLCEzNwzFU2lskrVVRL0sYZA0dVrSS0y20
sFuZpS8eprsGETXCZNIg7PlYtk9YSDzHKxMs42g/0f5hvL5KhEm6HumMTWukrXy97LhHuEXBl4LM
ZIFwS4jEGpO1KwNWPJ3WMIcmCasKXrZAxoF1PfWJe9URPl/6xW0+zwyluGBjEB2wPQdHODVF/bmE
wZqWEQj3QLje1SwRP3t8nKyXwRfLsVWVRK5xrVij9E5FC1II3YChlyNYpu0QJdiWSerYlsf0eI4N
2wKSnMxbtJot9LQE2RxTSR3bFYzFNVaGIyiqguX4tLVIzJJoB2JOAlrIWdiWy6o1ZTqAlDBhD7FZ
2ESUrqB2ak131X5JLJvG8Aw036m8wMX3LVWw0UyXCVOjV7GYyRlYTguupxLChrb24vOQNS//fKQW
oJQNnLPymLKAI9pQAS+ZwJmYQhWtKKsgVbCQMiAsrubR//gTrHrhBqATpEC33LKxpcWlhWXjjo4U
+wssqwXH0Mh7xxFamQW3SM5szy8TrNBMAivfgW8YdK16Qc3xQeSxMj7EwDGiowbOOReUt9m6Uxcp
hVNbQ+54OCWC9cgjj3DPPfdwzz33kEqluPnmm3nXu97FjTfeuFj2NdHESeHHwz/DEx7XnX01jzxw
lMnRLL3WJNsnHib88sv54bKL2H1XEHbfuKZIrM55fhGr2TjnrPW85g9M7vrRL1k5vZnzgXvvOUjf
ey6hb0UHiqrS994PMJHJUHj8N7Rs2MiKa35b1NaaeL7hP//zP5fahJNGzsmR9zJlZ8n3JP48i3wK
KdgTexo3rQXT+wp4BRO3UyESlriGgCg4jt9Y/KKBY6kmMoyGbCy9m7b2lppjpCvLkuQl2WffB70Q
RbFVZNTFjc/gV9VNuLNmkhUlWLhUSIHrCVRV4oqKg2Q5PoQr3+tj0zlApTtUq8KnqrVRubyXplVE
CBPcpySIJJhTExgtUVau214+NmflWd3Ri+d6TI5miLZHiLSEy6RMSIHu5wlVdZnrFCNuPnhVaU5B
JCtPyrTpz6dw2qLlAKAQPq5lVq0dVomWTSQ0WtZV+snLBClXilshm4ZmY+sSmaz0j/AljiGJtAdu
nmf7pMeS9NgF2tf11fRJIWMjaUGP1DvnTw8m67YF/VZx7Kf0GaQUrKnen04j26KEVB1/7ca684/l
j6EUJL0yEFdQhCRZsEDWR1FTZgalYBCKNv77LMK1Y9Z0fPAchKwSelEoL5SdKdhE8xbri9ljJYe+
Wnbd9QST5ihxK4EQ7XQQmbMWamYyIBCh7vp30JXBc3JTKeTW9eXt+azF8IExwpNjhDcsw5GteL4s
xmaq7sV2mPJm0J0wngwhpVJeAs10reI7UXHlJxI6K5a3lceolBLfl4RClb7TDBfDsYm2gEAEBCsX
3INimYCKbnr4xfctlCugo2HtS7Bq3eV1CnymI8B0y0TLcjzSOQ/Pa8VIzi3Xn/VS9Kc1zl2+tdb3
atTNQpI2M/ihCGtnq/fMkpIXmgaoqHoeKdfhewI9YzE2lCKyom32lU8bTklF8Hvf+x7f/va3AVi/
fj0/+MEPuOOOOxbFsCaaOFnE9Di/md5NX/sa8rvaGB1MscKY4oLpRzh4wVX8dWITuwfSbF7bxUfe
+GI+9a6dXLSt93lNrkrYvuocXn31BYxsfxI/6rDGl/zXN/dQKOY+q5EI6275MKFly0je/T30A03l
0Caem1i/fj3r16+nt7eXQ4cOsWvXLnbt2sXjjz/O3XffvdTmzYuknQIpCFlFYYExl5GBuZW5LN9C
SkHBLtRwpbxWSZnKWDmGc2P0pwfxhc/YRIGphI4iQRElYYLab2TectH92ap3QQOZXAuZyQLJ8Sy+
51PKGJIC1EwOoWm0DfWXz/I8n7zulJW9chmT2FSenO6SypvE9TQHR0ZJxQN5+FTOqvXD5Kx/63YE
sH2TjJdA9wsMm0dIaHmk76MYFmqmsTPoJBM4qRyFREDeympp0i1HdcqtHUeVL2fnkUoQgSzBHxok
ufsxOIG0KjVfqW0pObZusSalYDq4xehZm1qJquQHhpk+MISoWlrHExLDLyBEUQtRypqanvkktSu1
TnPbrXiVtqq7JKHl8YVXvobnBI559dOaKEyRsXLkjic1X+zsyrMojqGCQ1azy89rLmENZUpH8YO2
PekxZY9QcAoUnGBsC7wioWx8vptIID2/HO2CamGKxgPBMl3MxDSuF6I1E9xfMmdRCnqGirVCcXsa
wy4R5/r283a+Tvyl+lctZTKeFHi+xC8UEEIynTbqVCVFjb0KiuuiFFUxFSTpvE02b5HOW+V+dL3A
xqxuM50yynZMJfV5hGAq70jGTVBwCjXRp+p7qVk3Thb/JwReSUmx1LVzrAARnh6r22F6BrqfW/CC
0qeCU4pgua5boxT4XFFeauK3Gw8M/xQpJS+YegVDR5P0mDHOS/2Gb/e9mklrNees7+H3f+dsXrh5
RZNUNcAr1r2UmRfGeaTtl2wefBlthS7uuO1J/uBdL2HZinbCy5ax7paPMPHFv2H6tn9l06c+S2TF
yqU2u4kmTgof+tCHME2TsbExLrnkEnbt2sWOHTuW2qx5oQCtuTwR14RMK9AaqIA1mDLN5VOEDAvO
mr0YZzBzP5MxSGtZlHUqvq0SUVUeffRRRP80du9Kuu2K/LOZjdIdrUSIpFSQyKJ/W/stFUJBt5OI
RJplZ28jnwkFaVuegLCkYLhEqmpa0jmL8ZSDyFn0LmvD8wRZzcZ2fAQ+Qkg84ZFO6nX3mC6UUvkE
hUIG3csQVWfHAiowPA3XMkCFlJnibI6/tqFq6vitQfqg8H2cVIpGynal/MHGC/eCYfmgKjWiE5m4
gbcygRJdjww1qhNSahxC3S/gKSo9pRjYrBSxdM4iE7VZvXxWH0jwhUQISWRipLzZFDppJwksx3Lk
gtcImoxryCIp1CyPnkZk7LiObDHqUE6nrBxvew6Oa6EmMrCsC1Z0Vk6bZWTGTQZpfFVNjswU8Loj
2LrL2lUdrJD1YgaFTI5IXCcsNdjQR8HLYguLofwQXaYBfphy/5dqy3yHNir+rpfLBfvO7gVAnYzR
EkujrFpTpCtzpPCWokzFoRByHTwjDITL48OTLnnDoau18XjO2wVao1GgNopXas+1AiLleqAfOkj4
nEp6nWFW7iGX96h+jbumYzhOBL1vTU1XW45Hd3sE3VLLqbK+dBFSkMmbpKdyc5Ir6YHv+eRmdNp7
WmoYiJCC/pEJUtMO7dHgfMtqIz3s0l39OfYE9vg4zopI+Q3y/VntSYknXHxqhV4ARvQhDN+hNTT3
92GxcEoRrKuuuop3vvOd3HHHHdxxxx28+93v5sorr1ws25po4oQxVphgb2w/587sJH7UpstKsjH5
JN/su5qWrefwZzft4GN/eDEXbFnZJFfz4PXnvJbz+85hcPuvya1Ig+Nz53/sLhegt23ZQu8fvA2h
68zc9q/lGdQmmniuYXh4mG9961tcffXVvPe97+Wuu+4iHo8f/8QlQJDuIwgnM0H0SoJ+MIVi2XPW
YGWHj6KNDVdFRyoEwHIC5yuayUKhQlysovhF2LIJmRaOCNKcQo6L6zVeP6thVk8ugedahBPTyKo8
OsVysFyvxvc27MAWvVgnkU4VyEzOzHHloEU7lccXkDeCdLVQKoaeCZ6dIyySOStooxRVSufoiMUJ
H8sSPThBNF8ISOICv1+hTAJFL+COjWIOHCWUSRUtqbJRzH8tw3YJWxaRbCUC5XiltEKJ6wtcaSOk
wPadssM7YvUzkB0M+sgvYPrVRLPiAZeczUp0qb7/JJKQlq/egCMchC9IFgLiDEHkc9oeDSJ1ro+d
rL23WNYgkQ0IuGa4ZDW7pgYoq7nouiiSk/q+UAp6ZXNZfKDGLBTNRLEcQsNTtefOStfLesma8yAY
32k9iUCQqoq8QFBHBjAwPDG7cwBQNYPQ+AyReKp4fKUGK2vVRtRs38H3Xbzis1eTWSTB+zLXor2B
WmVV/qKE1lQG88hIw+MdJ0Qj2q7mNFzRuG6t5oYA2/U4NHYIzQ9UNBsGJ2ct2RDN5cEL3knVdREy
iHYVPA3DD6LJGS9J1k0xFivMG7lyE+AYga2FrI6UYBUEpumQMJKMTU8DUFrL2fcDWxqN4WA9MAVf
yIYRvLSXIOela6KrTlUacilqdjr9wFMiWH/2Z3/G29/+doaHhxkfH+cd73gHH/3oRxfLtiaaOGHc
f+wheqc2Ex3vpd3JsTa1hwe3XctNN7yUv3z7Szj/7GbUaiFQFZU/euEfcFbXWsbPeZyplTF81+fe
b+9jYiRYnb3n8ivofMklmANHSf3w/iW2uIkmTg4rVwaTLZs3b6a/v581a9bgOHOLKSwlho4mGdw/
gZoPnGspVXQL1FjwTuI6hBIzlBzuaqljx/PRrEDKWFIv262mKtLlSsnJKqaLeSURASFQ53AYZ5VB
FH+uSv1p8N3NanNnvXjTk6iF+VPDrIJdQ9KUWYITulXreKoFAwUI2Q6eF6YjliBypB9t71N4QpDV
bYYzM7PuS2I7Poptopg6kfFj5DLTaI5GOJWYZZFE5OJ4srj4aoMIV8ncsFWJDPpCIJCkChapnI3h
F9D8HJpXQPPymHkbxXSxDh3CdiWW1YriVuqtlIa5kqXfJIptEkrNlO/ncLIfXRhYViteFWGu7ktL
GIzmxrGEie4X0DOBvYoQqFVtz5bTNovS8Y5wmMk52JbA1hu/T0q6QvIkCoavYds5QrZdtn32cHN8
v04pT5nlfwt8JJCzchiOjuEHzni1I274OqZn4SPKMvcSCOl5egZHQCver+cT0vWamrNqWJ5Fzsox
o8eZ0WIIX1LQIvi+UjWVUby+lOzpTzA6U0nVlRLkrDpBpX8UI+bhG5QfiucrDSNhimHVrUknPA9R
TB8WwqfgFPCFx1iswHgiixlcGL06TVCKoCkFFK/yfCOmiepUjrM1g1hGx5cunnRxROWdszyzjuzM
futLkUpHWngpKMQ9xoYzOFVtSn9W4LOYHlgmXKULK7UiLJZT6odKq56WL0vRj1Yt1j1Xuuhi4pQI
FsDWrVu59tprueqqq+jp6WHXrl2LYVcTTZwwBjLHmDlisGbyfFpcjbWp3SSufSufuOUKXvbCviax
OkG0hlv54x3vYXXbKtJb9zCyLIbrCX70vWcYGUiiKApr3vlHhFeuJP3D+zEHBpba5CaaOGFs27aN
v/qrv+LSSy/lm9/8Jt/4xjdw3flmhJcOpu4E6S9FP0JSVTshJNFjhwilZlD1gIDpbpDO5/sK0ykD
x/UxS/UcxVnz2dB0F8sOUqmUksdVhCJE2ZudfWqjRV3Lis+ONWcgSik6PxXXqOTtVjvutSfbnoMv
BbpfKM+iN2ogFJ/EHBrCKlTIozpr0SjhOqSsdLnWKJWukDpPCGK5PGlDq7l8wkwwYySRVURDSoW8
EUbzdAxfQ0qI9j9DRNcq1/NFQzt96ZMrVEiIWpXuZQsLXIdwrIBqWkynBZ4XwvUiZWfWNN1Kutms
a7sFH1XLoRRTPV3hYXgWWcPF80JYViutmSyqaeJr+fIFCl4V4UbBKzqvbekMbakUuHNPQkgEltDR
vKDf5irRauTkhhPTtGaypQNqYLl+UAukObPq5arqdGRQE2b4GqptErKtchRDzmrRNPUaTz6UTdE6
NYljRtBGbLxijVE4l4NYksJwrYAKUBU9Co41Mj62E0IzgskDD7csdqJpDlreZDqtcyg2WiY4obxR
Y4eXdxCGT/RYiqhWilTWR2k8IfCFJKPZKJaB4gRkJ9d/iNzewBcv+DlM1yJt5xDUyvv7Iugw3c/h
Sq983a6hkZr2yoSm+JvlVInNCKN8aNKdxqqqLWyEmhorF+IZk4m4hlIUN/ElTE+65LXgG5TMW6Sy
Fr6QmI6HKBJdAbhmsE5XWd2yHNWsIl2HDxEeG6rpt+Df0yfPXsIpEazPfOYzvO997+MrX/kKX/3q
V/nqV7/K1772tcWyrYkmFgwpJff9+tesH7mAiG+xOb2LzR+6hZte+yJao6e8nvbzFt3RLj60430s
a+lBP/cpBntm8KXkJ/ceZHw4Tai9g7XvvRmAmX+/rbiORhNNPHfw6U9/mmuvvZZzzjmHD3/4w8Tj
cb785S8vtVnzwnTqFwd1ElVpjaK22N/zVHy3yvlQlDrZawk4vmBs0MAWcxfVlza5vijvNayqdL85
UoRUvd45BWhPpAjrRlC75PsomTiiGEEM4jqy4rTJQElwUpsi7SSxfB3d1/BKzuEsEXY/nsDKpXGn
a6NS1dBdHd2pOIkdE5VIj2a65J3Z6XhBH+S1KBm99m9LySEHyDsWnnCJarX3bdmlGqvKsbqfxxOC
kO0Q0TVsWYluqb5PKDFJRDNqJdP9ECXhSNf3yqmVQcf5dMzECI1OI8aTZXJVsh0p8Lxa26OZXA1h
hIocffUiy4oXCD4ojo1iBv2mmbXnVQ8BIedImZS1Pyha/fhQNAM1FxDUvBbFyBWjY55PKparH51S
4k0FUUVfuoRcj9ZMcJwvBMlccZxJieJ7iIkJiFWikCEjSFn0vDCK7+MVIyYK4EykEfHZgi5Vkv2K
QiHmYeeL6Z6eiltQMLwKicv3D2JOJZFCMpFN4bo+jiuxCpXaMNeNUNCiICVqdVSnQRdqpkvBdEBI
Quk4oeQ0IEmZKcYKEyD88nsuhB+snzYr3OdKFy8zTdrKkDLTZN0ktlNPDeabni7F6pDgUUu8Vcct
T6IAmJ6OL2vHi+N5hAnj+oJMIRirtrCK/SgZO6SRTHvYro9pefhSMDZlksu5mJZfnyJY9bOQkMs4
+NKj4FcmDUo39KyVaX/00Ud56KGHygsMHw9CCD796U/T399PNBrlc5/7HJs2bao77hOf+AQ9PT38
6Z/+6amY18TzCHc98hjtB88iJHzOyzzBjo//b7p6Vyy1Wb8VWNm2nA/veC//8NS/oJ27l4HDOzlX
7+XB7x/guje/iPXbtrH8f72GzE8eJHH391jztrcvtclNNLFgfPjDH+b3f//3cRyHV7/61bz61a9e
apMa4niqV6Koyubh4eVjKJ2dSCqCCW5WEpYSiYJiu3XrQZmORzymIX0FqRT9jwaTvKbVQmuLxLBc
iut6Yjt+OTYg63N7AIKUttBsx60YIbCDmqNIcgbF0LFngloMzcsRUkIIKZCoqMksYo1AcRxCiTRq
cX2ltJGBltV13C5vqCzvCZxyT0gaVY/NrhlRhQ+e38B5r3SG6rq4biW9sTRzXn2OIyyStosMtYEU
ZSVG3w8hRH1lTjQcIpoOiJxX9KkkAcEq9W26YNOjBFEP3wuR01VWdRdTrkpsS4JaFbELmSaogTR1
3rRwQgatdm+DnoCMVrwD36NjJkO6pRe5TMXKuURnuYuR8WMIF3y1E62tsiZSYEKlr2xpEp3VPw3l
zgs6dFa3IVEzufJ1hVAwkz4lKuJ4Ak0P0doisIvPpiWZxrTqI2tS+rjSYebxo0g/yXLHIyN7MUSo
PIZL9vlCpRRPFUIJiG9VsNcTEtvxcFwfUPD8CkH07J6adoWlBOOkLVioWEiJYueRrC0ZVu6piF4h
8XO/6UrptBpEcllKIhfBlERRVdG18Yr1k0HAuj7NUUpBxDChuw2BIOR4yJCKolS9/LJiVfAuNo7+
+LpCQY/SWRGupDWdQUqB3heI+GfcFLjQEa4VlpkaipHOmkhFJdKgBxyrtEAyJNImhqZidQZETffz
eBkI9UjUqXjNPYrie1fwM7jCpav4TfSFRDNOb6bCKUWwNmzYcEJShw8//DCO43DnnXdy66238oUv
fKHumO9+97scPXr0VMxq4nkEKSX3P3yU5JMmqoQXpn7FpX9+c5NcLTL6Otbwxxe+h5ZwFOf8XQx0
JBG+5MHvHyAZK7Dy9W8gum4duV/8HOPwoaU2t4kmFoy3vOUtPPzww1x11VX85V/+JU888cRSm9QQ
maSBRJKz515XJuXFyXlpLGFg2Blq0nyKfudcqTGO65dJQEfbSrwAACAASURBVKnQPWKa+PNJh1dH
ISrec3m3WnKsF+An+L5Ay9nYvkW8EMP2g2h4Kf3HLqYHCQRK3kB1HKKFSvqdkD6GqI8wFEwXy/VJ
Zi0cX9Q5mK4vcGYRqvDBQZhuvP6T6wVCEpbtlzvALxOI2mhNSaWtNZOjPZEi5AQLytp2/aR0qCp1
UZndX8VfTbO1SrK7kmLnWD5C+mTMHFknSAVrBEc4CAGhoYmG+2dD5AoIA3xnHuEO2yasV6eF1abh
KRIszSkuLByQg8xUAS8d/JzTHFw3kOifXUs1Hzw/jGGFmYipmLkWpKSmNqwa4dgkKTNOIeOUo79h
00Kz7WLKbKXvq5X1DDOM7fiYwsQWJgKJbrm4viiLstRwxYbrZCkIGdQDJo0MBS+DbVh1R0UL9eqY
9QiuX9A6gihXEWox4qv7eXKZY+XXseAk5o081aCYIqscR0LS8PNzp9c5AiHUYmS1tmVF+GXlx7pe
siz0gUlainL1xyuP8mSQ7Fma2LCFibCCSSQ1kQmCtDIQyYjl89jCKkfyPCM4R/cLjFfVnZ4OnFIE
q6enh+uuu46LLrqoRq7985//fMPj9+zZw2WXXQbAjh07OHDgQM3+p556iqeffpobb7yRoaGhRpdo
ookyfCH49o8Okzs0TViEeGHsES764FuJrl5z/JObOGFs7D6LWy58N/+479+wzn+SsaMvY0NuOT+6
6xluePvF9L37/Yz9zWeJ3f4fbPrM51Bb6iVxm2ji2YYrrriCK664AsuyeOSRR/jbv/1bMpkMv/jF
L5batBoYuoPh6WStWqegWHZSUY8rOieebOxs+gioWmS3Gm2pDG54FZKKe+Q1IFizz7Mcj7whyBRs
wpZXPl/xPAiHkAQOZmdblLDa2OXTszaqq+BIG8+QeF797LIUIH0PRa+v88i6SVobOH7Zgl2urzIs
l0hdFA1mUqJyn6Wbm6xVkhQIhPTIGVFMI4zr+YRDYtYxDYiIEIS8WlLk+2pdP8zpU1ZFD3w/hKxq
sxxVcXwsT0N1w5iOQ6jqaq60kULQpnYUr6Fi2qGGLU6lszjCpVUN5OgxBOFkATqCaEP9kwu2tBQK
+K1RStoTpqiQBdUHVcsxZHu0ez5SUVBzaYh4+EJiuT5Wo5rHkvLgXAxBgmV7+EIQNi0c2VNeMLjO
yqJjXx3FVITAxSEiG2dg1cRhi4sh52eJdQhRq8ZgOR6q61FZyrY24lRKjZtOTiPn9b4l4UyO2b1S
8PJEZUfD44Pr+7QUNGR3QBJdr57IHZe9AK4bJhKttF59hi1MTNejvWpbRDOQoUB1whE2sQI4orZf
2+PBhIW5fhWRggE9lftQ9QKoBBMQ1E4wRAt5oqpaHw7yPdpn4ghFQRECfU0v1f1tWa34fohESxZb
RBAyGMOuXrl2wc+d1hzBUyJYl112WZkwLQSaptHZWVnHIBQK4Xke4XCYeDzO17/+df7xH/+RBx98
8FTMauJ5AMvx+NcfPIM/kqGNMOcmH2fLdTtpP2/7Upv2W41zlm3mIxe9j68+dRvZc39D57HLId3J
j+7azxv+8CKWX30NmZ88SOqB++h901uW2twmmlgQBgcH+dGPfsRDDz3E2rVrecc73jHv8a7r8vGP
f5zJyUkcx+GDH/zgGUkttFNpaEAuTNsjZVtQ5dO0JVPIvkoqWGO3SmJalZPCBQOWieM7HQ0uZtoe
thv8l/dsIuGKRySEUjzGpaut0TpPgOcitEwQ4XF9QnNoZgjDrDHB88JIxZ9TDlsswKF0nIpN1Ylu
1dD9YEHXjpCK5xcFDWbVZjUKYMwVEai3turkupqSCtPwRYWs6V4eXwSuruO76KbEF4JqUTopJa50
aFFay23GU437RPOCiGCJYIUcBxwHPZxGD+WJqhXqYDlg+y5K0Y1sT6TwWsNFPlh7z2o+gxsCOgJR
E1XP0+5lMIDOUOOU0tI48H1BR1sEFaUundOv+l05rtR+aSqiFq5wah7GQoNoMS2JPWOytnsVvgzW
gssVbFpyeUJqkEJbYiGa6eCICtkR0m+YflttQ3U0zvNCRIvM0JMuvhA1cvi+rKSRQjB8PF/B0HRK
HwXTs4FWfLdEQmbfaSW1EED4as2+movPQkkR0+0ojkUxdy12WzoDpk/UnKYlKrGX9eCaaWRHQLhC
tk2kql4zopuEwiHoqCT4+sJHMYPvQOn9Un0fIVQs16eQ9/H9UENzLcsDJC25PH44guPbwOmZDD4l
gvWGN7yBiYkJBgcHeeUrX8n09DQbNmyY8/jOzk70qjxTIQThYg71Qw89RCaT4f3vfz+JRALLstiy
ZQs33HDDqZjYxG8hdMvl77+7j+iMRjcqG7IH6ThLZ/3Vv7fUpj0vsKVnEx99yQf48q5/ZWLLL+kM
vZpMAh6+/zDXvO56tD27yfz0IbovfRktGzYutblNNDEvXve61xEKhbj++uu5/fbbWb169XHPuf/+
+1m2bBlf+tKXyGazvP71rz8jBMtLpFELBrPdc8fzaZ3119zHIz42QZSqhU6LuTNBqlxx1rtGpjso
NmkkqV7dVsk5qxZKKDgapjDKkRLXq3iQuvP/s/fm4ZaV1Z3/5x32eOZ77lD33qq6NVMFxTxFwWhU
jHb0QZzAAQz666jRjkkn6bTdNv3EtJqnNT+T1jbG/GIiaMBAGMQxgiQaERUVBCkUkaKoouaqO557
ztnT7499hn2mO9TALeF8n6fgnLP3ft/1vvvd+671rrW+q1lzJgij9lI7KM8jPLiXcrAwjbIUgqjN
8+N5Bh5+TwOrR7RdTwShpFKVmO01ehssfQsRPC8jxq39ysSl7aFyya9e1OoNmwqOEEVjEDXnPKjR
6ic9OrVMoY6+unXSfljUcoUq4XxDaZycDygFc1g6QeFdXlxRbW87Gc6YpAMXUMtzgpm5KrmUxfSc
l/AOtRoJYXnxue82boFIHGg1woKuExXDD0KmSh46ipgp1Txbbnx+qdzq/fGCsIVdLy6DsLBBGDZy
9iTlso1Rc/hUo9h4SYaKBpHfwvpIBJNTFqIyj8w1n9GKHxGURVd7Yr7i4zrN8Mi6gQIxyYmofVVV
j9DosUmyhPUvgjgc14sq6HJIMG/GRm6taHaDQbJnA3E+lUOhtWchEBEcma4SJNbCTKm+5kMQkiAC
c3oGPV9GU8YLu3v6TwSOKwfrK1/5Cu9617v44Ac/yNTUFFdddRV33HFHz/PPO+88vvWtbwHwwAMP
sGXLlsaxa665hltvvZUbbriB3/md3+GVr3xl37jqowMzpSof+ccfwb5ZsgiGZp9kqPIAW9/+3pUW
7TmFiewafv+8dyAjk5+tuxsx4LPrl0f44Q+eZvgt10AYsu+zf0+0SDx3H32sND760Y9y++23c+21
1y7JuAJ4+ctfznvfG79zoihCqe7Fd08kDk3O8+ShQ7Uk9ibCHs9YzII311CydW3Ht+IFlMoepYrf
oXAGoVpSzkZHKFYYUq7M4oXVRlHiJGaDJvX5zHyV6blqqyoWRYRet3CmVkghma/vji/ZlolPVLVd
9vaQx/Y5qFaNlvyWdpTD7myI9eK8S0WSKRBgdq6p9dpHjradnVT6WxXCih/G3rXEugijuLjvXIui
H6HkUlW+dg9a59jqtdHa5enVVimc45C3r4NVsEmzD/bkVMd1yU9JNjqI750ha6QF04t5XeP58XsU
yq7LkvS+NAynHpifd0jYhHFoW3IDQLT8b8kIgqhhLM/P10zKqOV/C6KebyR9H/twvJamZ0z8rjli
tT7DqGfjdc9hs2h39xONLt71dtQ3D+oyWlO9c0qTqDRIXJLGcCuiSkQUta5x34sNTHOu1MgB0+Xm
PW5nUz2ROC4P1t/+7d9y44038pa3vIVischtt93Gtddey+WXX971/Msuu4zvfOc7XHXVVURRxIc+
9CHuvPNOSqUSV1555fGI0sdzAFNzVf7iph8THJxjBEmmeojTDn6LvW99Kfns4EqL95zDhsIafueM
t/M3D32GR9bdw+nlF/Oje3cxdMUZZC5+HjPf+y6T37ybwksvW2lR++ijJ0477bRlX5OqhbPMzs7y
e7/3e/z+7//+otcUCi5aH7shNlWj2pZSorUiqoXdRYDWCtPQ+NpAiIhKJe5HSYFSAm0qUjPzsUrS
IoNqCSdTGgxTYYVB23kLwzQ0oZToUOFTRsvWa6U0kBK0ko3wQ9NQBFFItbY1LmSIFs3rJLKtag8Y
hsKXHoal0X6ACJrH5exRZJvMFeZwtYEOJY7vxaFGSblNTRTRmC+o5bMRR7n1ul+yCx+haSqiQCKi
pc+b0qCERqMAidatyp6hFUYk8ML5+J5HCiUlUdjsw/PShGkfqyZrL5kNLTGURJghutJ5jlKy4blU
OqrJVB+bJkq0H98bHy1Vyz3t2q9SmFJT8gMcQyJkhGnolva7yZ09crRlDRqmIlUuY9bLrghFEChM
ZRAFAYahO9pQQjUUetPQGIbE91O4bqlxrmEoRBAiws61pv2FlW+tFYbZ7NfUCm3oRs6doRWGoTCF
RifuWWQqAhX1vFfVoPOYaSiCMCISUQ8XZBNlLyTrms0xWvGczZVnARvTVF37No3uvwshMAyFlBIT
CWbnXC+MxNhF5zoHsBZpUwmN1BFCCKSM0FKhvObLyzTiMXpeCtvQLR44iMMYnXKZ6trhOA+zZgjn
sjZDQxlOBo7LwJJStuRUDQ8PIxfYIZFS8oEPfKDlt40bN3ac1/dc9dGOydkKH7nxx8wfLrEZiU2F
s/fcxQ/Pz3HlRa9eafGeszhrbD2vP3oNX3jy8/x8w71s3nEpd39pB5e/9tXIh3/CodtuIX3ueRjF
4kqL2kcfJxR79+7l3e9+N29605t41atetej5R49293wsFftmDmARh035XtDhMani4xEgZYhfz8/w
A2aIGbCXgigK4ehU8/olooqPl+w3Aa1V4/cojHOEACp4hFGAX8udafeDCBF1sJUdmSpDukSm4uN7
YaOtuIGgI63FJyAKBX4UEM2WOvqYmtbYVrlD7hk/IEItax6q1QDP95e1I16JfLQA3w8QUdThXZsN
AgJtEPoBEQG+L4hknIPSMs5qFeUHLXPdDi/yiUJJxat2PSd5byb9qZZjVc/HOjJJKCS+HyBF1Bxn
F7lb+g0DhPTx/YBKxWc+PIThjbXIsJDcdVSqPl4gqdY8Zp4Xz5sKA/wwwKv6GG1tRKLJQulVfTwv
IIgCqlW/0Z+SAt8PGkyQDbk9uaBMdZkrZa9xnlf1ifyAqPbdr3h41YCq8FvaqlQ8QhWgl7G+5mte
56VAypCj05XG/axWfMpeQKUqEEY8/va5ApiarSxwLyM8TwNzBFov+x3RaEVGXfMSD0+WGu+Cdmit
COY9wiBAiHh8c9EcfuI5qFZ9BBHaD/BCv+VYCw4exk/ktx0+PI1rpLufu0T0MtCOK0Rw8+bNfO5z
n8P3fXbs2MH/+B//g61b+yQDfZxYzM57/MVNDzB5uMRmqVAy4qxdX2f3aMjal78aRzuLN9LHScOL
ztjMJfZrmY8Eu9b9GN8L+cbXniB3xZVElQoH/vGGZZVz6KOPUx2HDh3ibW97G3/8x3/M6173umek
z3oAWKXaaVxFtY3NKBKUy0urS9kNQSCXFObTDZXqUpS/1kSfhZLhu4UABZHP3FxlScQVyWt6vX4C
X/UM7Xtq7/ICu6JILjvcqBTMNELtukVvRQkGwTq69WFNddLTd21L9A4pXSz4TJUrGLUw06QMi81S
SMBcLQywPtfH8tegKb/ocT87f2xfJ/VwxiBB4HC8HHKlIBnq2dpfr3wi5+BBjNnZrsd69rNE4wog
DGXL5kMQho3nU8/NY/ZYLwsayl7TH2PMLx7Ou1y0E8a0o05iUr/3XtjKsSjq3PSLwGijyA+CUzQH
67rrrmP//v1YlsV/+2//jXQ6zf/8n//zRMnWRx/MV3w+9k8Psv/QHGeaBoQRp+//FlpM8qMXrOEF
q5+/0iL2AbzxRWewZvqlHNUeB0cfZ3qyzH0Hs9hbtjL34APM/vD+lRaxjz66Ys+ePVx77bW87GUv
48CBA1xzzTXs3r1wnaBPfepTTE9P88lPfpKrr76aq6++mnL5xCsd3RB0U5BrIVrVqtkRGnMqIekl
iAgbxkVX9FCWzNlSjS57aWp6EC28095rvirV5ZkBlUrvvK0Fr6uxy7Uz5DVQ+7lOKHKsm1WxfdLb
nFggPQdV7m0Ie8bCgVDVsNIg3SiXbcplh0p4DEZ8Tb5SyW3mJSXQ3chsDkomaP+7XX+sSHpZjbkS
einGRxBgTC7RrXyMEAmmlNn51uesbigfKxZaD88EetnXolFna+k0NKY8tud2KRDRr/jW8sGDi+/c
9PGrCc8P+Ng/PcjPdk1yoWsRlTw2R0+y9vF7+PKlWS55+W9z0arzVlrMPmqYnK3wp//wPUrDP2TT
oTWkpwfZfnaGVXf8NTKVZt2ffQjluos31Ecf9A67ONF4+9vfzrXXXstf/MVfcOutt3LzzTdzxx13
8PnPf/6E9nO8f6tuuuu72Lv3dA/vkhILs4UK+plEVheY9tuJGWL0Cv9yVWrRXetuiJRaAiX38WMp
YWsnEg2mxzaEWiN9vxYyubC/ZSGZbekSETYMuo7+hTgm4y1wbNQyPRpKGA2jC5Y212nHZBLZMGDq
82FJu+eYjgeubbSwAbajLrNtasoLeG8Nw4PQxnsG1mwSIkmOmMAzva7bEUm5aDHjdrTLLFXYRiO/
dEgZF0Ou42WXv5zsUO6Y2qrjpIQIbt26lW3btrX8+/Vf//XjabKPPoB4l/ZTd/yUR3dNcl7BJSp5
rM4HrHn8Hn6+1mL+9PVcMHLOSovZRwL5tMUfvP5c1O5zeTJ7GM8o89CD00xf+psEU5McuvWWlRax
jz46cPToUS699FKiKE6gfsMb3sDsMsN3ngkspFqLMFwx4wqOjYlrMc9ST/xq7wn3RK9RyRphQjcm
v+ViIUPkeDxjzwwiRJJ2vmFsnrxCsScKwUKe2pOEU/UxWa5xdbJxMpmOj4vk4tFHH2189jyPu+66
iwceeOC4herjuY0oivjHbzzGjx87xJlDaeTBErmswaYHb6LqGNxzQYa3bfwPyPZiKn2sONaOZHj3
FWfxlzdHPFXcy/pDq/juvhwvXLWaqX+7h+yvPQ9n0+aVFrOPPhqwbZt9+/YhamF2999/P6Z58sJG
jhWiujBl9EoiScO+VCwnj6oPToAl86s937PzXhfuxpOIJVooSzlrsbrdJwuhYSC9Z964O9k4Vu9V
V5xES/SESWkYBq94xSu47777TlSTfTxH8fXvP8U9P97DugGXzFQFrSVnT34X5ZX5xgUuG8a3cUZx
+dTKfTwzOGP9AL/9iq0cPriKg9k5lG/yr4PnEkaw//p/IPJPXlJpH30sF//1v/5X3vGOd7Bz504u
v/xy/uiP/oj//t//+0qL1QHraGsInmsbPc5cOpRcud1/b0GCi+5QKjjldsCXA8NYvrJr6BOjpi2U
l3J87XZfQ7Z8boSDL+b58zxj2TXSThTmiwMnrW1XHR/z3jONaqYub9u9OIkG1nF5sG6//fbG5yiK
eOyxxzCM43/p9/Hcxf2PHuDme35BIWWwWUomqwEXralg3PMwT2zI8MRah/dvWpwSuY+VxSVnjnJk
usxt334C16mSni9y36Zf4/m/+C5Hvv5Vir/Vv4d9nBo466yzuOWWW9i5cydBELBhw4ZTzoMVRRHC
81qU2eNV2U5W7srJhOOUmZ1LnRRnjFLBSScIWSlFGxYODzwuHNOQfrW9aUl4i9TKgpj58GSgPZ/o
VISSooMCfyVQf3d2SHISRTsuA+t73/tey/dCocDHPvax4xKoj+cuHt8zxd9+6RFMU/Hi8QK7fn6I
LZuyZO/6JF7K4V/OtfiN1ZcykhpeaVH7WAJe+fx1lL2Ar9+3izNVCMFp7CrsJfzibaQvuBBrZNVK
i9jHcxjve9/7Fjz+4Q9/+BmSZHGEXXLClDpxSluvhPhTCVLW6y6dmPYCy0RVmmGXz6SBJYTsqPH1
TMO2y8dF6Q/EC6fnMnxmjMlnzGTtwULyTPHESSkI2wyVbqGHx0pWsnwsbeblMRpYtkxRDpdPghOY
JqpbOHXNDm3f5DiW/NGl4rgMrFPpD1Afv9o4MDnP//nnn+AHIW88bw2PfX8PxeEU6x/9Mr7v841L
shjpLK9Y/5KVFrWPJUIIweteuBEiuOd7uziDiMcGL2Vw9ov85G8+wnnv/98oeepSSvfx7MZFF120
0iIsGVEY1Hm2AVBq8V1rISMM7VGt9vLGLU81XY7iJhCNkDTLaGUAq2QySN8ntEysyaXlbllWBa1P
XGhxYJqUCwVS+/bX2q82DbgloNdc1H83zWrHvCspMAyvZsTF14aGRnrLH9ep5X3svo4WWl31elSn
GgLLRAYhwvc76k71Ynl8puBauoNuXYiQE5jpsyLo9SxpcWzmiZdJoQ53Glj1WoHP5E08LgPrxS9+
cSMxOIk6G9Pdd999PM338RzBXNnjr25+kJmSxxueP8ETP9iDaSkuyu6jeu/j7N+2ip+Ph7x54yv6
RYV/xSCE4HUv2ohSknvv3cnGSPOjNS/heU/cyZdv+Qv+w+v+M1oe12uojz6OCVdccUXj844dO7jv
vvtQSnHJJZewcePGFZSsC4QkCCVCxdqBFCAXMZCUDJZhNCSsty6QMiJlmczML41oQwmNjw9RhGMr
krWLQ1Pjp1xErcCnIU28sHe7QkQYxolRyBuGScdQI5aieWnt1+pRLU/dVlKQcgy8sBT3VlMoA9NE
ej5CCExhLcNoWp5xrIRetlFjW5ryMorbduAZcC35tk3UZel4KRdjrrSstixTUwKqKadrTS0pT6yH
03OcZdWj6lbDrNvzLQUEUTw3K4mG4bTEx6R9s0EeIzNIr+6iXpu5JzF88bhM31e96lVcccUV3Hjj
jdx8881cc801nHvuudxwww1cf/31J0rGPp7F8IOQ/3vrQ+w9XOJl549z9GeH8b2QSy4YoPovtxHl
Mtx6hs/67Fp+bfT8lRa3j2OAEILX/PoG/sPLtnCAiIrMsWPkeaz9tx38/ff/jmrw7GM56uNXB5/5
zGd473vfy4EDB9i9ezfvete7+Od//ueVFqsVov1r/INcgKQijE7cznbURYZFr6kpSAsVt7Wkg2L5
edvGEouDtpN4GNJCic7+hIiwTb0o6Ue9eGvduDV6eBLTupUAIOOYLQZxU35BJOWCnkHVZSc/OaeL
semmVQ4plm8Y6B5zYcvWTc4ooQgr1fRUttz3+lqoyTq3aqTpUViKLF28l6HWVPLZrueHevnjtYxY
Ht/pvombLNy7VHRzQDSwBC90a2NdfpKdMrmOQWmo2HNunil0G7rvdDP6akXSM81aUmnHYKhwjJvp
3Zw+Sras09aDpyjJxbe//W1uvfXWxve3vvWtvOY1r2F8fPy4Bevj2Y8oivj7rzzKo7smOX/zIMX5
gF8cLnHmeaPYX/8HqmHIv1ycIbQi3rzt9X1a9l9xvPi81WRsg3/74g72pzeSLR9m9F9+xCeNkHee
fS22Xtkdtz6em/jCF77ArbfeSjodK8Xvfve7eeMb38hrX/vaFZasCSEkQeSja0TV7bpCt3yaMJAs
ZLvUm1BSEhIsuNMcHUOSvpdysWbnuio8ArBVClMGeOEiGyztlwtwZAqJ7OnxUUKBCFmOp2lJG+ai
40NXWNJEiaij/lFdSVdoPGLXy3yxgHvwcM+2HOVQ9lrdNCphMKVVrmeRZyFiA0wcE7mGwJDW4myP
iYnrZYQIIUipLJVwHq+by2mJkDJqEEZ46VRNyuVDqZCgG9W3EIDAt210+dhCMC1TUakGjeZ66e/t
Py+aB9lloFr5VLBaflMILNuk7DW9WynHYG7+2DYyTa3QWi5YdHmJ4nY1ciSCgNhgjpQia6rjYjft
xmoZKt1zoZwsdk04AcGb9957b+PzPffcQyqVOt4m+3iO4Ivf2cl3f7qP9aNZLp0Y4BePHGBkLMum
Q/dT3beX/eet59FBn5evewmjqZGVFrePE4ALTx/hN19zOh7wWPFCCoeHkQ/s4P888LfMecsL6eij
jxOBXC6H1s29Rtd1T8m/Y7K2W+04861KrYwaim2kNeWRVVhy8c0KQ5qY0kKKpRlQQnQPSeqG+ZER
UiMFMm5s4dlm615uRhdIGwVEF/+W5y68c512FlZ8beliSQdDWh2bcr1G2TQM4jOCHiySHbZemyIY
ZeLx9tIPkwaIEBEIGBhY3lpbyCuiddD2PVaK6+shef8WMihjwwycJVCtRz0MrPqdjaRkfqCARC4Y
ClpHYC3NOxmHwMV91L18ss2j04sWXykf0wh7U+Afo34vBJiqi/dMdHrhMgMSx2rKq7WPYXqNsXTZ
V+jaXy852q/t5glth9fFeyckmMv1tjV6bUVgWfH7q+clC5GmNOG5Dhmd73m87kmVbuzFixaQ/2Ty
gRyXgfWBD3yAD37wg1x88cVcfPHFfPrTn+aDH/zgiZKtj2cx/v0ne7nj359gMGfzlhdu4Hv3PI5l
a16w3WD6m98gGi5y86ZZxtOjXDbxopUWt48TiDO3DPOK124nEvDwyAu58EGLw3t38pc/+hTT1c7Y
9z76OJlYs2YNV155JZ/+9Kf5zGc+wzXXXEM6neYTn/gEn/jEJ1ZaPACqtfwZISOUChs6SFflQEos
uXh4jUDGtYqWoUxaVmVJipqQCttUSARpN8A2VKcyKCRhYbCj/zBh3HgpF8ts9aBoHbJQBJgpLbQw
cFWKjM4S6e7KN4Bh+BiGh5Rhy1zaAx6O0+m5cZSLNbYNPx8rd7apsIymMFbKx3VL6C5TZBhh6xyI
2IvRK8ywMd42N6Qj0/S6afNrV3XNvakbO6aZNHA656MlbK/HulBStoyZHiGC9QbKhTzhcsr3LORJ
7dInxDWZUqk5HKd1k651vDUZhW4YjmnL6m6kLCRDuPAbaQAAIABJREFU2/0yDB/bThj9XdqzrXJD
FlumqOSypI0MGaP5nEYRWGY19jgicKyFn7P6XC8112rILTTCY9tFtAzF/OAA1VxnWKGhe22qLGyZ
1Oc1MOM+I60JLBM/191ojwCRqJG54GtJyN5niObceNkcXsqNww9XIETwuAys7du38+Uvf5mvfvWr
fPOb3+TGG29k7dq1J0q2Pp6l+OnOI3z2a4+SsjXvvvwM7v3azwmCiBe9ZILpz/8dSMlXfy1NqCVv
3vq6PgnCsxCbNw/y0svPIJSKR4Z/gxd9x+Tp2b381Y8/3Tey+nhGsX79ei677DKq1SqlUolLLrmE
888/1fI9E0rsEvJp/FWrCY2lvTfbPSKWVcG2yy3KqePM4zo+SoUYwlowr6qaSTVybQwjJJ0KCMaG
EsNoXhs6ncVKQ60aHqRQKVJmq3JuNbxhCc9JKtdrdEQ1+WPvikDX5s92TSyrgmVVO3UvKTq8QQAS
iZtxE98FTsI7J6RAyjhIqb1J3VNR7Q5TWnExVyFa8saU6E1vErV5TxqU1EJQyXTOdbuHp5qJvWmx
ytnqHUrCTxADJD1Y1Wyzj+QasYRda+vYSCLqcppGtaPPel+mFghBq7HTFRG2dFG5YfJ2jtbRNtt1
VaqrJ7ibd3Mhz6ChJUI0vcymNMmbQ6R0DktauKrdgym6N5RA/RnthfYrh5wiSkoc5SKICVfqYXhS
xmu4V8HoOlLJwua1U5dSbLixJqP4+ZtbNdzY9GgX2ndsDGPx52TJ6aVSUc1kahsHv2I5WHv27OH9
738/e/bs4fOf/zzvete7+NCHPsTq1atPlHx9PMuw+8Asn7ztIYSAd1+xnUe/t5vpyTLnXrwG864v
UJqaZOcLtvBYepLfnHgxE9k1Ky1yHycJW7YNU56r8J27Hmdf6vls/8GDPHzRfv7qR3/D7537DnJW
ZvFG+ujjOPGe97xnpUVYFBLVe784ag89EyA10gB66CpKaCLbQZTnMaTGl34jtyU0NdIRyJJPLU0I
pUKstVlmfz6DEpKMzjMTTCKFIOPEitfBSOK5LpHWuMICykSDeRgScEQjhSaseeJcWxPU2m5XeyIE
5YECEKHny5DIH7K0wjIkHq0KvCkdKrRSvkdRq45qSwcRCUzpIJRCWQUgJnPwwwgp4snSOmB+oEh6
zzwZnWPGb7YbESGFIOpiYsYhYHUDQuDaFUpeiO9rItvEtcvMtOnEdc9ON8IHu+ZlMXVsUDUZDLvN
WufPjjOP59WUYqWJdACLpdEkFpkkXj6uSuOFVco1BkQhRCvzWqJPL5MiO11iruwlfo9I6xwwhasy
zPiTLV2GWmMakiAIgZBQKRSxZ2jOzmHOzKFUSCo1Rz5lsudAoutaH6aW5LMO+46WGuPo5WmNiBCG
RWpoPerQ7sSR2Biv21RKGAQEsdJfboa19SRLaIMQ8RpxLI3X9hwqVGJxtv8fqikXM2PDvjg3Lzb0
mo1o7S+cM9gWo6qVour5SCSmNAGPtGPG67nLWkqrHKVwpqVGlKEkUgpMc55KJZ4kLQwsaVMyI1S5
1eNbb1UKRShky/srAhzpIIWmEpbizQwhiIQgk6oyNeU0WghMEx22bnZEUnV9AjKOiZWxiKZAqpAo
sQYMaSFE1KXY9ynqwbruuut4+9vfjuu6DA4O8spXvpI/+ZM/OVGy9fEsw9GZCh+7+UHmKwFv/63T
8Q6V+OXPDjK6OsfGuUco/fRhvC0T3LH6KOuya/mt9ZettMh9nGScdcEazr9wFWUjg1vdzsCjo+wr
HeBjP/xrpip9T1YfJx+f/exnueiii9i2bRvbtm1j69atbNu2baXFakEYidhAALQwu5IJ1HeYBYL8
aBoz1TvfxTIkkRErSWmdJee2qgKz46PMjQy2/BYUYi+R7KG4VrPNcDyzFqIY5jKwdhSpBBlVDz+K
lkigJghMA0MYmMLCkjaFjNXQ3LTQNe+OThB2iI58L4RAiKgRNimAgVQas65J1y6WEobSkpGhUSKj
ngcjWxgLTWmBEASOE+eKJYo9W1ZSwRQYRtT0EIl2h0TE/GCR0I4JCuphZ7pjYgQpWzNUWHxXP+Oa
rBlON8ajVEjWNVFSAhGRUh3rpp3tUBCHUtpWGUM115Mhm96LpH2lpKiFazXRLeRR1FRNgWh4N9tR
twmqmTSG4WGaFaixLNb7tdvC5lzLYLSQo5CxGXYH21oUDXk6PFFSQgTm8GinHIkINiEirJyH65aQ
KoxzlJaZnyW16liThq0aIaJ1QzDpSPFdhzDtMreqmXuuhESpAKWDRQlZGsaNDVqaXfP2lBQMugPd
ZRaStMqRs3LYZtPrmLINbCN5/wTl4VWUa6GFMblMs69KPtfwYCWZHYUdP3/NWlfNwdeXjy4WCbWm
PNCZa9Urp0oq2cgXUzLASHh+TdtkOKM7WTBPVZr2o0ePcumllwKxtf6GN7yB2S4V5/voY77i85c3
P8jRmQqve9FG1hcc7r37cWzX4JIzLY7ecSsil+XzZ1extc21Z7yxX4T2OYILX3waW9dZlMw8Gw9v
xNi5noPlQ/zv7/9fpirTKy1eH89yfPazn+X2229nx44d7Nixg0cffZQdO3astFgtCIkpzQtmESkE
2XS5haJZCokUgsDQCARSSfK6sKS2HUuTMmzcmvIqopgmWchYVyoWFLlNY5w1vJ1hcyzm5TNbmcva
k6JStbbqr/D8Gk0hHVJXpJKn10dRD3lK2ZpsqubZUZr5zaeTN4qkZazECQRRFO+Mp1S2EaZUyaQ7
qnlFUZIuvtaPobEM1bFzHxHh6hRGLSRdCLDNqOFJAuLwwpqBUM1miRK5RTmdbyiXi+ngkVIEhtEg
Aalf1BGWKCDtSlJu1PJjRIjXRsSihOgoQK2VJJ+uhVtqg0qxGUoZRYKcbWNZHlLFBpyUIa5bQqmA
rNU9/Cvpu1Pt9NcaLG2RdgxkTb2MVJtRJNN0iyyzLU2pOABSUqmFGoaGQbmQw7ctipvHGz3X6b5X
p1ZjKI0QApWg+U7ZulFHybE1hrBQZvM+hm7cvs5mGiF/naQStRA6HddkMrSHNxyvQVMm13+zaHQl
2xl1EYyPIIhajKJ03mr02O4LDRyHSCmsXDx/paEiAEVnAMcp47SFBnajahcIhAG6QMv67Twv+Tlu
x9ASt0a+kU+bLecoIbA3ryGwE+NXGlEzglMqS0blSde82r5t46fTeOk0lUR+l5dJw/BY0zivhw4n
l1K+QHVjEWOVwBsfaCHgyNqrug9Itu5k1DdRVB6sAU1Kx4bdM8VIfVy92LbNvn37Gtbx/fffj9mD
faeP5y48P+ATtz7EUwdmedG547zozFH+5bafEoYRv/HiCWY+97cgBP/yggGmjIA3bX0dg05xpcXu
4xmCEIIXXflrbMqVmDeynL1/PfKpTUx6R/iz73ycw6XJxRvpo49jxMaNGxkcbN/9PpUR5/mYiVwF
gSAYLFDJZtAi/hvsqgxFY3H2Va0EY8UUpq6H3YREUYSQ4LpzFNI+xtgwSjWNgchxaYTwjA0RjA61
tFnIxYqNUfOMCSmQIg4pEoBrW82d8Zp+WPeujOTybCquiQ0DAVLVDZ6m4lRXKWPvlcCyRcPrlFTS
lrs3rdtC9dJ2RJJoTogIJTu5D7UwsJXbZrTFn13baBFqfnCAanGY0QG3YdQSQdqtkkoQa8ia0hmG
ELbliUihqGbSbYp+bAi05IQJgVaS9NgYxghUB7KxV1CaZHUBKcA0PKQTh6HZNaNNCoGZGwDDihXo
hPxSCkxho0RMXJIdSKPyGdS6IfJGkZyVZSw9jKEgpTIUrNY1KNJ5kGBarfGKhpQNgpNKLs/UxgnE
KhM1ZuJtWIvMdrItFjLx+LUCa9vWlplPOwaDOSfxm6xPNcHAUG1+WtvLpeI1F69vaueLxrnSir1m
LdTeOZPZsVWUCwUCy+yan6hqns4WxV6ItgUanxONxl6lVEERO3ia1+ou3mM7XaHgtpGIJEIEF8qX
NFTnsUhrLKOVVbPl+ECayKmPI2JiuNWoDNKZmte0KYufTjcK/daY8BGW3TCsGr3UKisYKn5PiJq9
ZIzb+Al2Ua0XI/Fpym1rG+WAlbjGFE2Ppud15lqeKBxXDtb73vc+3vGOd7Br1y4uv/xypqam+Ku/
+qsTJVsfzwIEYcin7vgpO548yrmbB3njSzbxlX96iJnpCuc/bw3qy9dTPXqUx5+/gUeys7xk7a9z
/sjZKy12H88whBC85D++DD56E78Qqznv4CYe1Razoz/lA9/+OH9w/jtZNzi0eEN99LFMXH311bzq
Va/i7LPPRiVIAj784Q+voFStCFpSXuo73xGh1ki/FgqYduDoPCmVqZ/YgkgpfNvEmJsnJA6nqcNW
Jsi47pEIq7XCwqKpgEpB1KIc1j6bBnQh0xgYTDEfGiizVQhHpkHlSBspfLNCuRrEYVI6aBhYw04R
oU02F9fymDqEK+sKXKL/NstJSIgi2RJmVSOuR1WroLuQDzgGSJuQplGTTZkc9pqNpB2DydlYATOU
RAiaRlEXrDLGCQwLIQRSCARxyFJSIQ+1QWy1xe3mrRyVVAUR7sMw2va8azq4peK8mebPgiFzFC2r
pO2I6UTgkGtrpmppY5ayEUIwvnaQJ6f2EaGoDAxgRC4KD7wZDGFydP0wxXIGq2hReugJAMzhYfyq
BVFIIRsws2OSUmGA0eII0Z4pRHUeYR/BHjM5JPJYmRz50AH2ghCk7BBvThMJAyFgIB1yZFaCNoEK
gWHAfFgLK21uFqR1Fk+WiFTTAwoQpV0i18aypqkMF2E+NvYmhpuGk795gumZkOycwNCy1YCQ9RxD
s3E+xCGRddY514WjGgLbhpn5hmMFwBSxMWebiiqSgADthJRXDRJIg2gWEJJoqIAnZrDm5hu9pGyD
gi6ytyxbwmMbZoxohgiaeU1xwsWxO8PnfMeBUmvovBRgthGoCCCtsqyyRyjrn9ELg1nF/tk4L0ml
Imw/TWZsGI7simWRmlLC/shlKviGwEgFpCoRWScLqpVJE22CB0btN6dWwLukApKRmq32ZYSWirSv
WZMe4+ly855CvAk0MJhhujRPYFu4ANqAGulgJGtEIoOFRoxhNZXCADKqgG0qHNm9HELpGOuDLQXH
ZWAdPnyYW265hZ07dxIEARs2bOh7sPpoIKwVEv7xY4fYNlHgnZefwff+9Zc8vWuSdZuLrPnlvzLz
2M+Z3LaaL03MsKWwmcs3vGKlxe5jhSC15jfe+VuIj36ex9Lb2fT0BAe1w56h+/nIDz7JG9ddw6Xb
1q+0mH08y/DBD36QV73qVYyPj6+0KD0R1f4jAIkGERsF1VwW+/AhTC2oSmphUd13rE1DMe+6GHPz
eF6AdFKYpkQFc5iGYjgzwt75STwxHYfWtV1fDwNyzIgZJ0U1kyE33Iz1MkbA218/mQ7jCmqekS4U
8oW0oOy37pobyiBrFBqeq0Zr40NEB6PmqbV5ibrUvEoaXElOgfWrbX6uFVJr5oM2OvbERUpKIgKU
MMi7DmuGXbyCw+N72y6pjw8BNaa8lMwQqvkW4xhAD4DhK7QZ4s1HDDgFDucEZUtiTD6BVhI/CFtu
47Bb5DHmWuZnKOcQTJoI0c4mFx/P6QI5K0OlGnsQR8zV7K/uBgWR0IQjY/DEw2R1gchZg5vKkM/N
8CTtzUmibIbZVSMM5xzcVIqUD07Zo6QhyEjSUhHMyRZSFSHAMqK6Dky4eh384mlCxwVmmB8aACqx
57E017hOojC1ooKPoSVaSobyTpz7tnkCL11BhmUog5syETOicQ+GxwZwJqsET841Qj0lgmzaYm9l
vnGzBE0mREMYWM4UgwVJlNM8LVuN8brnJeuswjNWkbemmJQwj8fM+CjaoG4rx+e7NtVUQKYUknYD
orREz0QUnRSeVwIi/LprpgcyrklUX4d1gggpqeQLWKUl5iYLgRSKDaM203NR19eCkpJt41n2zx3k
iOtgzJhobSeakJjKbHh55GnjGEoymLPYYDocmZEY2qGYSSGcLEHCVjGVZChv4xgmESHZrORQab5+
CwjD5vNeyGjmUFjKxlQmGSfCA+qpf4M5h+kjVeyxIvOVeEX5w+OIg/NE8zNESlJeXSRtxyGNwZZ1
lOd9Ul78zqmHEQfZAcJIwNS+hpzpdu/fCcRxhQh+5CMfwTAMNm/ezNatWxc1rsIw5LrrruPKK6/k
6quv5sknWx/lL33pS7z+9a/nqquu4rrrriMMl0dr2sepgyiKuPGux7j34biQ8HtecyaP7zjIQ/fv
oTDocn7qaWa+822qo4N8/swKI6lh/p/tb+nnXT3HofMFLrn25Zx54FuEnk/hiWFOP/DrYM7x+V9+
lr+/68d4fv+90MeJg2mavOc97+GKK65o+XcqQSZCbhphYQmFyTQk6cH2/dI275GlSFm6SbcsJNbq
EdYOKaQQKNeNi8H2KPRrGYpCxiZlRyAkQTqFSlCot9g3CyUhRVFHgdcWCvOWIsrJnwWr0sNs23AB
W37jQvSadY1csNDQRIZqIVhoN7DqMFSc73PupiGGCs1itXWEdhweFknZoLK2RzYxkkmTcWIFrmfY
VaJDKRRpXcs7SbLzWWDm25U6QWDFBB6pVAnbLjdCBAEsw8A0Yzp5Q8WN2ZbGNNrY1ESTAMDUaaxa
sptp6oaSaRRhdE0ao54nJyQpleWMdcVaOGM31MPU4s+mVowMOA2jGzpZ+wYydguZXZjNM7h1M/ms
g5aCoWIa33VbcriG8jarh5pGu6kkL9y8nWwi/6ZoxPk3xgiMTzTzDAVxGKptaorOAG4iJMwyFdRz
wWpzUl8rGauz9lNrnGnE5Pq1+Ou2YCmXjJlCALayG2cJFZNjJJ0kpo5YlcmyajyHW6t3JaRACBGv
WyEa3ixR2xhRQnHumtZNxEhKKoOD2GsnGMgsXt9O6bjeWn24AxnVJI5oU68EkDZTC5JmNMJTpSCq
0finHYNs7T2hpWIiN85QpjmPthUbZEoKdE2nSzua0WJzgupPvGX4hAO5mpc3frIGc5INW4doXVIC
19IUs7EBKCSkzDicMlQalSh1VT+nHcHYWsJMDq8wgM5DKufgDvQq8XD8OC4Da82aNbzvfe/jpptu
4vbbb2/864W77rqLarXKF77wBf7wD/+QP//zP28cK5fL/OVf/iXXX389N910E7Ozs9xzzz3HI14f
K4Qoirjl3x7n7h/uZnwwxR+84WxmjpT41td+hmlpXnimweQ/30SYdvncxeC4GX737LeTMhavGt/H
sx/Oxk2c+doXc/6er2KFZeTONGfvvAxD+3y/egd/duO/c+BoafGG+uhjCXj+85/Pn//5n3Pvvffy
gx/8oPHvVEIm47Bqy3qidKZFue+W3t5ynZvI0ZGSdNrCUBLfav5e9xCpTBZ7zVpKQ8U4VK2L1lXM
2rjW4ju+3fS1enO5lEWukFAUpWztKkrWFGptydUurmEzsbbAeRdPEOQHCfNFJk6bYHS9TWZdnsFs
07tV352WiXZMQyGkwjJVy1z6roNcu45gZIy0ysLE5kboU2RamKvX4GzaXKN/F6CgONQsjNoue13+
YHy4czLaLD9TyzhPrZZfp7vUApIyIpWaiw0sI65FZq5dhz+xhagwRDgcGxtRNoWXcvHWn8bgaI6B
nGZotJknU8zbbFgXK5XVzWdS3bwdiMMja9O2JCTPM5SJK1uZK+r5Ue0nD+UdxodSOKZCSdE0sLRE
K4mRDKkUkDUzZMxm26trCvFCRoFrOA22Qqum7Is1Obx0CnesyPhgmjUjcZujAylGCjXdo+phGwoz
rwCBaQQIojiMUdc3JgSOZVBwswyaMQthNmWic2Bkm0JZZsBg3mZbcQurUnEemj+ymuqWM8GMSS7S
tqKQNuN5iGDUGWN1sZM1L5NLIbRqkEdIIZnIj2LqRF5TDdl0FXtAYmRruYtS0E6s2TCYExZAYMX3
v56LWWcWVarzLSMXqU2acmJXVs7KMZTIp0/bmoxbc8REdQ+iBVGdir7p2WsnbOk2gjCbJ8zlCdak
SadaZWq8b9JNY2vdqiyjRZfQcSitGaH4gjVkhjrn+0ThmEIE9+/fz8jICIVC/EA/+OCDLcdf/epX
d73uhz/8IS94wQsAOOecc3j44Ycbx0zT5KabbsKp7VT4vo9lWV3b6ePURRRF/PO//ZKv3reLkQGX
P7zqHMKKz1dveZggiHjxpQPMXv9/iJTklktsvIzD7511LYNOd7rQPp6byF1yKev278P+2m08uu43
OXhwgDNmXsIv197PweI3+dPPzXPtZedywdYuyksffSwDjzzyCAA//elPG78JIbj++utXSqSucAp5
js5VoRLv80Z0Kpnt3owOSEmwdhXVKY/6XrLO5/EnJ5HaYGjEpmpOk0prZg7NdE2CryOnCxiqSkq7
TFam0MKgumihpU54hQJBMcQ/NAWzNLfe6TSw2iGUInQzGIZkwLGQ/hyq1MzxT9kGucBiIB+RlSkm
ZwJsU2NNTNRbaLQVCYEcGISDswyZY2QzDkz/pHFcpjMYxSJq7igCiZYx/buQArueXNJmOFkazjtt
O5UjT3JoV7y+RszVVMPWsL6RAZeSXcY4qihV/ERdrE7l1h+bwC+LmGlcaCItcTNjDAyoOFxQSlID
68EwSZ9+BqkoapnHlKObrarOiJHhgttSe0yIhYkSAIbtQSZLcbzmcF5SrkZoJShmbbKDKZ4+Mtdy
vpQKbUUoJYhMjUgN1Bgw68NOhmm2yjiYS0FbiGYHEtdnUh5rs5LdjkE1ncayTYYKDrPTFearAaYy
yKeGOFqZolqpks9YHA0iBjIRpTBgLuqco1zKxHEMpurFetunpzbfZrvLKB587Zy4aLbjmDw9G/9Q
v02b8hvZNfNU45KUY5ArOMzO1oweNOOFAhUxxVxbNQYpwMwoqo1Iwu73zjJDdO35DseGodqs2zU0
UM8BEzw1HTJbElim37WdRnvjqwnkNMJvvgOKTqEmgUTU6sy1bqaAqWzG0qOUPAtE9/DHiIgBu8Dh
+cO4RoqSFhiOQXmmSnrVAP5AheG8w9S032hXpmLP78jqLAem4+dt1YBLdbLMLw8C8jg9TEvAMRlY
73znO7ntttv48Ic/zGc+8xne9ra3Lem62dlZ0unmToRSCt/30VojpWwwOd1www2USiUuueSSYxGv
jxVCFEXc+q1f8pX7nmSk4PBf3ngutpLcduODlOaqXHzRMOKf/pqgWuZrl+Q5POzynrPf3i8m3EdX
FK94Lf70FGf++xfZe9pL+Fl1Let+fiGTxT08PfEDPvnlgJfv3cJrX7ihlbWojz6WgRtuuGGlRVgi
IkhnCMwxxOQRIFa8QiGwTFmjauitCAtJk7wiybK1YQORHyCemEYR7zIrKZgYSWPOdoYk1RUkQ5ic
OTHB5JESWSvLvmieXTyFo9xFFXKAfNpicrZKaFkEo0PM2Ck8GSESO9dSL/Jc13KwGvkctdyxQjrE
1KJB2x7vyCvCkQlG1w0iu6QzVIaLHVILIOOEBKaiMBibpIaSjA2kmBNx4d4hZ5B983N0q+osBAyN
ZDmSGYeageWqNJWwjDLAq6UF2aYiW3Q5+mRN4a0r5ok0CSkEYRQRWQ5Uq/Wh1+6nxNUO88SKZJ1J
Mj7ceS+ibrGT1IrwAjJhWKcHY++Ojms/x/e2zWirv38l4FoC14qPubaBZ3eqmduKm9jPXibOCJmd
8bHKFuX5ZuXpsGYC5o3WjVdDdXpP7Q0bEEoztwBnpEilEG1GIwkvWcpIkTJS7J3bz+FaupJQIr6l
C1BRGrpeK2zxvz+mIVsMv4jYa9XtSclZGc60TudBcYiAENOVDI9mObLLwNKKnNu8v75lQrmVCa91
o0V0/gaYZuKatoOubTSYDw0dUsj5SBklUs06pZaOTWRWEL4XMy4mPF95K4tvTrZcWZ8KS0eYyqT5
pHZHzsqQMVKYpmZ/KmB+ulKTTzJcCz2sPxMRtRDCgoPR5r5bvS7PtDXAoerCBuOJwDEZWMmH8847
71yygZVOp5mba+5khGGI1rrl+0c+8hGeeOIJPv7xjy+6e9XHqYO6cfXl79aMqzedR8bW3PmFnzB5
uMSZZw2R//rf4c/McM+FGXauT/Ous65lY37dSovexykKIQQjV/82wcwMYw/ezfDpF/FI4ULYP056
apC944/z9R9F7Nw7zTsv394Icemjj+Xg/vvv5+/+7u8olUpEUUQYhjz99NN885vfXGnROiCEICyO
wGQc/eFYGp0yMQ0Pv0YqYSQU2mRdWccJafUjxJTT0jDBAKjVnKv92VVadag71uo1CHkUjsbfZW3r
XkuFQ4oBYwhVK+4phOihyMfKeUselgCEYKCtvo2uUblHUiHCTjrl2JOXYOTwPKIoZq1TCpLpmrYp
2HRaEeUkqbvjMYRCg92Zt6GVRCvIDqcxEgVXLVNTDmISgdahtRpZjbC7tg0gSyucgkIaokmQkEz7
qZ2fSzWjeIbyNtW1Y8xNxoMVSeq5ZaCXcVU72i5KQw+TJtCjdnUuZeLgkJcm4eMHgdgzauVcvMSU
ZPIOVAOyWZdqJcdcdY6sK6nUHHqCOAxUK4lPHM65GMyhWhRDZarr8WDDaqw1m1k/dZBoap4wCttH
mBh9VJufhDFG05hMenSzrsR2bFZHGxhJO/xwtukBt02N0xZKaxma08YK7Kg7aer32BUoFaKN+Q7Z
68vJr8QflONQyFjYuvm3rlzIEVTLMNX0/iRVZ9Eo1CzwV68nOPIUujxF1L52F0B7mGA9hHMkL/FT
JqWEG63O3zGQqzC+Ns/e3dOcsW0twljLgwceasgWASk7xDSa76koKXc3I05KBkcy7N8zmTivO+o5
du3HpZQM5h3KMyeP3KLR17Fc1FKPYhkP+Hnnnce3vvUtAB544AG2bNnScvy6666jUqnwyU9+shEq
2MepjyiKuO3bT/Dl7z7JcM24yjoGX7/tp+ySG27DAAAgAElEQVTbPcXGzQOM3/eP+IcPc+9ZKX55
epHfO+d32FLYuNKi93GKQyjF6Dt/F3f7WehHvs9FR/+Ni1+wFiMyWfPk6Wyzqzy5dx9/+g8/4PGn
u/+B7aOPhfD+97+fl770pQRBwJvf/GYmJiZ46UtfutJi9UTHxmMU56loSzAyNkS6WP/bKSjmTbIp
k3xGN4yh2iUAjYK+SQzUwnps0blhYY2PI0ZXY1iKKAKzjbK8blxJIZjIru0qfxR26gyi4QHprvSN
XvJrrBupJdEnQghTAw5KCzKFpiHS1Ek6VS/RzjR4znZSm8aRW9YymLNbNNOzNg+2KPj1eS8OpxnM
51mzdpjVmbG2wbV+zaetxrX+lgmmJlYDsGl1DikFTi7ZfqtstqFwkvObz6IKWQazDoaSSClaPDF1
pVcZAqvLfU2KGC3klukCkdSKuybYwerhNHZivtzTtmIPxvk3qmZMF4ZSbNo2jO4WOgekzTRCQCFr
UszazdyoNhT0IANGa3h41OIdSvyeifOXCimnkXentGSg5pHMZ1SijRBR9/AmxiyFYGIkw5Y1TVKN
kQHNeZuHOW/TaCOMUQhBqqgYGrJZVexMcUnWKGvmGglymSpBzsHc2KoTK0fU5Iq/Twyswz3nHIbc
RJ1QKfGzTYKJpti1i2qGhk6nY7dOF9OlngcX1r2wSWO/21qpvUscWzAy3koSUi8sHTkWbtpi49Yh
TEu3mKyubHqcALQRE8qknNZ3Tt0z2itCRau2/M3El7WrMpy/Zbjrel3+tsSx4bho2mHxGOkkLrvs
Mr7zne9w1VVXEUURH/rQh7jzzjsplUps376dW265hQsuuIC3vvWtAFxzzTVcdtllxytiHycRURRx
+7ef4Ev37mQ4H4cFZt3YuNr1yyOsmciy8aGb8Z7ew4+3ODx+/jj/+Zy3N5I+++hjMUjDZOzd/4m9
f/0J5n7yIEO+xxve8jt88Rs/gb0FziDiYDjHRz/3I668bAsvPGes7/3uY8mwbZvXvva17Nmzh2w2
y//6X/+L17zmNSstVgcW2st0tcMFq87FGwzw/BDXNijpKfyjRylkqygVkU6PEhop5rymH6vbLvH6
/FqkmuBnux+KKbbbwvSEEKSLLoEfYnUJ/yqkLWxLkXcyONpmtxfX1GF4FKO0l9C0OgyzxZDPONgX
XUBUriCNpvFg2JrcqgzalFACoqY6WJfaNV1Sde9T23tBSIkcKdKtzHQhY3MEWDOcJliVbRg7hqFY
t34IGKJ0uMRIvsJ8ue6m6X6TTGmCYyNrhAsNW7fne6rz92AiNubOOG2YgZTFfKlaayI2tOpsb/k1
Bv6RBTaol6FdFjIWgwWXw3OVBUklups2kBtwCaOIg37QONzwiNUMSmWIuHhxWjJsDRIGEQLBQMZC
d8kRA8gbvQuDm47AMhX5nMQjx1GpsZTFsDvImvWzZKI8qbSJEILNp49QevQIfo2pP4xCMq5BStrw
1HTNOxqRTZmMFlNMHmklV7JMhYVitJgifdRgIGNhGQo7B3K2i3BJCBGHDRLnxZU3TrB+fWu6RHtO
mqlMtoxsZXrnZMt5UdtVLTZHzTgxdDKkU7QYTsX0IKOpMsGMg2vrFk9lt/dONJDHMF3M0bGWvtcM
p/EDlyC3hshuNTDjwtSSrM4RtZFkKClZP5qhGk5Tr/cmhGD1UIqZkhd7gg91yrFuVYbds9OdB4if
f0PLhqc8WaurzjCZs04egyAco4H12GOP8ZKXvASICS/qn6NaXO7dd9/d9TopJR/4wAdaftu4senF
ePTRR49FnD5WCHXP1Zfu3clQ3ua/vOlccq7JN+54hCd/cZjxNRk2PPJPBHv38dAmm4OXncefbH8T
aaN7wbc++ugFaRiMvus97Pv//obZH95P8Kn/lzf//h/y1Z8/zC/vm2a4nGZARHzt6z/jF3smueY3
ty4pvKSPPizLYnJykvXr1/Pggw/yvOc9j1LpFGWprOlIYULrkVJh12rXGFph6OZuevxbfO74ttOY
3ncQ5WgOTtaV8y5dCIEUknR+kNK6MSLXaTlRCBBSNML32jGQbSpWSfY+MTKGmRsiZSiyeZsDe5uK
0VL2Q+JQxu5hwC1KYBTFu+i1RtNGimE30/U6M5HTc9rAZma66GqWockO9Ga4dSzRrdZyC1zDYXNh
I5XDcQfJUKihVRmqFZ8y9RCxhS0gpSWr1xUIgpDZuSrTu2Nl25QGgVAMp4fYdaTzulzKgvlY6WwP
g2ucY2aYJPaIVoHBnM3akTSjUZrwcIbZ3UH3/Lr6DWgTXUpBcSiNPNhpbQw6RWa9EiPuEE/lnmR2
JkJMxVk0deVfdlkYvZZK/RohBavHXSpPCaDAxPCZtbYk20Y29bg6ITMwkLeZqvW9Km9j5Noq5LbB
UJLRtjWi5cIhaOtGc9jegVqfgkLaQrcx5xXdPPtLh3GXwbDsGg6TQmDUPNA6X8DcP4tnu5yzcYin
H5vk0O4pZCKAbTy7mrFsmidLXRZODXWKdgCUxNkce9t8vxm6a5sKUEylO+UVQnD+yNn84OgBoNVA
TLmK6TIM5DVETZIMU0uKWatjw/T8LUNMHp1n8uAc+VSGkN7W7FDewfNCBvPNe5izspw2sPmkM1cf
k4H19a9//UTL0cevGKIo4uZ7Hudr398Ve67edC5pS/OVWx5i986jDI86jP/kevThKX6y2cV5/av5
3fUvbexa9dHHciENg9F3/C4Hb/pHJr95F099+M+47F3/ie9dsY9v/vuDjDy9mYlAM/XwAT66e5r/
eNU5cYHKPvpYAL/927/NH/zBH/Dxj3+c173uddx5551s3759pcVaFErGCmXW6l7zpR1WLkdRznJ4
fp5cyoQKFHOdYUx1ZWY8PcrcUInZ6iw6sePcTeltub4lGKh111xIgWkbLQqTXWyr1NqtzW6GYOJz
3sqSNtMMpMc4yIGY+KGbmG3b8avcYQSCQaeIqQxmRXuW2olDzsqiRKvhLoB8TTHfdaSp3EZLMDiV
ktiJulVSSs5atQ1pCHaxr+P8rWvzzFuDhHi1kLZOQy5n5VibXY2Wmv2J39OOgakl2TGBmOxCmlEP
T8xmUZkM5khrhMrqwTT7jpRa8mSlkGzIxWyORV3ADX38qWaLLe0nvtY9El0Z+o4TQ84QT0vNWGqE
KfFko596CF1vh2PrgY359SjdzWHQHMhgzmFukWfpvHXr2G8XMHWrsSakJErWiU1M0LA7iJWd4Om5
+Jkyx8YZTWepSItU2mTuqTSz2mbQaW46SK1RXQhE6jKH48OEhe7eHq0VmZxNueQ1ChIvhGQo5+hA
iqwLWks2nz7C3COHCLoQCW4b2sTPdh5ufDe0YnAwhaEk69JFfj7zOPNeidSgYvaAj5VJEOUIwerh
dEebSer/k4VjMrBO5Yr3fZx8hFHEjd94jLt/tJvRossfXXUuthLcedODHNg7Q3rYY+zBG8lMV3hs
+yAXvu29fabAPk4IhJQMvfHN6GKRQ7f8E7v/94c49y1vxfqNC7jpoS8ytnsbmYOjuJMV/v5vvscl
L97IJRf2114fvfGKV7yCl7/85QghuPXWW9m5cydbt25dabE6EEVRi1FhW8H/z96dR8lV1gkf/957
a6/q6up9704nnX0hCWEnILswIKIsIRwYhUEcFcYFD4KD4hhR5nXg1fGVEZ0ZFZRFEFBBlkAwEkjI
vu9J73t3dde+3OX9o9KV7vSeVHdVh+dzTk66q2/d+6tbt+o+v2dFzZXJ0YdbeDuxdZGzgIgWxSSb
yLfn0RXuptSdT5bFNWBh2aoZecRjWnKslizJzPRMpyHQRJGj4PheRyv89x+o3q9CrazAxf4GL8V5
x2uNs50W8IDGwGnLx0uRFebkziTcc4R2o31AGIo7C5LTexiDnlfqGjixxngMHoM+1q7JQ7RSmUxg
kolmZ0MkmtxV3x5LXSUE42NrWa0udg9o5YRE4jwvv4b2UCeF9nzq6Bj0PNlqxVFajik7G5oTrZxK
vy6iJsvQr6/vWJIs45w3f9DfywtdlBU4h+26Pa9wJh2SnyMnxNSXoOfaPHSFuyhyFGC3mphXlTtw
fBonNd/HsYP0S/ZNVhYfa/FSS6vQmv1oxfmjTlZw4qvKsXkYuuPaCE8agizJg5IrAMeChQR3bMd6
bLILSZJwOEIYRqL12WPJpkVKzEQjyTIWTzZ9qa3F6cBtzcKek4u1wI3q7UY+NueB3WFOfAcci022
2zHCJvQcd3IsFzBgEWeA4rJsOtsCeLuCx+KRsZuGr/hxemwEeyJ4XIlFkI+P8TuWqDsHtizlOXKA
rgGPSZKUrJxwmOyE4yGsLhmrK3MmuzrlMVjCx4tuGPz2jf2s3d5MWYGT+1csIR6M8sKLOwj54sSy
Gpm/ZQ2uiEbggjO46o57MSniMhNSR5Ikcq+6GmtZOS1PPUnbr/+bGZdexu0XX89vLC9iL66jpvE8
sryw/Z1D7N/ewmdvXEC2RyxkLQy0Zs0aampqqKioYPXq1bz44ovMnTuXWbNmIWfw1P99a2DZsxUs
rnLsWcOPae2bghoStbZLi87gcPfgwrXFaho8aYWsMO2EySrGMgV7cp+ymVy7B5eeTY7LyjlziwYU
sgtz7JRPK2Jjcy955iJOfeLk4yNLJEmiIFvGNLsapS4xw9tJF8LHKM9twa+aiIw2BXQyjv5dLyXU
+TMJt/mx1DUNesp4EsGiYbo02kw2Kt3lIz7XVjUNgAprHE3TB01IIZ/k6kHjGRebGEYQPTZ9d6L1
b3HhwmRL6tAzxhpD/niyDJudcEUlbstwFRjH9e+ZM8NTDYBkNqM4Br8PZ8zIRzeM5LpQIxnulPU9
XuYswWurJBKsRZb7Jigf+VzPWjaDA4pCwYwiFJOCtV+DSVlVYhIP/0eHAbBXTydrWg7R0PHkJt+R
R5mzZNB+Y9HENW+xmlhauGjEGKwuC1aXBVPAO+REeVK/pHKoqfkHbT/Eofreh3TK3DuIkHF03eB/
X9vL2u3NVBQ6WXldIW9vWcsLv/6IkC9O3LWXy3e8gzOq47nlFpZ+/msiuRImjHPBQir/9REsZeX0
vPsOhb9+nS+VfhrNFWJHzV/JPkclLstEO0P87hcf8be3DxKNTPzaF8LU8N///d/87Gc/IxqNsm/f
Pu6//34uu+wyQqEQjz32WLrDG1JfUcRlOt69RS7Ix5xfMPQThnCq3bT7z0Z4YsHGYjchSWAyHx8H
Nq9wJmWukuTvfYrLsikqdWM321mcfwZuUw6pkCywSeC0SRQUZ2EtLUMymZCtg7tEppIiS8y5Yjnx
6lFaQJNdzobqbpf4+1gS2fHOBtifaZQKBJvdjNM1VBdSEq1tJGaKO9U4hlLqLGa6Zxq5Nk/yMZM8
clnipCPo9x6Y84efPGMk/T9TOcdizlp6Jo45cwdta7eacNrMyFYr5ry8QX/vr6/1UFGGea+kxFIB
DtmFWbLiMR3b3wiXjsNppWx2KcoQXSwTE6YMfHJlVjmzc2cyL2825VllTHNXDpn09K1FpyiD99Ff
rvt4y5bp2PeW7Bp6jOSSwkUszJ83/Ivpi/uEFyzLSvJ9SCdR+hXGRNV0nnxtMzva9pEzv5eIy8tL
f6kgr30ahqxS7N7D/K0fIVuslHzhn3GdsTjdIQsfA5bCQiof/FfanvkN/vUfYvrp/3Lvp6/hKdsO
1sXf4pxrzqZ3YwV6W4g9m5s4sLOVcy6qZv6S0uFvWsLHwquvvsrzzz+P3W7nxz/+MZdeeik33XQT
hmFwzTXXpDu8AfpmjFOjKiaXGU/hdOwEaLc5yLcPU0jrV+ZwzBu6kDKe1qjkc/o9RVb6FQA1DWeO
nbKqHOyO0Wuds7LHNnbsxGOOyDCwW03E4wyYMtxaUYG1YuK7CitFJchWK8Ywk3EcN0I6MMSfJmJW
1HnTcmnvCdHaPb4JXSRkTNkerLludP1YopXipkFFNpFrc4++4QCnFoNkNmObPnDpGLvsBPwU2PsS
r9S+D6acHOJdXcP+PSvbRjymjfpZkSWFHPPx7wGXzYzNbKIo99THIMuSnByvNNJkG33XwGjX6rTi
LDp7ExO62KZNw1JchOIYeuKzvunvR+O2ZNER6qTAkY9m6BQ7xl7hNJFEgiUMSzd06nwN7OzYy9+O
bCPi6sbiAoJZlO1cijnkxO6WWBbdj2XLRhSPh7J7v5rsYiAIk0G22Sj5p3twLjqD9md+S+z5l/nC
ogW8tMjKhvaPmLawhRkzL2XLunaKYyrrVh9i56ZGzlpeTc3cgozuCiZMHEmSkustbtiwgZUrVyYf
zzTHG2WOjY+qnE5RURZjWexCttkwZY23sDq8/mdHUWQqp+diMiu0729HkiUcKVrwuy9pg/F17Sv0
2MkyFBzS6APuTzTe997kyUHz+1ELS9Fz8lFyx9YK57Fm0xZsp9hROPxG/UKxVqe+u5PDZmJasXv8
CZaUmKzEWlqK1N6JoWspa8E6lY/ewGtk/PHIZvOg99+huFhUUJnsqniyHLPnEG1uwpw3dAvZtOwK
HAWDJ9aRJIm8ISZo6C/PbcPrPf67vWYmsiyxeObJtcYNMMw0+UMZa4LVf6ZESZYHJFeyxYpGAMky
vpbmHJuHBfnzsCqWjPr+FgmWMIBhGNT6Gtjcto0t7dvpjSWmdDFkCXukkLM4k569EoYOc6bZKN/8
Ioa3E8e8+RT/0z2Y3Km7kQvCeLjPPhd7zUxa/+dXhHfs4vpDDg6cX81rRj2dlhe55rOf5u3VIUw9
EYyeCO/8eS+b3q9l6XmVzJxfJFq0PmYURcHn8xEKhdi7dy8XXHABAE1NTZhMmXVrTJYZTqbskOLW
hf5dBAGsttFbq0Z1wuuy2kyUVnhoP5gYKzaelyBJEi6bGT06+hiXE1lOWDPHPmMGeiw2/PYlJZg8
2ehHfQOzg1HepyyLiyWFiwbU0PcvxOebi7FrOrLFQuE5C7Bk0Gyo/Vs9bYqVkB5CkdK/JEaquyn2
GWtyVZ09DdUYugu6yePB5Bm+y5osKQOWDBiPmeUetjU19y0RPGq3w7FwLlyI5vcPOYZsOEbfGsXy
6F9SFQWuQZOUAFinTUO227AUDx7jlTzOMF8GNtPEdv89GZl1FxHSJqyGWd+ymbVNH9AeSqzo5jDZ
cQSr8TZ7mGevpiyu0N0RxO4wscRSj331WxiKQt6nP0PuNdcmF7QThHQx5+ZR/vVv0vu39+h86QVq
Vu/mi1UlvLDIzwvx33H+xecQq5/Nhq3tlCJBT5g1r+9n0/u1LDmvkjkLSwbMmiWcvr7whS/w6U9/
GlVVufHGGyksLOT111/niSee4Mtf/vKIz9V1nUceeYT9+/djsVhYtWoVVVVVExZrslZ24GRboz1p
YmM5wfQSN9H4+JOaoThd1uQ4LhhH4TnZ1Hdyrz0ny0p1sRtPVqKwNtrYNkmSEjXwkr8v0DE7sftT
gT0PCTB8KpJci9MWprDYgnuE5GqiJ+3oz2VxEdfjVFTlJgvRNZ5qOsJdFDtHaIkbh8l8PaOpKsoa
8lpXXC7U/k1Gx+TZUzOGcGwmtpVGcTiH7bY3nORMkmMIraxg6FY52WzGWn76zPorEqyPua6wl7fr
32ND62ZiWgyTbGJZ0WLmZM3nz28G6eiKsMzjwGiO0A1U52lU7n4VU8CLtbKK4s/fhbWictTjCMJk
kWQZzyWX4ly8hPbf/Ra2beX2RoV983NYE/+QrKwDfOraK1izNkqTL8oMqxk5GGPtmwfZ+Pda5p5R
wrzFpeMaIyJMPZ/85CdZsmQJXq83OS270+lk1apVnHPOOSM+d/Xq1cRiMZ5//nm2bdvGj370I558
8skJi7WvoHcyY6ZSbbgCVGFOCmfplEb8dXinWECXJGnYGfgmmizJFDoKaKID7dhJnoxG9TNm5A9q
lRzKnNyZiaUC+l0AFsWSnMAk3UzHElaTbAJ1/BfCiS0jJXlDJxjj7b42opOsCJBtNizFxYnp9DOE
w2nB1xMe0/jLjwuRYH1MdYW9vFX3Lh+2bEIzNHKsHq6uuozzS8+mp9fg/z6/DUsgxpmKgtETIdtu
MLv9A7IOHUS228m7eQWeSy9HyrCuNILQx5yTQ+mX7yOwdQsdz/+eOTs6qTls492FMV6rfp5558/F
3rWQdZv8KAYsznWiBmNs+bCerevrqarJY+4ZJVRU54rug6epoqIiivotinrxxReP6XmbN29m+fLl
ACxevJhdu3aN+pycHMegKa/HKhpR6ekI0eKN4M6y4/E4KCgYeuatPqFwPsFYAGtBAe4Ttm3LSqzS
k1/gGndMmizTFYgDjBpDn9G2swSiuLvCROxh3Fl2cnOcFBRk8YmzrHh9ESqLB3c9N1nNuL2RAfv3
dTqIqkEUhwP3vGVg6Jic46uJH0/cfdxZiRVyc/NcFOQ6cGc5kGwW3G7bgH0U5gWIxLQR95vVGiDm
sODSNbKz7eSOsG0oEsfdHsRUKuHOspOdY8dmn5oF3IKCLLpaA6hxnZyc0a/vE+UbLpxuM/mOHOju
xe/vwux24xllP74OJ1EtjOIY+VxDYubFSCCO7LIjKQr544zxREaeE78ewVZSgsUzzn0VLEz+mOWy
EbMlujKO5byN99yORV6ei1AgijPLOiHjoNxZ9uRxPGmqBBkvUTr+mOmOeHmz9nhiVWjP5+rqyzmz
8AwUWWHL/nZe/NMeKlQDiyRj0jWqfTsoO7QdxWwi+9LLybvuepSs1H9ABSHVJEkia+mZOBcsxPvm
X+n+62tcud7HuQfsrJ2/jX0V+znniiU07SlmU2MQkyRxXpUHR1il9mAXtQe7sFhNVM/Kp2ZuAWVV
OSLZEggEArhcx7u5KIqCqqojjt3yesc3mUB/mqbj84fJybLi84cp8djo6PCP+BzD6kbNL8Pw5BA9
YVufPzGLV2dHYNxdYr094eTzR4sBEoW50bbzh2L4/GFC4Tg+fxiTTUYyJQppdkUa8vm9geigOMLe
IHFfBDkOWvDYeJjQ6DGebNx9+uLo6gogaxq+QATFmkUsr3TAPqoLneiGMeJ+fb4weiiGRYvR2xtC
G2HbcFRNHFuCwvIsbHbzmGPOJH3n2ucLo6o6KIkp/8fLigt/LI4h2VHzy9A9nlHPRzQmEfVFMFvd
I55rAH9vBJ8/jFFYSt55M1JzrvPLiMaBU9iXXzOhR2I4CwtGjWk81/XJCEfjKd9nQUFWv8+Ymbg2
/glsJtJwCatIsD4mvJEe3qh7lw+bN6IZGgX2PK6edjnLihajyAqqqvPcH7fTcaCdCtmMjE6Fdw9V
3p3YnDayr70Wz6VXiEkshClJtljIu+563OdfSNerf4QPP+Dav4fpzovx/oIP6Sq1Mmf6LDoOlvD3
Oh2r2cQF8wooMZtoOtLN/p2t7N/ZisksU1aZQ3l1DhXVOXhyHRk1a5EwOVwuF8FgMPm7rusTOjGG
osiUVeVQWppNW2cgOQnDSCRZxlyQ+umKM+V6dx5rqSnJHaqFKv0xaiUVWAoHjk2SJAlllPN3suOQ
MuV9yQSSJI352reUlCRm2hxhEoo+LrcVdzDRUphRY87tDmIz5pGVd/qWz+wOC+FQbNBi6Jls6kQq
nJSucDdv1b83KLE6s2ARekcHvg/Xs3tHK/u77cQUB2ZJoaT3ANMDe8g/Yy7uc+/FMXce0jim6xSE
TGXOy6P4zrvJveZauv70KmzcwKf+FqG7wMGHs3cQLt9PUVU+weYi3j0YQtFsnDu/iAurcult9dNw
pJu6w13UHU6sXeLMslIxLYeyaTmUVXmGXJhTOP0sXbqUNWvWcM0117Bt2zZmzZo14cd0OC2YLaYx
JVdjdhJl8kwpx5sUmXPmFg1ILMY6VfREm1OZM6ZxTUOxmmUikpQYf5VBkz5MhuwcB10dAVxZk/c9
KsnymGfekySJotIMTWLMmTVFeaqVVnqIx9TUzFo6SUSCdZpqD3XwZu0aPmrbgq5rTFPdXCbVUNYA
sfff5lD9/9JkKafOs4CoOQ9J1igM1bKw2kLpP5yDY/Y/Tviq94KQLpbiEkq+8EVy/+Fauv70Cmze
xD90QMRlZfPMCDtndOAo3oscymd9Wz7r9hUxq6iIi86v5KoSN22NPhqOdtNY62Xfzlb27WwFICfP
QVlVItkqq/JMqZuBMHZXXHEF69atY8WKFRiGwaOPPprukCZNJhXiMimW/jynUNEys9xDc6edHE0b
tTlLOckkLlPlFjjJzrGLmVzHyW5ViMTV1Fa+ZBhZlqbc/VQkWKcRwzA46D3C+v1r6Dywk8LuODf2
yBR1q8jhDuAwLeYsGrPn0lx6PZpkxjB0wlqIxRfP4tILLs3YG5YgTARrWTml//wVYq2teN95G2nd
37lgq5/zdoWpm+lhQ3kzbVXtMG0vdYFsfr2lAHOomLOrZ3LemWVc9qm5dLcHaazz0lTXQ0tDD7u2
NLFrSxOSBPlFWZRP81A+LZeSimwxfus0Icsy//Zv/5buMNJiIm4RKb/vpOk2looGJ6tFobQ8j2hD
CMU58iKzFrPCnMqcIdcUmqpEcjV+00uz6ewNU5TKmTyFU3b6fCo/plSfj+6Du2nYuwn/kQNktwc5
NzJwLRJTQSH+2edRK5fT7EvUcMQw6ECnpCaPz105mzwxJbXwMWYpLqbottvJ//Rn6H1/LT3vrKZ6
TyfVe0DNy+ZITTbvF/fgL+8FDrEhvoEP1udhjxWzpGQO58+czjVnVWAYBm3NPppqEwlXW7OPjlY/
W9c3YLEqVFTnUlWTR9WMvCk725fw8ea0mXHZzORPwOK3Jkvi/mQ+jWvix8JSXIJstmDKGX1tpVNp
LRNOD2aTPOy08kL6iARrCtGCQSK1RwnXHqX38D5itbUovsRA66xj/yJZVqipJm/mfMK5FdT7bRzY
3004kFiNPoBBGwbZxVmsuHg6C6pPfdVvQThdKE4nuVddTc7lVxLauxvfBx8Q2LqZWRt6mSVJaJWl
NFVlsSEnSHNeK3Fa+YhtrN9lQ/kon5R8AYcAACAASURBVApHJcsq5nHusmrOWl5NPKbR0thD/eFu
ag91cXhfB4f3dSBJUFSWzbRjyVZOvpgsQ5g8FquJWFQ9qXFCZpPMgukTc99w5dkpKnWf/Bp0yS51
k/tZmlGaTX1bgJwUJTsTNUGJIAiTRyRYGSoeCtC0byvx+nri9Q3Q0IzJ6xuwTcQm01ZqIVaaT/7M
+cxacBFq3M2+PW18cKiL8M4eAFQMuoEuYPqMXO46q4I5VTmiQCcIw5AUBeeCRTgXLEILhQhs3ojv
ww8IHzxAZZ1BJSAXFdIzo5id2To7XT1Ecxqpp5H69g94qdGGQy2kyjmNxWUzWXbxdC64vIbuziB1
h7qoO9RFa2MvrY29rH/vCFluK5Uz8qickUtZZQ5my8e7Bl+YWJXTc9F1I+PuAbIi4z6VlrGTnYLv
FBV47BRMQIueIAhTl0iwMoAeixFtqCdSe5RI7VGitbVEW5qTdXAmIGyRaCq20JZnwpfvQSuqxGyr
wBUoIdoN9XsjbNl0GOXY/UXHoBfokSG/zM15swo5Z14R2U5Lml6lIExNisNB9vKLyV5+MarPR3DH
NgLbthLasxv3B+1cAFxgMiFXlNNW4GK/Ncphd4Aedx37jHr2Na7l2ToFm5pHobWEmpwqln5yJlc5
5lN/xEvDkS7qj3jZvbWZ3VubURSJ0koPFdWJcVt5hS4xdktIKUmSUJTMSa5SNVeDKS8PtbcXc35+
anYoCIJwkkSCNYHiqk44qhKOqoSO/Qv3BlCbGzFampDbm7B2NmPv7UDqV/MWk820uwto8Zhodbjo
cOQT1HOwxB1Y41bsTSZsTYlOEH4SrVoKEAeiNgVHnoPK6hyml3moKcvGKmrDBSElTG432RdeRPaF
F6FHo4T27iG8fx+hA/uJ1tZRcNSgALgQwGYjkO2i0ybR4QjT664jZG1gb+cmttRLRGUrZsmD25FP
4TlFVEiFmHot9DRGaTjqpeGoN3FMk0RBnpWCXDMF2TLZNh27FMeIx9FjUYxoLPF/LAa6jufSy0X3
ImFKcdjMFOU4yDnF6bktBYWY3NliBlxBENJOJFj9GIaBYYCmG8RUjUhUIxJTicS0Y/9UwlFtcNLU
9y8SJxzViEbiaOEIWdEA2WoQtxomRw3gUUM49CiaZEKTzaiymbhchq9wNgGTi7DJQcxkA8WMSZIw
GWBXDap8QwQrgT3LiqfASWGpm+rqXIpLsjKuy4cgnK5kqxXX4iW4Fi8BQAuFEi3Q9XVE6+uI1Nfh
6ujApWlMG3YvLcmfdAnkY/UsEcVBj72YHnshvbZCWuI5tLRFktuatCiumBdnrBdb3I9dDWCP+7Gq
IdZGD9GztJrb596MRREt1sLUUF2SmvWFRHIlCEImmNQES9d1HnnkEfbv34/FYmHVqlVUVVUl//7u
u+/y//7f/8NkMvHZz36Wm2++eULjMQyDHz+3jUNNvRiahqLGkdCRDQMJAxkDyTCwAKWKE0WSkADp
2GSs0rFOfDLgBJxS/248FrDmgjUXDeg89m8kpmP/0MFilbE7LDicFhyuxP9OtxVPrgNPrgO3xya6
DQlCBlEcDpzz5uOcNz/5mKFpqD1e4h0dxDs70QJ+NL8fLRBAj4SJRWP4gwEi0RBqLErc0IhJOqqi
oyktoLTgNINZNhNV8omTh0o2qpyNVymix148OJBm0Nri/OqtD5EVBRQZSZZAlpBkibICJzluG4pJ
TlTISIkuYxIkf4bj03Ent+HYBgwxVXfftkhIElTV5J3aWBpBEARBmMImNcFavXo1sViM559/nm3b
tvGjH/2IJ598EoB4PM4Pf/hDXnzxRex2O7feeiuXXnop+RPYl1qSJEryHERiGldufZ6cQMeQ20VM
DnYUX4oqm5GOJV2JJOv4z5JhIBsqMjoms4LJasFst2J22jE7HVg8bixZThRFwWzp98+c+N/S7zGT
WRHJkyCcBiRFwZyXjzlvfN9jcVWn2xehszdCZ2+Ybl+YcDhMOBQjGFEJhOKEwipSXEfWDBQNrIaE
GTAjYdJMmDQdEzpyskooob47TH1KX+Vg3Z1BLv7k7Ak+iiAIgiBkpklNsDZv3szy5csBWLx4Mbt2
7Ur+7fDhw1RWVpKdnQ3AmWeeycaNG7n66qtH3GdBQdYpxfS125Yd++mSEbe77JSOIgiCMD6lJdnp
DkFIoVO9V6V6P5NpKsYMUzPuqRgzTM24p2LMMDXjnooxT2ozSSAQwOU6vjK5oiioqpr8W1bW8RPo
dDoJBAKTGZ4gCIIgCIIgCMIpmdQEy+VyEQwGk7/ruo7JZBryb8FgcEDCJQiCIAiCIAiCkOkmNcFa
unQpa9euBWDbtm3MmjUr+bcZM2ZQV1dHT08PsViMTZs2sWTJkskMTxAEQRAEQRAE4ZRIhjF5S5/3
zSJ44MABDMPg0UcfZc+ePYRCIW655ZbkLIKGYfDZz36W2267bbJCEwRBEARBEARBOGWTmmAJgiAI
giAIgiCczsRc4IIgCIIgCIIgCCkiEixBEARBEARBEIQUEQlWhtB1ne985zvccsst3H777dTV1aU7
pClh+/bt3H777ekOY0qIx+N885vfZOXKldx4442888476Q4p42maxoMPPsiKFSu49dZbOXDgQLpD
mjK6urq4+OKLOXz4cLpDmdIy/d4w1PdKXV0dt956KytXruS73/0uuq4D8MILL/CZz3yGm2++mTVr
1qQ58oHX6FSJ+Re/+AW33HILn/nMZ/jDH/6Q8XHH43G+8Y1vsGLFClauXDklznX/csV4Yo1EItx7
772sXLmSu+++m+7u7rTEvHfvXlauXMntt9/OXXfdRWdnZ0bGfGLcff785z9zyy23JH/PxLjHxBAy
wptvvmk88MADhmEYxtatW40vfvGLaY4o8z311FPGtddea9x0003pDmVKePHFF41Vq1YZhmEYXq/X
uPjii9Mb0BTw9ttvG9/61rcMwzCM9evXi8/lGMViMeNLX/qSceWVVxqHDh1KdzhTWqbfG4b6Xrnn
nnuM9evXG4ZhGA8//LDx1ltvGe3t7ca1115rRKNRw+fzJX9OlxOv0akQ8/r164177rnH0DTNCAQC
xk9/+tOMj/vtt9827rvvPsMwDOP99983vvKVr2R0zCeWK8YT6//8z/8YP/3pTw3DMIy//OUvxve/
//20xHzbbbcZe/bsMQzDMJ599lnj0UcfzbiYh4rbMAxj9+7dxh133JF8LBPjHivRgpUhNm/ezPLl
ywFYvHgxu3btSnNEma+yspL//M//THcYU8YnP/lJ/uVf/gUAwzBQFCXNEWW+yy+/nO9///sANDc3
43a70xzR1PDYY4+xYsUKCgsL0x3KlJfp94ahvld2797N2WefDcBFF13EBx98wI4dO1iyZAkWi4Ws
rCwqKyvZt29f2uI+8RqdCjG///77zJo1iy9/+ct88Ytf5BOf+ETGx11dXY2maei6TiAQwGQyZXTM
J5YrxhNr/8/qRRddxIcffpiWmB9//HHmzp0LJHphWK3WjIt5qLi9Xi+PP/44Dz30UPKxTIx7rESC
lSECgQAulyv5u6IoqKqaxogy31VXXZVcqFoYndPpxOVyEQgEuO+++/jqV7+a7pCmBJPJxAMPPMD3
v/99rrvuunSHk/H++Mc/kpubm7z5Cacm0+8NQ32vGIaBJEnJv/v9fgKBAFlZWQOeFwgE0hLzUNdo
pscMiQLorl27+MlPfsL3vvc97r///oyP2+Fw0NTUxNVXX83DDz/M7bffntExn1iuGE+s/R/v2zYd
MfdVGmzZsoVnnnmGz33ucxkX84lxa5rGt7/9bR588EGcTmdym0yMe6xEgpUhXC4XwWAw+buu6yJ5
EFKupaWFO+64g+uvv14kC+Pw2GOP8eabb/Lwww8TCoXSHU5Ge+mll/jggw+4/fbb2bt3Lw888AAd
HR3pDmvKmgr3hhO/V2T5eNEiGAzidrsHvY5gMDig4DSZhrpG+4/hyMSYATweDxdeeCEWi4Xp06dj
tVoHFCwzMe5f//rXXHjhhbz55pu8+uqrfOtb3yIejw+ILdNi7m8813L/x/u2TZfXX3+d7373uzz1
1FPk5uZmfMy7d++mrq6ORx55hK9//escOnSIH/zgBxkf90hEgpUhli5dytq1awHYtm0bs2bNSnNE
wumms7OTO++8k29+85vceOON6Q5nSnjllVf4xS9+AYDdbkeSpAE3XGGw3/3udzzzzDM8/fTTzJ07
l8cee4yCgoJ0hzVlZfq9YajvlXnz5rFhwwYA1q5dy7Jly1i0aBGbN28mGo3i9/s5fPhw2l7LUNfo
RRddlNExA5x55pn8/e9/xzAM2traCIfDnHfeeRkdt9vtTiZK2dnZqKqa8ddHf+OJdenSpfztb39L
bnvmmWemJeZXX301eX1XVFQAZHzMixYt4rXXXuPpp5/m8ccfp6amhm9/+9sZH/dIMqsa7GPsiiuu
YN26daxYsQLDMHj00UfTHZJwmvmv//ovfD4fP//5z/n5z38OwC9/+UtsNluaI8tcV155JQ8++CC3
3XYbqqry0EMPifMlTKpMvzcM9b3y7W9/m1WrVvH4448zffp0rrrqKhRF4fbbb2flypUYhsHXvvY1
rFZrmqM/7oEHHuDhhx/O6JgvueQSNm7cyI033ohhGHznO9+hvLw8o+P+3Oc+x0MPPcTKlSuJx+N8
7WtfY8GCBRkdc3/juS5uvfVWHnjgAW699VbMZjP/8R//MenxaprGD37wA0pKSrj33nsBOOuss7jv
vvsyNuaRFBQUTMm4ASTDMIx0ByEIgiAIgiAIgnA6EH1dBEEQBEEQBEEQUkQkWIIgCIIgCIIgCCki
EixBEARBEARBEIQUEQmWIAiCIAiCIAhCiogESxAEQRAEQRAEIUVEgiUIgiAIgiAIgpAiIsESBEEQ
BEEQBEFIEZFgCYIgCIIgCIIgpIhIsARBEARBEARBEFJEJFiCIAiCIAiCIAgpIhIsQRAEQRAEQRCE
FBEJliAIgiAIgiAIQoqIBEsQMtRPfvITXnnllUGPd3d3M3v27FPa989+9jNWr159SvsQBEEQBHGv
EoTBTOkOQBCEof3Lv/zLhO17w4YN1NTUTNj+BUEQhI8Hca8ShMFEgiUIKabrOo8++ijbt28nGAxi
GAarVq1izpw5rFq1ii1btqAoCpdffjlf+9rXCIVCQz7+4IMPMnPmTO666y7eeustnnjiCex2OwsW
LBhwvD/84Q88++yz6LqOx+Ph4YcfZsaMGXzrW9/C5XKxf/9+WltbmT59Oo8//jivvPIKu3bt4t//
/d9RFIUrrrgiTWdKEARBSBdxrxKEiSMSLEFIse3bt9Pe3s7zzz+PLMs89dRT/PKXv6SqqopoNMrr
r7+OpmnceeedfPTRR7z77rtDPt6ns7OThx56iOeee46amhp+8YtfJP/20Ucf8corr/C73/0Ou93O
+++/z7333svrr78OwK5du/jtb3+LJEncfPPNvPHGG9x2223J/8UNSxAE4eNJ3KsEYeKIBEsQUmzJ
kiVkZ2fz3HPP0dDQwIYNG3A6nXzwwQc8+OCDKIqCoig888wzAKxatWrIx19++WUANm/ezKxZs5Ld
JG655RYef/xxAN577z3q6upYsWJF8vi9vb309PQAsHz5ciwWCwCzZs2it7d3ck6CIAiCkNHEvUoQ
Jo5IsAQhxd577z1+8IMf8PnPf57LLruM6dOn86c//QmTyYQkScntWlpasNlswz7eR5IkDMNI/m4y
Hf/Y6rrO9ddfzze/+c3k7+3t7WRnZwOMuB9BEATh40vcqwRh4ohZBAUhxdatW8cll1zCypUrWbhw
IatXr0bTNM477zxefvlldF0nFotx3333sXHjxmEf77Ns2TIOHTrEvn37APjjH/+Y/NsFF1zAa6+9
Rnt7OwDPPvss//iP/zhqjIqioKpqil+5IAiCMFWIe5UgTBzRgiUIKbZixQruv/9+rrvuOhRFYdmy
Zbz11lv86le/4gc/+AHXX389mqZxzTXXcOWVV3LhhRcO+fi7774LQG5uLj/+8Y+5//77MZvNnHXW
WcljLV++nLvvvps777wTSZJwuVz87Gc/G1DLOJRLLrmExx57jHg8zg033DCh50MQBEHIPOJeJQgT
RzJEO6wgCIIgCIIgCEJKiC6CgiAIgiAIgiAIKSISLEEQBEEQBEEQhBQRCZYgCIIgCIIgCEKKiARL
EARBEARBEAQhRab8LIIdHf50hyAIgiCkWEFBVrpDSKlU3Ktychx4vaEURDN5pmLMMDXjnooxw9SM
eyrGDFMz7kyPebh7lWjBEgRBEIQpwGRS0h3CuE3FmGFqxj0VY4apGfdUjBmmZtxTMWY4DVqwBEEQ
BGEyaZrGv/7rv3L06FEkSeJ73/ses2bNSndYgiAIQoYQLViCIAiCMA5r1qwB4LnnnuOrX/0qTzzx
RJojEgRBmDoMw+B0X4Z3yi80LMZgCaczwzBoDrZyuOcoR3rr6I548ccDxLQ4DpMdp9lBmauEGZ5q
Znqmk2VxpTtkQUiJTB+DpaoqJpOJl19+mfXr1/PYY4+Nsr02Zbu6CIIgpNJHjdvQDI3zKs5MdygT
RnQRFIQM5I30sL5lMxtaN9ER7ko+LiHhMjuxKBa80V6ag60c7DnCe43rkCWZRfnzOL/0HObmzkSW
RAO1IEwUk8nEAw88wNtvv81Pf/rTUbdPxSDtgoKsKVepOBVjhqkZ91SMGaZm3FMxZsicuLu8PgA6
bKPHkikxD2e4ykCRYAlCBvFGenij9h0+aNmIbuiYZTPLihYzO2cmM7KrKHDkD0ic4lqcOn8jh3uO
srl9O9s6drGtYxeVWWXcUPMPzMqpSeOrEYTT22OPPcb999/PzTffzGuvvYbD4Uh3SIIgCEIGEAmW
IGQAVVd5s/Zd3qpbg2poFNrzubzyYhbmLqClI0ZjS5B1hwIochCrWcFpN1GU46Aox86M7GnUeKq5
suoS6v2NvFO/ls3t2/nJ1qdYlD+fW2Z/Go81O90vURBOG6+88gptbW3cc8892O12JElClkWLsZBa
rY29+H0RauYWIklSusMRBGEcRIIlCGlW52vg6b0v0BJsw2PN5trqKykzzebtjxp55sAmojFtxOdb
LQrFOQ6K8xyU5DpYkHc586cvY13nu+zo2M3R1iY+WX4Fi4vn43JbxTgQQThFV155JQ8++CC33XYb
qqry0EMPYbPZ0h3WhNJ1A003MJsyK5Hc2r4TtyWLGZ5p6Q4l5fy+CJA494pyeiZYhqqix+PpDkMQ
Uk4kWIKQRh80b+S5/X9EMzQuLDuXqyuu5C/vN/GrLZswgPxsGxefUcq0kixyXFZ03SAa1/GFYrR5
Q7R3h2nzhmjuClLXluij7ABykXAzh3nSbGRDZu+WIHv5CABnloWqGXlMn11AWZVH1LwLwjg5HA5+
8pOfpDuMSbXtUCcxVePcecXpDmUATVfxRrzAtHSHIgCarqHIY6/E82/ehOS2wdzFExiVIEw+kWAJ
QhpousbLh15jTeP7OE0OPr9gJXlSOf/x+500dQYpyXNw0yU1nDEjb0xdQ+KqxraNjezb0UrAGwbA
AKJIhOQ4enYHiqJjDrswQh72bGthz7YWPLl2zr5oOtNn54suKIIgDCumjtySng5TfBLk086R3jq6
w90sKpiPRbGkOxxBSCuRYAnCJNN0jd/seY7N7dspdhbxxYWfI+y3sOq5zQTCcS47s5ybL6kZU1cc
wzA4uKedjX8/iq8ngiRBVU0ecxcVU16di8kk4/VHOdzq5bX6v9Ak78CIWbDsX0YV+fR4w7z1ym6K
y9xcdt1c3B77JJwBQRCmKsMwRGWMMKTucDcAITUsEixhWB+XihGRYAnCJOqfXE3PnsaXzriTbq/K
/3l2C8GIyu1XzuKSpeVj2ld3Z5C/vbGf1kYfsiwxd46HOZ4gJu8B4mv+TstL3eihEHokQr7Fwp1O
J16bk23mDo6UrWVP1xJM4TIWumy0Nvl44X82cdFVs5g1v2iCz4IgCFOVAWRKemWQWQW1YDxES7CN
anfluLrJTTXtPWGsJplslzXdoWQ81ecjcuQwjjlzkU/zcZpjlWmf24kiEixBmCSGYfD03j+wuX07
M44lV6EQPP7CdoIRlc9fPYflZ5SOup94XGPT6j3s2NGFbkAxXdQ0fYD1QBe+/hsqCordgWSzoodC
xDs7sGsa5wHnbYde5xoOlc9gbe+ZeCw2pmkG7/x5L51tfs67ZIaopRYEQRiHfd0HMAyDDrOTYmfh
mJ8X1+K0hzspdhROicTsSHMvQMaNxxuN6vNhxOOY8/Im7ZjhQwcx4nFiLc3YqqdP2nEzmWjBEgQh
pf5y9C02tm2h2l3Jl864EwULP39lC15/lBs/MWPY5EqPRAgfPkTkyGEaDrSxPVxCyJSFNR5gdsd6
CsJNWIpLsC46H1vVNKxl5ZiLijB5cpD6TWBhGAaq10v40AE6Nn6AY+dOztx/kDPMR9mUPZ+P3PNZ
6HCw/aNGAr4ol147R8w4KAgZoqs9gK87jMtjQ5YnvvJj2K6AmdSElWH6Co79a+gjaoSOcBdlrpJh
F3+v9TXQG+1FMzQqs8bWg0EYv9DePQCTmmD1+ZjkFGOiixYsQRBS5cPmjbxR+w75tlzuWfQ5bCYb
T7+1nyPNPs6bX8TV51QO2F71+/Bv2EBw+1bCBw8Q1RUO5Z9Fi3sWKAYzrN0sXuwka8YdWCsqx9T1
QJIkzLm5mM8+F/fZ59Le3ciaF37CrF1dnNu5nXn+o7yedy45ORUc3tdBJBzn6hsXYjaLJEsQ0q27
M4g7y47VYcJqM0/osTpa/fh6wlTNyMOUwZ//qVATvt97iHAoRmdthLk1ldjsg9+7iJaYjl3V1ckO
T5hgksTHJJ0QTiQSLEGYYEd6a/n9/pdwmOx86Yw7ybK4WLezhTVbmigvcHLHJ+cgSRKGphHYthXf
h+sI7twBmoYBdE47m32WOcR0mbx8O5/4h7kUlrhPOa7C3HI++fnv8F+bfkHVh4dZst/Hiua32BCc
R1fZuTTV9fDXF3eKJEsQMshk5BQ93SEAYjFtUIJlfAyasDRdo9nXiqzbTrnLXlyLE+rWkA2NjlY/
FdW5I2w9tc6roetEGxswFxSi2I9NkKTp6JEoZNjwrLiqo+t6WpYlMQyDUFjDqhuT0vqcabRQkMjR
o9hnzEC22T82zXkiwRKECeSPBfjvXb/DMAzuXng7Rc5C2rpDPP3WfuxWE1/+zEIskkHv3/9G9+uv
Ee9oB8BaWQVLL2CbN5fmpgAmRea8i6tZdFZZSm8Q2dYsvnzWP/NT81Psr6rnho1xzuneQ3Okje3T
rqGproc3XtrF1TcuEN0FBSED6PrkFU6GaiHqe6jbF8FiVnAN0SIzWcY7WD4e0+jqCJBf6CIc15Ak
acj46/1NxCIhbKqTKnfFKccpSYAB0cjxFipdN+j2Rch129ANPbHdFEuw4h3txFpaiHd2krX0TABM
B2qJWXoxzrkIyZQ5RczNB9qxtPioKfNM+rH9YYPeSBgt15+SytGJMlEzhEaOHEELBonU1eGYPSfl
+89UYoVRQZgguqHzmz3P0RPt5VPTP8msnBo0XeeXf9lDLK7zj1fNwll/gNrvfpu23/wvqreb7Isv
oexfv0fbJz7HG3utNDcFqJyRyy13ncXicyompPbNZXFy3+IvYK6q5L8vt9O5oILSaBcXHfwDuhSj
sdbL6lf3out6yo8tCJmgsbGR9957D03TaGhoSHc4IzImM8Ea5lhxVeNAYw+7j3ZPWiyp0Nbsw98b
oaMtwO7abnYd7RoyiYyoiS57US0GJMbBRhsbMIb4Dmys7aalsXfkAx8rs/Y/VkN7gEPNvTS0B5Jp
ojTGmv1M6RppaIm10Yx4/PiDsfiAv0018Viqu2lKRI+dnlAgluJ9D60vYR+PYCDKob3tBP3RIf8e
jcRP/rqTkh+AxH8nt5dTFlGjJ3VuTpZIsARhgrxVt4a93QeYnzeHy6suBuD1D+s40uzjoml2yt7+
Pc3/+X+Jd3SQfcmlTPvh/yF6wbW88kYLmz+ow+6wcNUN87nmxoUTvj6Vy+LkviVfoMhTyu8WRam/
ajF2Ylx86HlkI8zRg52seX1/xtzYBSFVXn/9df75n/+ZVatW0dPTw4oVK3j11VfTHdawJvMzONyh
NH3wZA79heJhWoJtExVW0njPRF/rX//EUdUGF7hOfF3BPbuJNjWhdncN2jYcihPwRQY8dmIbwFCN
AoFIotQdOlZwVY40YGzbNer7axgG/o82EDqwf8TthqPqKvX+RmJafPSNT5qUvHiicY1wdHxJy0RX
5sW0OO2hjkHnuqc7RO2hrmQXWYCujgDtLb4TdzEqwzCIRk44x5PQQBlWw2xp2z7uz5+3M/GavV2h
QX/z9YSpP9JNV3sgJTGmQygeZlfnHo701k3aMUWCJQgToCnQwutHV+OxZnPHvFuQJZnaVh9/ev8o
50ePcMH7vyG4cwf2OXOp+u6/IV92A2+93cBrL+zE3xvhjLPLufXus5g+u2DSpkt3mRNJVqmzmJfz
mmm89RNIFivLj7yESQ9xYFcb61YfEkmWcFr55S9/ybPPPovL5SIvL4+XX36Zp556Kt1hDckfCxJV
U1MwjjY3ofb2JH9XdZWusHfA57vHH6W+zT/gebuOdicTrOHs6dpHk78Zf2yCC2Tj/i4a3/Z937x9
LTSGbhDX4skWrrZQB97IKK1Xx456YtKWPM+ShIGB5A8lDjhUctH/qcdahlSvdxyv5Li67iaau9uo
9aWgoDmG07n1YAfbD3eOeZehQJTD+zoGJDmnqv813RvzsaNjF/W+RrojA89hX6Ic8B1vxenuCNLr
DY/pOFo4jB45dm00+6g/0k0oqk/qkCNfNPF5bfI3p2yf4VDi+vf1REbZciB/b4R4TBvUgtXH0A3a
mn3ExpmA96fGNdT48K2lhmHgQLEPPgAAIABJREFU7QziDyeup55ID9FIHF/P2N7TUyESLEFIMU3X
eHrvC2iGxq2zP4PL7CQW1/jtH7dwQ9O7XNTwPpIkUfT5u3B9/ius3dTLi7/eTP2RbkorPdz0+WWc
f2kNZsvk9193mZ18ZfE/UWDP42VjF213XYXqcHJ+7SuY9DA7Nzexad3k1QAJwkSTZRmXy5X8vbCw
MC0D4UcTVWM0+Vo50HV4wOP+WIBgfPTCqKHryS5uhqoSbWggtG8f3kgPR3vrOdhzhKO9tQMKnUeO
tNJ6pIG4erzQH4mp9IzS1UkHWrpD9AQmvhAzmvDBg8Tahq/NH6rwe/yhgZVbkqKwvWMXuzr3AlDf
24A3Mr5EJzmV+7GDyFL/BEAeurkrhZqP+ultUomf5IyFqaxgG2pfPS3dRBsb6WpKXffT/ofpCh3f
b1xPbStecMd2Atu3AYnkAiAa6381je+97Wj109k2dCVFT6SXTc1b8MUGVoAocurLDX0Tc4znvY9G
4rQ29XLkYMcQrcSJ/UR8Or6eME11J1dZAHD0YCdHDw6fwPt7I3S2B+hoPH4ejxxqp63ZN2JilgoT
VoLTdZ1HHnmE/fv3Y7FYWLVqFVVVVQB0dHTw9a9/Pbnt3r17+cY3vsGtt97KDTfckLzZlZeX88Mf
/nCiQhSECfF2/d9o8DdxTvGZLMifC8BfX/obV+94kWw1hGPufFy3/CPbdnnZ+6uNGAYUFLs45+Lp
lE/LSfsCv9lWN/cu/gKPb/k5L/Ws49Z7rsP5qzc4t+5PbKi6nk3v12K1mlh0llivRZj6Zs6cyTPP
PIOqquzdu5ff//73zJmTeQOxNSNRGIjHBxaM93cfBGBZ8ZIRn+/f+BGSomBbtASF4wWewz1HB2wX
61foNDXXYrIYGFrNgG1UdWCB6cTB8aGISiAc52BTL0tnj/bKEmNGhlsjaiSjFfcMVSXe3UW8uwtL
UdGgmHW0YfaRePTE72JJUaBfmXy08qYeTbSEmG0SHCvf6bqBokjJwurAYxgc6jlKgTOfbHMWsbY2
FHfqJkUwdB1isWM9+NLfEyHePjjxjbe3okcixL1eIDX3GP0UXqthGITU8IBr1DCMEQvngViQ7rAX
Cy7kvs/KSUy+2deKl1/kwjAMgv4odqcFRZFp+WgtpmiI1mVurLoNk1lBU3VkY/jPkR6PoceGqxwZ
/hzJskRUjdEZ7qJMdWMzHV8Wpu+z7++NEArGKCpNXK+altjf0WYffq2XWTmmQWMY+4ZD9fqjHGn2
UZpjxzAM7A7LaKdmWInWqQj5RS4kSUKNJw6ixY+/vkA8hMfqRtMMTBM4R8+EJVirV68mFovx/PPP
s23bNn70ox/x5JNPAlBQUMDTTz8NwNatW3niiSe4+eabiUajGIaR/JsgTDXNgVb+evRtsi1Z3Djz
OgxdZ99zf2T2u68BYLv6Buo989n13D40VceT5+Ds5dVMn52f9sSqvzx7Dvct+QJPbH6S51rf5o4v
3YD6X3/hrPq/8FHldax75xBWm4nZC4vTHaognJLvfOc7PPnkk1itVh566CHOPfdcHnjggXSHNUhf
FzNdG39hMdaemJ00EFRpPdRFtsvEcCvnKZIMxxKwZLn0hENG+xUu9XiMwJYtWEpKsFVWMdITewJR
DjX2Mr3UTa47EUFbqIMGXyOzc2eSZXExPn2tQQaapqMoY0/SWmJ1RPUIi7UCrGNdhuLE7+hhhwpJ
xLs6CR86hGwPgG3wzHXRsIohGRAKIIUSY3yiapRAuIfemI8zLFVofj+a349xxqnNZOiN9BANK+j1
DdDcimwCZp1c4jb2q88YNYnTAkO0zvRNnDGGyVx8MT82xYpFGblAPtb8qv92Bxt7MAzwRnvpifTQ
HLBSnlUKQEtjL621XtymMI6yEqQTWrz3dR+gOxIjHlDJ8QUpcY++TuVo/L0R2pp9OJwWyqpykOIq
6AZa3KC+pRutsx1DMaN67DDMOsqBLVuQ3TaYu3jEY3n9UTp9IWpKPUiShCxLtIc7iKlxtjQc5szy
OcnPzPaOXdhMNpTWxPUUliDXbUsmF4m3ceDn5sS3o6U7RMRuItoZxGSSmVaTj9lycrMW1x9JtFDa
7Gaysm0E1SChWIRIXEMi0W1PkeREBcsEj/WbsH4QmzdvZvny5QAsXryYXbt2DdrGMAy+//3v88gj
j6AoCvv27SMcDnPnnXdyxx13sG3btokKTxBSTtM1ntn3B1RDY8Xsz2CNaDT838dR3v0LfpOThsvv
5u36XLZvbMTuMPOJq2dzy13LmDFn8sZZjUeRo4CvLP6nxKLIDX9Gue9Gglkezmz8K4oRZ83r+zh6
YOx96wUhEzkcDr7xjW/w0ksv8fLLL/PAAw8M6DKYccaZXxmqSuToEQDC0UQisnV3M95hZgtTpMEF
mxO/nnoCx5+rBYIAxFpaRo2lrTuEqus0dwWTjzUHWoFEEnCyfC0qR/Z3oJ3YFemEwPsK0MFANLm4
b1QfXKOf7ManqsnfNV3HMIwBBf/+b0X/CQ1iqobanSjoyT0Du3BhJArLve0BQj0RpKMHUOoS42Um
Ypr23qiPwz1HWVe7i9YjjYnjRGLgm/gJC/qP1TNUlUjt0WSr3nCS495G2Xdci3Og+xA7OnYfP4am
EW1sGNRKoxuJZG9wopU4WlxX2d6xC2/ER48/SiSqUl/npcMbIqIm4g3Ej1+zQX+UcHMzviP1xFpb
h43xxGqGkW7zwUicNu/wXX2jwSh6NEYoOPC1aWpi77EeH/GuLjR1/ElD+PAhYp3HJ3DZVdfGptbt
HOisGxB4OKZSv9fPtj3HX7OqqwSOjbWMxXWaOwPsqese+FqTb+rgd7V/9+O+59QeOvVyRd+ENrW9
9RzorKe1O0SPP0pHb5hYzOBobx0Hug+PspdTM2EtWIFAYMBNSlEUVFXF1G9dhHfffZeZM2cyffp0
AGw2G3fddRc33XQTtbW13H333bzxxhsDniMImerdhr9T52vgrKIlzOwxU/fUd4h7vezMW0p30WK0
2hg2u5nzL5vG/CWlU2JdqfKsUr58xp38dNsv+c3hF7nrvlvx/eQFljS+yZbyq3nr1d38w02LKJ+W
k+5QBeGkzJkzZ1AFR0FBAWvXrk1TREMbqhJmLFMOx/p3ZZIgFFWRwiG6wmFysgavBjvk2JwTCkaS
dHyWuJEqhwxDJ9bTg6FLA2r5DWP4NXd8MT+arpFzrNXHF/MTViMUOQoG7//Y/2rEAFuidU/p/7U6
UtNFX5et4bYJhGB3I5EZFupa/ai6Tp7bT3dvnNxpx/oV9Tv99Ue6IRd6gzG62r0ocZWsE19bMEZ9
ux+nJGEYoIYHjgGSJBkw0FUDrMfPjaFqMMb7Rb2/kWAsxNy8WcDxqeZj+sDExogcn7Cgq81H++EW
apZMw2wfZYXgvtOlaUQb6sc8Ziza2EisrQ2vt414TTmVWeXoukFcNYjFDfraoE6cD8EXjCFJkHVC
tzFtiGs/2NBE9+FGcnt7kXQdk8eDtaISwzDoafFjDcrkDlF30hPpJa7FaepuwhwsIBhRiUZVtLiO
1TH069GjUQzZoDfQha8nyozsaYO2OfHMeCO9ZEWU5LXd384jiQQn22khaoQJqQOTrfD+fUQDGvYZ
M9B1g2BYZoi6kJMS7+wk3q0hVyVaoSJ64tgdkU7sPhNZRuL+nlwDryM45H4MA5SuNnBlAf0W1E5+
Xxxvce7TNMy+AI701tIb9XNlwfkn8aqOHyMa07HIiYohTTdQe8JYXODzD3/sVJiwFiyXy0UweDx4
XdcHJUp/+tOfuPnmm5O/V1dX86lPfQpJkqiursbj8dDR0TFRIQpCyrQG2/nL0bfIMju56qiNhv/z
I7whiXen3URHziIMzWDJuRWsvOcczjirYkokV32qs6v44sLPgSTxv4efJ+ertxG2WjmjeTWGqvHX
l3bS1vz/2XvveMvK+t7//ayy2ynTBwYGkCagNFHBAqIiGgQUlIh4A5pijLEkXpNXromX601uUCPR
xF+KIVeviokR9QK2IDEXGwnY6Awww9TTy+571af8/li7733O2VMOM+r++MLZZ6+1nvVdz3rW2t/6
+e4/je0QQxwJeOKJJ9i2bRvbtm3j4Ycf5uMf/ziXXXbZ4RZrSTT1W61WNLBm8x4PPDVHxUuU7IbC
l5cLFOL6b2uXgbGvMkmxEnZ4lrttkFKcJx8naYd7+lA3+3mNDqA09wSlhx8h3LsXoxSisAhaYaKI
yo/uJ5ya7DhOac1T+R0ddWFP5XewrzzR91qNMUQ13Xa8Ys6bR+ml62NK1ajj2vop6gCUa4Agmp5G
1lOJ9kxViGJNaVIysbvQUdfRQKVuNJXqUT4vUuyZrSKVZrbgM71YQ6skmiJE0lOsYw5iQ2FvzMxs
iyDE37G9NX5YWfK+F/Me++ZmqbVFW9qjYqZN47cm55qfdz32FN7cNMUnB/foi8DriDL11Nd0TY1R
ieE+W5pkrjaPNprd+zwmFzV7Zpcmm3h8T57HdvcSXvQzzudnqhRrhvxsFeV5hFNTTVmMhu7gjsFQ
iarN9NtGADSo98IyWje3GQx7yxPsLU80DsYAE9UpikGxh2im4/Lrk7HgL/TUPHZDacP2wg4mK1Oo
tjltprMZTXHRo+rZVGqD1yqVwnJHfeWgmKnOEeqIWMUIq2H9du7TNJjiCHthFnfvDoQQ1JqNtfsf
BxC3RZ2r1YA4bv2d9wsoLZdMNS0tE/Hre0j9u8a8rnZLrIEMrLe//e3867/+K3E8+M0577zzmh7A
Bx98kGc/+9k9+zz66KOcd955zb+/8pWv8JGPfASA2dlZqtUqmzb1eq2GGOJIgjaaL2z7Mo4f8db/
hNIdX2fXUedz/9YrwRnh2JPWc93bz+dFLz+ZdObnMxp72vpT+K0zfw1pFJ95+ots+P3rCRzBmTPf
RcaKb9728JJerSGG+HmB67pcdtll3HfffYdblB401UkDxbDEA/OPsH1iAm8xUdC1MWyfKFJuSyGa
q1MRV7siJcYYYpN8Z012Eg1U/Zj5UsDkfK2ppJg2z3N5vsZMfoaSTJTehXLQoc08tbdAcT5CFsCp
R0lkuZREOyb3IGd2USskzZzDtqbOe2Yr/PiJOaQ2aG3wvc5UqP5KVqeBNVWbSZTgymSffRPyjbmi
z742o7A/i6AB0RuBqAUx+UqIig1eLcTLL23IBaFi+2RIvhxhDMy10Y4rpUn4EgxSdgrQCCDW2u6j
LCcOrFrs8VRhB1O1zrS0veUJZmqzTOwuUJ2VGN2qfxICiGLG9k0htKb7qvwoZq44R8EvNtP35qbL
TO0t9p3zyep0Ypx31R1VfvyjzuvwvAMi0hANLXiFwFi/dMq4kS6nljf2GpisTPFkfntPeqrl17Aq
JYyBmt+K6M5480xVGs9Lv5TDjrM2P/ltS1muUFsmB6ixjMoVDBDHFvnJrpTLPodro9k+9TgT5eS5
KNUinp4s9dyfUi2iXGuNF1U1hd0xlWKACCOsIOrb+641TGve/VAytVDXCURyV3XThmgbQ2ucqd24
u54k/+R29u7Y2zF2UFJs3zZLFMoeavW56a7022VhmmtKN+Rc5cqMgbS93/7t3+b222/nYx/7GBdf
fDFXX301Z5999rLHXHrppdx77728+c1vxhjDTTfdxNe//nU8z+Paa68ln88zOjra4YW45ppr+MAH
PsB1112HEIKbbrppmB44xBGP7+77IbVdO3jrf/hEvs9jp76RkhkhxnDSeVu48tUD0Gj9HOCsjc/h
bc+5jv/z2D/z2d1f5Ib3Xk/pE5/hjNl72XbUhXz9Sw9x9a89b9WbIg8xxKHEHXfc0fxsjGH79u24
7ipSSx0o2pSBYlgCY9g7kSh7uQ02xUrIYjlgsRxwzrPW47h9/Kd9FAprsYTe2iKradTNxEo3iTB2
TyW9nrTUyFChIoM9AspIoNPImC/6RH3qQLTvg4BibS/Yo5Db3LG97EVscJOIjizARLHAsSe0Uo+F
ENQqIZmci21b7Jouo4k7lNxGvYwv+9PDN2q02muDGgxzYRBjO1ZbdkGfyTItJS2px4Iw1iyWAjas
6SQyKJc1UTVE5pLxpDak60NLZbAE2PMziKxoUzdbaVT9EMgQDESy0/ic85Jo5HGcnFx/UcOWxlaB
PTmXGLttAY9QxsRa8sBTs1TLPmmjESI5d6PvU4PxsIFIRUzXZijKMuNieeIGf8d2rKOOhnY6FWOS
eiiSZr/VuEamHkbQdaOwNRv7r/1qrfBlQKlaYjS3lpyzRH5fF2r1dLyGwWHlF6ipMmbUwRY6YZoz
MDVfw48ka1Map1HT1SeSFtV03yhPGCvmpj3c+GnOOGYLObdXvu37itCbQdiEEODv3Nk+8ookHsYY
nB2t9irb9iTOkY1rs6wZaS2KuYKP60fN80dVjUAQBhI5OU8cxHDM8b0naDzupnXZcZ93gOkTpBH1
tSxCH7Cwqp1GU21REWRjFuYTp4jjWORGe9NYpydLbNzUyv9sn5PWRwEYKqqEK9NkzepmEg1kvbzw
hS/khS98IUEQcNddd/He976X0dFRrrnmGt7ylreQSvWGKS3L4k//9E87vjv55JObn9evX8+dd97Z
sT2VSvGXf/mXB3IdQwxxWDBbm2PHt7/Km35copg5lsdPvoRIWSxgWHfyeq64tDdy+/OM5x91DlJL
bt12G1+YuI3r3v02gr/+R06d/xHbOZ87//lBXnfduaxZNzSyhvj5wP3339/x97p16/jEJz6x7DFx
HPPHf/zHTE5OEkUR73znO7nkkktWU8wW+ihTXih5Kp944c3MNDsqZWbtIu7oOGk6lbhE8egzSBBi
z+Uxa9b0bCrO5bGigNr4GLEOyC7mGa+WmHy2A0GN0+1RXOEkCqoxZIpFZDoFjdeAkkjfb51Wt6f1
9dZ8VYuShVTAprYGpNVywOxkhWwuxZbj1zBb8Hrqig7EIW3qbHcN9rFTn3NUwmQ3s8ByKlKoI4w2
+HtnKOYc1o4dhzIGK4iwlM9yhlLD0BNxCJkuKngvwJ4qoTcfONGKik3SvBiB8XzEEoQWU9Vp7Nkp
LCmJgWJFsbnvnvVxje65qkJVE8aGo9f1Kqsqn4fMMc2/Z/IeM75PabJEztlFOSyjpUWONLuemkdr
w8gSNV1GqYQmfxlM1qYxoU86A+Ww3DSwTHcuWBRDHMNIfwNMoVBGUpMl1rojACxMeNQ8iZWCSGps
uiJjbWJXZmXrK6NRSmNhCAIJLmxf3IdJl/u2VpBa9111HUZC27miQRuP94mcCZLo1qw3T6TGwG6c
qxm6Tq4nrhBG/fumFaohlULM2qzukLL9bOXpMru9HMdv6rp/BsqqwNoOa0gzm/eIduWb749IRUgt
cSyn2SLCGIMqV7ByOeYrEdsnSpx8rI/xfHQYoMcs2JBrzZXWSQStvoby5ZA1G1bXkTZweOj+++/n
zjvv5N577+VlL3sZr33ta7n33nt55zvfyac//enVlHGIIY5IyMDn0b/7C17+ZIm9G89i+9rnI4xg
FwprbYb3X/ncI5Id8GBxwZbnI7Xkn5/8Kv8y91V+9R1vY+3f/yMnL9g8zfO5458e4HVvPod1G0cO
t6hDDLEiDqTX4te+9jXWrl3Lxz72MYrFIlddddWqG1iNd0k/tb1SCxNmZiVhfpZqwUZtGacczXMM
JwBJVCdSgmpbY9L2RqvOzn0QKxwFMZsA04zu1LbvQxlYOHGBSLqMxBLSoFSMTUIv7qYcdk2VkjqM
IMQOwqaCpKMYpSXTwSTYTqLsGUOMwpDuSFUyQL4SYqfCZi0MQBQlkTLfizoUW+EFiKrs6/XvSVET
gFZYgQdrE5YLYzRaG6K5OXTgEx/tIGYX8GrgaAFLBEFipSgWPOwwIownkN56dlYLrNu9h6w9jmyE
kJq/AS1ZpNT1YFiDaaNNxMUSxApZXJlVsXz/fWSe9aye72WQsEVajoWuVvECG60FUksaWrQhITQR
pRZjW3tT3MZOtSBmetHjxC1jHYZKg02xVGulj8rQYGmQCozdOZYxhmob06IX16NkbRGsjnlo+2zP
TVGp7GbkrLMRroOqVGBNi0LESEnppz/BlGt0V75obdj15ELb3xA/sIea5zL2kuOw053kKx0ySJWk
VSpN6CtkHlJHN0hewIsMpUXByFGGmdocm4FABiDqC19phFJUVcyYVSFTnoFj0ri+DxuydXk0WhmU
VNhttdleKMkZjd1dyWM650Zp2SG4IbkPTjtJyhLGvmUJ8n6BWlSj6MdYYQ4yCce74/kgbLBSlMPW
O0N01drNzgSIuYCatQMx1qL+b5dR6kSGfNWwjs53WKQjYhMArahU2Ytx/AinPo27i5MEfsRJa5/V
GnNxgWhuFiuToVSnk5ne+STaKzHijjAWFdhw3CZEuYYVhrjFSdLlWRZPOIku+3HVMJCB9YpXvIKt
W7fyxje+kRtvvJFMJgn5nn/++VxzzTWrKuAQQxyJiGZn2f7Jj7J1tsCjx7+U2dSppLMOj0tFxbL4
k6vPIvdzWm81CF567AXEWvLl7XdyW+lOrrj+LWz+7OcRRrGD87njnx/kymvPYeNRRzDd9RC/1Hjl
K1+5rAPk3//935fc9iu/8iu85jWvARLF0V7Bs35IYToVQVGuMTY7AcduhkwOYwT5kk8kJNbxKag7
aaersyzkM9Rcp8l2sGNxL4VKiNkZMFKwGB9VTa1jMZ4llouMsaFJDiC6alssmfSWmarOsc7ezHy2
iljCox6bViqhMEmERRrNvpkqtYIip0epuDXSba8Mo2h61fvdqbhYwexbxALMsYAReKEkCgP0uv7q
kzOT1H056yRqNIfGYJRC1euc/O3bwStSrrrYyoZ1iRLoiM73ebESEESKHBDriOKe7TiVCjJlN2Ve
CmE9Mpfo6S0ShUgayjUbgaqny4E0CszSnvZg925Km6xEDW+QGyrDzifmOe7EDQS+xvNcfD9L7Ia0
W4zFoFgnK6l/12VhGAyP7cqjjWEk4zA61qbIG92RCioVlCZj7GIGFRiiUcOGDe1jdWKpQGo7du9Y
SJ6v/Bwcs4Y9OyYZC0o4OiTd1gxclkrsnqpQrMQJC19g4aUsGKGnIXC5mkLKxGiJappsujeVtiFW
bn6BTLUGloITj+9ahIZIQowgqmomFhcI5xWhM4MZPzExnevPgh9KxnIuBkFuPjH2nCd2MWWdRrnk
Y1sWpZka67cmBkqxFjFf9NFVn2M3jXVNYN+Z7EBpKmbDiSl+MvMAz15/CmHJII2NV7OoeLrl+DCG
KJZNcg+rUoRNyU0bm5zGYZzp8U2MZltrP1J+h+NCFSFT85G5LHa5RXYV9WnE3Lf8TByYoaOCgFCF
uL6GbDJHtcoiaWFRCSv49XtsT82RyRuwRhLmzsXlRj20GEgD/NznPsfIyAgbNmwgCAL27NnDCSec
gG3b3H777ast4xBDHFGoPvgAU//7H3CCkP885eX4nMCadVkek4q8H/FbV5zB8Ud1E/T+4uHlx70U
27L40pN3cKe5m0tfdwXH3XE7YNix6QK+9sUHee01Z3H01t6UoyGGONw4mIb2IyNJdLZarfLe976X
3//931/xmHXrcgfFHjo2liGfh2wuxbp1OSLXo5YF5hfJpF1GlI8Y24CfcnFjhSzX2DBbYvz0s8jl
UmCncGsO6bRD7NqkhEMgNY7rEM4UMCJNNicIR9II5VJC4Dg22UyKVL2kKZ2xMLGN49ikUg7ZtIPQ
Fv6iwrEDxkfThLt3NK8zXVde01kXGUjSGYfIchEZl9HxDCkjieYiXM+QDquUxxUnjeZIpQJyWZex
DIxZNn5gMV8ukMvlSNspNm4cZXyqQu1n07i5RI0ZG01h1mZ5cuccal4zTYmzzzkaMZZGKti0aYyx
sSIpN9nf8ixSm1KsW5tl09gY2WxinYyPp8jKepTHUThZg2cVSVkpspkTSGGTyaaoVGtkhG5eayWQ
uJbAkCGTThHjQOQQpxwyGZeUdMm5LrmRFHYYks7aVFRIVo0wlkoxna8gwwypMRuhbQrVCJwI4Uqm
1AYysWbtuhxjYQZyKSzXJp2ycW2LSuARlzRshUw2KdmI3YDFhQq5kSxu2sEOHdJpF4ODthWZbApn
Zo6yazevIZdLsWnTGFOjRaYrc2TGNjE6mjjU167NMb7GMBZlyWYD9KIh7ztks4nVkc05ZBCQdvCM
R1alSMeGkXVpbMdibUaRzaQwtQxWxaHiKxzXJlV3clQpkXOzjI2mqfkGdySN7QrG3Cx2JrneQBm8
+QLueIVMMMLYeGIprLVdQpXUz7kpGyEsAukyPp5BpV1qkSKXdcF1EHaKRl/p3Eia0XEXS9gorUin
Q3KWi3ZtnLqVHKo4YYsMIzKuxXitzNgxLvE8ZLMuaVsyMpqhXItxUwZtpyhUI9Ipm/GUTTp2kRoy
GYcRBVK7eF6a2XwKuTBBjZhjxjaTy7qMj2Uxs1OUJybIHns0uioYH8uyadMYszkXiWF8PItVTZGu
OQgFbibF+FgGMg4gYDRNxtOMbd8FJ21lT3kv670t1MIREJrdsx6p50DGzjI2nuXBn+2hvJBibCSF
69pU7BKjmQykHLQUOI5FEGtGHRuhDa6brOf160dASVKpZP1oBCNZFxWncGzI5NLJnAO4DtlsipGs
zfoNo0zn4+TehTFuysXtes+ksi65tEutaDXXcyabIjuSnHfdhhEm8wrPeKRxyWXdhCHRdXDtxjvH
Zv2oQybr4jgK17YRrkuqrYHxyEiaTZtWT1cbyMD67ne/y+23387tt9/O4uIiv/M7v8Pb3vY2rr32
2lUTbIghjjQYYyjcfRcLX7kNaVvc89yXY4UnsGnLGDuEYXqqxhUvOYGXnLll5cF+QXDRsS8mZaW4
ddttfHvsfl5x4cWc9sN7MJZg58bz+dq/PMSrrjyDk04bsoEOcWTh2GOPBSCKIr73ve8124oopZiY
mOD3fu/3lj1+enqad71R43MrAAAgAElEQVTrXbzlLW/hyiuvXPF8hWUohQdBqZxoH+XFgPxijUro
E/gRdiQRWhJXSjhWBhnFxLFEakXkBZTKHrYXUbVCgkAQBsn2CIljCSIJKvKI0w75gsJPxQR+RBQp
4ljhBxFRbGEA34c4SuFKRaViE/oRQmtqfoTjxASlGqV8hKxTj4ehYqHos3e6RKwjQksS2oBlUy77
xGjCmmJkZg5tZQiUz/yiIYoUng6Z/enDePY0U+njcKwaOVJsiF1mN6QoV3yiWGLq0aBK0UOmfXw/
Iq7A5MI8m/bAnn0+fmiQT+yjvFjCj0JsYRN4GtuPWMhXSQdp/DrTYrms8b0QyBIGMYvFMlGUzJex
YkQlTxj5hMbCmi/QSGK0tEJqhdaaIIyYi6axQ4NOKYIgxoQxnoyZW9xG2nPwfI0b+UwXJeXIRiKJ
Q0kh9hi3DLaU1IIaYzmbXZMFZu7bzTEnp/GkD5WQp+Zbrnj/hAyyaoirVQIvBCGY9OdYdCocnXKI
Q4mUijCUaCSxlARejB0WiCLZvF+eFzE/X2FycY68X+A/dzyEqiS/Z/mCQygl5XIyx6ZQJh1XiHXE
mvQayhVF4FiIMKbixajQYmohxniLrDlqlGLg4QcRXs0iyMbMLUjSJR9hCcYzY8wVCiAKjCnwfUme
PMW4ALkxNgUR5XJI1Q0xvk/aifAfegL5nJPBdcjrKkEQI6UiihQWGmxBuRzwyMMTlOrsc04ssaxW
2mmt4rOrUETphCBD5jW2E2PFqjknsrlvQLZcQFhFdtYeJRX4pMkQZgzVckA4XaQYCLQTElmJHCnl
kolioliwWPSJkfghRJEhikKqswuMjmeZlot4XrKm4z0PoWOP9GKJIJOivGmUx57YxlOze8mJDYhC
FacaEAQRSmuUH1Ku+OxdnMaxbNZUNxDNFvBzEWbXFN76o5BRljiUoDSLJs/E3ifIqpiosIXFiQgV
xmQci6K/gOfZ1Cpl1kUSLXUzNVdKhdKGOJb4QcR/fPsRxmoT6EIVGSsCnTgYlIqwLZjcV8CrP1NO
LPH9CMvY/OSRKSbyBYJUgexECVUzxFKxUMuz4CtG7TXoqo+agVgECBLjKvAjFotlxhazSK1ZWCwR
hjGlMMDJxsxHU2yQBqveHNyrhUw+uI1qNbmXsVYYoZrXA0nD8fn5/WEi7I+ljLSBDKzbbruN2267
DUh+lP7v//2/vOlNbxoaWEP80sBIyewXPk/5h98nHsvx7VNeRLZyPJu3jDE/nuKpJ+d5wWmbuOqi
kw63qM84LtjyfFJ2iv/z2D/z7896EpM/k9MffxRhW+zedD7fvv0xLrz0FM56/tbDLeoQQ/Tg3e9+
N77vs3fvXl7wghfw4x//mHPPPXfZYxYWFviN3/gNbrzxRl784hevuoxeINk9UyY9KhHA9NNVAquz
6Nwq53GrHskeSVRAa4GnylhVWIwlnh/h6DrVV5+cu4UisEa2Vem04Ksq0Mrfi2OH9uahNVUhU5js
IRSYrzPStdeBxNKQr2jGeso0DfOL9Qa49XEqZZ8FNyCdssAvsEaMEU5NEIedLHZGKxb3+egokWhh
cZI5bzd+mMMYw/yDj1LKV6ipMhkrh9CJ+rNzqsRRJy5N29bBY2AMjh+ApdFOeglSDdHcN9YRmTDq
SL2TRuFqC6UltuOgdVseJCY5rk6i0ECDFCOKFQjIVwI2usmwBkBrUpUa/sQEIqpg1tUbxkofL25N
cqN2qPu6mufRrXqouWJAPC7ZaIHRGn/nLlLPatXYVFWZclTFQqBSCmPqaXexTuqt/IhMKouKWxRz
2hiElIzMzBFt3IyBxBEgI0Q1xrQtCF8FQJaaqrCJeilb14Rbc4voY49iNl9r9vjar3Qzk7BlCiPI
Ts6j8wqOOrrvrkIbRKyJLQcd+NhGUwrLkBnDyyvsUpUykLVERxlYQ56yF2AcB2il0Dbq2AhCUrNz
2NmQyGoxRNqlAkYey9RPfwi4BDogOzvNYjCP0vV1JmUz905qRSEsIZRCKYFdC3AK+5gfkR2VXKoM
ZibPxL4y1ppj6U7mc6vtLVd6XxbaaKqTeynHPinPQ9t2vZauNfuzs/17YzbaSJSn86jpEmnfxthp
aqoMjBCbENskxDO1MHECZbLJffLaSG+0tggiRSTVkoaMNJowtGkwnfp+FjoSala3CmugPlhxHHcw
BR6RFLZDDLFKUF6NiU/cTPmH38faeizfOPMFZCvHs+HoESobs/zoyXlOOXYNv3nFc7B+AUktBsHz
Np/F75z9Nixhcfc5c+zcegwnLDzKs/zHyI64/PDfdvCf9zx9QH1RhhhiNbFr1y4+//nPc+mll/Jb
v/VbfPnLX2Zubm7ZYz71qU9RLpf5u7/7O66//nquv/56gnrPp9VAoxlto+FtECkWF4OE7jrW1IIY
T1XQRvUo/Uorgljg+clvuFJ1I6FnP0MYaYIG0YFOeOggqUGSJu7RRxKWtOSzNDG1uad6lOBSXCDZ
rXVwVHMoVjWl5VrnGUOsZFOpCiPd7I21c/silXmvc1SdKIKyzg1h/JBd0yWqfsxiOWBivkaokuOT
HmDJkZFSLBQDlJFoo9FG0c6Vlw8KbTIl81/zY2TczarWS/3egFtLopci8BBSUa4lSraxrFavsQ5a
6c7jrSjG3rcTgrBJdQ5QrFnkKxaZYhkrismXAyr5aufRbb9JxajIchaWL4OkebNUaG2acvozeeZ3
LZB/YDtBSfVlovTCuKNpbDfDozGJUdb8u97vDBLDQARRUy5lVJOKXiBYjGdZDFoRu27RZ9t6jLVv
3Lvgs9ffgTKdtPYdTeWAaMEg5w1GC5y5yV62i/oFGCMIw3SX1d25WxS31H0RRc19lVFoI9C6N03Y
TCfvG39momNoUydlgcRZYmHhVwIqfmtuRaHSsV4jGeMFkoVCqnUZYbQkxabldUbW7SBgdLrV6ytV
7o3w1CqTFP0ikWnJ0XzODcRKMhdOU1EtohalNdPVaarBQp2SnVYNnxHEUbp+xtb6rHoxxWpEzY9Z
LIWJIV2/qNmpuFefsFoXmQ9KbH/6gebfjXHtcPXe090YKIL1qle9ire+9a3N7vZ33303r3zlK1dV
sCGGOBIgSyUm/+pmwn37GHneeXx583rGn95KZtxGbxnn+w9McvxRo/z+r55N2n0GC92PQDxnw2m8
77x38ncPfYZvvaTMtd8e4YTd95N6yRam0lt48P59VMshr7z8dGxnIN/OEEOsOjZs2IAQghNPPJEn
n3ySq666iiiKlj3mgx/8IB/84AefIQnp7LWjDQtln0ArsmlNEMaMZiDSMcWoRE60mAVi6VAoLrDV
tD9vBqF1vYdP+7CJAhJXbKxghhFvHmML/MgQ6CUsIWM69LZ+bGWeqpEiiUz4kYSsizECrU2fpqqt
9rEaw3R1kVi6ZIpFhFSodBqpNAvlGmxe13Fk97ljnXxTbXR5dVvXqEwMJlO/BI3RkkWvAAKq05LR
nG4fuAWVGFVKG1yvf68tgPmKRtsaASjpoAsKk61glWNGZB4pNpD2fIzuR23dR7kXEEcaFuZga0tt
U3V2tiQtysIPJEolrIhCJHNMG5uf5+VIjSTz4QUOKVfjtDUMni3n+cnMA2idTiIjdRUx8mOyQKkm
CJ+YR69tRQ81Bj+ULJYkxdGgSeYoSAzz9pWXUIo3mDgSmzjuIk5ZKGsWqkUWLYGdibDSNsooqrLW
HKtBQY8QiGIFMV/D09XWJNYX0Xy1QmSN45kaY05vOyE06ABM3VY2RiBN3LeJsSXjZvpZ2YvJNFn6
OveNIhuyYMURVtWHLhINY1r7+5EkU+83J3VEsVxl3Zq2/UXrnzi2cFNQrAV4xGTcdNKnjBYTIyTs
gyqSxMawQQmMEYgggH59qWg1dm44b3KL+WbrMmPACUKi8YYU9ZBp6CXhGQNCKbBtpNJUPEk2BdXQ
YHKGKCpRsyVrqPems6D85P3E5RxyY472UI2M04AiNjEOSWRcaI2xLErVxJATSlJ96EFmM+sRorfZ
tNEGjcFC4PsOvt96VnyVvMMyhRLBOlDpzKqzCA6k5fzhH/4h119/Pbt27WLfvn3ccMMNvO9971tl
0YYY4vAiXlxk3198mHDfPtZc/Ar+85wzSD19LKQV9nHrufuBSbZsyPFfrz2XXGYY1QU4buxY/uD5
72bTmqP5+ssz+Cmbzf/xNZ5zis3RW8fZsW2Ob9z2MGHQv6fGEEM80zj11FP5sz/7My644AI++9nP
cssttxD3aYh5ONFQI6xyDXvfDKpBm9ymIURRioqXor1dTRimUJ5A0voytZh4mpVqOYTajZNUvoKI
EoVGK4uK36bE9GgkXZSGdKqbWtWp5Y1BNqMIdYVO6WbqWztUw24xilhaRGEKOwiTXk1a4wUy6VMF
zbQkA9S8uIPq2wsloW4ZQQYQpv1akn09XW1FAAwEgYNUrf2qntOcK1FZmTpda1HPnGxcp43AoGIf
T1VwgoBsvojwAywpu6bU9MxxLWg1U/a9Xpa/iiwilGpGGy0psfdOs7hYZaEUUAn7G8dRZFOtuQRB
ix47Vjoxegt5cguL2KV65K2NwU4EIU41mVelDOVaRLkWoTVk8oWW+AJ8XW3e93hxgQ6VUyfRkg4D
S2vQmrCeVhiValBcRCmLmuey6C+Sjxfq/ZsMs6WAYGaRUPvE9YhZRwQIuvtgE0iFVIpiLSRfCZGF
zu2eqhKZ3ihHplBqfrakJAiX/w0TWqOM6oiyBN29pEzCMJhMRz066mXQWtTXnGj7r/PaolhT9WOC
StL3qSlb1as7UAzFcjppBxAs8z6rPzPVer1Ud9vrxqeGHS7qa7QWxDh1R6ldd0iFsSSSAi8yaAPZ
hTxidh/SRAQyxJiEeTTWuvWcN+QW9cRkkzgmgqknyM3Nd/TMIwxRGsL5/lSAfiSbKYjt19Ad6bLq
GQGrnVAzsBv55JNP5rLLLuNVr3oVa9as4cc//vFqyjXEEIcV0cwM+z7658SzM6z7ldcy/dIXMfMf
Ism93zLOdx+bZeumUf7wuucxnuvjGfslxobsOt5/3u+yZetpfOuiMYTQ6Nv+Ny+8YDMnPnsjU3uL
3PFPD1CthCsPNsQQq4wPfehDXHbZZZxyyim85z3vYW5u7shreF93Hlv1hrHCT9LjhO8jlMYYQRwn
Tp5mbUZdL8stdCkjqtGo01qmMW+PmtoUox1RwSKudXno2z4H1eTdOB9PU5VVgiBNHCWKVLEWMr1Y
SzzgbUe2B8tM11kFSSqa1oqaLOHrWuJQl4q5okex2vZOEUnaYqRbCldjbqCl/IQ6QJqW4quMYb4Y
NBXoIHDw/cSlX5ErG1gt4evxqGahlKgr3KKPZteIrKm66AKNRhlJrDSxjlBGolRCwa5UklqoG/t3
UWInSnVIplDAFEv0g8IQxIpYtmVeGCjc+wheYR4AvSBbG7rkFYDsy7vdJoOU+GGxeb0dxrffZZh7
Afa+RmpasmemUGJkbp4gyBBGDvm8RxBmCCKbINKI2UUqMwUUvYZ6P0Q6ua+1uoOv5vePVMs+za/b
0WEYW/2fokyhRKBr1HTiDFF1J4dSredFkERxa2GAJ1sL3/NyBEEGEOybr1L1EuMnbjPQGvVyUSzJ
L0w2v3eKrTTMYi0kX+6SryO9s9dB0rsvffeJlSaMeqnYYyk6HBuQRLGV1rQHrIUxHU6e7miU9IvN
/RrCWDImHxSoxTVMLJtRN3txlt41ChU/olTr1TNMz4fVwUApgv/zf/5P7rnnHo477rjmd0IIPv/5
z6+aYEMMcbgQzc6w72MfQZWKbHzDNZgLL+L7n/kPXJ2lcrTkid0VTt26ht+75uxh5GoJ5Nws7z7n
N7ktdyc/KPw/Lv5ZlV1//wle+qH/xchoikd/NsXtt/6My3/1bNZvGjYkHuLw4T3veQ+ve93riKKI
Sy65ZNUbBh8I6vYVkKRkYcCOYpxStWfffuZQu/ISRkspzS1EuuURb49udetZJgYdLKegieYxStlI
6aD8FMZNrkgqgaV6lTQAr7CHNT0u4GQwX1cRU4nhoLUhjhW2SVLVWplMyTUHuoYiJqtzHZmWljak
H59GbR1lJ3uJpCLl2CilCWUKISSjaWdgHay75qgbTrx02mkkNYkDvyVg3DVeTVUY1WMU9il8P4vv
1qjVUxZ1RNftTBRnO4yoaQthdxogQax6oykkURevkmWkniKW8gKEX6tPqWkagMkp2tNDTcf37cgs
zMNJJybjt90AUV06qtKRdmrqnoJGeqe0qVRdEmb5eupY2/lrQYRjW6zNtX6bYxNRlgWcLsV/qZu7
VGPebhkjleQ5NswEqXVPw+RIh6SMJNa99z+zWCAOch3kDe3wTQ0dK1SYRGe9MGbUdRJni2UBSapv
qMLWnHXZU36oSDtdqn6z8G/lqxRNB0Hj2NbHauAAUcemPu2vgMTYWee2ag6F1mjdnvTZvoY6hAXA
rdWwPVhUEohx45HmXIvQx8q2LrziR4xlU6glHADpShUsC3vnIvKEU3HWLE1yczAYyMC69957ueuu
u5oNhocY4hcV8cI8E3/5F6hSkU1vfgtjr7iEW/7Pt3GDHKX1JZ6aGeOckzfwO1ed+Utfc7USbMvm
utPewPdyR/Hk4q2ctmeR7/1/H+OyD3yQ0fEM9313J7d/4QEuu+ZMjjludV5wQwyxEt70pjfxjW98
g5tuuomLLrqI173udVxwwQWHW6wOxJHC1GuKwlgjbAuM7jF4lFGoelH8Eo71NjRSj3qVkLDDwGpD
H30lXa5gRkBKB8eR4Kab3NZCa6LIRQhZ98i3n3uZQQEr8InT3Yoh+LrWoej7oWwqUkIr0uUS0cgo
ncZKhDa5jlM5vo/UFtmpRWbzKTKhRBtT98pbNHTSoIfMYjCIttQmZxkCFFM3DNNpO4lu1M/bfg8a
KAcLOCObAVj0fRr1TJGfRrsta7StDCmJgHWlvPYzrtrhtaWU6R3b0alNeNIDnXyfqVSRnoCUUz9f
a2Ij2alhxzpCyqC+Vlv3pJlyt4KSH0ZJmpvUhlStRcggldUczmBanw3EXfVGyiiU8dsU3mSGlC9W
bAq9FKQ2CcV9LWLdeL0vnr9Uk+2ljWu35mEsq7le2tNmYyNJt6XUJmQzDsZYPWydDfRLu+2GaLOv
el4T9S+ktJNgtzGkqu2OHNNcz7pHBtMRMe5GzZe49YQfq22d9DonOiWzohi7UsPYI/ixIuvaFCsL
ze3KSGir6FrKsGpHulTGrLfRQdDFLHjoMFCK4HHHHTdk/xriFx5xPs/EzX+BzOfZ+MZfZd2rXs2X
v/EDxEKO6liRp/KjvOTMo3nXG84aGlf7gYuPeykn/ea7WFzj8OxdO7ntc5/gnAu28sorTkfGim/8
y0M8/cTyrG1DDLFaePnLX87NN9/M3XffzUUXXcRHP/pRXvGKVxxusTrQVJoE6DbFvfG73PIEN5ja
uoq/+4zZ/pMey4SJsD9aO9pRhOO3ogDpSpLWFAQZwjBNHLuYNtIEO4paNUl9IE1MJl9sfu5GEMpO
3cPonkhA07gyGrdcxvEDcguLOHHcMS/NSEgfZAqJDN0pTxqDlA3F91AR8/SXodMA7Q9VVoiwni4W
L5M90ZYG1qsEr4z221Xxk+uuRa37bkmJsEwzIhjoluHT715LmSjkjTqbZc9tEmOpwbyoGk2ftWlL
J6VjGpU2xFHnfMQq7nhW+kEchF7bONQp9E/B3B+0G+M9xkHg0X6xNVVCKavTiKwfEindY1w2oDHU
wjgZvyOC1Xk+GTtoLQiCTHOspQhdup0AsY461kI32q9t0LkXxmCFLYbJZgS+bSx1AGscGjWiq8f8
PFAEa82aNVx++eU873nP66Br//CHP7xqgg0xxDMJWS4z8Zd/Qbwwz4bXXcX6yy7nx49to7gNwrTH
U16aV7/weN70ylN+aanYDwbPPeYsdrz9PQR/9Vecfd8j3LL5k7ztNe/ktSNn8e3bH+PuOx7nwldF
nPWCYa+sIZ557Nixg29+85vcddddbNmyhRtuuOFwi9SBpVR7r1knlOkg+YKGMtNWa9EHg6RCtSOz
WOxQ7IVssOolSrA2FnG30hU7GNNrPBg6FbRBDAE7XNo7bhUWsHVQ17sMdhAQ2S0Ch0iCcdqudwAv
d3vBfDv728EgitLLbl/xLEswXArTFsVp/l/Du39gMG3rp73eKopcrDpLYnKOJfLC6kjv3s7i6DgW
SVrbUudR2jBb8Jq1Ne3ouV1tQY6y59LeX2pF1J8VO4zQzsGn+Xf3Les92YHDy+9iZKSNiL1tOCFM
x7pcKnJjaK3lqheTKyfRn0gqhNVrPMVxb115Mx1vGVnbjatUqbcPVrvDY7nar06nSu/HWOmON5ci
BpNeQbpexNKi5klWq4p+IAProosu4qKLLlolEYYY4vBCRxFTf/PXTUKL9Ve+ntniIvfdtRdbuOy0
Q6560fN47YtO6MghH2L/cMrp57Dtjb+G/aVbed53HuETo3/Nb5//dl7/lnP55pcf5off2UEcK857
8QmHW9Qhfolw5ZVXYts2r3/96/nc5z7H5s2bD7dIfdDFItZHN2km+/XJ++mfgSIIQqtT6ekDOYiC
3mbHBcrrUTbbi9lbMh2Y13l/0B51qqkK7d0hGkr9oTKcBoZ2gYNgUe3zGySkxJYSsgOcfj+Nam00
NVUhMbI0Vj33VCsLPZAGCVEsWCiFdQW8U501GPwwRmOItV5S2e3uLdYxvlrauHJqNSypsNqON3Wy
k1SlSjxyYDXA7c9UZd88lm4Zmaly/ya7B4ogyCDaDFOtrI5loIxBL5MaqNqj3l3brDjuaTTcb6ob
xluDqn4luH6v4Tbos6ZMO7umwYoiDDTbRfSLtlsyxmpLhR1knRvAsg9zBOvqq69mYmKCHTt2cOGF
FzI9Pd1BeDHEED+vMMYw+9lPE+x8mrEXvZiNb/xVIhXzxdt+QDpey+ToIm+88EVcfO6xh1vUXwic
cekl/HT7Ltb97Iecc88O/kJ8kt899ze4+tfO42tffJD7v7cLow3Pf+mzDreoQ/yS4Oabb+a00047
3GIsi4YypTRIZRBOO3V2guUybmqyt1moMaIjLc11Y2Lp9mhgUZ9aIADbsjoUt+b3K/QQa56/z3eW
jIEsdrT/DKP9UgzboYzEGUzl6YtBUvgOFO3KupASs0SfwP2JOC6VgrW/dm1VlQjVGGljqPoxYyMt
E6idqlx3RaYyVq4Z0Sj7AlsoTKo30iWVYb5SRkpFJrX0/el2EtSCCMgumbrZ8DOkK71EMIcaru8T
t92z7pS6eIW1uRISB0U9VbWRMmla74ViNcQJYtxD1F9SyqXvQ7pcXXIVCiFWLCdqGFnLmTVFucBo
Y0ydkLUsZ9YZY0iXOt9x3XTt/Q8Ex169npwDjfytb32Ld77znfz5n/85pVKJN7/5zdx5552rJtQQ
QzxTWPzaHVR+dD+ZU07lqLf+BgCfuvNrpPNrqWQqvP6Slw6Nq0OM897x6xQ3Hc8pExGnPTTPXz/w
D0ybCV7/lnMZG0/zox/s5sc/3H24xRzilwRHunEFNLWRWBmMSdLfnGUa3Q4CKXujSiv5chupgEvB
DJCiY2mN0Ip+JpZbS66pUZe1P/BUdcU0w7APucMgMh9aDOBZP9iSd2OaNUzd8MIDUfZXFqib9bB9
NYXax1M1oqg3PtXdJyrWUV/Gvc5jkn+Xq4vT2hDI5VMX2+sFV0vRNjwz0VpYet0sz8R+6PgV+jVo
7obv18Osy8xJu/NIDBAxW8m50jG26AhjI1ZmAzpgDLSi/vEf/5EvfvGLjIyMsGHDBm6//XZuueWW
VRNqiCGeCZTv+w/yX78Td9MmjnnXe7Bcl7/5zu04O9Yh7ZhLLj2PF55x9OEW8xcOwrY5+w/fh5ca
4SUPVTlq0uPvH/oMO6MdvO4t5zK2JsNPfribn9y7+3CLOsQQRwT6ZSYvHykaJD1mMMXCtCmeul2h
7Xf4ALqakBLHDwbbeT9hBuyH1HXQEQdZT/cK1nbRm5nl71t3FKkfBmFY60Z7FCgYsEl8vzXbL1W0
G77u3xi5HYOQrlX8aMVmwLpOFWmHweqZ2QNOdzg2dtDnkAMwCK4mBrnUZprgcjsbg2sltYoHQ0TS
D65oGfmGI8DAsiyL0dHR5t+bN2+uc/AvDa01N954I9deey3XX389e/bs6dj+2c9+lssvv5zrr7+e
66+/np07d654zBBDHCr427cz+9nPYGWzHPOe92GPjvG33/0G8nEHy9icc+EJPP+5w8jVaiGzfh1H
veN3McLiNd+vMuYZPv3YP7EzfjqJZK3J8OMf7ObxB6cOt6hDDHHY0fQMH0pdYEC9RWaWJ2XYX7hW
atnTp56BlK52rFSDdqixEiFEB7rud6w0JW/p9MlynET+VlPNjgdU4lczLtieHncgsC0Lu06N7vg+
1gFS8bdjKfa+QaEyvdG9QeudVsJyj/qBsEwufaJDYwzZUSsi1Z0KfSDIpBxcxyJjjeCIdlITs6It
czAYaORTTz2VL3zhC0gp2bZtG//9v/93Tj/99GWP+c53vkMURXzpS1/i/e9/Px/5yEc6tj/66KN8
9KMf5dZbb+XWW2/lpJNOWvGYIYY4FIjm55j6209itGbLO9+Nu2ULf/Pv/0Z+5zwZf5xjThvnZS9+
9uEW8xceW855Lrz6KnIy4jX/FpLSNp9+9Avsip7mimvPJpN1+f63n2LXUwsrDzbEEAeIyclJfv3X
f51Xv/rVzM3NccMNNzAxMXG4xepEQ5kcUKsMTWf6YD9WtiXP0Q3dGcUCGM32pyJYKYUQqDPJwVJq
n1tbOYKxFA5EWVyW7vwIhFJHYMiNfnezc0GpQ9hH1dA/xXV/YUlJulQmVT3wNbcSlqtn6sBBWIzd
fdYs+/BEsg5FjSB0vwMOzbWkHBtL9CYxilX0rww09I033sjs7CzpdJo//uM/ZnR0lP/xP/7Hssf8
9Kc/bTIPnnvuubw9G+0AACAASURBVDz66KMd2x977DFuueUWrrvuOv7hH/5hoGOGGOJgobwaU5/8
K1S1wub/cj3p087gk9/6LhPFR9g4dyK5NQ6vveKcwy3mLw1Ov+ZKyiefxVHVApf8MIuDxf9+9AvM
MsVrf/UsbMfi3772ONMTB99rZIgh+uHGG2/kN3/zNxkZGWHTpk1cccUV/NEf/dHhFqsDFgIhJVYY
DaSHdadQDeJYHrFG++4nTML9nrYyTRZV21qi2uLI1P1/oeAOYgwcAffB6lIvw/GDSIHrgrdkz7af
Zxy4gdXtANFrO43ZON6PqOkzhRVeSk3CjkNhX4mG/dqHgXMVmaEHMrByuRzvf//7+epXv8rtt9/O
H/3RH3WkDPZDtVrt2Me2bWRbuPPyyy/nQx/6EJ/73Of46U9/yj333LPiMUMMcTAwUjL9qb8jmp5i
7aWvIfuSl3Hznd9jJ99j6+4zERZc8cZzcYdNhJ8xCCE4933vojy2kVP27eDSJ49DALc88nnkeI3X
XP1cjDZ868uPUFhcuoHhEEMcKAqFAhdeeCHGGIQQvOlNb6JafWbT1FaCEOCUCodgnJYyYYuWZ12l
Utjrt/Q9RmbT9Uaogqx1YJTWXVIgxID0722wD1GtxGqyhh16JNdsWfunZQZ9yDxWH20NqYVNT7Pr
I6zDSbdefbht0oOan27hcxbZbCuKPWhaZzs6U+mWx2rc2sbjbseHzpjulrNPn+VDioHeNKeffjpn
nHFGx38ve9nLlj1mdHSUWluYT2uNUy8oNMbw1re+lfXr15NKpbj44ot5/PHHlz1miCEOBsYY5r74
T3iPP8bIOeeSu/INfOQr32Vv7jts3fNcHJnmJa88hQ2bl3ccDHHo4WQyPPsP/wDPzXHyT+7ntcXn
EqiAv33w04weY3HxrzybKJTc9dVHCAcssB5iiEGRyWSYmZlpGh8/+clPSKVWq/XkAUIIRDwY/fly
aG+SnrYyWPX8GO3YqHUbm0X/7VDpDOGa8UOmRCWxL7HfDXBHsonCZw6yZsLuY2Adue0N65TWB1iI
YrXlP4nNxx+0NN1zbwu3WVPXg55crJUnWQjRQQO/mug22Puti2dEjoaj44AWYX19dKcAW2K/jfJu
ZKzcsttdNzF8gnVreralnIN3Ujdi5IeqDq19QTafp1W2sAZaUU888QTbtm1j27ZtPPzww3z84x/n
sssuW/aY8847j+9///sAPPjggzz72a2almq1yhVXXEGtVsMYw/3338+ZZ5657DFDDHEwKH7nbkrf
u4f0cceRve7XuenLP2B23T1szB/LWHkTx5+0nrOePyS1OFwYP+Zo1r39XUhhc+y/fpcr7RdQisp8
6uHPcuJzN3DO+Vsp5n2+8/XHmx3lhxjiUOC//bf/xjve8Q52797N61//ev7gD/6AP/mTPzncYnXg
YA0ANxX3HaedUUtg9aQaNaBSnd7sgxNHHNT16ENg/I4768hYOXL2GKNZl7HcM2tQp6wM9gB1Mmud
DYnhv8J8uUvct5zVlpbXVWwyMrJ8qmGjJ1W7UaVP7O+AdN24p/5mEMrubqRT9mBK6SFAJtVpBKwk
rW3vf5pd+3NjWf1/t1pGcL/0tQP7rVvqOR4UaSuz5IQ0n91Gb750sm/7/c6mDy4wcqgdHi2Rrfr4
z4wOsd+z4Loul112GZ/61KeW3e/SSy/l3nvv5c1vfjPGGG666Sa+/vWv43ke1157Le973/u44YYb
SKVSvPjFL+biiy9Ga91zzBBDHCyqDz3I/G3/gr1mLanr38FNd/wIb+v3ycYZtkw8h2zO5RWXn76q
ubhDrIwTznsuP738Oka/8QWO/pfvcfHbXsz3qg9z6+Nf4tcv/i/k52vsfTrPj76/ixe9/KTDLe4Q
vyA4++yz+cpXvsLu3btRSnHSSScdcREsx7YGUldzmUSha9SoCAQGg1i6NWjzkyUs3LHN6LAPwccA
78bNzz2L/PZtyBXS04QYjBnMtVId/ZAaCtwgqpEQgpTIEOreXmFpKwUaUlaaNaMpKn0igzl7hIj+
DZZXOu8gFOJpV+PJ/vsJGk1yE0PUuC5CD+7F166L1ZZW5ToWxoAWnatgpVtq28kO2nFaLQG6Dkqn
YuI4RdrVhF32Rx86gRVldyxroP2WhRADFR0m8zvY/QJwXDkQzXyPLHVksx5BmGk2Cm7IoJdhlcxk
glbfqP2Ba1E84Xjcxxf3/9hlkHbtZrph+2pSqRRupJtRaUFvI3LXjQnsMdwwWpGMxjnEzH6C+q0Q
omNpGCCMQ1aL5mYgA+uOO+5oCWQM27dvx3WXF8myLP70T/+047uTTz65+fmqq67iqquuWvGYIYY4
GIT79jJ9y98jXBeu+y0+ctfjRCfei21Lzth5CbGGV15xOrlnKC1hiOXx/Ktexb/vneK4h/8fp37x
Z8xe8ywemH+EY/few6Wvfxlf/dzPeOC+vWzYPMKpzznqcIs7xM8xPvCBDyy7/cMf/vAzJMnKEAI2
rs0yVeytQ7Qs3aQaFyTRjO69bFuRstIYlqlnEOBm1xAy0Txpqw+TYDnF13UEuYxLxbGxRY0wbFG7
5zIu5WpLiRxEfRbLUHsJYXAda1labEfYpK3+BlbWzuHXG9BmXJtKDFZPfdfqOtvSjk0kehX2lGOT
cm2qflQ3FBo6+uAe9/ZaHsuCkZRLLYzZ32tq7t02N91SZNOatBtgLEUneXyXIWZlqJH09coUlyYs
EgL0xrVQm11yHzcVE0ct/TPt2oRtJA7GEoj9ZFpMp0MwDpmUs2z9Wi7n4XnLp84th25Hh2n8bwmD
YpAoZ5+TJGO7+xc/iUdyHc2pu0lKhBBkUw7S73VIRGMjuIutulWTSUEXEYltK7QQpK0svuofPXVt
C8u2eiKyubRDeYWm0SuhRXLRuTblfjgv9hcD3YH777+/4+9169bxiU98YlUEGmKIQwVZLDL5yb/C
hCHRG97K3947Caf8J1ba54KF11Atw9kv2MrxJ2043KIO0YaX/e5/4a7/lee0iQe5+Jsp8q8e45u7
7ubENcdz2RvP5Kuf/xnf/dcn2bh5lHUbD0XR/RC/jDj//PMPtwj7BYEgm3aoep2KRiYTNJW+tem1
1GS5ua2RtmXbmoyTo6pXYuNsKTb+hnVoJ1FknU3Act0ShEmiZV0RDsvS9GQrZUbB7zUUBeA4Frbj
EUUpGupJJuXUIzqJcSVscOsG5VJG1lh6FNllSzYM0fZMBSEsNubWA4JSdR5l9qOgvk+kZMDgCeOj
KYKaJLJSGN05Zw3xGvE6yxKwX7pl+/XVo1ArpFULy/TI0UBHY+OuXVIiTWSFS4qXstJYWFgiMSRV
JoNK+Us3yU67mFSv8961LZSRCMuQcqMOA0tYgvGRFOVaMqYRFmIF6rmGPdNxvwRkHHtJA0tg9j+1
rGOt9UnRtVyUUcjs0r3mUqkIGWcGbkGQRED7p5UuF7VUKRe3y+7J2aN4KjGc+kX6upkhXcdK0gOX
SFHMrIlx8yn8JRaMbTlk3P1zBKSsNJEOeyLeDcQjWYyKWZdeSyFsPN8NK9QM1Jz7QDGQgXUkefKG
GGIQ6DBk8m/+GlnIU3nJa/j7bRGp0+9HZD0usi+hsNNmw+aRYarZEQjXsXnpf/0dfvBnH+eM+ad4
ww+O4zMvFnz2sS/ygfN/n1e89jTuvuNx7r7zcd5ww3lD1schDghXX3118/O2bdu47777sG2bl770
pR3ZFsvhoYce4uabb+bWW29dLTGbiHWiHLSrOdms36ztcK0UjpU8CzKbwfGTFLdMJvnXsU0P5bHr
CsJ66EHQSYrQQNq10PWCeUe4OMJlTWacatBKP2qkWw0C0WSY61TYUq6dpEI6CssKiIOEajpTf76z
bgacLHl6o1LdGMu6FJZoHmtbAj22FuOmgBqWsHHWrUOXixC2UisPBK5wCVmZjKRVdWM6DZg2CEQ9
lfIg6Lvro1uWQB1oCnz9MNeNUV2yOMIlMks3Pm4nSljjrKck80Tjo2QX8v1PtQRT5EjOAitAKoO/
YT1BKkumkDRVTjkWFoJ0yiGMJNgClglKpF2bdFv9Vc4eQ4lwv7kOonXjWPP7x+xpWQpwEEKQs0ax
hENVlRBtBrDjyI7eWY4jsWQWIxShbqWtWsLC9DUOOtkcDaZpnKVcp+MyjeMgliWRWHrN2MJGpVMd
+426IxiiupHbNZIA4YKzTC3biD2C6om/Lw9HuGScHJEOievPXsYaIdCJtRiNjaNH0vz/7L15tB1X
def/OadOzXe+97375qenWbJkW5YnPIKHADYEB2wwZAUSCJB0hl4k/Uv/+KVXfqwsMzVJJ2nyS7pJ
QwNOgk3cZjA2GE/EDGaw8YxkW7ZlW5I16413rOH3R93x3fsGyZLl4X7X8rJe1alzdp06VXfvs/f+
bkfEODL9Iq3zI2W47BDRY8GyDKxLLrmk68ezTmt71113HXfBeujhWBEGAXu/9E+Udz7L1OrT+MeD
aexTfgH2LBdlLmT2HhelAi7/9Y1o6uQwB/WwODIJi3X/4SM88bf/nXXPP89v6SN8+cwZvvz41/ij
LR9m0xlDPPbLPfzojqd40xWLFz3voYfF8KUvfYkbbriBSy+9FN/3+f3f/30++tGP8q53vWvR6/7p
n/6Jb3/729j2MeRIHCW8wKNQrSkeLfpAPYTI0eKoFtr1SiwGIcTCIkr5GMKMFJx5ao9UAa2xXaLF
g1VX/FNxk8PCp65E5RwXU5u/476QmQCGpjM3kED4AVm9HzE9tewk9jY2MiG6hPJ1RzdDsQ5Tl2jJ
DEoTMBUpYULTqA7l0J97Dhm2hJ/VlfYuCIVYtFjq4ojuo5RKYgTA5Ezr4Wabev9L3fa885pQaEJD
I1KobVMxs0zJQikbtPywuAdLtKyobt6DVhgyMpjrXtE6dN3DNH0kHrbldlWvpYjkmB4dxLcsAi+A
I1HoWF2Vry+NQEbBbUr5XYsRW4Zqu42jicA0lUbdyeObJijVleWuzsZZjsdwSs0XTCkPIULKZbvB
HqjrFUpm0xMUUwkmveYsCBE9AUPabQaWo9nMLhBqtxha773i2ljTMyjl4ceiOW2FEqqrZyhaX82e
dBU9BVd3mPUrGHbIfMlySYvDlsFi7tjlbtIY0qISzCuuLAVVy6Yai5GYAukJikE0j0IqpNGsDRYS
kbzEHJ1wOUXYjxHLMrDe/va3o+s67373u1FKccstt/Doo4/ysY997IQJ1kMPx4qD3/g/zD5wP9O5
Uf6nWI97yoME9jTnDZyDfv8wlfI0b3zrul542Ssc6ydy7HzPb/PM1/6JlU/v4lovww3n7uC7O+/i
zZdcwt7d02x/ZC9DYynWbRo42eL28CrFjTfeyM0339yowfgHf/AHvPe9713SwBobG+Pzn/88f/Zn
f3bCZVzKkeEaknJrdJsAYhprYzEOHNExpEXJh2omBfsXSHxv6d8yFMLWCTQtCs3TBLbS8IOaJ2y+
fIYedVBjvNM0n7CeDCRA6IIQDaHMoyITamcjE5h6WCNwiAyopjHQHVEeVqmW0xXgWDpSSlYMRApt
sSViUlMCpXkEno4mo+LKtq4taGB1Q9ytUJlZRphgnYHNMokrjZmagSW6pbp1j/aifkEpnUKfat5I
KCVKSExp45oeVeUjtRAvo1N4sVsX7XGNlXgMX9eJzdVMsprlIkQ7T2A5EUcQRN4RERn7vh89l9Yc
Gj+br3kLF0KIYfikXJ3Q8Sl0eZxSgh9GIYYAKidgd8s9tDL0SVkLx5tPKS9xnci4GspJDqeSTD+6
r+YJam4teJaFKpW6rq3569E39a4GVtW18ezOXC0hOtkITaPC3ISF2B/9bQgDQbER3isEGHpAyjah
kKQcFCDQiDkelVKFSsXA1eLM+TNtA43lYxyYF9era7KtJlagFLZdQMqQqtW+aB1Tp1CuYksXDUUp
aBp9mtAQonnfCdfAislaCDPoTueKlQJ82yJMSCiW23JH2+ZICsIgRKmgIa+UomHoWYYCT+Jocbyw
gpsfxi7O4IWCqWqRUEqCRAJ1xGtsOmnzc9zCZsjm0eaqHQ2W1fMPf/hDbr755sbfH/jAB3jnO9/J
8HCP1rqHVxamfnQvR757K7N2ii8lzyG2+VGq1hHOHjiDVQdO58E9L7B6Qz/rT+0p5K8GvPkNE3xp
/7WEd9/Iquf28M6qwzfCO1iTmuDXrjqFm758P/fe/iT9A/GewdzDMSGZTLbVW3QcB9ddei29+c1v
ZteuLox7CyCddlDHWB8mUbKwHZvZmQJKaXiug5orYNSotJNKZ3JGYFkGBaHQDYUdM0maAr8SKXoh
oExJWJPBUBrKCFFlDXSNmGtBOUBZSUy7SjztgK6IZRXFWZ/8RIKqF2AVp1G6jtK1ZlLHcI543EY3
FQgN14mUr7m5qNaWaSu0jMAp6TCnqFZbrq1BKQ1DlwilCALA0zDMpmK0ckgxeRgOzChUEGlHQQCi
ljvUutNu2gqjEKATQ4UCA5sik7iOgW0bJOI2Tsxg7+7Im5LIOCQpoR0MkIFHLmaxf8ZDKQ1VXsCD
JUVj7DpsS5DS5vA8jWIx8vJJBEJI/Ba2OMc2yAYZZkyTfCxOSR0kBHRdw9Q1wpiPCBS6bRJ4GhKN
MOyydoRANxSa0hr5b8rUMUOBIRW2Del0ARB4rk2lZf0ZhkIpjdB2kOU5wkAgREhgKKRtYlQLqKqG
7LNQ1SKOExJYJtT6qKbimGvWUnjwTpRUJLRq474NU2EEtXfKNsGM5sJAazzrOnQ9IDJ+DULXxKoa
FGvnfdtCK5awLEUodGy7xVBTGkpX6IaGFCF+4FP1Q0LLxPQ8pGgx8oGEnkAzo/Uxkkvij2U5vO0g
pmaQsgaQUqcQzmJmspQqHsLzsfZHBko5l0GfexHTUg3ZqwJ0pTB1o+3ZAgS6QtbCEE3dwHZnmVuz
CvvF/ahCkUpZw6gp9nYqT9E2EEaAUhqmqRMLYhT95vutCUUyYZFK2lT9FIfNJM7sM1RUSDlmMFyy
eOZwZABJGSIsnb5sjCNKgxB8fAxdw7Z1qrPlxj0YusKqrSvL0vFqx5WmkUnbeEUHCnMYKLxqGd1U
qKpPTIuD4ZEYU5SnFY5l4Lg6jmkQx8GM2Sg12/aOu67EdgzcU1bgFfaiaT6zs00jVCkf11GUUARh
SNz16M+FVP2QcllQLkvCICKBQSoMYYKKMbpmhOC5Z5mtBOiFKhgaVqCDrlDECewYY5khYkEVQ1d4
IShfwzAU2mie7MQAfbH2XLLjhWWbbj/5yU8477zzALjnnnuW9QPUQw8vJwrbfsW+r36ZkmbyL/0X
4p7xFAX9IFv6T+WN1mXcdt9jJFIWF715bY+S/VUCIQTvf9tm/nq6THj/t1m9Zxe//u8e/6z/C//X
+X/KG9/ay8fq4aVhdHSU97znPVx55ZUopbjjjjuIxWL8/d//PQB/+Id/eFzGOXLk6HILWlEpe1Rq
ir7n+ZQ0DcvzmY7FsA5PUvIqVKoapVIFS9hUKz6lcpWCF1IsRe9EpSLwqjphjY2rEvqYpsLzfKpV
n7m5Mr4EPEml4hMUKwg/RMz5VEpVZrNRvmp18hBuqYoTxpkNClSCEmE1YG62TLXqQ9UjrCmcVc/H
Vhaiqsim+ylOVZEVD7/q4/nt3gFNgqkCKhWPMATfg2qLcVMplymWBF7VxwsFJaUjmcOr1LwmUuIF
PinXpFz0iNuKUjVABTGUrDBd8KmWPYpaBTlTpOp7FIuR228krlHYX8GqeuhCgO+jlEa17NVCiEQb
5TR0955NZQcpTfrII8VGHk1MSyKRTHvN+KtSyUPzLBJhHyoI0URI6Jt41SiLxPOr2CqkIBTVqo+s
enheJxlGiKBS9bD9AM+ryVyt5fAIj1I5ICxERsWsWcarsycaikrFi9aS7WDPThOGIsrnsypUKjrV
io/n+ZRtjawxix0mOVgs49d0Pz/0mZkrU65WkSLEC0IgGtvXfCp+9Ow0EVCuzXOlEo2v+0HDayaE
j2VBsVQlkBVKQaUh55wbw52ZI/RDTEtSLFaQQhKEAbrn43kSGYaYpk/Vi96NMgLD8wmE3zZngYCg
tp5mZkvMzRSpVn0qgYeSIUL6VAMPS5d4HhQC0GIaXlmjgqScTlIqTdFwWAlBteqheUGHgVXxAvz6
vQY2nneEUrGCKpQJKh6e71OpJYrZ/RvoC3X2mLvQQwuCCsJXyFBSqXlPtWqVYrEaFWK2Baeekufx
e5+g4nnM2hbebHPOpAxRns/MdBHPC4AQP/QxlWy8T/W2nlZtjFEqVaF2XAt8ysUqJU8hq81rqmWP
StWjGviEVCmXq1R8KBUhKSTFYhVbuMyVorVWf8elDKiUBcVChSAoomserqWYnPSxpE0pKBLiR/0J
jzAIKWtVZC2c0DACPC/AC3x8BUFVYRpVLKnhV32KhQplr0q1GlCp+JT8akNu009QLnrMemUqVQ8v
9ECWo/Vv2hw8NIssNsMHjwV9fd0NtGUloPzlX/4ln/zkJznnnHM455xz+MIXvsAnP/nJlyRQDz0c
T5T37OaFv//veCHcNHgxsQtepKC/yObcRq4ZfSf33PoEUgouf8dGTOvEuYR7OP7QleQ/XH069657
M0+4Y4zuq3DJ7bu44cF/YeW6PjadMcThA3P86I6nTraoPbwKMTExweWXX06lUqFQKHD++eezdevW
ky1WJ2oRPKZZJtA0KsMDFLMZAGRrvR1lkVI5TOmQMpON44NZt4NEIKa3bJQKcNM2hqpl3bTEqw1k
mzvNQSqLMTCIFLIRgiNljeRintEUMf8J0nofthb1ETgx4noS20oT0xKNtuVcBjvXzO0aSCbb+rKV
TcKJzmtSYhkSw2jmhsxP+NeVRtzW6UsITL3JgtZta03KZv5HKmaSXDPROBe3DeK2jlJLhwoG8QR9
I5n2vrvkg7mqqdApoRiM59CFgdbyfOp1e1oFbg1R0/XIaHEsA9ts31jSaqx9htaaw9b8p6m357a1
ooMiv3be1lykDPFsGz9uozIg0DpIAqSQuCpWGycklupkyBODqyj05QiVaniZxDwZISoGbOgaiMgT
2BePwtXmwzKbzyZYsIZSZEDG3CoBYZeF0MzlsmtMhuV8mkoyWqNml2LUUguBMKJ5r6HqOi3kD817
ChFUY51hg6GuownFaGYFp43H0RVk4wExp9lnfQ1pErIJibFEId9MwiRlpUgadmOdzN9TFiJET8xn
wVw6Ga1cM7Blra0WAycJ8dZbm8ci2BmhF2LqGq6lY+iqFqa6cGhtu+xR5p8UELcFybRNJi5qLKZR
C9ecXxi92YGk5bsRRt/LE4VlaZqbNm3i1ltv5fDhw5im2fNe9fCKQnVqih3/9XPo5RLfH7wA9/I5
ni8/z4bMWj6w7r3cdsNjFOeqnH/ZavoHE0t32MMrDgnH4I/evYVPf9XD3/1DNh58BuPffs698QnO
v+TSRj7W8Hiataf06mP1sHwcLw/ViYbwazviuoetxQn1Cn5HwnikSJjKIO2OooUHGmdMNZ9dbWHm
P1OZRIFl0XnHVHgd+fRNbSiRiBRK6XnMZ/tWyXgbe2Fo2qixVWgHDyOnm16d0NAJskk4ECWjmLqC
CpiGT1/CwlIWVhrSJZOw6COlzmwyCdNNRbReaLkVi/FixGxBqRKRdVnSoWpbZGNxZDzW0dYwKm3s
bgiwNJdyUCSsU2hLgdmyb92NjdDRYs15r9mwSdcglxQ8tb8UKf/UDb55xk/t/1XXxqhWScVMfEdH
TDZHSqscohJRa/enbQ55kiAIWDHosu3h9n7mo2qZmLE+xJG5xtMVmmgaiVJgDLbIIyT1ssjN+4uj
SQVUWCgiVkhJqGkUclni+1+kSXzQLplrqxopRWQkxPQEUtfYV2mG5ioVoKsQL52kXKiAlARhRCbo
OAU8T1GtmqTiBkUCLNMnCNtlto2QUsu7ESnd80gUfB/L0EgYSWbSaaCCGavghiXKtaLRUgtwBrNU
ZlsLZDfflEAuHmGRcE32TxZr+UEhmhD4QYip1d6v2mPQNclg1qFw8EhE/iJEg33QlDYbc2vJpAY5
tDFgdnYHPDXZGCPQFSyQV2jbEUOnV7K6LpIgHafqaRhemYoWvXdCAychUc3qEJi6STxWYHrWwPcl
ujCAakeXqbUp5K6AuSpRTThDLcoACXUjLNp4yMQF/YMJDu8UGJoBovbMFnnn47agJAyqYfSM4kbn
u368sCwP1u7du/md3/kdrr32WgqFAu9///uPKva8hx5OFMqFAg9f91n02Ul+MbAF/SqT58tPsSa1
kg9v+i1+etez7H9xhrWb8mze2ssZfDVjuC/GR3/jVL7TfwEP59aSm/RxvnAju1/4FZe/YyO6oXHv
7U8yefjYQ7F6eP3hK1/5CmeffTYbNmxgw4YNrF+/ng0bNizr2pGREb7+9a+fYAnBn5sDIbAMDVNp
rMhl0Du010irGMpItm4eYMv6vrazbstGrdQCAsdF1kgsWi5v6U7QqeJ3QtcklqVhykgJtDSHuBZ5
nwJdQ8/3M9IXYzzfDKMRQrYxt9V32XWzczR33CE23vx2yxaBS+kUhf4+dGWjSwujS/0dy2ifp6Yp
IsglJCO56PzWsQnWnX0po2+4hG6WgRBhx066zE8Qc6INHd/Qo579oNEuJERpUfK/3mCsbfFKhXWv
iSDu1Iy3lu1+icSWLklXJ5XwScRrinvdYG2xHhOuQcJt97IYiTiuHrkW4pZDoa+95qOmBYSiSRJR
ScTRNYusngdCLKtE2spirzkPLz+Cb0XeqIzeR0bvQwhBYJoR4WHNWxaa9lERy7fmSS0UuW8onaHY
IAPWII4WY8JeTzXbT2gaWGZknAWWiec4hILGXEsZYhhVYraOoRQZK8VIfLBR+6ixEgTY5jxvjgHS
ApUF6URzJYC+pIttpiNPiNQbxBWmWca2ihjO/DUosGt09X1pu6OI7mJvWKBHZRFMPSRuCfqTEhG5
W6NcPWq5FHakPgAAIABJREFUlZokZhsIIJ+MkbAiJ0hou1QTCXzDaKz7SjbVMXZdJinDRtmH7qi1
V/O9TWFk4ADGwAArE6P0W/nG83RMRcKMkzHbv0n6UAIQGLLmmbZiBMl412lpHc/P9redttetR7ku
WjIylrQOd130P0NFxrRR+1adOIL2CMsysP7iL/6CD33oQziOQy6X421vexv/+T//5xMsWg89LI7J
qQI//X8/S+LIHp7tX0f5mjy/mnqUicQYv3fqb7Pj0YNsf2QvuXyMi3t5V68JbF6Z5Xeu3MB3k+fw
i9G1JGd9jvzN3yFn9nLxW9ZSrfh8/5uPN2LMe+hhKXzlK1/hm9/8Jtu2bWPbtm1s376dbdu2nWyx
2tAI7ROCdNzENhW2oZFJNK0mr38oonPWBdn+OFJrNxKklFiZECkDbKuI78ZBCEbTeYZjA129LVJK
lKboc3L0JTvp6OtU00JEbTNWFlfGsbVIwQt0PcobcXTiLSFWml73fERMf6ZZwRmWaEaXb7RhoA0P
Yg4PY46ONeek9j2XiUiNWfbXvR7y5uoIvebx0jRyKZs1g3k0Y5lMh0KA0vGzecrJBOVUCiEkoRsZ
eobSiDsGmYRJ0jVxamFdgqYCON/4QwBKq4lY8x4qhxWpYU4ZyqOrIDL0whAltIgxr+YVEYjINqvL
LjU02yZnZRiM5cnZaUJNNcIHASyriJEDKYLGPQUt+USpmCSt96FbCYJ0rnHckXGSqmas1damZ5lM
jw1jrRxuexahptpCH+fDli35KwuE7SktxFJmG02/NB28vkzntQ06/84xbd3G0EziRjRma52uTEwQ
29g0PvQsCAnSAJUEGUTzImoG8Ir8BH36YH1IdN2L3gPVsVNBXKVYYa8jbWVYDPGtZ1JZGW3uKKEI
lYYSOpqEXEpDVwJnw0YQoGs6nh9iSgspBYPxDNmEi93C2GgqDYGgEndBQhB3CIOoOG+dNt8xFYa+
gCkgJYHd3cNTZzk8Y2wFq1IrcWvhxkI3wA9QQiFCDU1K4o5Bzs6S1rP1i2v9R6F9tuaSUOm2b1Db
GmqxhGJuBcNSWHoYlVoAzFQSc2iYbM4mn1m4bEY+7eDWvNyVmNug0z9RWJaBdeTIES644AIg+qi9
+93vZnZ29oQK1kMPi+HJ5w/zw+v+G4NHnuNQ/wSH37eaBw79krH4MP/htA9xeE+JH92xA8vWecs7
N0WMVz28JnD+5kGuedNq7jLP5ccbJnAKHrv+66cYMWfYePogh/bP8ZO7nj7ZYvbwKsGqVavI5XJL
NzyJkJY9L5EhUkXSsdZcD9H132P5eJR/JQTKAseJQpDitWstXSebaColsUwq8uDUxluXXsN4YpRV
w+05UQ0pWnRZbXAEv6/J0Fqq5a/Mz61wYiZ1LUsTkcKTipmAoD9t059qUZJqnjRzZBRzaKihmyVc
vTY3HWLRFok1T9+1DcHIijSZnIu7cSPm0DB6dt7zX8S+qsvb2ljG3Yj0AghjLrGz1uOYkWGRjrX/
9gS2i5SClYMJRvvbk+NTiTK261PpT1AcGqrdjEQI2VBgbbsIKZ2sFac/P9EW4hT2t3uoIPIW2spG
CEGfMYjbkvcmBDhmuwIdc+rhaIKY3RlyGc1BlxyuMMQ3TYzYvAdimOTTnblHdVhtdbE6Jz6VKKPU
wr4Gf3yQYLgfyzLQlSAdtzqLx9a69fyaUa8MTs9vxlDtcoU170k3A1vUDF/NtBjI2Jy1ZaTW7TzP
FxCzojnU5hfc7VafrfW1VQqMyJtjyzhJlSGmNd87c2gIrVZOwpAGGdVHvzlCLmlhK5uMla55hyOZ
NqxIA4JANzDHxgkzSQIZhcPqTo5Q1g3Y9jWq6x6aFNiOIr5+HEtXxFoZHFsNHtvsyOWUtRw0R8bQ
pU5cb1mjrRcL0ZkbVpuQuNmdOELXA87aOshIn9tgYpRSMrE2xxmnrGEotjBDtBACpUlAUMqkFmx3
vLAs882yLPbu3dtYdPfffz+GsVhdgx56ODEIw5A7fvECB7/+Nc6c3EGpf4Q979vEj/b/jOHYIH94
+ocpTvp89/88BsCvXbWRePLEJTH2cHLwlnPGmJqr8P37z6d0VpE33b+X5//qM5z++/+RvbtdHn9w
D0NjKVZv6D/ZovbwCsdv/dZv8fa3v53TTjsNrUXR+PSnP30SpWqHkALGhqARmt8MLcsnY9Trjepd
dFVDaY2CvSGC2cE8WrlC33CC/MEi+w7UduYFuJZOeuMadh7YA9W5tn7mo02trCWpC9OMqLwn90Zt
agpUGC5o/wFQ6MuS0zWogCF1hhKD7Kqnj7V4c1qhSUkuaXDgYAkZ0LZd3No6ulziWjmSlorCwWpG
hLBszNHRjr4d18TUA6qtRZhrnZrSpOA3i45JTTQmozFuPQwshOEzNzLz0x0wF53101mEmIm8LM1o
xZrzKsSxfUpuBqXF0QpAOovmljEHEnDgaaQMCQcdfDWBa9rMzTRrHQnXprVytOggfGgPx7OtHMoY
ZD/bG6eHcza+MHmBFEFflW5oLUidM/PMVV5sL7o873ElkiZ7Ds2hNIHnhw0lW0tAvBQQaC5ZO8YB
UW1bHAJQxiL2rhCEtkWQcHHwkcJCMwSlEHRpUchZWJOT6IkYRnoQDr7YuE4g8PPDhHMzEB4EETae
oyY6VWOvL0Ng61gyj6XrZBIWgd3JR6A0xcaBEZ7dM1u7h+fb5AUa48Tt9lywVkghcKWLJ2ZbvL/t
bZXUCYWsGQ21rlseg2UobOkwQxHLciiXpwgReEPjICAwXqQ02M/g+AoKz06DmKQalknZkhV5xcrB
PPbIII8/t5PKnGhEpnaYuy2DCkClM4TpLNrhA8Q0E21e7llhaIB0XHWNCe2zs0zPFEnoLoeqkx33
BFF+pjMxFhmk9blQGhmVRhs1eXxve1ntbgZziGC2WCUIwmUXMD9aLMvA+vjHP85HP/pRnn/+ed7x
jncwNTXF3/3d350QgXroYSEUyx5f/u529J/cyUWT2whzeXa+dyt377+PQTfPH5/+EYKC5NavP0Sl
7HHp29YzPJ4+2WL3cAIghODdl6xmulDhZzvOp3DBPbz1x4fZ9//9Lee/98N8d1Lyg+8+QS4fI5VZ
ePe0hx4++clP8va3v/0VX9exjQmrRWEYig2yuzhHst9hdMVpOIboolg3UY25VGMuUpO4VrPdqSuz
JNMOzz9zqE0heSnKR7Obebv8YfQfgGMGFGqW4brsWoKDkiAMkZYBcxBaRlcVtPWYOc+NJYChrKRU
CVFSRIbe4AR2ZRK9f+md65QVZ+WAy/bnJ6lUFo9+MF0DUUvwV9IAvDa9UUskUakUHN5Vk03gbDyF
WFLn4HOzndpq42+B6erIENIbxhBzk5i1kEJLOjiuy8hIivLuWTr4R+o9tGwYCCFYORinVLQ5WCjg
rB2DYLSD5VCoaJffyfQxvLYfyjGe3Tvd1iYVs5iaiww5W7Mw9T5mG0VtRYdCvHJ1jmRfjJ89epgD
k03yCM2FicE48Xgf5WKVA0F7YVygLc9svqIsAaXpnDlwGvfzIG5OY2p3FdWfhiBBOp9mb/oFsqGN
VHrLohFoSpBLOxzyK1BsHzJlJilpgsPV/c2xlEYw2Ic8pNd6iBg1Z8UI1vM7G+3yThZVVdQHS8UN
oNhmTDfRjc2wCUNJPFoLNzeoR4CIGbSScVH7okLa3ZBSOSoqJG0mmSpPYZkKygInZTIF+OkEawbX
U53aTfHFQ3hKoNeiRA1TtRfpFeBYGrTWNEYguhTrFck0IQfajlmmBh7IbIIg1T2UT9d0kmayY1p0
YWBIi6ThYWkW2lintxYgOc9bmnB1HFPhN3lHMIUNAuIyy4Gp4qIe1peCZRlYhw4d4qabbmLnzp34
vs/KlSt7HqweXlbsPjjHP3zjUUZ3/JyLDj+ETGd56tpzuf3AfeSdPv54y0egrLjlxoeYm6nwhjet
ZO2mXjHh1zKkEHzoyg2E34H7901RftP9vOPeaQr/8j85680f4L4d8N2bHuWd7z8Dswu7WA89ABiG
8apgEowbsRa9pql+2DXlRlMSZduoLnTSy0FdeQ3n6WlaF+KIbH+MAwqqSzGX1/ts8fCEtcOJpEa5
FGLqAX49LEpZDWPBGBwiUHOEMRclu6gqIiJ2mJrs7mUxlMBQgmIlaLQ3R0fRY5204Z1iC/rcLNuZ
JEgnkEciA0MTGtJNkyjoyL48dZ2tbvya0oQa/2JrX/MFF0pFYZ/MEoattPg0amsJwElZnLoig+ua
lKeqSCGxDA1fmkgBpqXI5l2mnCpShvPtBLRYnPDwvubfmkQKSTxWQQxmsYpJCi0XhQgMS5IciDPu
ZiM6cBcOTpWYKVYYMEbxQ581I0nuf2J/Y16FEIQt9xwEYcQCKT02jkd5R9mk1ZGLlXANDE9i6lqj
VlbbeSNO4CTQpzsXmikthCyQMds3UaUSiIQLfmKe8djuQlVKY/OpgzzwSBleAA1FwkxiOgKJJKky
VIISs3707M35aQYi6se3Lebyfbj7DpCOmVF+YXV+wxqWfGHaMZhxmKJK0u3+TruWTn9/jLl9LGio
CSHQpYWUksFkHsNQVGYFZUtBAeKqFjJq2XijqwgtG3Y8Xru4891POAZ9SY/ZFptbc1o9eYIgCKEe
+illY8NhKOtScHxmWlZqPiOYmY2ikjVtgTDn5DDBXJGSB+NjOaSzuEHUGobYajzV/YVKKsbNNaDs
BcNgjweWZWB97nOf441vfCNr1qw5YYL00MNC+Nmv9vHl725n84HHuOTQL9HSaZ557wV85/B95Ows
f7zlI4iizre+9iDTkyW2vGGM088ZW7rjHl710KTkd9+2AXErPFA4xE2X7uTqf5/F+d6XWXfu+3ji
INzx7W1ccfXmExYG0MOrG+eddx6f+cxnuOiii9BbYuzOOuuskyjVfIh5BlZ7HkOzVRPSjLw6ei5H
9eDBlutqxoDW7vWpG0H6vJpK+nwiBiCTc0k4IbOljlMLoi2CTAiUITGdEE3p9OeTbfciAKFJDDcB
hA0mvNZ7DInIIsYHEvBcgJKSQ6VCazcA2LpG0jVwa/lOHfk5C0AIGI7n2Skjdc2SNuHIJgIkQSoX
EVTUismahkap4pOJW8ws3i3QrkgKrZ2kQwR1a1RgGzquGxmE0nYQQDmVJCRsGG4xI4ZtRXKUa984
TUbE52YmiV/Ioblum39jItNPNTbAYD7Pjl1TFPUkh7w5hB4ppwt5/evkJUqTJF2TqbkyMqz5Tlqe
XaAbpA0YPWtLB7NhHTFbJ5+yYao1pLH9G50yU/StyuLs3IvVP0CrH82QNiPxDP1O5Mnoc3IRq2F2
P5oShJ7ZmFNTazeq6+vDMFWDDEYIWJEYZc/kJH6tnlta72PWn2Y8H8eoMRM2Gfbr9yup2lbjXDJr
MlcUHWMBhLYFIQRqkSTBFmhaPTdxHpYkYem+xjdtGKdS9CgUKzxzcI4V1joGW8ouhbH2MjbdQuuy
Kw3SpVGeePwAutOdECYMQrAdvGweadkIP/r+SCnIJi0K081rDAXpWEgQBDhx1fhOtL6mUkh0TcPO
pnA3bFzi3qGvz0KbDJgtLuJ91mr5WyeQ/GxZBtbo6Cgf//jHOe2007Cs5kf5qquuOmGC9dCD5wd8
/e4d3PnALs6afYpLD/4CLZni+fe+iW9O/oisleY/bvkI4YziW19/kJnpMlvPH+esC1acbNF7eBmh
ScnvXrkRbvN5MLiFf71U8L57ywz/9GtMbb6WF545zE9/8DTnXbL6ZIvawysQv/rVrwB4/PHHG8eE
EHz1q189WSItiRWr08xOzQunCtvSxxGaRvyssxFSNgysupchq+cZySXgcFsHAOQHExwODqKJMrKq
sF7CDq8wBEEYYDl6m2EjBPjpfkS5zIY3bOTxuWe6Xj/g5hnPpbAWKQaqKYnSVZT2VDP4tHSaRtyX
EOTTNlOOQbFQWbaBBZCJazxX8JvzKiV9cYtCySOXsnluX2RO2YaGnrIYysV44jBddeaGAVXzPAEM
jibZrwn6x+P4k4dg6gUIuhdE1jMZgnUrKU0V0fwmZbxruBH1eBhy6tmb+Nn+BzFkgC8lhqnDqui7
Vy5FbhUjnyc5amPGIga8taMpjhgw41eoJEX3+WkRRtVCxkb6XGYKFfrSNvsKcy0asSDI5TGTBtLq
7i3UhYGpNNZnVhM892j7OIJG3p5l6QzE+2FzLRrlQDu5mt7i2RxPRLl0++cO4GQ1CgcjtrwBY5Qx
J051ttWt1LyhsDUErp6DpUm2rMlx/5NRzlaTYr8d+bRD9dAQh4N6bld0bf9gjCdfmJw3kiB0XaZW
jFDeFcJUyHAuhjmU5NkX20MwKys3RN6jbiGT8zE//28eTl2Z4+maF1cIGF2R4cCBGZ45OLc0W2bN
4JWODXNArQBz3/ggrj+Oa3U3IepryMqnCWarxLqxgwKO7mBrRZTmUXYTqKERqrsPdLQ7ilcWAGVI
jGaUZtvtNLpqUNOfJANr37595PN50unIBfvwww+3ne8ZWD2cKByZKfOP33yMHbunuLzyJFv3/hQt
nuCF972JmyZ/RNpM8cdbPkphb8jt3/gllbLP2RdNsPW88ZMteg8nAVIKfveKzfyv26s8JL/BV98o
+cCPTdY9fhOza67h4Z/vIpNzWX/q4NKd9fC6wvXXX3+yRThqKKVF5Arz4Hnt8X0L5WIpTZJwDbpx
AWtK4qZ0SkWBYS/NvuqqOIOxIYKajthq5kkX+kYc0maM2WJLeJQQoBTeyARWLAFzIKXWoRBpMmLA
64a6EqdpgpFcxNb2Ys19pMUTUGqLfWsoUkGwfG1NayWiqEEpyVBftOu/blUWBOzYC6o1V2h+P6o9
VK0ui+0YjK+OGAwrcQd2P443OgDTXtTLvI6EbcN0qc2DBWDUPDR98TRKi4hFukR2RrLE45hD7cXY
hQDDqCIk5J2+jmvSMZOZQhQQGTHTQdwxOHtDnuKOafbtbjcELEtHmt2NK9vSwIeBeI6Y4TI973xm
VBLunsPzNAZH5+XLtTy6hKujlOxQvk/JbWCmMsMzhyOv3hkrR9GBXTsOkHJl84Zbb772f61RPysK
Wzx1VRZ/34vdjRcBE4MJRvpiPPCkwZ7ci6TzUV2nhTYlDGng6waaDLHNAF1p5NMOM4Vqu6evFjIr
pUVQbmFaqXvNWln/ahPgWIKZYkjCaZfWsVSjHEDrmknGDA5NlxrlA1rRLMkQtTcGauG6tdQgIUQj
bLF1tIG0pBIziCVMmCphOgaxjIt1oNNoAtiYXYdvF/B3HMBTZrNsArVHbehQaRrGR+tsSjrt38O+
pOTQVG3B1Ig3TpoH6/d+7/f4xje+wac//Wm+9KUv8cEPfnDZHQdBwCc+8QmeeOIJDMPguuuuY3y8
qfx+5zvf4Stf+QqaprF27Vo+8YlPIKXkN37jN4jVaChHRkZeUUxOPbw82LbzMP/j248zM1fhfWxn
7PlfoNJpnn33Bdw89WNSZpI/Ov132f3YHPfd/TRCCi5523rW9XKuXteQUvDht5zB9T+u8FPtm3zx
Qo0P/jTL5mdu4f7xd/CD7z5RU2i6J8f28PrE/fffzxe/+EUKhQJhGIWq7Nmzh7vvvvtki9ZEXQcY
zBE4FprRVF6lZUXhStrR1HTpEtZzjFU3DWGQtlIcmu4014QAJ6Ejq7LNM9JmbgjBptwGNKmgVOno
Yz6ydpZqUKnX6I1II0aGKO3e0zawSqXwJiebh2T3HLOlEM9IgrkKxbpDrGXuRoYTzM6UYe+8i+ZN
78hIjMN7KkzO1fK4uhi+wrLwNq6KFMvpKbrVJRNCRIbxFOTTnYZnm67YJfdrYTTDEh29MzxwMOuQ
ihnYpurwepjjKwh3TVKworC/jWMZjrw4i1f1UV0KNiccg34rhtPV+yEa/SvltxFcAMQdHVNpDPfH
KFkG5VK1I4TVVha2snim9lCkAMcxWL0+x+xDLzTus46YY1Ih8lJZts7AcBLLiRR9y9C7GiCRpBHq
3q1yOoXoSyIQGKbCTVvMHSm1JB5C2kgz4o5wyCiglZpG6eouJRAArFWrqO4/QHlXTe7ac5KGgb16
NdJxwY8MSdsQjPXJZXtkVg4lyCVtUrH2EM6EY5CshyXWwz6laBhXa9KrFpgJsAxBbiTRvr4FSNsm
KBaRhoFZY7eprzPNccBxoVLbgKl9J8IQvPUTqMeeJBmTHJ4U9A0sXOOqG3Qt0guCICQ71o85WSGY
LLRLfwIzBxatg9X6QbzllluOquM777yTSqXCjTfeyJ/+6Z/ymc98pnGuVCrxt3/7t3z1q1/lhhtu
YHZ2lnvuuYdyuUwYhlx//fVcf/31PePqdQY/CLj53qf5qxseolis8EfqMcae/gX6wABPvu8ibp75
GRkrze+v/13uv3UvP7nraUxb59evPa1nXPUARArI+y94Axenr6Roh3zhHA360py66/uI0Of2bzzG
nhcml+6oh9cN/st/+S9cdtll+L7Pb/7mbzI+Ps5ll112ssXqDsskyGfblFx386msvfwChvoTpOOL
Ezi0c5C1n+mmaCxlcykNSKXRWwkA6rlcIlLIdK3GutaaKzZvMEtZbeFe9fPdQphihomu6UgZNtpY
Y+PEzz6n5Xqw167Dqm3qavE4qu6d6OL5WwzrBsdJOkZb/aiF0Mb0KCVJK7pGKQ3TCEi4BhODia4y
aKI9FyvowgonEMRtnfXjKXIthZ/ttetwNrbnpiTnkZ0sFhq51HMWQuBYetfnIXWdYGSCoOZ9SLgG
oyvS5IcSuF3Wo2DhkLv6zadjJqaudYSgJWMmW9b20Z+yGR5LMTCcJLbEmhctHqqWg41/rluRZSgX
wzaiseJJq309L9xxx6HWOs91g77RSmogBKeNjpFJuDiWtqjJCyB1A3N4uJGnFwbNNaFnc2i23bYz
cjThbpqUpONLFNXucq6tBhrdvyXzj7sbT8HdvBlpWaTMJKtSE10Mtc4hT81vZkViDE0K+lMS01zG
cwGMTJbQsKgMNZ06QkrcjaeQO/MMquNNPomTFiLYOvFHE7cM8MADD3DhhRcCcPrpp/PYY481zhmG
wQ033IBtRx8Iz/MwTZPt27dTLBb54Ac/iOd5/Mmf/Amnn376UY3bw6sTB6eKfOHbv2LH7imGXMH7
Z38OT23HnFjJI2/fzPcO3kfOzPDr5jXc8c87KBWrjE6kueTK9bWilT300MR7tl4Ej5S49+Cd/K+z
NK79ucupe+7m4cFLue3fHuFt7zmNgQV2DXt4fcGyLN71rnexe/duEokE1113He985ztPtliLolWR
F1Lixl3c7nU5511YV/pqSpBhMpCuUHRM4snW3eGllY5geAVOzsPJZIglTIa0FKWdR6jWdqiTKo2v
58nHchwqzRF3dDIJG9vQotA7f2H5AFasziK7xLnlV+TQnt5NadBlhuZctCpKoZQRe1p+AGEYqEQS
U0RetHT26CiZbd1iKDbAvmqBg4CpL7IvLWBdZg3lSpU99nxOP5AINCm6GypCsjV/Og/seygaV3bW
WNKlTolSB6uiXkvjaO13YN59mpYikbIXqAt5jO7LGtpsFwRKlyS60HCvXznOiwcOsmKsv6tOOZ4c
oxJLsFd/FlPXFlX+NSWPrsZlm33V/EPX1ZJeqqXObVnTBwf2LN6yVivOsRR9aYdq2IeRX6b8UgM/
IPQ7jW7pukjTxMjnqezbR1AuI/Tjw/K9ZI5W1Kjr4fGBOJVdARODCYRSaC01q9LWwqUSTEOjShTi
amg6pW7FmZdAXzbOrpXrWTGQYG5/u2c9258mlkrw4I4obPGkk1zAMie6BbOzs41QPwBN0/A8D6UU
UkpyuSju+Prrr6dQKHD++efz5JNP8qEPfYhrrrmGnTt38uEPf5jvfe97KHU0oQ89vNrwwBP7+d+3
badQ9njTiOS8x27FP3gAZ/Np/Pyyce7adx9D/hjrnz2Xn+96AaVLzrt0FaeeOXLU67KH1w/ec+qv
Ibd5/ODFH/Cv55pc+ROLU/bdy+P5i7jlaw/z1qs3M7KiVyft9Q7TNJmcnGRiYoKHH36YN7zhDRQK
haUvfBnREex1jN89y1DgeY0QIHv1GrR9e+kbGu5gcFsKQTINFQ87Fu2CuzGT/uEEcy9OU1m5AREE
JJTdoMoWQnDq5gEO7pulfziBOjhH3zwlvPW2dKP7774zPsZILssz1f1QnmrzdmWcJHOVIpZuNo7p
mSgkWAP6B5f2Qi2EfNqBeIrFjBGBIGa42MLn8MTKjnjEdLK86DenzcuH6BhqIjnGi3P7GHIXj9hY
KLwwP9T9/quDIxTnulm8y8NIX4yZ0mrWDCcWXZvjAwOMDywsu61skk5fR8TlccdLUBssW6dUrLb1
YepaIwev29ynYyb7oC3PSs+kMfLL2+QTmkZYrULQ+YyElMRO3wKAyvURFItR2N1RIp92mC5U6E/b
UGf2fwnzZJuKU1cdfTh+Pm9hzAnidpdNCGN5m+mmoXHuxmidPb2/M3R5kTKBxxWLWi5PPfUUl156
KRARXtT/HYZRguVdd9214LWxWIy5uWb5uyAI2gylIAj43Oc+x7PPPsvnP/95hBBMTEwwPj7e+Hcq
leLAgQMMDvYS01+LqFR9brx7B/c8uBtDST66LiBz59fxyyVSb72S763zefC5R1i97xysfVn2M8v4
6iwXXr7m6Hauenjd4poNV5CwbL797He59YKA839os3nvD3h04GJu+/ojXPaOjaxc15nU3cPrB7/9
27/Nxz72MT7/+c9z9dVXc8stt7Bp06aTLVYb6sn3dkKju+tneYg7OnrMYrBGwy1NE2vspREDdVMo
MUy2ruvv2Ny2bL1hYEwco7EjhECLxeDIvo7xHd1ACaNBvX180Br2B8E8J0Ki9lsU61PNloJaWFdT
k1uRHAUE9jLqlA3lXIJpweA8L5ShGQ22vIWw6cz1lLc9hrli5ZLjNMRN9VOyprHlsRVczSQszts4
fBwedlQSAAAgAElEQVQ2PAXSsgjTcYJEbOnmy+mx9aHU0M0T1A2t4XAJM04Ywkg+TRgutsnRflxI
QS4bZ3x9viMcrVsfmbjF4Zn2+gfSNAlKpQYxw0KQuo7Uj431M5u0SMcjGRvEI4t5j2pUj1pHm2NY
Ay0bCUqTJJzOca3xFej9/Uffd1e8PBvzixpYt99++zF3fMYZZ3DPPfdwxRVX8NBDD7F27dq283/x
F3+BYRj8wz/8QyMh7qabbuLJJ5/kE5/4BPv27WN2dpa+vp7y81rE8/tm+Kfv/IrdB+YYy5q8X3sK
79YfgGGQ+/CH+dfy08z8RLHm8MUIBLl8jHPfuJLRiczJFr2HVxnePPEmbN3k609+i59cMkvhxzFO
23Mnjwxeyu3feJyzL1zBGeeN97yhr1O89a1v5S1veQtCCG6++WZ27tzJ+vXrT7ZYbRBCsHpDP08W
jzAz3Rl6dhQdYS0RerVsLBFVphaisVsUy5craSaYKk+TaQk3aua/HMct6iXmSjcU2ZWR0aRkLd+s
g/4PpFi+0eeaitVr+khZR2/w2Mk49rlvOKprNKEYs9YgF0/LXxTHY00JEfUzvOnso05LWbDPRt9N
+VR6eZELrdeMxocbjJZHc6vmipXETh3sOj/d+lkzkuT5fRrZlk1ka+UqKnv2YAwPL3/gFui6hlos
tLWGjnyk2p+2Y2DNWpS8EqYWrfWNmbUcKB4iY0Vzaa9dhz81hbRewuZ3nRCkC4xFPJ9HPczL9FO/
qIE1fIwPE+Dyyy/nxz/+Mddeey1hGPKpT32KW265hUKhwKZNm7jppps488wz+cAHPgDA+9//fq6+
+mo+/vGP8973vhchBJ/61Kd64YGvMXh+wG33PcctP9mJH4RcsVJn62O3Ud2zG31gkOKV1/LPDz+H
fniMFJDpd9n6hnFWre/rKcA9HDMuGjmPrJ3hi4/9Cw9dOE35AZtzn72VR4Yu4+c/3MnB/bO88a3r
MK0TV9W9h1ce7rnnHlavXs3o6Ch33nknN910Exs2bGDt2rVdmd5OJo7L9++o+1iGknuSPsv9Th9x
I46ldSFSeJl/K1zdZa46h1Ej9Dgec6K9zOtvPnnBywVnw0bYXauDVntu/V2o4o8V3Qx9Mc/rYq9e
09FmPpZr781fekJ0z7nr2rjWfnygPaFSGgbWihXLE6ALVqzJHd0FNUOn7g0bGktRmKsQhH6DtMbR
HcZbGCf1dLqRC/iScHzs6lcETpj1IqXkL//yL9uOrVrVZA3Zvn171+v++q//+kSJ1MNJxq4Ds3zx
O9t4bt8M2ZjO76T3ot99OxXPZ+6cK9kuh5n69yPoJNByVX7tTVsYX5ntGVY9HBeckl3Pn535R3zh
0a+ybet+vJTOxQ/cyvb8RTzzBOzfM8Olb9/A0NjCCbg9vHbwxS9+kdtuu43PfvazbN++nf/0n/4T
f/7nf86OHTv47Gc/y5//+Z+fbBGPP5b5KV3OJ7dU9ZZudLQ4ym+9Pa8AcaQ4B8d1h3qxvuo05Osz
a9rqf71af7Iy8Zc/9F4lEsj9Oo0q0ccZXRkL53lq9OzSuULdWB27oe69rOst+iLMd6/UdRI7bQv+
3CzSijx2UtZqq70MRrgWj4xL/QRGr70iPFg99HA84AcB3/vZ83zrR8/i+SFvHayydcedlB/ex+6+
Tewa2MrMoYCQMjPpfazb2s87tl7WM6x6OO4YcPv5v8/6Y7799Pe4hx8xlbS48t/v5qCzgWfD0/nW
vz7EKVuGOOfiiZ436zWOb33rW9x4443Yts1f/dVfcckll3DNNdcQhiFXXHHFyRZvSbiG22FgLIll
flPrxBTzFaoz1/V37ORrJ5Dm+GhhrVkDherSDY8KnfeXyjiks04jN04IMS8ssIOW5BhGffnndSma
/xOFZjjg8bvn8XycSjXorkccg26xVMjieGKMPXN7SRiRgaBMDSdlMTS/WHKbGK+cd6cV0jQXLBR9
oqFSKdzNp760UMMl8HK9Wz0Dq4cTit0H5/jft23jmT3TDBpVftPYQfjjh3kquZ7day6lHGhQ8jnc
9wKzI7v5za1XsSm34WSL3cNrGIZmcPXaX2dzbiM3PHkzN16xj7fdu40zd+/h0fxFPP7gHp7efoA3
XLKKdZvyr9gfwR5eGoQQjVIhP/vZz3jf+97XOP5qwIbM2qUbzUPayXCkPIXbpZhsK4bcAfzAZ9DN
tx3vFm6VTbxyCIeElI2aQS8VRjoN0y8iWtIUHNdgdqaMaSnUIrWSjssSenUsw+OD429fMZjtpLmv
41je8WCJCtV9TpY+p+kJE0JgxQyMBSjge1gYx8KCeFToebB6eDWj6gXcet9Obr3vOVS1xHvVToZ3
PMbz7mpemLgGT+jouqQ8+CLPpB8ln8rwsc0fpt85yljhHno4RqzLrOb/OftPuPv5e/m2dSfn/eIw
5+34BjvTp/BsZgv33Lqdxx/czbkXr2R4vEfn/lqDpmlMT09TKBTYtm0b559/PgC7d+9+zeb+TiRX
MBxUsdTiu9NKKiaSi7MLrh5Ocni6TMx5bXp6Exs3UEz2R8Vca8gPJ0gUqjju8uoMvRQv+MnwYJ08
vMyJN0dRW2kwNsCLs3uX3JQ4JjFeT4/4FYSXa9pfm78iPZxUPPnCJF/53nYOHJjmwtIOth7ZzgvW
BD8euQpP6li2IrnO4z79DiqizDkDW3n32quW/NHvoYfjDV0q3rziEs4dPIs7Ru/h9vvu4U0/e4yh
mWf45eD57N8zxLe/9jAjK9Kcc/HES6qj08MrCx/5yEe46qqr8DyPq6++mv7+fm677Tb+5m/+hj/4
gz9Y9NogCPjEJz7BE088gWEYXHfddYyPvzS685cDUkoseXy+s7mkTS7ZWUz2tQKhaW3GFUTz5y6j
sL0QglXrO2nqj2r8Y7/01Yd6+N2JtDjaqiEvf5zh2CDDsaMvFaSkxAuCTma+NpFeV0/5FQMhBOP5
OPYJ9i72DKwejhsKpSr/9oOnue+Xz7Fl6gmuKTzNXmsFPxl8G540sGzF2i1p7jfv5f65Z4nrMX57
/bWc1nfKyRa9h9c5kmacq9f+OkfGLuLuU24j/Y0fcN4L3+eAm+PhwfPZtRN27TzC6ESa088ZY3g8
1ftxfJXjLW95C1u2bOHIkSMNWnbXdbnuuus455xzFr32zjvvpFKpcOONN/LQQw/xmc98hn/8x398
OcTu4VWCNsX6mL4Vve/LccUxGljHitPX5PD9EPka+Z04kfUi6wQxJ2amunO/LxZCerzQM7B6eMkI
w5AHnjjA//neI6zd/Qi/N/MU+2OruH/gCqqaiWUpTj1rgGdSj/Jv+24h9EK29p/Gu9deRcw48Yu8
hx6Wi7SV4l1nvo/JU67gkVuuJ3PPL7lsx7d4Kj/MU5lzeOFZeOHZI+TyMbacO8bKdblXHJ13D8tH
Pp8nn2/mGV188cXLuu6BBx7gwgsvBOD000/nscceOyHyzUc+lsP2Twzb2vHAyVYlgyBSpBbzGrxa
8BrRy5eFpgPrxN20kBIjP0D18KFlsQa+VChNopYg3Xs1PWPtOOU2vp7QM7B6eEk4MFnk5u/8ktiD
P+J9M0+zP76KB0bfQUWzMEyN088aYF/f09y498uU91XIO328a83bOSX7yirk2UMPrUjZKS569x8x
e8k+dnz1f7DmV8+yav/NPLxyiBdiW2Af3PGtXxFPWpx29gjrNw+iGyenjkwPLz9mZ2eJxWKNvzVN
w/O8RXO30mmnQet9rOgjDsegG1prxvGLRVJ98aUbvwRksjFQipH+GMmWULq+ZY7rlw1kjTRjude0
4tDeWTxDJ5V1jun6+Wj08RJkAijLCtPL7CNeiMISc7k4jn70IZjHImMiPgVAJuPS9zLs7M/HgW0h
iYSFk3ZxT+Qa7dt8fLs7Rln3xacByGbjmNbLr4Yfj3fjeCK5b5ZSxSeV7vLevsR37+CLM/heSCp1
fL4JR4OegdXDMaFc9bnr+7+kePf3+f/bu9PgqMp8j+Pf06eXdNLdCVkhQIAEAggiBHBkLhgXFB2w
mAtIFibMYk3pzCgzaFlsBVojMsOtGl4wNQ7yYsYZ9VqjaI0loxfDomyCgRhlkyVAWAIkZO/uLL08
90WwJZAVknR3+H+qurr7PH3Sv6efk3PO0+f0eabWnKbMkUbBkB/TqEdiMhkYOT6WK4mneK9yC40X
m3CY7TyR+hj3D5yCbpAdUREebPFJjH/+JeoOfc2F//0nE4pLGauXcmBkAqX2cVA1iN35pziw+yyj
7xnAXeOTccT03d+liGY2mw2XyxV47vf7O7wwRlWV+7bfNyHBTnl5XddnjG3+DcktzdtFiXYzTfVN
lNc3AV3L7Pd4cNZeO0J3C1n9KGrr6rHaTbdd1+tz195GJgBvtQt3J/9GXW09ANWWelx618Yau9Xl
o7au+T0rK83o/s6N99SdjLYoKkuv0uTy4O6FZbQ73PL/It9/3lev1vX6VQZvJ3dPqamtp9Hjw2KA
8hsumlNb24DDEXHLmb1+P3V1Dd2yTmhLWx036WCJLlFKUfT5V1zevJmh1SVccaTy5bA5NOhR6EYD
/UeZOBd/hPfdJ6AcYizRzEqdwdTk+74f5V6IMGO/+x5Grf4favbs4soHm5hytJwG83YOjozhomM0
sZVpfLXvPF/tO8+QtFjGZAxk8LDYPnGqkrhZRkYGO3bs4Ec/+hFFRUWkp3f9kuniZgaTCevwERis
t/YlRXySDUdMRFhfGvvuhDE0eBsw3UHbS8eYu3DpZzElJAY7Sq9ISnZQV9MgZz10gn3SZOLibVRU
1d/S/IkDHDhirFiDcLXT8F0LiV6lfD7O7NhD2Sef4Kgtx+RI54vUJ2kyRKAZQB/i5ERsIV/rTnBD
er/hZA76IXfHjZYjVqJP0HSdmPsfwHHvfVRty+fqJ5/wX4eqaDDt4+CoQ5xJTiW2Mo2SYigprsQe
HcGYCcmMGtcfa2TnLusswsMjjzzCnj17yM7ORinFmjVrgh2pz7id38domtYjA4QbLBb0604J7fL8
1uZLfBsdHV+F1KKbseh31vpCt1iwJCcHO0avccRY5UyH67VzlX5N1zHcxrAZBoPW6WEVupt0sES7
fG4XF7ZspXr7VgxNXqqjR/LNsIfwGUyg+6lOOsflxJN4zY3ERcQyLelBftA/g/43DFApRF9hiIgg
buYT9Ht4OhVbt3L1k4/5r0N1TDQe5vDwYo6mJRJZOwxVMZB9n53my11nSE1PYNS4/gwc0k+OavUB
BoOB3//+98GOIXqJbfyE25rfYLFgGz8BzXTnHJUSosv62KZROljiJkopGs6c5nL+NuoLC6g1x3Mu
ehIVUYNBM+A1NlGRdILKpBJskVamJU1mUtJ4htgHy6WrxR3DEGElYdYTxE2fzpX/+xRP/hYmfetk
wnEXp1IqODjyEH7vUOLLh3LqmOLUsTKi7BZG3p3EqLv7E92vh0erF0KEDIMl9Md5lK23CIaBCVGc
vlTb58bVkw6WCPA5ndTs20v5tu00VNVRZhvG+UFP0GhqPq2h3lpLZdI57EMUk5NGMjb+cQbbB2Lo
wqjoQvQ1hggrA348m6SZj1Py6WfUffoJI0uqGFkCpXENHEk/xdnBcTiqBqMqB1G49xyFe8+R0N9G
2qhE0kYlyOkiQggh7kiJ/SKJj7H2mTHDviMdrDucz+3GVfQVV/fupeb0eSqsA7liz6B2aPOPTf2a
D2fcJWLSNX6QNoyx8Q8Qben4PHIh7jQGk5lhMx9l6OPTOb1zP+WffExyxXmSv4AGYy1nhlVwePDX
uIyDiK8cQvkVRfllJ/s+O018ko3UkQmkpMYSn2STI8FCCCHuGH2tcwXSwbrjKKVouniBum++4ULh
1ziv1lMdMYCKyHTcKZObX4OiMbqa+FQz99ydyuj+mZgMsqgI0RmawUDaA1NIe2AKpSfOcvKjLdhP
FjH6ZCWjT4LLUkvxkGKKk604jYPo7xpORRlcveLky51niIg0MXhoPwYPi2XgkBhs18YBEUIIIUR4
kL3mPs7nduM8c4aSoiOUnb2Et9ZLgx5NTUQCbvMPoX/z6/yaF18/F4NTo5k8fgSD4uUiFULcruT0
oSS/8DRNjU0c2voFNQVfEld6inEnnIw74cSjX6U04QglSZHUWAdjNabhqYnh5NEyTh4tAyDKbiYp
OZr+Ax0kDXQQl2jDZJIrcwohukdiTCRl1W6irHIRDiG6i3SwekjjxYvUFewHTUMzGNB0nQafgUtO
Exg0dKMRg66jG3UMJh2DUUc3GdHNJnSzMfDYaDFjNBnRI8wYzSYMpuYm83p8eN1uPO4G3DUuqspr
qS6vpLa6hkZnPf4GHz6fEa8hknqjDZ+eCHoi9GvOpzQP3sg6IpOs3DM6hQmj0jDKTpsQPcJsMTNx
ZibMzKShvpHjOwuoKCzCerGYIZerGHK5CagGDlFl0ymNjaMyahA+PZn6+hhOH2/i9PFyADQNovtZ
iUu0EZcQ1XyfaMPmsMiphUKILktNdpCSZMOoy++pheguPdbB8vv9vPzyyxw/fhyz2czq1asZMmRI
oHz79u385S9/wWg0MnfuXObPn9/hPOGk+rNt1OzY3mLaqdgMSmLHXTfFd+3WXUxAfPPDa5f915QH
g1YPllqIMZEwOJEJI4YybHB/uVy0EEEQYbVwz4ypMGMqSinKz1/h3JdF1J0+jaHsPDHOcsacKwPK
gEIU0GC0URGVSGVUIk5zP2or+lFdWU/xt+Xf/2HNj9HswRzhwxwJEXYNi8OINcpERKSp+d5iwWQ0
Y9ItmI2ma4/NmIwWzEYzui7fuQlxJ5LOlRDdq8e2plu3bqWpqYl//etfFBUV8cc//pG//vWvAHg8
Hv7whz+wadMmrFYrOTk5PPTQQxQWFrY5T0858G0ZZy7XNj9R18Y7U82/Q1LXBj9T1563+poW09T3
46VFZ2DLHAB+Pyg/mt+P3+cnutFNnXYBr3Jj8GtofprvlYbm1zAoDU0ZAjeUhqZ0UBooA6BjUH6U
du1m8OE3KDD48Fs0DJEmTNE2HAMSSU8bSmpiMmbjnTVooRDhQtM0ElP6k5jyWGCax+vj0pmLlB0/
TdWFC3gqSzHWVRDlvszw8hKsnuYvZRqNUTjN/aizxOI096PeZKfea8PdGIm7Brh0/Tt5AA8Gfw0m
XyO68mFQPgzKi+5vvgc/lxLNNFn0wJEwDQ008FkaqUs9h0HT0K7dDLSxQ6ZpPJKSyX0DJvXERyaE
EEKEvB7rYB08eJBp06YBMH78eA4fPhwoKy4uJiUlhejoaAAmTpxIQUEBRUVFbc7TE5RSvLPtJFV1
jT30Dvq1W0umIY0YHDXNnSY0NDQMaGiaAcO1m0kzYTaYsRjNRBiNWI0WIi0R9LM4iItykGiPITbS
gd1sI0KXU4OE6CtMRp2UESmkjEhpMd2vFDXOJsoqnFSXV+GurKa+qoammmpodGFpqsDkLcXv8aB8
BsCEwozSTPg0E36DGb9mxqub8WBEaToKHa4bZsHsar7dyG/0UjfkPF7dh/Ir/PhRSt38QprXaA3e
nlqnCiGEEKGvxzpYTqcTm80WeK7rOl6vF6PRiNPpxG63B8qioqJwOp3tztOWhAR7m2Wd8c+XH+v4
Rd1udhDeUwgR7pISIT01HhgahHf/7yC8Z99xu9uq7v47vSkcM0N45g7HzBCeucMxM4Rn7nDM3GMn
3dpsNlyu778K9fv9gY7SjWUulwu73d7uPEIIIYQQQggR6nqsg5WRkcHOnTsBKCoqIj09PVCWlpZG
SUkJ1dXVNDU1ceDAASZMmNDuPEIIIYQQQggR6jTV1on0t+m7KwKeOHECpRRr1qzh6NGjuN1usrKy
AlcRVEoxd+5cFixY0Oo8aWlpPRFPCCGEEEIIIbpdj3WwhBBCCCGEEOJOIwMfCCGEEEIIIUQ3kQ6W
EEIIIYQQQnQT6WAJIYQQQgghRDcJuw5WQ0MDzz33HLm5ufzyl7+ksrLyptesXr2aOXPmkJeXR15e
HnV1dUFI2ja/38+qVavIysoiLy+PkpKSFuXbt29n7ty5ZGVl8e677wYpZed0VJc33niDmTNnBtri
9OnTQUraeV9//TV5eXk3TQ+ndvlOW3UJt3bxeDy8+OKL5ObmMm/ePLZt29aiPJzapqO6hFvb+Hw+
li1bRnZ2Njk5OZw4caJFeTi1TSjqaB0bbK0tzyUlJeTk5JCbm8tLL72E3+8H4N1332XOnDnMnz+f
HTt2BDk5VFRUkJmZSXFxcdhkfv3118nKymLOnDm89957IZ/b4/HwwgsvkJ2dTW5ublh81tdvN7uS
tTP7p72R+dixY+Tm5pKXl8dTTz3F1atXQzLzjbm/89FHH5GVlRV4Hoq5O0WFmb/97W9q/fr1Siml
Nm/erF555ZWbXpOdna0qKip6O1qnbdmyRS1ZskQppdRXX32lnnnmmUBZU1OTmj59uqqurlaNjY1q
zpw5qry8PFhRO9ReXZRS6oUXXlCHDh0KRrRbsnHjRjVr1iz15JNPtpgebu2iVNt1USr82mXTpk1q
9erVSimlqqqqVGZmZqAs3NqmvbooFX5tk5+fr5YuXaqUUmrfvn1hvT4LRR2tY4OtteX56aefVvv2
7VNKKbVy5Ur16aefqrKyMjVr1izV2NioamtrA4+DpampSf36179Wjz76qDp16lRYZN63b596+umn
lc/nU06nU61fvz7kc+fn56tFixYppZTavXu3evbZZ0M6843bza5k7cz+aW9kXrBggTp69KhSSql3
3nlHrVmzJuQyt5ZbKaWOHDmiFi5cGJgWirk7K+yOYB08eJBp06YBcP/99/PFF1+0KPf7/ZSUlLBq
1Sqys7PZtGlTMGK26/o6jB8/nsOHDwfKiouLSUlJITo6GrPZzMSJEykoKAhW1A61VxeAI0eOsHHj
RnJycnj99deDEbFLUlJS+POf/3zT9HBrF2i7LhB+7fLYY4/x29/+FgClFLquB8rCrW3aqwuEX9tM
nz6dV155BYDS0lIcDkegLNzaJhR1tI4NttaW5yNHjnDvvfcCzdvpvXv38s033zBhwgTMZjN2u52U
lBS+/fbboOVeu3Yt2dnZJCYmAoRF5t27d5Oens5vfvMbnnnmGR544IGQzz1s2DB8Ph9+vx+n04nR
aAzpzDduN7uStaP9097KvG7dOkaPHg00n2FgsVhCLnNruauqqli3bh3Lly8PTAvF3J1lDHaA9rz3
3nv84x//aDEtLi4Ou90OQFRU1E2n/7ndbn7yk5/w85//HJ/Px8KFCxk7diyjRo3qtdwdcTqd2Gy2
wHNd1/F6vRiNRpxOZ6B+0FxHp9MZjJid0l5dAGbOnElubi42m41nn32WHTt28OCDDwYrbodmzJjB
hQsXbpoebu0CbdcFwq9doqKigOZ2WLRoEb/73e8CZeHWNu3VBcKvbQCMRiNLliwhPz+f9evXB6aH
W9uEoo7WscHW2vK8du1aNE0LlNfV1YXUsvDBBx8QGxvLtGnT2LhxI9DcOQzlzNC8A1paWsqGDRu4
cOECv/rVr0I+d2RkJBcvXuTxxx+nqqqKDRs2UFBQELKZb9xuduXzvX56a/unvZX5uy8NCgsLeeut
t3j77bfZtWtXSGW+MbfP52PFihUsW7YMi8USeE2ofdZdEdJHsJ588kk2b97c4ma323G5XAC4XK4W
35YCWK1WFi5ciNVqxWazcd999wX1G6fW2Gy2QB2g+ajbdxvLG8tcLleLhSvUtFcXpRQ//elPiY2N
xWw2k5mZydGjR4MV9baEW7u0J1zb5dKlSyxcuJDZs2fzxBNPBKaHY9u0VZdwbRtoPiKwZcsWVq5c
idvtBsKzbUJNe+vYUHHj8mwwfL9r8d12OpSWhffff5+9e/eSl5fHsWPHWLJkSYvfcIRiZoCYmBim
Tp2K2WwmNTUVi8XSYscyFHO/8cYbTJ06lS1btvDhhx+ydOlSPB5Pi2yhlvl6XVmWr5/e2v5pb/r4
44956aWX2LhxI7GxsSGf+ciRI5SUlPDyyy/z/PPPc+rUKV599dWQz92ekO5gtSYjI4PPP/8cgJ07
dzJx4sQW5WfPniUnJwefz4fH46GwsJAxY8YEI2qbMjIy2LlzJwBFRUWkp6cHytLS0igpKaG6upqm
piYOHDjAhAkTghW1Q+3Vxel0MmvWLFwuF0op9u/fz9ixY4MV9baEW7u0Jxzb5erVq/ziF7/gxRdf
ZN68eS3Kwq1t2qtLOLbNv//978CpjFarFU3TAjsl4dY2oai9dWwoaG15vuuuu9i/fz/QvJ2eNGkS
48aN4+DBgzQ2NlJXV0dxcXHQ6vL222/z1ltv8eabbzJ69GjWrl3L/fffH9KZASZOnMiuXbtQSnHl
yhXq6+uZMmVKSOd2OByBjlJ0dDRerzfkl4/rdSVrR/unveXDDz8MLN+DBw8GCPnM48aN4z//+Q9v
vvkm69atY/jw4axYsSLkc7cntL4G64ScnByWLFlCTk4OJpOJP/3pTwD8/e9/JyUlhYcffpjZs2cz
f/58TCYTs2fPZsSIEUFO3dIjjzzCnj17yM7ORinFmjVr+Oijj3C73WRlZbF06VKeeuoplFLMnTuX
pKSkYEduU0d1Wbx4MQsXLsRsNjNlyhQyMzODHblLwrVdWhPO7bJhwwZqa2t57bXXeO2114DmI9z1
9fVh1zYd1SXc2ubRRx9l2bJlLFiwAK/Xy/Lly8nPz+8z/zfB1to6NpS0tjyvWLGC1atXs27dOlJT
U5kxYwa6rpOXl0dubi5KKRYvXtziVKBgW7JkCStXrgzpzA8++CAFBQXMmzcPpRSrVq1i0KBBIZ37
Zz/7GcuXLyc3NxePx8PixYsZO3ZsSGe+XleWi7b2T3uTz+fj1VdfZcCAATz33HMATJ48mUWLFoVs
5vYkJCSEZW4ATSmlgh1CCCGEEEIIIfqCsDtFUAghhBBCCCFClXSwhBBCCCGEEKKbSAdLCCGEEEoN
wdMAAAA5SURBVEIIIbqJdLCEEEIIIYQQoptIB0sIIYQQQgghuol0sIQQQgghhBCim0gHSwghhBBC
CCG6yf8DIowHef8vnVgAAAAASUVORK5CYII=
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Compute-counterfactual-predictions"&gt;Compute counterfactual predictions&lt;a class="anchor-link" href="#Compute-counterfactual-predictions"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Our chains look stationary, well-mixing and similar to one another. This signals that our sampler probably did its job. Furthermore, the point estimates computed by the baseline model are contained in our posteriors. As the focus of this work is the overall analysis of the graphical model and not on these specific parameter estimates themselves, I don't dig further.&lt;/p&gt;
&lt;p&gt;The next task is to compute counterfactual predictions for $P(\text{Traffic}\ |\ \text{President}, \text{Accident})$, i.e. "given some inputs for $P$ and $A$ in our model, what's the probability of observing &lt;code&gt;traffic&lt;/code&gt;?" Remember, our model looks as follows:
$$
\text{traffic} \sim \text{Binomial}(1, p)\\
\log\bigg(\frac{p}{1 - p}\bigg) = \alpha + \beta_P P + \beta_A A
$$&lt;/p&gt;
&lt;p&gt;What values do we use for $\alpha, \beta_P, \beta_A$, you ask? Well, we've got a whole bunch of choices in the cell above.&lt;/p&gt;
&lt;p&gt;As such, we'll take the &lt;em&gt;trace&lt;/em&gt; for each variable - the values returned by the sampler, i.e. the squiggly plot on the right, from which we build the empirical distribution on the left - and plug in some new values for $P$ and $A$. The sampler used 4 chains of 2000 samples each, so we now have 8000 tuples of $(\alpha, \beta_P, \beta_A)$. Our new values for $P$ and $A$ can be whatever we want. (In fact, they don't even have to have been observed in our data to begin with!)&lt;/p&gt;
&lt;p&gt;First, we'll take the tuple $(P = 0, A = 0)$. We'll then plug this into our regression equation for each of the 8000 tuples of $(\alpha, \beta_P, \beta_A)$. Solving for $p$, this will give us 8000 values for $P(\text{Traffic} = 1\ |\ \text{President} = 0, \text{Accident} = 0)$. We then repeat for $(P = 0, A = 1), (P = 1, A = 0)$ and $(P = 1, A = 1)$.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accident_value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;log_odds_p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Intercept'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'president'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;president_value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'accident'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;accident_value&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log_odds_p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_prediction_interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;94&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;lower_percentile_bound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;upper_percentile_bound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lower_percentile_bound&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower_percentile_bound&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper_percentile_bound&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;input_combinations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;observed_proportions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;input_combinations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;observed_proportions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
        &lt;span class="n"&gt;observed_data&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;observed_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'president'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;observed_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'accident'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)][&lt;/span&gt;&lt;span class="s1"&gt;'traffic'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Plot-$P(\text{Traffic}\-|\-\text{President},-\text{Accident})$-posteriors"&gt;Plot $P(\text{Traffic}\ |\ \text{President}, \text{Accident})$ posteriors&lt;a class="anchor-link" href="#Plot-$P(\text{Traffic}\-|\-\text{President},-\text{Accident})$-posteriors"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Onto each posterior we also plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The observed traffic proportion, i.e. &lt;code&gt;observed_data['traffic'].mean()&lt;/code&gt; for the respective set of inputs $(P, A)$.&lt;/li&gt;
&lt;li&gt;The expected proportion, i.e. the original probability of observing traffic given the varying combinations of $(P, A)$. This is what we are trying to recover from our data. In the real world we'd never have this number, but when working with simulated data we can use it as a way to confirm that our model is reasonable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we note the uncertainty we're able to express in our estimates. If we instead used the point (i.e. single value) parameter estimates from the scikit-learn model above, we'd be looking at a single value in each plot instead of a distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;subplot_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]):&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;observed_proportion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;observed_proportions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;expected_proportion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TRAFFIC_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    
    &lt;span class="n"&gt;subplot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;221&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;subplot_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_yticklabels&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;visible&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;edgecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'white'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axvline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;observed_proportion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'--'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Observed Proportion'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axvline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expected_proportion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'#A60628'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'--'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Expected Proportion'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Posterior Distribution of &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;P(Traffic = 1 | President = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;, Accident = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;)'&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA/oAAALECAYAAAC44UvIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Plot-mean-and-prediction-interval-for-predicted-$P(\text{traffic})$"&gt;Plot mean and prediction interval for predicted $P(\text{traffic})$&lt;a class="anchor-link" href="#Plot-mean-and-prediction-interval-for-predicted-$P(\text{traffic})$"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;For each unique combination of $(P, A)$ we have 8000 guesses at what $P(\text{Traffic}\ |\ \text{President}, \text{Accident})$ is implied. For each set, we compute a mean and a 94% interval then plot against the observed proportion to see how we did.&lt;/p&gt;
&lt;p&gt;Given that our data were generated randomly, it is possible that we don't observe any positive occurrences of &lt;code&gt;traffic&lt;/code&gt; given varying values of &lt;code&gt;president&lt;/code&gt; and &lt;code&gt;accident&lt;/code&gt; - especially at $(T = 1\ |\ P = 1, A = 1)$. As such, there would be no data point in the following plot for the corresponding value of $x$.&lt;/p&gt;
&lt;p&gt;We can see that our model given the data is most sure about the probability of observing traffic given neither the president nor an accident. It is least sure about what happens with the former and without the latter. Finally, if the blue and green curves appear identical, do inspect the &lt;code&gt;y_observed&lt;/code&gt; and &lt;code&gt;y_predicted_mean&lt;/code&gt; objects to see what the true values are. (In most cases, they will not be identical even if they visually appear to be.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [19]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y_observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;observed_proportions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;observed_proportions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;

&lt;span class="n"&gt;y_predicted_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;joint_input&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;joint_input&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y_predicted_PI&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;compute_prediction_interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;joint_input&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;joint_input&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y_predicted_PI_lower_bound&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_predicted_PI_upper_bound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y_predicted_PI&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_observed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Observed Proportion'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_predicted_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Predicted Proportion'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill_between&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_predicted_PI_lower_bound&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_predicted_PI_upper_bound&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Counterfactual Predictions of P(Traffic = 1)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[19]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;lt;matplotlib.legend.Legend at 0x1192d19e8&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAsEAAAHfCAYAAACxhQUCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8VOWh//HPOZNACIEsEJQ17IggArIvKiCCgoJWK5ZL
tdq6t1WvS62K4FartrVotfqqV/3Z1qvWq6itiIKKoAioyCIoSwJhJ9skmfXMOc/vjyEjyBIQwmT5
vv8ymZlznpk4yYczz3mOZYwxiIiIiIg0IHayByAiIiIicrwpgkVERESkwVEEi4iIiEiDowgWERER
kQZHESwiIiIiDY4iWEREREQaHEWwSAPhui7PPfccF154IZMmTeLcc8/lkUceIRqN1sj+VqxYwfTp
04/4cZWVlUyZMoUJEybw7rvvHvHjCwsL+eUvf3nEj9vbtGnTmDNnzn7f/81vfsPIkSOZNGkSkydP
ZuLEiVx77bUUFxcf1f5WrlzJ6NGjAXjppZd45plnDnn/V199lX/84x+Hff+atH37diZOnMj555/P
l19+uc9t06ZNY/To0YnXa8KECdx+++2EQqHEfT766CP+9Kc/8cYbbzBp0iQmTZrEoEGDEq/zpEmT
WLZs2RGN6amnnuLMM8/kjjvuYM2aNZx11llccMEF/L//9/+4//77j8nzPph//etfXHPNNYmvd+zY
wfXXX4/neTW6XxE5cinJHoCIHB8zZszA7/fzwgsv0KxZM4LBILfccgt33nknjzzyyDHf3/r169m5
c+cRP27NmjUUFxfz3nvv/aD9btu2jfz8/B/02MNx+eWXc+WVVya+fuihh5g5cyazZs06Jtu/9NJL
q73P559/Trdu3Q77/jXps88+o2XLljz//PMHvP22225j/PjxABhj+PWvf82sWbO4/fbbqays5NFH
H+WVV16hSZMmTJ48GYj/Y6Nbt277vM5H4l//+hePPvooAwYM4IknnmDw4ME88MADP2hbh6usrIw/
/vGPvPnmmwwePDjx/RNPPJGePXvyz3/+k//6r/+q0TGIyJFRBIs0AIWFhbz11lssXLiQjIwMANLT
05k5c2bi6F1FRQUzZ85k7dq1WJbFyJEjufnmm0lJSaFHjx58+umn5OTkACS+XrduHX/6059o3749
69atIxqNMn36dPLy8pg1axYVFRXccccd/O53v2P+/Pk89dRTOI5DWloat99+O/369ePxxx9n+fLl
7Nq1i27durFy5Up27tzJpEmTePnll3n++ed5//33iUQihEIhbr/9dsaOHUssFuORRx7hww8/xOfz
0a9fP+655x7uuusudu7cyZVXXsnMmTM577zzEs9xy5Ytia+DwSAzZsygoKAAv99P06ZNefTRR+nc
ufMRvbZDhw5N/CNi9OjR9OnTh2+++Yabb76ZPn36cO+997J9+3Ycx2HChAmJo4T//Oc/eeGFF8jI
yKB79+6J7T3++OOUlpYyffp08vPzmT59OiUlJdi2zbXXXktqairz589n0aJFpKWlUVJSkrj/unXr
uPfeeykrK8OyLK644gomT57MZ599dsCf05AhQ1i2bBkPPfRQ4kjl1Vdfzbhx4/Z7ni+//DIvvvgi
tm3TsmVL7r77bnbu3Mljjz1GRUUF06ZN48UXXzzka2VZFoMHD2bBggWJ12DEiBE0adKk2td52rRp
ZGZmsnHjRi699FJOOeWUxCcZu3fvZtiwYTz44IPceOON7Ny5kzvvvJNrrrmGl156Cdd1CYfDDB8+
nHfffZenn36a3bt3c88997Bx40Zs22bKlCn89Kc/3Wefb7zxBs8999x+Y3n44Yfp0aPHPt975513
aNWqFbfddhsfffTRPrddfPHFXHTRRfz4xz+mUaNG1T5XETlOjIjUe3PmzDE/+tGPDnmf2267zdx3
333G8zwTiUTMFVdcYZ5++mljjDHdu3c3xcXFiftWfb148WLTs2dP8/XXXxtjjHn22WfN1KlTjTHG
vPbaa+aqq64yxhiTn59vJk6caEpKSowxxnz77bdm+PDhJhAImFmzZplx48YZx3GMMcYsXrzYTJgw
wRhjzJYtW8y0adNMKBQyxhjz9ttvm4kTJxpjjHnhhRfM1KlTTSgUMq7rml//+tfm9ddf3+fxhYWF
pm/fvolx7/31O++8Y+67777EbXfffbe59957jTHG/Nd//Zd555139nuNbr/9dvO3v/0t8XUoFDI3
3nhj4nGjRo0yTzzxROL2adOmmXnz5hljjAmHw2batGnm3//+t/n666/N0KFDza5duxL7HjVqlDHG
mFmzZpmZM2caY4yZPHmy+fvf/26MMWbbtm1mzJgxpqKiYp9xVN3fcRwzZswY8+677xpjjNmxY4cZ
OXKk+eKLLw75c/rpT39q3n77bWOMMWvWrDEzZszY73l/8skn5qyzzkr8P/Daa6+Zc845x3iet8/P
+fu+/zqWlZWZqVOnmmeffdYYY8wFF1xgFi9eXO3rXLWtO+64I/H1TTfdlHhsZWWlGTx4sFm5cmXi
57BixYr9Xs+9x3r99deb3//+98YYY8rLy82ECRNMQUHBAZ/HkTjY63HhhReaTz/99Ki3LyLHjo4E
izQAtm1XOydxwYIFvPTSS1iWRaNGjZgyZQovvPACV1111SEf16ZNG3r27AnAySefzOuvv77ffRYt
WsSuXbu4/PLLE9+zLIvNmzcD0LdvX1JS9v911LZtW37/+9/z1ltvsWnTJr766isCgQAAn3zyCZMm
TSItLQ2Axx57DIh/PH84xo8fT/v27XnxxRfZtGkTS5YsoV+/ftU+7vnnn+fNN98E4vOsBw4cyM03
35y4fcCAAQAEg0GWLl2K3+/nz3/+c+J7a9euZceOHQwfPpzc3FwALrnkEhYuXLjPfsrKyli7di0X
X3wxAK1bt+b9998/6LgKCgqIRCKcffbZAJxwwgmcffbZfPzxxwwePPigP6dzzjmHe++9l/nz5zNs
2LB9nkuVjz/+mHPPPTfxScCFF17IAw88wJYtW6p9vR5++GGeeuopjDEAjBo1KnHENT8/n7y8vGq3
UaXqtYX4NJQFCxbw17/+lY0bNxIOhwkGg4e9rU8++YRbb70VgGbNmvH222/vd58jORJcnQ4dOpCf
n8+QIUOO6HEiUnMUwSINQJ8+fdi4cSOVlZWJ6RAAO3fu5O6772bWrFn7RbLnecRisf229f0T6aoi
FOJhWxU739/W0KFDE6EK8ROqWrVqxXvvvUd6evoBx7169Wquu+46Lr/8coYPH87AgQOZOXMmwH7R
XFRUtN9z+P54HMdJ/Pc///lPXnnlFaZOncp5551HVlbWYUXd9+cEf1/Vc/E8D2MM//u//5v4uL+k
pITGjRvzyiuv7DMun8+333aqnp9lWYnvbdy4kTZt2hxwvwf6R44xJvEzPNjPacqUKYwaNYpFixbx
8ccf88QTT/Dmm2/SrFmzfbZzqG0fyt5zgr/Psixc1612G1X2/v9k6tSpnHTSSYwcOZJzzjmHr776
6oDjPJiUlJR9XtvCwkKys7P3eX9Mnjw5MU/5aLmue8Cfs4gkj1aHEGkATjjhBM477zx++9vfUllZ
CcRXYZgxYwZZWVmkpaUxYsQI/vGPf2CMIRqN8sorrzBs2DAAcnJyWLlyJcBhn7Dm8/kSkTRkyBAW
LVrEhg0bgPiKAOeffz6RSOSQ21i6dCm9e/fmZz/7GYMGDWLevHmJaBo6dChvv/020WgUz/OYMWMG
//73v/H5fInYbd68OY7jsH79+v3GvnDhQi644AIuvvhiOnXqxPz5848oyKqTkZFB3759E0cSy8vL
ufTSS5k3bx7Dhg1j0aJF7NixA+CAR88zMjLo1asXb7zxBhD/R8Oll15KRUXFPq9tlU6dOpGamsrc
uXOB+D9w3n333cTP8GCmTJnCmjVruPDCC7nvvvsoLy/H7/fvc58RI0bwn//8h5KSEgBee+01srKy
jugo7oF07NiRwsLCI36c3+9n1apV3HLLLZx99tns3LmTzZs3H9EKDEOHDuW1114D4vPhL7vsMgoK
Co54LIdry5YtRzzfXERqlo4EizQQ99xzD08++SRTpkzB5/MRjUY566yzEsuJ3XXXXdx///2cd955
OI7DyJEjEydx3XXXXdx77700b96cYcOGJT7GP5R+/frx2GOPcf311/OXv/yFe++9l5tvvhljDCkp
KTz11FMHPQJcZeLEicydO5dzzz2X1NRUhg4dit/vTyyjtnXrVi688EKMMQwaNIhp06YRCATw+Xxc
dNFFvPrqq9x666384he/ICcnZ58jkldccQXTp0/n//7v//D5fPTq1Ytvv/32KF7h/T366KPcd999
nHfeeUSj0cRSYgC33norl112GU2bNqVPnz4HfPwf/vAHZs6cyYsvvohlWTzwwAPk5uZy+umnc999
9+1z39TUVJ588knuv/9+Hn/8cVzX5frrr2fIkCGHnCJyyy238OCDD/LYY49h2zY33HAD7dq12+c+
w4cP5/LLL+eyyy7D8zxycnJ4+umnse2jO44yfvx4Pv744yOeIpCZmclVV13FBRdcQFZWFtnZ2fTv
359NmzYxdOjQw9rG9OnTmTFjBueddx7GGK6++mp69+79Q55GtYqKiiguLqZ///41sn0R+WEscySf
H4mIiBwjlZWV/PjHP+a11147rBUi6qrHH3+cnJwcpk6dmuyhiMheNB1CRESSIiMjg5tvvpknn3wy
2UOpMdu3b2f16tVMmTIl2UMRke/RkWARERERaXB0JFhEREREGhxFsIiIiIg0OIpgEREREWlwjvsS
abt3VxzvXSZkZ6dTWnr4VxQSaaj0XhGpnt4nIocnme+V3NxmB72tQR0JTknR1XpEDofeKyLV0/tE
5PDU1vdKg4pgERERERFQBIuIiIhIA6QIFhEREZEGRxEsIiIiIg2OIlhEREREGhxFsIiIiIg0OIpg
EREREWlwjvvFMkRERETqo23btvKXvzyG3+/HdWN06dKd6677JenpTXnggRmMGXM2Q4YMS9r4Fi/+
hHnz5nLnnTMS39u+fRuXXXYp3bv3wLIsotEo/fsP4Oqrr6/x8cye/X9MmHA++fkbWLhwAT/72S9q
fJ97UwSLiIiIHKVIJMxvfnMzt99+N7169QbgnXfeZsaMO3n44ceSPLpD69ixE0888QwAnudx7bVX
sn79Orp27Vaj+33xxecYP34C3br1oFu3HjW6rwNRBIuIiEi98tirX7FiQ/Ex3WafLi248eJTD3r7
J58spG/f/okABjjnnIm8/vq/2LZtKwCvv/4qL730Iq7r8pvf3E1ubiumT/8NgUCAcDjMVVddx6BB
Q5g//31efvkf2LZNnz59ufbaX/Lss0+zatUKQqEQo0ePpbKygiuuuIpoNMrll1/KCy/8L7Nnv8Z7
772LZVmMGXM2F188hYKCfH73u3tJS2tCkyZpNGvW/JDPMxqN4jhR0tLSeOCBGfj9fsrL/Tz88GO8
8MKzrFixHICxY8fz4x9fygMPzMAYw65dOwmFgtx1173k5XXkpZf+zrx5c/H5fAwdOpjLL79mn+cw
dux4SkqKmTHjt1x88aXMnv0aM2f+jrlz3+GVV14iNTWV9u07cNttdzJ37jt8+ukiIpEwW7duYerU
yzj33POO+meqCBYRERE5Stu2baVt23b7fb916zbs2LEdgN69T2XatMv59NOFPPnkLH7+82vw+/38
4Q+zKC0tpbBwE+Xlfv7nf57mb397kbS0NO67726WLl0MQF5eJ2688RbKy8u57rqf87Of/YKFCxcw
bNhItmwpZN6893jyyb8BcNNN1zN48BCefPLP/PznVzNw4BD+/vfn2bSpYL8xFhTkc8MNV2FZFrbt
4+KLL6Vdu/YAnHbaAC65ZCqLFn3M9u3beOaZ53Fdl2uvvZLTThsIQNu27bjrrpl7ntefueqq65k/
/z3++tf/wefzce+9v2XRoo/3eQ4AL730IjNmPMjq1SsB8PvLePbZp3nuuX+Qnt6UWbP+wOzZr9Gk
STqBQCV//OMTFBZu5vbbb1IEi4iIiHzfoY7Y1pTc3FZ8/fXq/b6/desWTjjhRAD69u0HxGP4L3+Z
RefOXZg06UJmzLiTWCzGRRdNYcuWQsrKSrnlll8BEAwG2bp1CwAdOuQB0Lx5c7p378GKFct55523
uOGGm1i/fh07d+7g17++FoCKigoKCwvZvHkzPXvGj06fckrfA0bw3tMhvq9qn5s25XPqqX2xLIuU
lBR69TqFgoKNAPTvPzDxvGbN+iObNhXQq9cppKTEM3PAgAHk52/YZ3sHsm3bVjp16kx6elMATj21
P0uXLubkk3vTtWt3AFq1OoFoNHrQbRwJrQ4hIiIicpRGjDiDZcs+4+uvVyW+99Zbb5CZmZU4Qrxm
TTySv/rqSzp37sKGDesJBgM88sifufPOmTz22CO0bt2WVq1O4LHHnuSJJ57hoosuoVevUwCwbSux
7fPOm8wrr/yTSCRCXl5HOnTIo2PHzjz++NM88cQznHvuRLp06UanTp1YtWoFAGvX7h/p1bGseCrm
5XVKTIWIxWKsWrWCdu06APDNN2sAWLnyKzp16kJeXke+/noVsVgMYwxLly6lffu8/Z6DZdkYYxJf
t27dloKCfEKhEADLl39B+/Yd9tz3u8cdKzoSLCIiInKU0tPT+f3v/8SsWX+gvNxPLObStWs3Zsx4
IHGf1atX8qtfXYNlWdxxx3Sys3N47rlnmD//fTzP48orryY7O5tLLpnKDTdcheu6tG7dhtGjx+63
v379TuPhhx/gpz+9AoBu3bozYMBArrvuSqJRh549e5Gbm8sNN9zE/fffw0svvUhWVhaNGjX+Qc9v
+PCRfPnl51x99c9wHIfRo8+iR4+TgPiqEwsXfoTnefz2t/fQpk1bRo8+i2uvvRJjDIMHD+T0089k
/fpv99nmqaf25ZZbfsUVV1wFQFZWFldccTW/+tXVWJZNu3btueaaG5g3b+4PGnN1LLN3gh8Hu3dX
HM/d7SM3t1lS9y9SV+i9IlI9vU9EOKyl35L5XsnNbXbQ2zQdQkREREQaHE2HEBEREZEfZO8Lb9Q1
OhIsIiIiIg2OIlhEREREakTQCbKtfFuyh3FAmg4hIiIiIsdUyAlRHvXjGpfs9HRSkz2gA1AEi4iI
iMgxEXJCVET9xDwX27axrdo76UARLCIiInKUvvhiGdOn30HHjp2wLItIJMLZZ4/nooumHPG2nnrq
cfLyOtKtW3cWLlzAz372iwPe76OPPqBXr960bJlb7TYXL/6EefPm7nMi2/bt27jsskvp3r0HlmUR
jUbp338AV199/RGPORKL4I+Ufhe/9qHjd/bs/2PChPPJz99wyOdYkxTBIiIiIsfAaacNYObM3wEQ
jUb5yU9+xLhxE2jW7OBr1R5Kt2496Natx0Fvf/XVl+jY8beHFcEHs/clkz3P49prr2T9+nV07drt
sB4fiUUoj5YRdR18tq/a+K3y4ovPMX78hGqfY01SBIuIiEi98uRX/8Pq4rXHdJu9WpzEdadecdj3
DwaD2LaNz+fjhhuuIjs7h/Lych555DH+8IeH2LKlEM/z+MUvrqV//wF8+OE8XnjhWbKysnEch7y8
jnzxxTJmz36NmTN/x9tvv8Hrr7+G57mMGHEGPXv2Yv36b7n//uk8+eSzzJ79Gu+99y6WZTFmzNlc
fPEUCgry+d3v7iUtrQlNmqTRrFnzQ445Go3iOFHS0tJ44IEZ+P1+ysv9PPzwY7zwwrOJyyaPHTue
SRdeyP0P3IPruZQUFRMOhfjlrTfRtkN73vzX6yz6aAE+n4+evXtzwy3X8OyzT7Nq1QpCoRBjx46n
pKSYGTN+y8UXX5p4jnPnvsMrr7xEamoq7dt34Lbb7mTu3Hf49NNFRCJhtm7dwtSpl3Huuecd1c+y
iiJYRERE5Bj4/PNl3HDDVdi2TUpKCjfddCvp6ekAnHXWOM44YxSvv/4vMjOzuOOO6fj9ZVx//VU8
//w/efzxP/E///N3mjfP5NZbf73PdktLS/j731/ghRdeolGjxvz1r0/Qt29/unbtzq23/pYtWwqZ
N+89nnzybwDcdNP1DB48hCef/DM///nVDBw4hL///Xk2bSrYb8wFBfnccMNVWJaFbfu4+OJLadeu
PRA/sn3JJVNZtOhjtm/fxjPPPE84Gua6666kfc8OGDxat2nNr267mS+WLOPFZ5/n0sun8cmChTzw
p0fw+Xw8ct/vWLxwMQB5eZ248cZbAHjppReZMeNBVq9eCYDfX8azzz7Nc8/9g/T0psya9Qdmz36N
Jk3SCQQq+eMfn6CwcDO3336TIlhERETkQI7kiO2xtPd0iO/r0CEPgA0b1rNixZd8/fUqAFw3RlFR
Ec2bNyczMwuA3r377PPYrVu30qlTFxo3TgPg2mt/uc/tGzduYOfOHfz619cCUFFRQWFhIZs3b6Zn
z94AnHJK3wNG8N7TIQ425k2b8ul9yikUh4sIuxG69ezBts1b42PtGx9rj5NP4vmn/8bWwi1079mD
lJR4YvbsfTIFGwsAO7G9A9m2bSudOnUmPb0pAKee2p+lSxdz8sm96dq1OwCtWp1ANBo96DaOVO09
ZU9ERESknqiaK5uX15GzzhrHE088wx/+MItRo86iRYsWVFZWUlpaCsDatV/v89i2bduxeXNBIgDv
uus2du/ehW3beJ5Hhw55dOzYmccff5onnniGc8+dSJcu3ejUqROrVq3Ys83VRzxmy7KJuTGyW+ew
bPkSYl4MXMM3X6+hddvWAGxctyG+/dVraJfXgbbt27Fu7be4rosxhjUrV9OuQ7s9r4G1z7aNMYmv
W7duS0FBPqFQCIDly7+gffsOe+773eOOJR0JFhERETlOJk26kN///n5uuOEqAoFKLrjgYlJTU7np
ptv47/++gWbNMhNHUatkZ2czdepliWkLw4ePJDe3Fb179+H+++/hT396ggEDBnLddVcSjTr07NmL
3NxcbrjhJu6//x5eeulFsrKyaNSo8WGP0zMeFdFydgS302/QaaxYvpzf3ngLMSfG0NNH0LlbVwC+
XPo5Sz/9DM9zuf6WGznhxBMZdvoI7rzpNozxOKnXyQw/Yzib1766z/ZPPbUvt9zyK6644ioAsrKy
uOKKq/nVr67GsmzatWvPNdfcwLx5c4/yFT84y+yd4cfB7t0Vx3N3+8jNbZbU/YvUFXqviFRP7xOp
j2JuDH+0jHAsjM/2HfK+Tzz6J4afcTr9Bp52yPtlZqXRxMk+lsM8bLm5B1+ZQ0eCRURERBo4z3iU
hUsJxUL4bF+1AVwfKIJFREREGijPePgjZQSd4BHH7w233FSDI6t5imARERGRBuZo4re+UASLiIiI
NBDGGPyRMgJOoMHGbxVFsIiIiEg9F49fP8FYANuyG3T8VlEEi4iIiNRTxhjKo+UEogFs28K2dImI
KopgERERkXpm7/i1rH0vVCFximARERGResIYQ0W0goBTCSh+D0URLCIiIlIPlEfKE/FbU5cark8U
wSIiIiJ1WGW0kspoOQbF75FQBIuIiIjUQQEnQHnED8TjV/l7ZBTBIiIiInVIwAlQESnHw9NqD0dB
ESwiIiJSB3w/fm0UwEdDESwiIiJSi4WcEOVRP65xFb/HkCJYREREpBYKOSEqon5inott25r6cIwp
gkVERERqkUgsgj9S+l382orfmqAIFhEREakFIrEI5dEyHDem+D0OFMEiIiIiSVQVv1HXwWf7FL/H
iSJYREREJAmibpTySBkRN4rP9uGzfckeUoOiCBYRERE5jhzXwR8pJexFSbEUv8miCBYRERE5DhzX
wR8tI+xGSLF8pFiK32RSBIuIiIjUoJgbwx8tI+SGFb+1iCJYREREpAZUxW84FsZnK35rG0WwiIiI
yDHkGY+ycCmhWEgnvNViimARERGRY+C7+A3js23Fby2nCBYRERE5Cp7x8EfKCDrBPUd+tc5vXaAI
FhEREfkBPONRHvETcAKa9lAHKYJFREREjoAxBn/ETzAWwLY07aGuUgSLiIiIHAZjDOXRciqjlfhs
G9vStIe6TBEsIiIicghV8RuIBrAsNOe3nlAEi4iIiByAMYaKaAWV0UosC2zbSvaQ5BhSBIuIiIjs
pSp+A04loPitrxTBIiIiIntURCoIOBUYwLIUv/WZIlhEREQavMpoJRXRciAev8rf+k8RLCIiIg1W
wAlQESnHw9NqDw2MIlhEREQanO/Hr40CuKFRBIuIiEiDEXJClEf9uMZV/DZwimARERGp90JOiIqo
n5jnYutCF4IiWEREROqxcCxMeaTsu/jVhS5kD0WwiIiI1DuRWITyaBmOG1P8ygFVG8Ge5zFjxgy+
+eYbGjVqxP33309eXl7i9jfffJPnnnsO27b50Y9+xE9+8pMaHbCIiIjIwVTFb9R18Nk+xa8cVLUR
/P777xONRnn55ZdZvnw5Dz30EE899VTi9ocffpi3336b9PR0JkyYwIQJE8jMzKzRQYuIiIjsLepG
KY+UEXGj+GwfPtuX7CFJLVdtBH/++eeMHDkSgL59+7Jq1ap9bu/RowcVFRWkpKRgjNHVVUREROS4
cVwHf6SUsBclxVL8yuGrNoIrKyvJyMhIfO3z+YjFYqSkxB/arVs3fvSjH9GkSRPGjh1L8+bND7m9
7Ox0UlKS9z9obm6zpO1bpC7Re0WkenqfJI/jOpSFy3BjYTKbNiGTJskekhxEzI3VyvdKtRGckZFB
IBBIfO15XiKA165dy4cffsi8efNIT0/n1ltv5Z133uGcc8456PZKS4PHYNg/TG5uM3bvrkja/kXq
Cr1XRKqn90lyxNwYZdFSwm6EFEtHfeuCzKy0pL1XDhXf1c4W79+/PwsWLABg+fLldO/ePXFbs2bN
SEtLo3Hjxvh8PnJycigvLz8GQxYRERH5TsyNURwqYmdwBzEvpgCWo1btkeCxY8eyaNEipkyZgjGG
Bx98kLfeeotgMMgll1zCJZdcwk9+8hNSU1Pp0KEDF1xwwfEYt4iIiDQArufij5QRioV0wpscU5Yx
xhzPHSZY4ZY1AAAgAElEQVTzoyN9dCVyePReEame3ic1yzMeZeHSRPxK3ZWZlUYTJzsp+z7UdAhd
LENERERqDc94+CNlBJ2gjvxKjVIEi4iISNIpfuV4UwSLiIhI0hhj4vEbC2JbtuJXjhtFsIiIiBx3
xhjKo+VURivx2Ta2pcsby/GlCBYREZHjpip+A9EAlgU+W/EryaEIFhERkRpnjKEiWkFltBLLAtu2
kj0kaeAUwSIiIlJjquI34FQCil+pPRTBIiIiUiMqIhUEnAoMYFmKX6ldFMEiIiJyTFVGK6mIlgPx
+FX+Sm2kCBYREZFjIuAEqIiU4+FptQep9RTBIiIiclS+H782CmCp/RTBIiIi8oOEnBDlUT+ucRW/
UucogkVEROSIJOLXc7F1oQupoxTBIiIiclhCToiKqJ9YVfzqQhdShymCRURE5JAisQjl0TIcN6b4
lXpDESwiIiIHVBW/UdfBZ/sUv1KvKIJFRERkH1E3SnmkjIgbxWf78Nm+ZA9J5JhTBIuIiAgAjuvg
j5QS8Rx8lq34lXpNESwiItLAOa6DP1pG2I2QYvnwabUHaQAUwSIiIg1UzI1RFi1NxG+KpSO/0nAo
gkVERBqYmBuLH/mNhfHZil9pmBTBIiIiDcT341dzfqUhUwSLiIjUc57xKAuXEoqFFL8ieyiCRURE
6inPePgjZQSdoOJX5HsUwSIiIvWM4lekeopgERGResIYE4/fWBBb6/yKHJIiWEREpI6Lx6+fgBPA
Z9vYWudXaolQLExRyS56ZWRhWVayh7MPRbCIiEgdZYyhPFpOIBrAssBnK36l9lhXls9bG+cScIL8
sm86J+V0S/aQ9qEIFhERqWOMMVREK6iMVmJZYNu16wibNGxR1+H9zQv4fNcKADplt6dTZl6SR7U/
RbCIiEgdURW/AacSUPxK7bO1cjtvrJ9DSaQM27IZ1W4443oOo3GsUbKHth9FsIiISB1QEamg0qkA
qHVzK0Vcz+XjbZ+xcOsSDIZWTVoyqct4TmyaW2vnqCuCRUREarHKaCUV0XJA8Su1U1GohDc2zGF7
YCcAQ1ufxpnthpFi1+7MrN2jExERaaACToCKSDkeXq09kiYNmzGGZTu/4v3NC4gZl8xGzTi/yzg6
Nm+f7KEdFkWwiIhILfL9+LVRAEvtUx6t5K2Nc9no3wRAn5Y9GZc3irSUxkke2eFTBIuIiNQCQSdI
RbQc17iKX6nVVhd/w3/y5xF2IzRJSWNCp7PoWcuWPzscimAREZEkCjkhyqP+7+JXUx+klgrHwrxT
8AGritcC0DWzIxM7j6VZo4wkj+yHUQSLiIgkQcgJURH1E/NcbF3lTWq5fP9m3tz4LuXRSlLtFMZ2
OJ3+rfrU6ZM1FcEiIiLHUSQWwR8p/S5+dZU3qcUcL8YHhQv5bMeXALRpeiKTu4ynRZPsJI/s6CmC
RUREjoNILEJ5tIyo6+CzfYpfqfW2B3bxxoZ3KAqVYGFxershjGgzqN58aqEIFhERqUGRWISKqJ+I
G8Vn+/DZvmQPSeSQPOPxybZlfLT1Uzzj0SItm8ldxtMm48RkD+2YUgSLiIjUAMd18EdKiXgOPstW
/EqdUBouY/aGdyms3AbAwBP6Mqb9CFJ9qUke2bGnCBYRETmGHNfBHy0j7EZIsXz46slHx1K/GWNY
vns1czd9SNRzyEhtyvmdz6ZLVsdkD63GKIJFRESOgZgboyxamojfFEtHfqVuqHQCvL3xfdaVbQSg
Z043zu04hvTUJkkeWc1SBIuIiByFmBuLH/mNhfHZil+pW74p3cDbG98jGAvR2NeYczqOoneLk+r0
0meHSxEsIiLyA8TcGMWhokT8as6v1CURN8rcTR+xfPcqADo2b8/5nceR2bhZkkd2/CiCRUREDpMx
hopoBZFYiHCjxjieo/iVOqewYitvbJhDWaQcn+VjTPsRDDqxX4M4+rs3RbCIiEg1Qk6IYCxAOBbG
tmwsy1L8Sp3jei4fbvmUT7cvw2A4IT2XyV3G0yq9ZbKHlhSKYBERkQNwXIdKp5JwLITBYGuZM6nD
dgWLeGPDHHYGd2NhMbzNQM5oO7RB/z+tCBYREdnDMx6V0UrCsSBRzyHFTsGyLCwa1sfEUn8YY/hs
x5fML1yIa1yyG2cyqcs42jdrm+yhJZ0iWEREGrygEyTkBAi7kcR0hxRbfyKlbvNHynlz41wKygsB
6Jfbm7F5Z9DY1yjJI6sd9A4XEZEGyXGd+ElubljTHaReMcawqngt7xR8QMSNkJ7ShImdx9Iju0uy
h1arKIJFRKTB8IyXWN3B8WL4bJ+mO0i9EoqF+U/+PL4u+RaA7lmdmdh5LE1T05M8stpHESwiIvWa
MYZgLEjYCRJyw4lpDjrqK/XNhrIC3tw4l0onQKqdyri8M+mb26vBLX12uBTBIiJSL0Vj0T2rO4TB
ik930DxfqY8c1+H9wo9ZtvMrANpltGZSl/HkpGUleWS1m34biIhIvVE13SHsBIkZF5/tw7Yt0HQH
qae2Ve7g9Q1zKAmXYls2Z7QdyrA2A7AtO9lDq/UUwSIiUqdVTXcIOQEibjQ+zcECn6XpDlJ/ecZj
4dYlLNi6GIOhZZMcJnc5h9ZNWyV7aHWGIlhEROqkSCxCwKkkHItgWegqbtJgFIdKeWPDHLYFdgAw
+MT+jG4/XNN9jpBeLRERqTNcz02s7uAaD9u290x3EKn/jDF8sWsF721egOPFaN4og/M7j6NTZodk
D+2gSssdNu+KMLBjVq07QU8RLCIitdre0x3CbiR+tMtCcx6lQamIVvL2xvdY7y8AoHeLkzin4yjS
UtKSO7CDKC2P8uGyYj5fU4bnQdNLmtOrU06yh7UPRbCIiNRK4ViYoBMgFAtjW5au4iYN1pqSdfw7
/31CsTBpvsac22kMvVr0SPawDqjEH+XDZUV8sdaP54FlweA+OXRvn5nsoe1Hv01ERKTWiLmxPcua
BfGMwbZtfLaO+ErDFI5FeHfTB6woWgNA58w8zut8Ns0bZSR5ZPsr8Uf5YFkRX67x45l4/PbrkcmZ
A1vQtVMmqU7tm6+vCBYRkaQyxlAZjYdvxIvume5gYdey+YMix9Om8i3M3jAHf7SCFMvHWR1OZ8AJ
p9a6ebXFZfH4Xb52r/g9KZNRA1rQMrtxsod3SIpgERFJinAsTMCpJOJGsNB0BxGAmBfjwy2f8On2
zwFo3fQEJncZT8smtWs+bVFZlA+XFrH8m3j82hb075nJqAEtaZHVKNnDOyz6bSMiIsdNzI1R4Xy3
uoPP9ukEN5E9dgZ388b6OewKFWFhMaLtIEa2GVyrlv4rKo3wwdJiln/rx+yJ39N6ZnLmwJa0yKwb
8VtFESwiIjXKGJNY1ixiHFIsH1iWLmYhsodnPBZv/5wPtnyCZzxyGmcxqct42jVrneyhJewujfDB
0iK++rY8Hr/2nvgd0JKcOha/VRTBIiJSI0JOiGAsQDgWxrbs+HQHha/IPsoifmZveJfNFVsBOK1V
H87qcDqNfKlJHlncrpJ4/K5Yt3f8ZnHmgBZ1Nn6rKIJFROSYcVxnz+oOIQwG27Jr1Ue5IrWFMYYV
RV8zp+BDol6UjNR0JnY+m25ZnZI9NCAev/OXFrHy23IM8fgdcHI8frOb1+34raIIFhGRo+IZL7G6
Q9RzSLFTsCwLi9p1FrtIbRFwgvw7fx7flK4H4KTsrkzodBbpqU2SPDLYWRyP31Xr4vHrs+G0k7M4
47SWZDevHUenjxVFsIiI/CBB57uruCWmO2h1B5FDWle6kbfy3yPgBGlkN2J8x1H0adkz6Uuf7SgO
88GSIlatr0jE74Be8fjNala/4reKfluJiMhhc1wnfpKbG9Z0B5EjEHWjvLd5AV/sWglAh2ZtmdRl
HFmNk3sltR1F4fiR3/UVAPhsa0/8tqi38VtFESwiIofkGS+xuoPjxfDZPk13EDkCWyq28caGOZRG
/PgsH6PaD2Pwif2Tujzg9qIw85cUsXrDd/E7sFcWZwxoQWZG/Y7fKopgERHZjzGGYCxI2AkScsOJ
aQ466ity+FzPZcHWz1i0bQkGQ6v0lkzuMp4T0nOTNqZtu+Px+/XGePym+OLxe/ppDSd+qyiCRUQk
IRqL7lndIQxWfLqD5vmKHLndoWJmb5jD9sAuAIa2HsCZ7YYm7f20bXeYeZ/tZk1+JbAnfntncUb/
FjRvYPFbRb/ZREQauKrpDmEnSMy48au42RZouoPIETPGsHTncuZt/piYccls1JxJXcaR17xdUsaz
dVeI+UuK9onfwadkM7J/Ds2bNsz4raIIFhFpgKqmO4ScABE3Gp/mYKGruIkchfJIBW9unEt++WYA
Tm15MuPyzqRxSuPjPpYtO+Pxu7YgHr+pKRaDemdzev8WNGuq/ANFsIhIgxKJRQg4lYRjESwLLMvS
PF+RY2B18Tf8J38eYTdCekoTJnQ6i5Nyuh73cWzZGWLekiK+2St+40d+W9AsXdm3N70aIiL1nOu5
idUdXONh2/ae6Q4icrRCsTDvFMxndfE3AHTL6sTETmPJaNT0uI6jcEeIeUt28+2mABCP3yGnZDNC
8XtQelVEROohYwwBJ0A4FiTsRuIn41gkdUkmkfpmo38Tb26YS4VTSaqdwtgOZ9C/1SnH9cIXm3eE
mP/Zbr7dHI/fRqkWQ07JYUS/HDIUv4ekV0dEpB4Jx8IEnQChWBjbsnQVN5Ea4Hgx5hcuZMmOLwFo
m9GaSV3G0SIt+7iNYdP2IPOXFLEuEb82Q/tkM7xfDhlN9J4/HHqVRETquJgb27OsWRDPGGzbxmfr
iK9ITdge2Mkb6+dQFC7BtmxObzuE4W0GHrdPWTZtCzJvSRHrC/eN3xH9cmiq+D0ierVEROogYwyV
0Xj4RrzonukOFvZx/BhWpCHxjMeibUtZsHUxnvFokZbD5C7jaJNx4nHZf/7WIPOX7GbDliAAjVNt
hp6azYi+OaQrfn8QvWoiInVIOBYm4FQScSNYaLqDyPFQEi5j9oY5bKncDsDAE/oypv0IUn01v85u
/tYg85bsZmNV/DayGXZqNsP7tiA9TSu7HA395hQRqeVibowK57vVHXy2Tye4iRwHxhi+3L2KuZs+
wvEcmqU25fwu4+icmVfj+964JcC8JUXkb/0ufoefmsPwvjk0UfweE4pgEZFayBiTWNYsYhxSLB9Y
li5mIXKcVDoB3t74HuvK8gE4Oac753YaQ5OUtBrbpzGGjVvjJ7xVxW9aI5thfXMYfqri91hTBIuI
1CIhJ0QwFiAcC2Nbdny6g8JX5LhaW7Kef+e/TzAWIs3XmHM6jqZ3y5NqbH/GGDZuiU97KNgWAiCt
sc3wvjkMOzWHJo31O6AmKIJFRJLMcZ09qzuEMBhsy9ZV3ESSIBKLMHfzRyzfvRqATs3bc17ncWQ2
blYj+zPGsKEwHr+btsfjt8le8Zum+K1RimARkSTwjJdY3SHqOaTYKViWhYVWdxBJhs3lW5i98V3K
IuX4LB9jOoxg0An9auTCF8YY1hfG5/xurorfNB8j+uYw9NRs0hopfo+HaiPY8zxmzJjBN998Q6NG
jbj//vvJy/tuQviKFSt46KGHMMaQm5vLI488QuPGjWt00CIidVXQCRJyAoTdyHfTHbS6g0jSxLwY
H235lE+2LwPgxPRWTO4yntz0Fsd8X8YY1m0OMH9JEZt3fBe/I/vmMETxe9xV+5v3/fffJxqN8vLL
L7N8+XIeeughnnrqKSD+w7z77ruZNWsWeXl5vPrqq2zdupXOnTvX+MBFROoKx3XiJ7m5YU13EKlF
dgWLeGPDHHYGd2NhMbzNQE5vO+SYvz+r4nfeZ7sp3BkGID3Nx8h+OQzpk01jxW9SVBvBn3/+OSNH
jgSgb9++rFq1KnFbfn4+WVlZPP/886xbt44zzjhDASwiQny6Q9XqDo4Xw2f7NN1BpJYwxvDZji+Y
X7gI17hkN85kUpfxtG/W5pjv59tNAeYt2c2WveO3fw5DTsmhcSMtdZhM1UZwZWUlGRkZia99Ph+x
WIyUlBRKS0v58ssvmT59Oh06dOCaa66hd+/eDB069KDby85OJyUlef/iyc2tmcntIvWN3itHzhhD
0AkSdIJEnBDpTVJIp2myhyU1qEWLjOrvJLVKacjPP76azfqSTQAMbd+PyT3PpnFKo2O2D2MMq9b5
efvDbRRsi1/euFl6CmOHn8gZA1o1uBPeYm6sVv5NqTaCMzIyCAQCia89zyMlJf6wrKws8vLy6NKl
CwAjR45k1apVh4zg0tLg0Y75B8vNbcbu3RVJ279IXaH3ypGJxqJ7VncIg2V0IYsGokWLDIqLK5M9
DDlMxhhWFq1hzqYPiLhRmqakM7HzWXTP7kKlP0ol0WOyj7UFlcxfUsTWXfEjv02b+Di9fwsGn5JN
o1SbQGWIQAP73yYzKy1pf1MOFd/VRnD//v354IMPOPfcc1m+fDndu3dP3Na+fXsCgQCbNm0iLy+P
ZcuWcdFFFx2bUYuI1GJV0x3CTpCYceNXcbMt0HQHkVon6IT4T8E81pSsA6B7dhcmdjqLpqnpx2T7
xhjW5lcyb0kR23bH4zcjPR6/g3rH41dqn2ojeOzYsSxatIgpU6ZgjOHBBx/krbfeIhgMcskll/DA
Aw/w3//93xhj6NevH2eeeeZxGLaIyPFnjCEYi6/uEHGj8ZNnLHQVN5FabH1ZAW9tnEulE6CRncq4
vDM5NbfXMVn6zBjDmo2VzF+6m227IwA0S/cxUvFbJ1jGGHM8d5jMj1j1Ea/I4dF7ZV+RWISAU0k4
FsGyqJF1Q6Xu0XSI2i3qOszb/DHLdn0FQPtmbZjUeTzZaZlHvW3PGNZsrGD+kiK2F+2J36Ype478
ZpGaovjdW2ZWGk2c7KTs+6imQ4iINESu5yZWd3CNh23be6Y7iEhtt7VyB29smENJuBTbsjmz3TCG
tj7tqOfre8bw9YZ4/O4o/i5+zzitBQN7KX7rGkWwiMgexhgqo5VE3BBhNxK/iIWFTnQTqSNcz2Xh
tiV8vPUzDIbcJi2Y3GU8JzZtdVTb9Yxh9YYKPtgrfpvvid8Bit86SxEsIg1eOBYm6AQIxcLYlqWr
uInUQUWhEmZvmMO2wE4AhpzYn1Hthx/Ve9kzhlXrK/hgaRE7947fAS0YcLLit67Tb3kRaZBibmzP
smZBPGOwbRufrT9oInWNMYbPd63gvc0LiHkxmjdqxqTO4+iY2f4Hb9Pz4vE7f+ludpXEl07LzEjh
jAEtGXByJik+/a6oDxTBItJgVE13CMeCRLzonukOFrZOdBOpkyqilby1cS4b/PELX5zSsifj884k
LSXtB23P8wwr15czf0kRu0u/i98zB7TkNMVvvaMIFpF6LxwL71ndIYxt2ZruIFIPfF38Lf8pmEco
FqZJShrndhzDyS26V//AA/A8w4p15Xyw9Lv4zWoWj9/+PbNI8ekfyvWR/gqISL0Uc2NUON+t7uCz
ffF1fUWkTgvHwswp+ICVxWsB6JKZx3mdz6ZZoyO/hLXrGVZ8G4/forJ4/GY3T+XM01rQT/Fb7ymC
RaTeMMYkljWLGIcUyweWpYtZiNQTBeWFzN7wLuXRClLsFMZ2OJ3TWvU54rW7q+J3/tIiiveK31ED
WtLvpEx8it8GQREsInVeyAkRjAX2ne6g8BWpN2JejA8KF7F4xxcAtGl6ApO6jKdlk5wj2o7rGb76
xs8HS4so9jsA5DRP5cyBLenXQ/Hb0CiCRaROclxnz+oOIQwG27I13UGkHtoR2MUbG+awO1SMhcXI
toMZ0WbQEb3fXc+wfE/8llTFb2Yqowa2pG93xW9DpQgWkTrDM15idYeo55Bip2BZFhb6AyZS33jG
49Ptn/Phlk/wjEdOWhaTu5xD24wTD3sbrmv48hs/Hy4toqQ8Hr8t9sTvqT0y8ekqkA2aIlhEar2g
EyTkBAi7Ea3uINIAlIb9zN44h8KKbQAMaHUqYzqMpJEv9bAe77qGL9f6+WBZEaVV8ZvViNEDW9Kn
e3PFrwCKYBGppRzXiZ/k5oY13UGkgTDG8NXu1by76UOinkNGajrndT6brlmdDuvxMdfw5ZoyPvy8
OBG/LbMaMUrxKwegCBaRWsMzXmJ1B8eL4bN9mu4g0kAEnCBv57/Pt6UbADgpuysTOp1FemqTah8b
cw1frCnjw2XFlFV8F7+jB7WkT7fm2IpfOQBFsIgklTGGYCxI2AkScsOJaQ466ivScHxbuoG3N75P
IBaksa8R4/NGcUrLntUufRZzPT7/2s9HnxdRVhEDIDc7Hr+ndFX8yqEpgkUkKSKxCAEnvqwZVny6
g+b5ijQsUTfK3E0f8eXuVQDkNWvHpC7jyGzc/JCPi7key77289GyIvyV8fhtldOI0QNz6d21meJX
Dov+4ojIcVM13SHsBIkZF5/t2/PHSn+wRBqawoptzN4wh9KIH5/lY3T74Qw+sf8hj/46MY9lX5ex
4PPiveK3MaMHtYzH7xFeNEMaNkWwiNSoqukOISdAxI3GpzlY6CpuIg2U67l8tHUxn2xbisFwQnou
k7uMp1V6y4M+xol5LFtdxkefF1MeiMfvCS3i8duri+JXfhhFsIjUiPh0h0rCsQiWBZZlaZ6vSAO3
O1jMGxvmsCO4C4BhrQdwRruhB50K5cQ8lq6OH/mtit8T98TvyYpfOUqKYBE5ZlzPTazu4BoP27Y1
N09EMMawZOeXzNu8ENe4ZDVuzqTO4+jQvN0B7+/EPJasKmPBF8VU7Inf1i3j8duzs+JXjg1FsIgc
FWPMnhPcgoTdSPyIjgW2ZSd7aCJSC5RHKnhz47vklxcC0De3F2d3OIPGKY33u2/U8ViyqpSPvyim
IugC8fgdMyiXkzpnKH7lmFIEi8gPEo6FCToBQrEwtmXpKm4isp9VRWt5p2A+YTdCekoTJnQ6i5Ny
uu53v6r4XfBFMZV74rdN7p747ZRR7VJpIj+E/mKJyGGLuTEqnUrCsSCeMdi2jc/WEV8R2VcoFuad
gvmsLv4GgG5ZnZjYeSwZqU33uV/U8fhsZTx+A6F4/LZtlcboQS05qaPiV2qWIlhEDskYQ2U0Hr4R
45Bi+cCy9LGkiBzQRv8m3tzwLhVOgFQ7lbPzzqBfbu99gjbqeCxeGZ/2sHf8jhnUkh6KXzlOFMEi
ckDhWHjP6g5hbMuOT3fQsmYichCO6zCvcCFLdy4HoF1GayZ1GU9OWlbiPpGox+KVJXz8RQnBcDx+
252QxphBuXTPa6r4leNKESwi+wg6QbZVVFAc8uOzfVrWTESqtT2wk9fXz6E4XIJt2ZzRdijD2gxI
nCAbibosXlHKx19+F7/tT2zCmEEt6dZB8SvJoQgWkQR/xE+lU0mrZs0VvyJSLc94LNq2lAVbF+MZ
j5ZpOUzuOp7WTU8AIBx1WfxVKR8vLyG0J347nNiEMYNb0rW94leSSxEsIhhjKA4XEXWj+LS0mYgc
hpJwGW9smMPWyu0ADDqxH6PbjyDVTiEcdfn0q1IW7hW/ea2bMHqQ4ldqD0WwSAPnei5Fod14xtPa
viJSLWMMX+xayXubP8LxYjRLzeD8LmfTOTOPcMTl4692s2h5CaGIB8Tjd8zgXLq0S1f8Sq2iCBZp
wKJulKJgEbZt6Y+TiFSrMhrgrfz3WF+WD0CvFj04p+NocFOZtyQev+E98duxTTpjBrWks+JXailF
sEgDFXAClIVLNfdXRA7LmpJ1/Dv/fUKxMGm+xpzTaQxdMrqy6PMSPlleQjgaj99Obavit2k1WxRJ
LkWwSANUFi4j4FQqgEWkWpFYhHc3fchXRV8D0Kl5B85uO4YVXzv831frE/HbuV06owe1pHNbxa/U
DYpgkQbEGENReDeO6yiARaRam8q3MHvDu/ij5aRYPka2HkFwa1ueWrCTyF7xO2ZQLp3apid5tCJH
RhEs0kDE3BjF4SIMRifAicghxbwYH275hE+3fw5Aqya5nBgYzLx/e0ScEgC6tm/K6EEt6dhG8St1
kyJYpAGIxCIUh4qxbZ2cIiKHtjO4mzc2zGFXsAgLi9ZeLwo/bcemaAyIx++YQS3JU/xKHacIFqnn
KqOV+CNlmv4gIofkGY/PdnzBB4Wf4BqXxiaD4LensMGfCUC3DvH47dBa8Sv1gyJYpB4rC5cSdIIK
YBE5pLJIOW9ueJdNFVsA8IraU1bQA7wUundoyujBuXQ4sUmSRylybCmCReqhvU+As23N/xWRAzPG
sKJoDXMKPiDqRTFOI6Ibe+P5W9EjrymjB+XSXvEr9ZQiWKSeibkxikK7wUInwInIQQWdELPXv8f6
8g0AuCWtiBb0pke7HEaf3VLxK/WeIlikHonEIpSEi3V1JhE5pBU71vPvgveIWWGM68PZ1JOuzXow
5sJc2p2g+JWG4f+3d6excd3n3fe/Z86ZObNxhjPctFKWbMuyLFu2ZHmJZCle5CRu0qZNirgNkL4I
0Hd3ixYB2jdNUyBN86AoEDxNmhfP/fRBETxtkrt3kERO7DjxpsiKF9mWLVm2VluStZCzcZ/1nP/9
giJFyaQWiuTMcH4fwLAkzlAXKZ45vznn+l9/hWCRRWKkMsJgZRBbV39FZAa54TF+fOA5svYxsMAb
StFbfYBPPbqK5d0Kv9JaFIJFFoFCKU+xWsRW/6+ITGNotMrTbx7msP8yVngM41ukixv4/MZPsKJb
0x6kNSkEizQxYwzZYoaaX9MCOBH5mMGRKi+9keHN/BsElh7HsgzBWoLfW/Up7lyxot7lidSVQrBI
k5q6AE49wCIy1Xj4zbHv2EfYN72NvWwIDNyZ3Mhn127HCej0L6KjQKQJlaol8uWcpj+IyCUGhi+E
33cL0HmS4PrDWAGfuBPnD2/9NDclVta7RJGGoRAs0mSGK8MMVYa0AE5EJo2H3yz73h3Es4uEbj2A
ncwBcFfn7Xxq1cOEHbfOVYo0FoVgkSaiBXAiMtXAcJUX92V549AAng92+hyxNe/hBypEnDC/t/ox
boFqWQwAACAASURBVE/fWu8yRRqSQrBIE/CNT7aYwfM9LYATEQpD4+H3zffGw69lV+m++yjDoVP4
wC3Jm/jsmp20heL1LlWkYSkEizS4mlcjU+zHsiwtgBNpcYWhCi/uy/HGewP4PlgW3Lq+SKF9H8O1
UYIBh52929nUfZdeL0SuQiFYpIEVq0UK5bwWwIm0uPxgZfzK7/uDk+H3rttiRFYd452Bt6EGy2JL
+PzNn6Yjkqp3uSJNQSFYpEENlYcYqg7hWHa9SxGROskPVnhhX5a33hvEN+Ph957bkmy40+fFzHMc
HchjYbF9+QNsW36f3jCLXAeFYJEGlC/mKNVKOAEFYJFWlBsYD7/73x8PvwELNq1Lsv3eNEfG3uEn
p3+Hb3zS4RR/ePOnWRZfUu+SRZqOQrBIA9ECOJHWlh2o8OLrWfYfnhJ+b0/y8L2dBMJj/Oz4zzg9
chaALT138+jKbQTtYJ2rFmlOCsEiDaLqVckVM6AFcCItJ1so88LrOfYfGcRcCL+b1yf55L2dpBNB
9mfe5dmjL1Lxq8SDMX5/zePc3H5TvcsWaWoKwSINoFgtki/lsNX+INJSMoUyL7ye5e0jQ+PhNwCb
b78QfpMhRqtj/PjI0xwZOAHA7elbeeKmR4kGI3WuXKT5KQSL1NlgeZCR6ogCsEgL6c+Ph993jl4M
v/eub2fH5g7SyRAAhwvHeerErxmrFXFtl8/c9DAbOtbpTpHIHFEIFqkTYwyFUp6yV9YWyCItoj9f
5vnXsxw4MoQB7ABsWt/OJ+/tIJUYD79lr8KzJ19if+YgADclVvL7ax4n6SbqWLnI4qMQLFIHvvHJ
jPXjG19XdURaQF9uPPwePHox/G5e386OzZ2kEhcXtp0ePsNPjz/DQHkI27J5ZOU27l9yj14nROaB
QrDIAqt4FXLFrHaAE2kB53MlXngty8FjwxfCr8W9dyTZsbmT9raL4dfzPV468zv2nt2HwdAT7eLz
N3+a7mhn/YoXWeQUgkUW0Fh1jEKpgK3xZyKL2vlsafzK77FhYCL8jvf8Tg2/AP1jWX56/Bn6xjJY
WGxdtoUdyx/UOgGReaYQLLJALi6AUwAWWazOZUs8/1qWd4+Ph1/Hvhh+k/FLw68xhlfPv8Xzp/fg
GY92N8Ef3PxpetuW16N0kfnToDc9FYJF5pkxhlwpS8WraAGcyCJ1NjMefg+duBh+t9zRzvZpwi/A
YHmIn594lg+HTgNwd9cGHl+1A9cOLWjdIvPJGIMFdEW7GKnU6l3OxygEi8yjqQvgAgrAIovOePjN
cOjECDAefu/b0M72TR0kpgm/xhgO5t7n6Q9foOyViToRPrtmJ7elbl7o0kXmled7xIJxkm6SSDDC
CMP1LuljFIJF5okWwIksPuWKR1+uzPlcmQ/OnuPtwwPAePi9/84UD21Kk4hNv41xsVbilx88x6H8
EQDWtq/hs2t2EgtGF6x+kfnm+z4hO0hntAvHbuyY2djViTQpLYATaW6eb8gOVOjLljmfK3E+V6Yv
V6YwVL3kcUHH4v4NKR7a1EFbbOZT6vGBD9l14lmGq6MEA0E+teqT3N11h94gy6JijCEZbicWjNW7
lGuiECwyxwZKA4xqBziRpmCMYWi0xvls+cIV3hJ9uTL9+Qqebz72eDtg0Z0O0dPhcsuqJLeuDNMW
nflUWvWqPHd6D6/37QdgRXwpf3Dzp0mH2+ftaxJZaJ7xiToR2t1UU72xUwgWmSPGGLKlDFWvqgAs
0oBKU1oZxq/wlunLlSiW/Wkfn0oE6elwWdLhsqQjTE+nS2cyhG2Pn+Q7OuLkciMz/n1nR87z0+PP
kCsVCFgBdix/kE8su1frA2TR8I1PMODQEe4kaE/fBtTIFIJF5kDNq5ErZbUATqQBeN54K8PEVd2Z
WhkmRNwASzrDUwKvS3eHSzg0uzezvvHZc+Y1dp95BYOhM5Lm8zd/hqWx7hv5skQaiu8bEm6SeChe
71JmTSFY5AaVa2VyxRyBgBbAiSyk6VoZzmfLZApXbmWYuKq7pMOlp8MlEXPm7NjNFQv87PgznBk9
D8D9S+7h4ZXbCAZ0upXFoWY8onaEVDTd9Oc8HZUiN2CkMsJgeUDtDyLzbLKVYUobw/lcmdIVWhkm
Qu50rQxzzRjDm/3v8OtTu6n6NRKhOL+/5lOsTvbOy98nstB84+NYNt2RbkKLZJ61QrDILA2UCoxW
RxWAReaQ5xkyA+XJwDvRzjAwPEMrQ9ieDLtLO1x6OsP0pEO4s2xlmI3hyghPnfg1xwY/BGBDxzo+
c9PDhJ3wgtUgMp9845MINXfrw3QUgkWukxbAidw4YwyDI7VLJjKMtzKU8aa5uOvYFl2pEEs6w1Ou
8Lq0zWErw2y8lz/KLz74DcVaibDt8sTqR7mj47a61SMylzzjE7ZdUuH0olzvohAsch1qXo1sMQMW
i/IFQWQ+lMoXpzKcz5U5ny3Rl5+5lSE9MZWh06WnYzz0drSHsAP17z80xjBaHSNXKvDMR4d5/cw7
AKxJruJzax4nsciulElrMsYQsAJ0hjtxHbfe5cwbhWCRa1SulcmXck2/EEBkvlzeyjDRuzswXJv2
8dGwfclEhiWdYbrTLm6o/m8wK16VXKlAvlQgVyyQKxUmf1/2KpOPcyybx3q3c2/PRr02yKLgG594
sI2Em6h3KfNOIVjkGoxURhisDGLr6q/IZCvD+eylI8iu1MowvsFE+ELYHW9naIvWt5XBNz4D5aGP
Bd1cscBwdeb5v2HbpSOcYkVqCZvSG+mMpBewapH54fkeruOSDne0zJ1OhWCRqyiU8hSrRW2BLC2p
VPYm2xj6Lowg68uVKVVmaGVITkxluNi7W89WBmMMY7XiZNDNTrm6WygP4hlv2ucFrABpt510JEVH
OEVnODX566gTwbKsq26WIdIMjDFYWKTDHUSCkXqXs6AUgkVmYIwhW8xQ82sEFIBlkat5hmzh4lXd
id7dwZGZWxkmruhO7KhWz1aGql8jXxogV8x/rI2h5JVnfF5bME7HhXDbMSXotruJlrkaJq3L833i
oTiJUKIl23kUgkWmMXUBXCu+MMjidXkrw7kLoTd7xVaGixtLLOkcD7zxqL3gx4YxhsHKELniALlS
fkroHWCwMjTj80KBEB2RdjrC6UuCbke4fdHMOxW5Hr7vE7KDdEa7cOzWjYKt+5WLzKBULZEv53QV
SJpesexdaF8oTV7hnamVweKyVoYLO6p1JEMEFriVoVgrXbiKOx5wc8XxwJsvFajN0L5gYZEKJy+E
2/Rk6E2H24kHY3ozK8JE6wOkwumWa32YjkKwyBTDlWGGKkNaACdNpeYZMoWpG0yMh96rtTJMtDH0
XLjKGwou3M99za9RKA1eXIxWKkyG3bFaccbnxYNR0hdaFybaGNLhFCk3qbndIlfg+R6xYIyk2643
hRcoBItcoAVw0uiMMQwM1yZD7tSpDP4MrQw9UzaWmNhCeKFaGYwxDFdGLgu64/26A+UhDGba5wUD
zseC7kTYDS/imaUi88H3fYK20/KtD9PRd0NanjGGTLEfz/e0AE4axuWtDBOBtzxDK0NHMnhxIsMC
tzKUauXxhWhTgu7E4rSqP/3VaAuLlJu8rEd3/L+2UFxXqkTmgDGGZLidWDBW71IakkKwtLSaVyNT
7MeyLJ10pS4mWhkmRo9NbCE8UytDLDJlg4kLWwh3p+e/lcHzPQrlwYthd8pc3dHq2IzPizqRaYNu
KpzECegUJDIfPOMTdSK0uymd265Ar0DSsorVIoVyXgvgZEGMtzJUx6/oZi9uIZwduHIrw9Q2hp7O
8Q0m5rPGkerotDulFUqDM7YvOJZNOpwiHW6nI5K+GHYjKSJOeN7qFZFL+cYnGHDoCHcStIP1Lqfh
KQRLSxoqDzFUHcKxtJBG5l6xNLHBxKU7qs3YytAemhJ2x/9Lz2MrQ8WrXAi3Fycv5Ep5csUBKn5l
xuclQ4mP9eh2RFIkQ2262iRSZ75vSLhJ4qF4vUtpGgrB0nLyxRylWglHK8nlBtU8n0y+cnGDiex4
/+7Q6MytDBNtDBOBd75aGSa2BJ66GO2atwSOpOkIXxwx1hEZ/39Q7QsiDadmPKJ2hFQ0rTej10mv
aNIyfOOTLWa0AE6u29RWhvPZi1sIZwcq07YyBJ3LN5gY792Nz3Erw8SWwJf36OaLBfLlAXwz/dbG
tmVPmal7sXUhPWVLYBFpbL7xcSyb7ki3Nn2Zpau+Ivu+zze+8Q0OHz5MKBTim9/8JqtWrfrY4/7u
7/6OZDLJ1772tXkpVORGVL0q2WJGC+DkqsZK3ngbQ7Z8yQYT5eqVWxmmBt50IjinrQxVr0q+PHBJ
0J24unulLYETofjFUWNTgq62BBZpbr7xaQsmaHPb6l1KU7tqCP7Nb35DpVLhRz/6Efv37+fb3/42
3//+9y95zA9/+EOOHDnCli1b5q1QkdkqVovkSzkN0pdL1Dyf/nzl4kSGC6F3plaGeNSe3FhiIvDO
ZSuDb3yGKsPTBt3ByvCMzxvfEvjSoNtxYZGarg6JLC6e8QnbLqlwWm9k58BVQ/Abb7zBQw89BMDd
d9/NwYMHL/n4m2++ydtvv82XvvQlTpw4MT9ViszSUHmI4eqwAnAL841hYKg6uUDt/IXe3dxABX+a
YQdBx6InPTFr92LonatWhvEtgfPTzNQdwJthS+CAFaDdTX4s6HaEU8SCUd3dEFnkjDEErACd4U5c
bRgzZ676qj4yMkI8fnGloW3b1Go1HMehv7+f733ve3z3u9/l6aefvqa/MJWK4jj1CyRdXbp10AqM
MeSLecIuRKxEvctpSh0dzbfCeHSsxpn+Mc70Ffmor8iZ/jHO9henn8pgQU9HmOXdEZb1RFjeHWFF
T5TOlHvDrQw1r0ZmLE//aI7MaJ7+kRyZsRz9IzlGqzNvCZxw43TFOuiOddAdS9Md66Ar1kFHtF1v
5BpUMx4n0lw83yMZTpJwm/tc1oj566ohOB6PMzo6Ovl73/dxnPGnPfPMMxQKBf78z/+cTCZDqVRi
zZo1/NEf/dGMn69QmHmo+nzr6mojk5n5tqIsDr7xyYz14xtfV8hmqaMjTi438wSBeqvWfDKFCuez
l44gu6ZWhs6LUxmCzuW3E2sUCtN/jssZYxiqjJArXRgxVsyPjxwr5RksD19hS+DglP7c8QkM45MY
UtNf4SnDQHnm4Cz10+jHiTQ3z/dwHZd0uINyxSJD8+aXeuavK4Xvq4bgTZs28cILL/DEE0+wf/9+
1q5dO/mxr3zlK3zlK18B4Cc/+QknTpy4YgAWmW8Vr0KumNUCuEViopVhctvgCyPIrtjKMNmze3EL
4Xhk9q0MpVr5Yz2617wlcORiwB2fqZumLRjTz6aIzMgYg4VFOtxBJBipdzmL2lXPDDt37uTll1/m
ySefxBjDt771LXbt2sXY2Bhf+tKXFqJGkWsyVh2jUCpga/xZUxor1i6G3Sn/r0w3lcGCzvYQSzqn
bjARJpUMEphFwJzYEni6mbqjtatsCTzN5hFpV+0LInL9PN8nHoqTCCX0ZnkBWMaY6e/ZzZN6tiOo
HWLxGiwPMlIdwdZq2Tkxn7d5p7YyTA28wzO0MrRF7Uuu6s7cynBlU7cEvrgYbfzXhfJVtgS+LOh2
Xhg1pi2BW5vaIWSu+L5PyA7S7qZx7MW3hUPTtkOINDJjDLlSlopXUQBuMJOtDNnxEWQTgfdaWhmm
7qgWu85WhrJXmQy3l7cxVPzqjM9rdxMfm6nbEU6R0JbAIjJPxlsfIBVOq/WhDhSCpWlNXQCneYn1
NXqhlaHvwo5qfbkSffkylerH0+7UVoaJNoaeTpdU4tpbGSa3BL5s1Fi+VGC4Ojrj8y5uCXxp0E1p
S2ARWWCe7xELxki67XqjXSd61ZempAVw9VGt+fTnL+3ZvWIrQ8y5uJPalA0mrqWVYXJL4I/N1B2g
cE1bAl9YlDYl9EZ1pUVE6sz3fYK2Q2e0a1G2PjQTffel6WgB3PzzfUNusHJhF7XShQ0myuQGK0y3
iiAUvLDBREf4ksVq19LKUPWqk6PFLh01VqB8lS2Bx8NtmnTk4qixpLYEFpEGZYwhGW4nFozVuxRB
IViazOQCOAXgOWeM4d3jw+zZn6cvW6Y8w1SGrlTowrzd8IV2Bpf2q7Qy+MZnsDx8yWK0iau7Q1fY
Eti1QxcXpF22OC1kB+fk6xYRmW+e8Yk6EdrdlO5eNhCFYGkKxhiypQxVr6oFcPOgP1/mqd19HDt9
sZ82EXMuaWNY0hmmKxW6YivDWLU4GXSzl8zUvfKWwKmJLYEjl44aiznaElhEmpdvfIIBh45wJ0G9
cW84CsHS8DzfI1vMaAHcPChVPJ5/Ncved/L4PkTcAI890MUn719GuVia9jk1vzbetnD5TN1SgWJt
+ucAxIOxaYNuyk3q31VEFh3fNyTcJPGQttZuVArB0tDKtTK5Yo5AQAvg5pJvDPvfH+SZvf2MjHlY
wH0b2tn5QBexiEM0YtM/MPTxmbqlAgPloRk/79QtgS+fqevaoYX7AkVE6qRmPKJ2hFQ0rfNWg1MI
loY1Wh1loFTQzltz7Ex/kV0v9XHqfBGA3qURPre9h2VdYd7Jvsdrx94iV8pfdUvgiz26FxalRVLE
tSWwiLQo3/g4lk13pJuQ3vQ3BYVgaUgDpQKj1VEF4Dk0Wqzx7O8y7Ht3AAPEozaf2drD3bclOD1y
lv/33Zc4N9o3+fiYE50ydeHiXN2Um9S/i4jIFL7xaQsmaHNn3p1MGo9CsDSUSxbAKWjNCc83vHaw
wG9eyVAs+wQCsHVjmkfu66ToD/O/j/2C9/JHAWgLxnh45VYeuHkjxaHprwSLiMg4z/iEbZdUOK21
DU1IIVgaRs2rkS1mwEIvJnPkgzNj7HrpPOdz4/N2b1kZ47Pbe0gkYM/Zvbx2/i084+EEHB5cuplP
LN1CyA4SDYYpMlLn6kVEGpMxhoAVoDPcieu49S5HZkkhWBrC1AVwcuMGR6o8/XI/7xwZX8TW3hbk
9x7qZt3qGPszB3nx7d8xVhvvCb6z83YeWbGVhG7jiYhcled7tIUSJNxEvUuRG6QQLHU3UhlhsDKo
DTDmQM3zefmtPC/sy1KpGhzbYsfmDrZv7uDUyGn+n4M/JVPMAbAyvozHV+1gWXxJnasWEWl8nu/h
Oi7d0R7drVwkFIKlrgZKBcaqYwrAc+DwhyM89ds+cgMVANavaeP3HuqmFhzmv4//nGMDHwDQ7iZ4
dOVD3J6+VZMcRESuwhiDhUU63EEkGKl3OTKHFIKlLowxZIsZan6NgALwDckNVvjFb/t4/4PxHt6u
VIjPbu9h+VKb3WdeZl/f2xgMoUCIbcvv4/4l9+AEdOiLiFyN5/vEQ3ESoYQuGixCOhPKgpu6AE4v
KrNXqfq8uC/Lnrfy1DyDGwzwyH2d3Hdnkrey7/DTt1+h5JWxsNjUfSc7VjxIPBird9kiIg3P931C
dpDOaBeOrai0WOlfVhZUqVoiX86pn+oGGGM4eGyYX+7pY3BkfIzZPeuSPP5gJ+crp/mfh35OvjQA
wJpEL4+t2kFPtLOeJYuINAVjDACpcFqtDy1AIVgWzOQCOAXgWevLldm1+zwnPhoDYFmXy+d2LMFN
jPDzUz/nw6HTAHSEU+zs3c4t7at1tV1E5Bp4vkcsGCPptut1s0UoBMuCKJTyFKtFLYCbpWLZ47lX
M7zyTgHfQDRss/PBLtbdEmT32ZfZf+ogABEnzPblD7C5+y5tNiIicg183ydoO2p9aEH615Z5pQVw
N8Y3hjffG+RXe/sZLXpYFtx/Z4pP3pfincJ+vn/gdap+lYAVYEvPRh5a/gARJ1zvskVEmoIxhmS4
nZjWS7QkhWCZNzWvRqbYj2VZurU0C6fPF9m1+zwf9ZUAWLU0wme391AInOQ/Dj/FYGUYgLXta3is
dzsdkVQ9yxURaRqe8Yk6EdrdlM5PLUwhWOZFsVqkUM5rAdwsjIzVePZ3/ew7NAhAIubw6a3ddCwb
5VenfsaZkXMAdEc7ebx3B6uTvfUsV0SkafjGx7FsOiLdBO1gvcuROlMIljk3VB5iqDqEY6kn9Xp4
vuHVAwV+80qGUsXHDsDWuzvYvDHEb8+/zK5DhwGIBaM8vOITbOy6Q28yRESuke8bEm6SeChe71Kk
QSgEy5zKF3OUaiUcLcq6Lic+GmXX7j76cmUA1vbGeHxbmsNjb/M/D71BzXjYls0DSzezddkWXDtU
54pFRJpDzXhE7QipaFqtD3IJhWCZE77xyRYzeL6nBXDXYWC4ytN7+jhwbLy/N5UI8nvbuii3neZH
J59mpDo+Cu2Ojtt4ZOU22t1EPcsVEWkaE60P3ZFuQrpwINNQCJYbVvWqZIsZLYC7DtWaz5638ry4
L0u1Zgg6Fjvu7aT3ljGe/2gXfdkMAMtjS3h81Q5WtC2rc8UiIs3DNz5twQRtblu9S5EGphAsN6RY
LZIv5TST9jq8/8EwT/22j/xgFYANt7TxiS0ur+X38vKR4wAkQm08snIbGzpu0xsLEZFr5BmfsO2S
Cqe1ZkKuSiFYZm2oPMRwdVgB+BplByr8Yvd5Dp8cBaA7HeJT21Kc5gD//4n9+MYnGAiyddkWHliy
SSuXRUSukTGGgBWgM9yJ67j1LkeahEKwXDdjDIVSnrJX1hbI16Bc8XlxX5Y9b+XxfIMbCvDIfR2E
lpzmF2efpVgbnwN8d9cdfHLFJ2jTymURkWvm+R5toQQJrZmQ66QQLNfFNz6ZsX584+s2/VUYY3jn
6BBP7+lnaLQGwKbbE6zdUGRP3y/IncoDsKptBTtX7WBprLue5YqINBXP93Adl+5oj1ofZFYUguWa
VbwKuWJWC+CuwblsiV0v9fHh2fHpDsu7wzz0YIgDY6/ys5MnAUi5SR7r3c5tqZv1/RQRuUbGGCws
0uEOIsFIvcuRJqYQLNdkrDpGoVTA1vizKyqWPH79aoZXDxQwBqJhm08+EGcwfoifnzuIweDaLtuX
38+WnrvVTy0ich083yceipMIJXTxQG6YQrBc1WB5kJHqiALwFfi+4Y1DA/zqdxnGSh4BC+67K0H7
6jPs7f815bEKFhb39mxkx/IHierqhYjINfN9n5AdpDPahWMrusjc0E+SzMgYQ66UpeJVtADuCk6d
L7LrpfOc6R9f4HbT8ggbNo3yxsAzvHNuCIBbkjfxWO92uqId9SxVRKSpGGMASIXTan2QOacQLNOa
ugBOCw6mNzxa41d7+3nz/UEAknGHB++3OcFrPNd3BoDOSJrHe3dwc/tNdaxURKT5eL5HLBgj6bar
9UHmhUKwfIwWwF2Z5xl+906e517LUq742AGL+zeFqXa+z0v59wCIOhF2rHiQTd136k2EiMh18H2f
oO2o9UHmnX665BJaAHdlx06P8tTu8/TnKwDcelOYpbefYX/hLar5GrZlc9+Su9m27D7CTrjO1YqI
NBdjDMlwO7FgrN6lSAtQCJZJWgA3s8JQlV/u6ePd48MApJMOd947zHulPXyUGwFgXeoWHu19iHS4
vZ6liog0Hc/4RJ0I7W5KdyBlwSgEixbAXUG15rP7zRy738hRrRmCjsWme336w6/x2nA/AEui3Ty+
agerEivqXK2ISHPxjY9j2XREurVVvCw4heAW5/ke2WJGC+AuY4zhvQ9G+MVv+ygMVQFYd1uA4Moj
vDN0DMagLRjj4ZVbuatzva5ciIhcJ983JNwkcW0VL3WiENzCyrUyuWKOQEAL4KbKFMo8tbuPo6dG
AejpDLDirjMcHj2IN+ThBBweXLqZTyzdQkhXLkRErkvNeETtCKloWuceqSuF4BY1Wh1loFTQjmVT
lCsez7+eZe/+PJ4PYRfWbR7gNG9zaKQIwJ0d63hk5TYSbludqxURaS4TrQ/dkW5Cdqje5YgoBLei
gVKB0eqoAvAFxhj2Hx7imb39DI/WsIB1d5YYSR3gcCkHwMr4Mnau2sHy+JL6Fisi0oR84xMPtpFw
E/UuRWSSQnALMcaQLWWoelUF4AvOZkrseuk8J8+NX+ldurxK/JajnCyeghK0uwkeWbmN9em1um0n
InKdPOMTtl1S4bTWnUjDUQhuETWvRq6UxWD0QgSMFWv8+tUMrx0cwBiIxj16N37Eqer7DBQNoUCI
bcvv4/4l9+AEdJiIiFwPYwwWFp3hTlzHrXc5ItPS2b0FTF0A1+p83/D6uwM8+0qGYskjEPC5+Z4s
WfddTlbLWFhs6r6THSseJK5h7SIi183zPdpCCbU+SMNTCF7kRiojDJYH1P4AnDw7xq7d5zmbKQOG
ZbcM4ve8x5nqIHiwOtHLzlU76Il21rtUEZGm4/keruPSHe3RHUdpCgrBi9hAqcBYdazlA/DQaJVn
Xu5n/+EhANo6xmi/7SjZ2jmoQkc4xWO927m1fbX6fkVErtNE60M63EEkGKl3OSLXTCF4ETLGkC1m
qPk1Ai28BXLNM+x9O8/zr2WpVH0ct8Kyu06SsY6TrUHECbN9+QNs7r6r5d8oiIjMhuf7xENxEqGE
LiJI01EIXmRqXo1sMQMWLf2CdPTUCLte6iM7UAHLY+kdZxltO0LGrxKwAmzp2chDyx8g4oTrXaqI
SNPxfZ+QHaQz2oVjK0pIc9JP7iJSqpbIl3Mt3YuVH6zwyz19HDoxAhjaV2axlx9mwB8BH9a2r+Gx
3u10RFL1LlVEpOmMtz5AKpxW64M0PYXgRWKkMsJgZRC7RQNwpeqz+40cu9/MUfMMoeQQqduOMkQG
fOiOdvJ47w5WJ3vrXaqISFPyfI9YMEbSbW/pO42yeCgELwKFUp5itYjdgv2/xhjePT7ML/f0MTBc
wwoV6b7zA4ZDpxgCYsEoD6/4BBu77mjpK+QiIrPl+z5B21Hrgyw6+mluYq2+AK4/X+ap3X0cOz0K
gRrtt56iljrOMB62ZfPA0k1sXXYfrvaoFxGZFWMMyXA7Mc1Nl0VIIbhJ1bwamWI/lmW13G2pug4D
LAAAGB1JREFUUsXj+Vez7H0nj+8bIkvPElp5jDLjWx+vT6/l0d5ttLvJOlcqItKcPOMTdSK0u6mW
O8dI61AIbkKtugDON4b97w/yzN5+RsY87LYcqduOUgoMUAGWxZbw+KodrGxbVu9SRUSakm98HMum
I9JN0A7WuxyReaUQ3GSGykMMVYdwrNaaa3umv8iul/o4db6I5Y6S3HCMSvQcJSARauORldvY0HGb
rliIiMyS7xsSbpJ4KF7vUkQWhEJwE5lYAOe00MYOo8Uaz/4uw753BzB2leiaE9B5kgo+wUCQrcu2
8MCSTbpiISIySzXjEbUjtEdTLXeHUVqbQnAT8I1PtpjB872WWQDn+YbXDhb49SsZSuUaTs9pIr0n
qFllADZ23cHDKz5Bm65YiIjMim98bMumO9JNSAuIpQUpBDe4qlclW8y01AK4D86Mseul85zPlQkk
M7StP0ItOEwNWNW2gp2rtrM01lPvMkVEmpZvfOLBNhJuot6liNSNQnADK1aLFMr5lrk9NThS5emX
+3nnyBBWZJjY+iP48Qw1IOUmeax3O7elbm6ZNwMiInPNMz5h2yUVTrfMuUVkJgrBDaqVFsDVPJ+X
38rzwr4sFVMmtPoodtdH+Bhc22X78vvZ0nM3dgv1QouIzKXx7Y4tOsOduI5b73JEGoJCcIMxxlAo
5SnVSi2xAO7whyM89ds+coMlnJ6TRFccxwRqWFhs7tnIjuUPEtX+9CIis+b5Hm2hhFofRC6jENxA
WmkBXG6wwi9+28f7HwwTSPUR3XgEExrDALckb+Kx3u10RTvqXaaISNPyfA/XcemO9qj1QWQaCsEN
ouJVyBWzi34BXKXq8+K+LHveyuO5A4RvP4zVlscAnZE0j/fu4Ob2m+pdpohI05pofUiHO4joTprI
jBSCG8BYdYxCqYC9iK/+GmM4cGyYp/f0MVgeIdh7hHDnWbAg6kTYseJBNnXfqasVIiI3wPN94qE4
iVBiUV9QEZkLCsF1NlgeZKQ6sqgD8Plciad293Hi7DDOkg+ILPsAAh4BK8D9S+5h27L7CDvhepcp
ItK0fN8nZAfpjHbh2Dq1i1wLHSl1YowhV8pS8SrYi/TqZ7Hs8dyrGV55J4+VPkvkrqMQKgGwLnUL
j/Y+RDrcXucqRUSa13jrA6TCabU+iFwnheA68I1PZqwf3/iL8va/bwxvvjfIr/b2U7SzBG9/n0B8
EIAl0W4eX7WDVYkVda5SRKS5eb5HLBgj6bar9UFkFhSCF9hiXwB3+nyRXbvPc6aQx1l5BLfjPABt
wRgPr9zKXZ3rF+XXLSKyUHzfJ2g7an0QuUE6ehbQYl4ANzJW41e/6+eN93M4y44TvuskBHycgMOD
SzfziaVbCNnBepcpItLUjDEkw+3EgrF6lyLS9BSCF8hiXQDn+YZX3inw3Kv9VJOnCG88ihWsAHBn
xzoeXrmNpNtW5ypFRJqbZ3yiToR2N6W7aSJzRCF4ni3mBXAnPhpl1+4+MtUzBNe+Tyg6AsDK+DJ2
rtrB8viSOlcoItLcfOPjWDYdkW6CupsmMqcUgueR53tkiv0YYxbVAriB4SpP7+nj4EfnCK48jJvK
AJAMJXi0dxvr02t1pUJE5Ab5viHhJomH4vUuRWRRUgieJxWvQnYsSyCweBbAVWs+e97K8+JbZ6Hn
KO6dp7EsQygQYtvyLdy/ZBNOQD9SIiI3omY8onaE9mhqUV1AEWk0SizzYLQ6ykCpgB2w613KnHn/
g2F27T7HUOQ4wTuOYzlVLCzu6b6THSseJK5FGiIiN8Q3PrZl0xXuwnXcepcjsugpBM+xgVKB0ero
ognA2YEKT+0+x7HBDwne9D6h8BgAqxO97Fy1nZ5oV50rFBFpfr7xiQfbSLiJepci0jIUgueIMYZs
KUPVqy6KAFyu+Ly4L8vL739IYMV7uD15ANLhFDt7t3Nr++pF0+YhIlIvnvEJ2y6pcFqtDyILTCF4
DtS8GrlSFkPzL4AzxvDO0SF++copSqn3cdZ/hGWBa7t8csWDbO6+a1GEfBGRehrf7tiiM9yp1geR
OlEIvkHlWplcMUcg0PxXRc9lS/z8pbOcMYdwbjmBY3tYBNiyZCPblz9AxAnXu0QRkabn+R5toYRa
H0TqTCH4BoxURhgsDzT9ldFiyePZV/vZd+Y9nBWHCbolAG5tX8NjvQ/RGUnXuUIRkebn+R6u49Id
7Wn6u4Yii4FC8CwNlAqMVceaOgD7vuGNQwM8s/8I3pJDhG4eAKAr3MmnbtrB6mRvnSuUheb7PpZl
YVs2ASyMMQAYDAYw5sKvL/w5lrnk+Rbjd0Qsy8Ji8YwHFLkRE60P6XAHkWCk3uWIyAUKwdfJGEO2
mKHm1wg08RbIp84X+emeE+QiB3BuPYcNROwIj/ZuZWPXHbpK0UI838O2ArhOhKgbxXVcuuJtBIrR
qz7XGDMZiif/bww+Pr4Z/2/iY8DHHsuUP2eazzX5HBTApTl5vk88FCcRSuhnUqTBKARfh5pXI1vM
gEXTvpgNj9Z4eu8ZDg6/jbP8A5yATwCbB5ZuYtvy+3DtUL1LlAVQ82uEAkFCdphYODbr7VgnAid1
OBwuD+AwfiX7RgP4xx7LxQA+8fdeHsCByeCtAC4w/rMYsoN0RrtwbJ1qRRqRjsxrVKqWKJTzTXty
8zzD3rdzPH98P9aSIwQTZQDWpdayc9U22t1knSuU+WSMwTMebiBEyAkTD8abupUHZgjgC3QDY7qr
1ZcH8EseN80V7slr3NcQwKf++dQAbmFhMArgDWS89QFS4bRaH0QanELwNRipjDBYGcRu0haBY6dG
+enrBxlNHSTQOwRAd7ibJ9Y8zMq2ZXWuTuaLMQb/wgzSkBMmFoqpzWWONEIAn/rraw3gE39+eQC/
5PFTAvj4n08TwI2BC2FbAfwiz/eIBWMk3faW/16INIOrhmDf9/nGN77B4cOHCYVCfPOb32TVqlWT
H3/qqaf4j//4D2zbZu3atXzjG99o6l7ZyxVKeYrVInYTfk2FoSo/23ucD82b2Cv6CACRQIxPrX6I
DR3r9CK9CPnGB2PhOiHCToSoE9W/8yIzGcDhYghvkAAOXNKGMi8BfIqpARzq16bm+z5B21Hrg0iT
uerR+pvf/IZKpcKPfvQj9u/fz7e//W2+//3vA1AqlfjOd77Drl27iEQi/PVf/zUvvPACjz766LwX
Pt+aeQFctebz/BvneKXvNayuk9gBQwCbrcu2sHXZvbPu/5TGNDHRwbVdosEYYc1zlnlS7wAOl7aL
TATwyd9PE8CnPmdqAL/8c13aP/7xAD7RdjLBwpr8+5LhdmLB2MJ8I0Rkzlw1BL/xxhs89NBDANx9
990cPHhw8mOhUIgf/vCHRCLjfU+1Wg3Xbf6db5p1AZwxhnePD7Hr3deodhwm0FMFA7e3r+NTqx+i
LRSvd4kyR2p+jaDlEHLCkxMdRBYza0r7RaME8I5ojFBJUx9EmtVVQ/DIyAjx+MXwZNs2tVoNx3EI
BAJ0dnYC8IMf/ICxsTG2bt16xc+XSkVxnPotyOnqarvix0vVEtmxQTpizRUYz2eL/H+/eZVzwTcJ
LBnFApZFl/Mn93yGlcml9S5P5kDNrxGyQ4SdMLHg7Cc6XKurHSsiAuFu3XkRuRaNeE65agiOx+OM
jo5O/t73fRzHueT3//zP/8wHH3zAv/7rv171HXGhMHYD5d6Yrq42MpnhGT8+XBlmsDKIYzXPqvly
xeMXrx/jwOhrBJLZ8b5fq40nbt7B7elbsGoWudxIvcuUWZiY6BC2XUJ2mHgoScAKUAUGKAGlefu7
r3asiIiOE5FrVc9j5Urh+6oheNOmTbzwwgs88cQT7N+/n7Vr117y8a9//euEQiH+7d/+rel6Z6ea
WADnNMnYKGMMr77Xx29O7sVPnSKQNARMkG3L7mPrik04AS3OaEbjEx0MYTukiQ4iIiLz6KpJaefO
nbz88ss8+eSTGGP41re+xa5duxgbG2PDhg3893//N/feey9/9md/BsBXvvIVdu7cOe+FzxXf+GSL
GTzfa5oQf7p/lP/11suMxA9jpWtYxmJd2x08ces2YsGr7/IljWXqRIeIEyXiRNRjKCIiMs8sc3Fy
+4Ko562jyy/HV70q2WKmaQLH6FiV/973Fh+aNwmEiwB0Ocv5w9sfoSfaWefq5Hr4vk/AsgjZYaLB
aMNNdNBtXpGr03Eicm2ath1isSpWixTK+aa41ez7hucOHOOV7F6I5wkAYT/JZ2/5JLd3ral3eXKN
pk50iLkxQo62qBYREamXlgzBQ+UhhqpDTbEA7r3TGX5++CXK8dNYcQh4IT6x5AF2rL6nKQJ8q6v5
NUKBIK4TIebENEhfRESkQbTUGdkYQ76Yo1QrNfwCuNzwGD96cw/Z4PtYbR6WsbglvIHPr9+q/egb
2MRWxa4dujDRIa43KyIiIg2opUJw32gfZa/c0AvgqjWfn+5/g/eK+7DCJSwgbVbyxQ2P0BNP17s8
mcb4rlEQtkO4wQixYKxp+sxFRERaVUuF4Eqt0tDh5OWjx3jx7G/xwwNYIQhV23ni5k9y59LV9S5N
LuP5HhYBwo5L2IloooOIiEiTaakQ3Kg+zGT534deYCz8EYTBqrlsST/A4+vuUbBqIJ7vYVuB8YkO
buNNdBAREZFrpxBcR8OlIj/e/1vOmPewwj7GD7DK2cAfb9lGNOTWuzwBasYjiK2JDiIiIouMQnAd
eL7HLw+9wf7B18GpYFmQqKziixs+yfKU+n7rzfM9QoHgePDVRAcREZFFSWf3BfbGR8d49sOXqAWH
wAG7lObx3h3cu/qmepfWsjTRQUREpPUoBC+Qs4NZ/teh5xgKnIUgUIlwZ/w+Pnfv3di2AtdCmzrR
IRyKEnWi6r8WERFpIQrB82y0WuQnB1/iw/L7EDAYz2aJdwdf2rSVZEwLqxaS53sErACurYkOIiIi
rU4heJ54vsdzx/fxWuZ1jF3FAJGRm/jD9du5ZWlHvctrGZroICIiItNRCJ5jxhgO9B/j6RMvUQkM
gw3WSCfbl2xj+/036crjApiY6OA6EWLhGEE7WO+SREREpMEoBM+hcyP9/OS958n75yAAfinKWmcL
n992FxFX3+r5MrGwbWKiQzwYx27wbbFFRESkvpTM5sBIZZRdR3ZzbOR9sMDUgrSPruePNz/A0o5o
vctblC6Z6OBoooOIiIhcH4XgG1Dza7x48nVe6duHsWoYY+EUbuIzt2zl7ls61fowx3zjg7FwnRBh
J6KJDiIiIjJrCsGzYIzhQPYwz5zYTZlRsMAf6GZz+/18aucago6uSM6VqRMdIs74wjYFXxEREblR
CsHX6czIeX525Hly1T4A/LE2llfv4QsP3kkqoS1158LERAfXiRB1o7iOtpAWERGRuaUQfI0Gy8M8
fWI3R4eOAGCqIcL52/mDjfdy26pEnatrfjW/Nr6wzQ5rooOIiIjMO4Xgq6h4FXZ/9DqvnHsDY3kY
PwD9q9m+cgsPPbgE29at+dkwxuAZDzcQ0kQHERERWXAKwTMwxrA/8y6//vBlymYMLKjllrA2dC+/
/9hq2mL61l2viYkOYdsl5ISJhWKa6CAiIiJ1oSQ3jZNDH/GLEy+QK2cB8EeSJIfu4g8fWM+qpRp5
dj000UFEREQakULwFPnSAM9+uJujg8cB8MthAn3r+PS6jWx5OEUgoPB2LXzfx7KsyYkOkWCk3iWJ
iIiIXEIhGCjVSuw+8yqvnd+Pwcd4NrVzq9nUcQ+Pf24p0bB6Va/G8z0cyybkhDXRQURERBpeS4dg
3/i82X+A50/tpeyXMAa87HKWVO/k89tXs7QrXO8SG5omOoiIiEizatkQfGzgQ3714Uvky3kAvKEU
bmYDv7/5VjauTahvdRpTJzq4ToRYMKaJDiIiItKUWi4EZ8ZyPHvqJU4MngTAL0XwPrqNB29azyNf
7MQNKdRNNT7RwRC2Q5roICIiIotGy4Tg4coIPz/6K353+i0MBlNzqJ29mdXu7XzuiWV0ptTDOkET
HURERGSxa4kQnBnL8X+9/n9T9IoYY+H19xIfWsdnt/ay7qa4Ah5TJzqEiQajhB31Q4uIiMji1RIh
+FxhiLFKFX+4E3N2HTs2rOahJ9IEnda+rV/zawQtZ7zNwY0RckL1LklERERkQbRECO50uwm9/xlu
WRXjsS920t7WulMMNNFBREREpEVC8LLOGN/5Hw9RDBYYHCjVu5wFNTHRIWy7hOww8VBcC9tERESk
5bVECG41l090iIfU9ywiIiIylULwIjF1okPEiRJxIgq+IiIiIjNQCG5inu9hWwFCmuggIiIicl0U
gpuMJjqIiIiI3DiF4CZQ82u4gQs7tjkxHFv/bCIiIiI3QmmqAY0vbPNx7ZAmOoiIiIjMA4XgBuEb
H2MgbIcIh6LaqlhERERkHikE15HnewSsAK7tEnYimuggIiIiskAUghfYJRMdXE10EBEREakHheAF
UDMeQWxcJ6KtikVEREQagELwPPF8j1AgqIkOIiIiIg1IyWyOaKKDiIiISPNQCL4BkxMdnPGFbZro
ICIiItIcFIKvkyY6iIiIiDQ/heBroIkOIiIiIouLQvAMasYjZDmE7LAmOoiIiIgsMgrBF0wsbJuY
6BAPxrEDdr3LEhEREZF50NIheOpEB9eJEAvFNNFBREREpAW0XAj2jQ/GwnVCmuggIiIi0qJaKgRH
g1Fww4SdsIKviIiISAtrqXv/nbFOIkGNNBMRERFpdS0VgkVEREREQCFYRERERFqQQrCIiIiItByF
YBERERFpOQrBIiIiItJyFIJFREREpOUoBIuIiIhIy1EIFhEREZGWoxAsIiIiIi1HIVhEREREWo5C
sIiIiIi0HIVgEREREWk5CsEiIiIi0nIUgkVERESk5SgEi4iIiEjLUQgWERERkZajECwiIiIiLUch
WERERERajmWMMfUuQkRERERkIelKsIiIiIi0HIVgEREREWk5CsEiIiIi0nIUgkVERESk5SgEi4iI
iEjLUQgWERERkZbT9CG4UCjw9a9/HYDnn3+eL3zhC3zpS1/ixz/+8RWft3//fv74j/+YJ598ku9+
97sAlEol/uZv/gZNjZPFaLbHCoDnefzFX/wFu3fvBnSsyOI29VgBKBaLPPnkkxw/fvyKz9N5RVrN
bI8VaIzzStOH4O985zv86Z/+KdVqlX/6p3/i3//93/nBD37Aj370I7LZ7IzP+/u//3v+5V/+hf/6
r//i7bff5tChQ4TDYe655x5++tOfLuBXILIwZnusnDp1ii9/+cscOHBg8s90rMhiNnGsABw4cIAv
f/nLnD59+qrP03lFWs1sj5VGOa80dQgeGRnhwIEDrFu3juPHj9Pb20symSQUCrF582Zef/31GZ9X
qVTo7e3Fsiy2bdvG3r17AfjMZz7Df/7nfy7klyEy72Z7rACMjY3xj//4j9x///2X/LmOFVmMph4r
AJVKhe9973usWbPmqs/TeUVayWyPFWic84qzYH/TPNi/fz+rV68Gxv8x2traJj8Wi8UYGRmZ9nkj
IyPE4/FLHjvxziWZTFIoFBgeHr7k84k0s9keK8DkC9zldKzIYjT1WAHYvHnzNT1P5xVpNbM9VqBx
zitNfSW4UCjQ2dkJQDweZ3R0dPJjo6OjM34Dp3tsIpGY/H1nZycDAwPzVLXIwpvtsXI1OlZksZl6
rFwPnVek1cz2WLmahTxWmjoEd3R0MDQ0BMDNN9/MyZMnGRgYoFKpsG/fPu65555pnxePxwkGg5w6
dQpjDHv27OHee++d/PjQ0BDpdHpBvgaRhTDbY+VqdKzIYjP1WLkeOq9Iq5ntsXI1C3msNHUI3rhx
I4cPHwYgGAzyt3/7t3z1q1/lySef5Atf+AI9PT1kMhn+6q/+6mPP/Yd/+Ae+9rWv8cUvfpH169ez
ceNGYPybn0gkiMViC/q1iMynGzlWZqJjRRajqcfKTHReEbmxY2UmC32sNHVPcCwWY8OGDRw6dIj1
69fzyCOP8Mgjj1zymFQqRU9Pz8eee/fdd087GmrXrl38yZ/8ybzVLFIPN3KsTPj2t799ye91rMhi
dPmxMuEHP/jB5K91XhG5sWNlQr3PK019JRjgL//yL6+4ktAYw1e/+tVr+lylUok333yTz33uc3NV
nkjD0LEicm10rIhcm2Y/ViyjCd4iIiIi0mKa/kqwiIiIiMj1UggWERERkZajECwiIiIiLUchWERE
RERajkKwiIiIiLQchWARERERaTn/B+ZqIwGwgfrmAAAAAElFTkSuQmCC
"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Returning-to-our-original-questions"&gt;Returning to our original questions&lt;a class="anchor-link" href="#Returning-to-our-original-questions"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To answer our original questions - $P(\text{Accident = 1}\ |\ \text{Traffic = 1})$ and $P(\text{Accident = 1}\ |\ \text{Traffic = 1}, \text{President = 1})$ - we invoke basic axioms of Bayes' Theorem, conditional probability and the factorization of probabilistic graphical models. Here are the key pieces that we'll need:
$$
P(P, A, T) = P(P)P(A)P(T\ |\ P, A)
$$&lt;/p&gt;
$$
P(X, Y) = P(X\ |\ Y)P(Y)
$$$$
P(X\ |\ Y) = \frac{P(Y\ |\ X)P(X)}{P(Y)}
$$&lt;p&gt;Starting from the beginning, our goal is to reduce each question down to an algebraic expression of the distributions we've estimated from our simulated data. From there, we simply plug and play.&lt;/p&gt;
&lt;p&gt;In the distributions that follow, we're able to express the &lt;em&gt;uncertainty&lt;/em&gt; in our estimates of the true answer to each question. This allows us to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contextualize our estimate.&lt;/strong&gt; Therein, we can make statements like: "There are a ton of plausible values of this probability. If you're going to use it to make a decision or communicate it to others, please do so with caution and don't bet the farm." Conversely, if the following posterior were to be very narrow, we could say things like: "There is a very narrow range of values that satisfy the model given the data. You can be very sure about this probability estimate - perhaps even share it with the CEO!"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Make intelligent choices.&lt;/strong&gt; If we're going to make bets on traffic, i.e. decide how much money to invest in new urban infrastructure given an estimate of $P(\text{Traffic}\ |\ \text{President})$, different values of $P(\text{Traffic}\ |\ \text{President})$ will suggest different decisions, and each decision will carry a different cost. For example, concurring that the President's appearance causes traffic 80% of the time might lead us to building a new highway - expensive - while concurring 5% might lead us to simply open the POV lane for the duration of his stay. So, given our estimate (a distribution) of $P(\text{Traffic}\ |\ \text{President})$, how do we know how much money we should budget for the project? This is a motivating example for Bayesian decision theory which, using the entire posterior, allows us to responsibly answer this question. In fact, it's a lot easier than it sounds. My favorite resource on the topic is Rasmus Bååth's &lt;a href="http://www.sumsar.net/blog/2015/01/probable-points-and-credible-intervals-part-two/"&gt;"Probable Points and Credible Intervals, Part 2: Decision Theory"&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we plot the point answer to each question computed from the original probabilities defined at the beginning of this notebook. This gives us an idea of how well were able to recover the initial generative process given the simulated data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="$P(\text{Accident-=-1}\-|\-\text{Traffic-=-1})$"&gt;$P(\text{Accident = 1}\ |\ \text{Traffic = 1})$&lt;a class="anchor-link" href="#$P(\text{Accident-=-1}\-|\-\text{Traffic-=-1})$"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;$\begin{align*}
P(A = 1|T = 1)
  &amp;amp;= \frac{P(T = 1 | A = 1)P(A = 1)}{P(T = 1)}\\
  &amp;amp;= \frac{P(T = 1, A = 1)P(A = 1)}{P(A = 1)P(T = 1)}\\
  &amp;amp;= \frac{P(T = 1, A = 1)}{P(T = 1)}\\
\end{align*}$
&lt;br/&gt;
$\begin{align*}
P(T = 1, A = 1)
  &amp;amp;= \sum_{P}P(P, A = 1, T = 1))\\
  &amp;amp;= P(P = 0, A = 1, T = 1) + P(P = 1, A = 1, T = 1)\\
  &amp;amp;= P(P = 0)P(A = 1)P(T = 1 \ |\  P = 0, A = 1) + P(P = 1)P(A = 1)P(T = 1 \ |\  P = 1, A = 1)
\end{align*}$
&lt;br/&gt;
$\begin{align*}
P(T = 1)
  &amp;amp;= \sum_{A, P}P(A, P, T = 1))\\
  &amp;amp;= \sum_{A, P}P(A)P(P)P(T = 1\ |\ A, P))\\
  &amp;amp;=
    P(A = 0)P(P = 0)P(T = 1\ |\ A = 0, P = 0) + P(A = 0)P(P = 1)P(T = 1\ |\ A = 0, P = 1) + P(A = 1)P(P = 0)P(T = 1\ |\ A = 1, P = 0) + P(A = 1)P(P = 1)P(T = 1\ |\ A = 1, P = 1)
\end{align*}$&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [20]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_P_accident_1_given_traffic_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;P_traffic_1_accident_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="n"&gt;P_traffic_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    
    &lt;span class="n"&gt;P_accident_1_given_traffic_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P_traffic_1_accident_1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;P_traffic_1&lt;/span&gt;
    
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;P_accident_1_given_traffic_1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [21]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;P_accident_1_given_traffic_1_point_estimate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
    &lt;span class="n"&gt;compute_P_accident_1_given_traffic_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PRESIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ACCIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;TRAFFIC_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
&lt;span class="n"&gt;P_accident_1_given_traffic_1_with_uncertainty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
    &lt;span class="n"&gt;compute_P_accident_1_given_traffic_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accident_probability_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [22]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;color_palette&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P_accident_1_given_traffic_1_with_uncertainty&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;edgecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'white'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Posterior Distribution of P(Accident = 1 | Traffic = 1)'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axvline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P_accident_1_given_traffic_1_point_estimate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'--'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Point Estimate'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Probability'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Count'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[22]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;lt;matplotlib.legend.Legend at 0x1194a6a20&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAtoAAAH0CAYAAAADoIroAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="$P(\text{Accident-=-1}\-|\-\text{Traffic-=-1},-\text{President-=-1})$"&gt;$P(\text{Accident = 1}\ |\ \text{Traffic = 1}, \text{President = 1})$&lt;a class="anchor-link" href="#$P(\text{Accident-=-1}\-|\-\text{Traffic-=-1},-\text{President-=-1})$"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;$\begin{align*}
P(A = 1\ |\ T = 1, P = 1)
  &amp;amp;= \frac{P(A = 1, T = 1, P = 1)}{P(T = 1, P = 1)}
\end{align*}$
&lt;br/&gt;
$\begin{align*}
P(A = 1, T = 1, P = 1)
  &amp;amp;= P(T = 1\ |\ A = 1, P = 1)P(A = 1)P(P = 1)
\end{align*}$
&lt;br/&gt;
$\begin{align*}
P(T = 1, P = 1)
  &amp;amp;= \sum_{A}P(A, P = 1, T = 1)\\
  &amp;amp;= P(A = 0, P = 1, T = 1) + P(A = 1, P = 1, T = 1)\\
  &amp;amp;= P(T = 1\ |\ A = 0, P = 1)P(P = 1)P(A = 0) + P(T = 1\ |\ A = 1, P = 1)P(P = 1)P(A = 1)
\end{align*}$&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [23]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_P_accident_1_given_traffic_1_president_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;P_accident_1_traffic_1_president_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
        &lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;
        
    &lt;span class="n"&gt;P_traffic_1_president_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
        &lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;accident_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;\
        &lt;span class="n"&gt;traffic_probability&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;JointInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;president_probability&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;accident_probability&lt;/span&gt;
        
    &lt;span class="n"&gt;P_accident_1_given_traffic_1_president_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P_accident_1_traffic_1_president_1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;P_traffic_1_president_1&lt;/span&gt;
    
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;P_accident_1_given_traffic_1_president_1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [24]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;P_accident_1_given_traffic_1_president_1_point_estimate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
    &lt;span class="n"&gt;compute_P_accident_1_given_traffic_1_president_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PRESIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ACCIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;TRAFFIC_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
&lt;span class="n"&gt;P_accident_1_given_traffic_1_president_1_with_uncertainty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
    &lt;span class="n"&gt;compute_P_accident_1_given_traffic_1_president_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;president_probability_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accident_probability_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counterfactual_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [25]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P_accident_1_given_traffic_1_president_1_with_uncertainty&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
         &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;edgecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'white'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Posterior Distribution of P(Accident = 1 | Traffic = 1, President = 1)'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axvline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P_accident_1_given_traffic_1_president_1_point_estimate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'--'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Point Estimate'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Probability'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Count'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[25]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;lt;matplotlib.legend.Legend at 0x119715f60&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAtsAAAH0CAYAAADsYuHWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="But-didn't-this-all-depend-on-how-the-dice-were-rolled?"&gt;But didn't this all depend on how the dice were rolled?&lt;a class="anchor-link" href="#But-didn't-this-all-depend-on-how-the-dice-were-rolled?"&gt;¶&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To conclude, I'd like to give some basic context on the stochasticity of the generative process itself. We said above that the probability of observing an accident is &lt;code&gt;ACCIDENT_PROBABILITY = .12&lt;/code&gt;. We then simulated 200 trials and observed a discrete count. While the expectation of this count is $200 * .12 = 24$ accidents observed, we could have in reality observed 23, or 31, or even (but implausibly) 0. Next, we used this data to infer a posterior distribution for the true value of &lt;code&gt;ACCIDENT_PROBABILITY&lt;/code&gt;, and later plugged it in to the expressions corresponding to the original questions at hand. How much did the stochasticity of these trials impact the final result?&lt;/p&gt;
&lt;p&gt;The following data cover 199 more trials. In each, we "flip the coin" 99 times, instead of 1 time as was done above. In each trial, we plot the inferred posterior distribution for the true probability of &lt;code&gt;accident&lt;/code&gt;. The distribution of given the median observed count is overlaid in bold.&lt;/p&gt;
&lt;p&gt;As is clear, the expected values of these posteriors can vary wildly! In fact, the left tails of some &lt;em&gt;barely touch&lt;/em&gt; the right tails of others. By all accounts, this will absolutely affect our final estimates. Is there anything we can do about it?&lt;/p&gt;
&lt;p&gt;As yet, I see two options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Give an informative prior.&lt;/strong&gt; The canonical Beta-Binomial formulation we've used implies a flat prior - $\text{Beta}(1, 1)$. A more informative prior would come from domain-level knowledge about how often traffic accidents occur.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build a better model.&lt;/strong&gt; Perhaps there are other variables that directly contribute to accidents like weather, time of day, etc.? If such data were available we should use them to try to explain the variance in our outcome &lt;code&gt;accident&lt;/code&gt;. Notwithstanding, this is a general point for a future analysis: the mentioned quantities are not actually found in our original graph itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, here's a fitting quote that makes me smile:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“And with real data, these models will never accurately recover the data-generating mechanism. At best, they describe it in a scientifically useful way.” - Richard McElreath&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [26]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;99&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1002&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;accident_trials&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ACCIDENT_PROBABILITY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TRIALS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_beta_densities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yes_count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;no_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_count&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;yes_count&lt;/span&gt;
    &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;no_count&lt;/span&gt;
    &lt;span class="n"&gt;distribution&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;yes_count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;accident_trials&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_beta_densities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yes_count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=.&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
&lt;span class="n"&gt;median_yes_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accident_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_beta_densities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;median_yes_count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Plausible Beta Posteriors of P(Accident = 1)'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Probability'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Density'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[26]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;lt;matplotlib.text.Text at 0x1197e7ef0&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea"&gt;
&lt;img class="img-responsive" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAuIAAAH0CAYAAAB4hEgkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Bayesian Inference via Simulated Annealing</title><link href="https://cavaunpeu.github.io/2017/02/07/bayesian-inference-via-simulated-annealing/" rel="alternate"></link><published>2017-02-07T15:33:00-05:00</published><updated>2017-02-07T15:33:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-02-07:/2017/02/07/bayesian-inference-via-simulated-annealing/</id><summary type="html">&lt;p&gt;A toy, hand-rolled Bayesian model, optimized via simulated annealing.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently finished a &lt;a href="https://www.coursera.org/learn/discrete-optimization"&gt;course&lt;/a&gt; on discrete optimization and am currently working through Richard McElreath's &lt;em&gt;excellent&lt;/em&gt; &lt;a href="http://xcelab.net/rm/statistical-rethinking/"&gt;textbook&lt;/a&gt; Statistical Rethinking. Combining the two, and duly jazzed by this &lt;a href="https://www.youtube.com/watch?v=SC5CX8drAtU"&gt;video&lt;/a&gt; on the Traveling Salesman Problem, I'd thought I'd build a toy Bayesian model and try to optimize it via simulated annealing.&lt;/p&gt;
&lt;p&gt;This work was brief, amusing and experimental. The result is a simple &lt;a href="https://willwolf.shinyapps.io/bayesian-inference-simulated-annealing/"&gt;Shiny app&lt;/a&gt; that contrasts MCMC search via simulated annealing versus the (more standard) Metropolis algorithm. While far from groundbreaking, I did pick up the following few bits of intuition along the way.&lt;/p&gt;
&lt;p&gt;&lt;img alt="traceplot" class="img-responsive" src="https://cavaunpeu.github.io/figures/bayesian_inference_simulated_annealing_traceplot.png"/&gt;&lt;/p&gt;
&lt;h3&gt;A new favorite analogy for Bayesian inference&lt;/h3&gt;
&lt;p&gt;I like teaching things to anyone who will listen. Fancy models are useless if your boss doesn't understand. Simple analogies are immensely effective for communicating almost anything at all.&lt;/p&gt;
&lt;p&gt;The goal of Bayesian inference is, given some data, to figure out which parameters were employed in generating that data. The data itself come from generative processes - a Binomial process, a Poisson process, a Normal process (as is used in this post), for example - which each require parameters to get off the ground (in the same way that an oven needs a temperature and a time limit before it can start making bread) - &lt;span class="math"&gt;\(n\)&lt;/span&gt; and &lt;span class="math"&gt;\(p\)&lt;/span&gt;; &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;; &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, respectively. In statistical inference, we work backwards: we're given the data, we hypothesize from which type of process(es) it was generated, and we then do our best to guess what these initial parameters were. Of course, we'll never actually know: if we did, we wouldn't need to do any modeling at all.&lt;/p&gt;
&lt;p&gt;Bayes' theorem is as follows: &lt;/p&gt;
&lt;div class="math"&gt;$$P(p | X) \sim P(X | p)P(p)$$&lt;/div&gt;
&lt;p&gt;Initially, all we have is &lt;span class="math"&gt;\(X\)&lt;/span&gt;: the data that we've observed. During inference, we pick a parameter value &lt;span class="math"&gt;\(p\)&lt;/span&gt; - let's start with &lt;span class="math"&gt;\(p = 123\)&lt;/span&gt; - and compute both &lt;span class="math"&gt;\(P(X | p)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt;. We then multiply these two together, leaving us with an expression of how likely &lt;span class="math"&gt;\(p\)&lt;/span&gt; is to be the &lt;em&gt;real&lt;/em&gt; parameter that was initially plugged into our generative process (that then generated the data we have on hand). This expression is called the posterior probability (of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, given &lt;span class="math"&gt;\(X\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The centerpiece of this process is the computation of the quantities &lt;span class="math"&gt;\(P(X | p)\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt;. To understand, let us use the example of &lt;em&gt;vetting&lt;/em&gt;, i.e. vetting an individual applying for citizenship in your country - a typically multi-step process. In this particular vetting process, there are two steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The first step, and perhaps the "broader stroke" of the two, is the prior probability of this parameter. In setting up our problem we choose a prior distribution - i.e. our &lt;em&gt;a priori&lt;/em&gt; belief of the possible range of values this true parameter &lt;span class="math"&gt;\(p\)&lt;/span&gt; can take - and the prior probability &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt; echoes how likely we thought &lt;span class="math"&gt;\(p = 123\)&lt;/span&gt; to be the real thing before we saw any data at all.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second step is the likelihood of our data given this parameter. It says: "assuming &lt;span class="math"&gt;\(p = 123\)&lt;/span&gt; is the real thing, how likely was it to have observed the data that we did?" For further clarity, let's assume in an altogether different problem that our data consists of 20 coin-flips - 17 heads, and 3 tails - and the parameter we're currently &lt;em&gt;vetting&lt;/em&gt; is &lt;span class="math"&gt;\(p_{heads} = .064\)&lt;/span&gt; (where &lt;span class="math"&gt;\(p_{heads}\)&lt;/span&gt; is the probability of flipping "heads"). So, "assuming &lt;span class="math"&gt;\(p = .064\)&lt;/span&gt; is the real thing, how likely was it to have observed the data that we did?" The answer: "Err, not likely whatsoever."&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, we multiply these values together to obtain the posterior probability, or the "yes admit this person into the country" score. If it's high, they check out.&lt;/p&gt;
&lt;h3&gt;Computing the posterior on the log scale is important&lt;/h3&gt;
&lt;p&gt;The prior probability &lt;span class="math"&gt;\(P(p)\)&lt;/span&gt; for a given parameter is a single floating point number. The likelihood of a single data point &lt;span class="math"&gt;\(x\)&lt;/span&gt; given that parameter &lt;span class="math"&gt;\(p\)&lt;/span&gt;, expressed &lt;span class="math"&gt;\(P(x | p)\)&lt;/span&gt;, is a single floating point number. To compute the likelihood of &lt;em&gt;all &lt;/em&gt;of our data &lt;span class="math"&gt;\(X\)&lt;/span&gt; given that parameter &lt;span class="math"&gt;\(p\)&lt;/span&gt;, expressed &lt;span class="math"&gt;\(P(X | p)\)&lt;/span&gt;, we must multiply the individual likelihoods together - one for each data point. For example, if we have 100 data points, &lt;span class="math"&gt;\(P(X | p)\)&lt;/span&gt; is the product of 100 floating point numbers. We can write this compactly as:&lt;/p&gt;
&lt;div class="math"&gt;$$P(X | p) = \prod\limits_{i =1}^{N} P(x_i | p)$$&lt;/div&gt;
&lt;p&gt;Likelihood values are often pretty small, and multiplying small numbers together makes them even smaller. As such, computing the posterior on the log scale allows us to &lt;em&gt;add&lt;/em&gt; instead of multiply, which solves some numerical precision troubles that computers often have. With 100 data points, computing the log posterior would be a sum of 101 numbers. On the natural scale, the posterior would be the product of 101 numbers.&lt;/p&gt;
&lt;h3&gt;Optimization is a thing&lt;/h3&gt;
&lt;p&gt;I used to get easily frustrated with oft-used big words that I personally felt conferred no meaning whatsoever. "Optimization," for example: "We here at XYZ Consulting undertake optimal processes for maximal profit." Optimal, eh? What does that actually mean?&lt;/p&gt;
&lt;p&gt;In recent months, I've realized optimization is just a collection of strategies for finding intelligent and warm-hearted prospective citizens: specifically, given one prospective citizen (parameter) that is sufficiently terrific (has a high posterior probability given the data we've observed), how do we then find a bunch more? In the discrete world, simulated annealing, mixed-integer programming, branch and bound, etc. would be a few of these strategies. In the continuous world, gradient descent, L-BFGS, the Powell algorithm and brute-force grid search are a few such examples.&lt;/p&gt;
&lt;p&gt;In most mathematical cases, the measure of "sufficiently terrific" refers to a good score on a relevant loss function. In the case of XYZ Consulting, this metric is certainly more vague. (And while there may be some complex numerical optimization routine guiding their decisions, well, I'd guess the PR-score is the metric they're more likely after.)&lt;/p&gt;
&lt;h3&gt;Findings&lt;/h3&gt;
&lt;p&gt;The most salient difference between the simulated annealing and Metropolis samplers is the "cooling schedule" of the former. In effect, simulated annealing becomes fundamentally more "narrow-minded" as time goes on: it finds a certain type of prospective citizen it likes, and it thereafter goes searching only for others that are very close in nature. Concretely, with a very quick cooling schedule, this can result in a skinny and tall posterior; when using simulating annealing for MCMC, we must take care to use a schedule that allows for sufficient exploration of the parameter space. With the Metropolis sampler, we don't have this problem.&lt;/p&gt;
&lt;p&gt;Finally, I've found that I quite enjoy using R. Plotting is miles easier than in Python and the pipe operators aren't so bad.&lt;/p&gt;
&lt;p&gt;Thanks a lot for reading.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;The code for this project can be found &lt;a href="https://github.com/cavaunpeu/bayesian-inference-simulated-annealing"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>RescueTime Inference via the "Poor Man's Dirichlet"</title><link href="https://cavaunpeu.github.io/2017/02/03/bayesian-estimation-of-rescuetime-productivity/" rel="alternate"></link><published>2017-02-03T18:42:00-05:00</published><updated>2017-02-03T18:42:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2017-02-03:/2017/02/03/bayesian-estimation-of-rescuetime-productivity/</id><summary type="html">&lt;p&gt;Modeling a typical week of RescueTime data via an alternative take on the Dirichlet distribution.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.rescuetime.com"&gt;RescueTime&lt;/a&gt; is "a personal analytics service that shows you how you spend your time [on the computer], and provides tools to help you be more productive." Personally, I've been a RescueTime user since late-January 2016, and while it does ping me guilty for harking back to a dangling Facebook tab in Chrome, I haven't yet dug much into the data it's thus-far stored.&lt;/p&gt;
&lt;p&gt;In short, I built a &lt;a href="https://willwolf.shinyapps.io/rescue-time-estimation/"&gt;Shiny app&lt;/a&gt; that estimates my/your typical week with RescueTime. In long, the analysis is as follows.&lt;/p&gt;
&lt;p&gt;The basic model of RescueTime is thus: track all activity, then categorize this activity by both "category" — "Software Development," "Reference and Learning," "Social Learning," etc. — and "productivity level" — "Very Productive Time," "Productive Time," "Neutral Time," "Distracting Time" and "Very Distracting Time." For example, 10 minutes spent on Twitter would be logged as (600 seconds, "Social Networking", "Very Distracting Time"), while 20 minutes on an &lt;a href="https://arxiv.org/"&gt;arxiv&lt;/a&gt; paper logged as (1200 seconds, "Reference &amp;amp; Learning," "Very Productive Time"). Finally, RescueTime maintains (among other minutia) an aggregate "productivity score" by day, week and year.&lt;/p&gt;
&lt;p&gt;The purpose of this post is to take my weekly summary for 2016 and examine how I'm doing thus far. More specifically, with a dataset containing the total seconds-per-week spent [viewing resources categorized] at each of the 5 distinct productivity levels, I'd like to infer the productivity breakdown of a typical week. Rows of this dataset — after dividing all values in each by its sum — will contain 5 numbers, with each expressing the percentage of that week spent at the respective productivity level. Examples might include: &lt;span class="math"&gt;\((.2, .3, .1, .2, .2)\)&lt;/span&gt;, &lt;span class="math"&gt;\((.1, .3, .2, .15, .25)\)&lt;/span&gt; or &lt;span class="math"&gt;\((.05, .25, .3, .25, .15)\)&lt;/span&gt;. Of course, the values in each row must sum to 1.&lt;/p&gt;
&lt;p&gt;In effect, we can view each row as an empirical probability distribution over the 5 levels at hand. As such, our goal is to infer the process that generated these samples in the first place. In the canonical case, this generative process would be a &lt;a href="https://en.wikipedia.org/wiki/Dirichlet_distribution"&gt;Dirichlet distribution&lt;/a&gt; — a thing that takes a vector &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and returns vectors of the length of &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; containing values that sum to 1. With a Dirichlet model conditional on the RescueTime data observed, the world becomes ours: we can generate new samples (a "week" of RescueTime log!) ad infinitum, ask questions of these samples (e.g. "what percentage of the time can we expect to log more 'Very Productive Time' than 'Productive Time?'"), and get some proxy lens into the brain cells and body fibers that spend our typical week in front of the computer in the manner that they do.&lt;/p&gt;
&lt;p&gt;To begin this analysis, I first download the data at the following &lt;a href="https://www.rescuetime.com/browse/productivity/by/week/for/the/year/of/2016-01-01"&gt;link&lt;/a&gt;. If you're not logged in, you'll be first prompted to do so. To do the same, you must be a paying RescueTime user. If you're not, you're welcome to use &lt;a href="https://github.com/cavaunpeu/rescue-time-estimation/blob/publish/data/rescue_time_report.csv"&gt;my personal data&lt;/a&gt; in order to follow along.&lt;/p&gt;
&lt;p&gt;The data at hand have 48 rows. First, let's see what they look like.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;report&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="n"&gt;week&lt;/span&gt; &lt;span class="n"&gt;very_distracting&lt;/span&gt; &lt;span class="n"&gt;distracting&lt;/span&gt; &lt;span class="n"&gt;neutral&lt;/span&gt; &lt;span class="n"&gt;productive&lt;/span&gt; &lt;span class="n"&gt;very_productive&lt;/span&gt;
&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2016-01-31&lt;/span&gt;&lt;span class="n"&gt;T00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt; &lt;span class="m"&gt;0.05802495&lt;/span&gt; &lt;span class="m"&gt;0.15878213&lt;/span&gt; &lt;span class="m"&gt;0.1179268&lt;/span&gt; &lt;span class="m"&gt;0.05471899&lt;/span&gt; &lt;span class="m"&gt;0.6105471&lt;/span&gt;
&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;2016-02-07&lt;/span&gt;&lt;span class="n"&gt;T00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt; &lt;span class="m"&gt;0.16082036&lt;/span&gt; &lt;span class="m"&gt;0.11625240&lt;/span&gt; &lt;span class="m"&gt;0.1251466&lt;/span&gt; &lt;span class="m"&gt;0.06762928&lt;/span&gt; &lt;span class="m"&gt;0.5301514&lt;/span&gt;
&lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;2016-02-14&lt;/span&gt;&lt;span class="n"&gt;T00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt; &lt;span class="m"&gt;0.07335485&lt;/span&gt; &lt;span class="m"&gt;0.18299335&lt;/span&gt; &lt;span class="m"&gt;0.1269896&lt;/span&gt; &lt;span class="m"&gt;0.08361825&lt;/span&gt; &lt;span class="m"&gt;0.5330439&lt;/span&gt;
&lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="m"&gt;2016-02-21&lt;/span&gt;&lt;span class="n"&gt;T00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt; &lt;span class="m"&gt;0.07911463&lt;/span&gt; &lt;span class="m"&gt;0.04051227&lt;/span&gt; &lt;span class="m"&gt;0.1445033&lt;/span&gt; &lt;span class="m"&gt;0.05395296&lt;/span&gt; &lt;span class="m"&gt;0.6819169&lt;/span&gt;
&lt;span class="m"&gt;5&lt;/span&gt; &lt;span class="m"&gt;2016-02-28&lt;/span&gt;&lt;span class="n"&gt;T00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt; &lt;span class="m"&gt;0.07513117&lt;/span&gt; &lt;span class="m"&gt;0.12542957&lt;/span&gt; &lt;span class="m"&gt;0.1560940&lt;/span&gt; &lt;span class="m"&gt;0.04884047&lt;/span&gt; &lt;span class="m"&gt;0.5945047&lt;/span&gt;
&lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="m"&gt;2016-03-06&lt;/span&gt;&lt;span class="n"&gt;T00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;00&lt;/span&gt; &lt;span class="m"&gt;0.04554125&lt;/span&gt; &lt;span class="m"&gt;0.12288119&lt;/span&gt; &lt;span class="m"&gt;0.1410541&lt;/span&gt; &lt;span class="m"&gt;0.08958757&lt;/span&gt; &lt;span class="m"&gt;0.6009359&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, let's see how each level is distributed:&lt;/p&gt;
&lt;p&gt;&lt;img alt="empirical boxplot" class="img-responsive" src="https://cavaunpeu.github.io/figures/observed_productivity_levels_empirical_boxplot.png"/&gt;&lt;/p&gt;
&lt;p&gt;Finally, let's choose a modeling approach. Once more, I venture that each week should be viewed as a draw from a Dirichlet distribution; at the very least, no matter how modeled, each week (draw) is inarguably a vector of values that sum to 1. To this effect, I see a few possible approaches.&lt;/p&gt;
&lt;h3&gt;Dirichlet Process&lt;/h3&gt;
&lt;p&gt;A &lt;a href="https://en.wikipedia.org/wiki/Dirichlet_process"&gt;Dirichlet Process&lt;/a&gt; (DP) is a model of &lt;em&gt;a distribution over distributions&lt;/em&gt;. In our example, this would imply that each week's vector is drawn from one of several possible Dirichlet distributions, each one governing a fundamentally different type of week altogether. For example, let's posit that we have several different kinds of work weeks: a "lazy" week, a "fire-power-super-charged week," a "start-slow-finish-strong week," a "cram-all-the-things-on Friday week." Each week, we arrive to work on Monday morning and our aforementioned brain cells and body fibers "decide" what type of week we're going to have. Finally, we play out the week and observe the resulting vector. Of course, while two weeks might have the same type, the resulting vectors will likely be (at least) slightly different.&lt;/p&gt;
&lt;p&gt;In this instance, given a Dirichlet Process &lt;span class="math"&gt;\(DP(G_0, \alpha)\)&lt;/span&gt; — where &lt;span class="math"&gt;\(G_0\)&lt;/span&gt; is the base distribution (from which the cells and fibers decide what type of week we'll have) and &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is some prior — we first draw a week-type distribution from the base, then draw our week-level probability vector from the result. As a bonus, a DP is able to infer an &lt;em&gt;infinite&lt;/em&gt; number of week-type distributions from our data (as compared to K-Means, for example, in which we would have to specify this value &lt;em&gt;a priori&lt;/em&gt;) which fits nicely with the problem at hand: &lt;em&gt;à la base&lt;/em&gt;, how many distinct week-types do we truly have? How would we ever know?&lt;/p&gt;
&lt;p&gt;Dirichlet Processes are best understood through one of several simple generative statistical processes, namely the Chinese Restaurant Process, Polya Urn Model or Stick-Breaking Process. Edwin Chen has an &lt;em&gt;excellent&lt;/em&gt; &lt;a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"&gt;post&lt;/a&gt; dissecting each and its relation to the DP itself.&lt;/p&gt;
&lt;h3&gt;Dirichlet Inference via Conjugacy&lt;/h3&gt;
&lt;p&gt;Given that a Dirichlet distribution is an exponential model, "all members of the exponential family have conjugate priors"&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; and our data can be intuitively viewed as Dirichlet draws, it would be fortuitous if there existed some nice algebraic conjugacy to make inference a breeze. We know how to use &lt;em&gt;Beta-Binomial&lt;/em&gt; and &lt;em&gt;Dirichlet-Multinomial&lt;/em&gt;, but unfortunately there doesn't seem to be much in the way of &lt;em&gt;X-Dirichlet&lt;/em&gt;&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. As such, this approach unfortunately dead-ends here.&lt;/p&gt;
&lt;h3&gt;The "Poor-Man's Dirichlet" via Linear Models&lt;/h3&gt;
&lt;p&gt;A final approach has us modeling the mean of each productivity-level proportion &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu_i = \frac{exp(\phi_j)}{\sum\limits_{j = 0}^{4}
exp(\phi_j)}, \text{for i} \in \{0, 1, 2, 3, 4\}
$$&lt;/div&gt;
&lt;p&gt;For each &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt;, we place a normal prior &lt;span class="math"&gt;\(\phi_j \sim \text{Normal}(\mu_j, \sigma_j)\)&lt;/span&gt;, and finally give the likelihood of each productivity-level proportion &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; as &lt;span class="math"&gt;\(p_i \sim \text{Normal}(\mu_i, \sigma)\)&lt;/span&gt; as in the canonical Bayesian linear regression. There's two key points to make on this approach.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As each &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; is given by the softmax function the values &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt; are not uniquely identifiable, i.e. &lt;code&gt;softmax(vector) = softmax(100 + vector)&lt;/code&gt;. In other words, because the magnitude of the values &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt; (how big they are) is unimportant, we cannot (nor do we need to) solve for these values exactly. I like to think of this as inference on two multi-collinear variables in a linear regression: with &lt;span class="math"&gt;\(corr(x_1, x_2) \approx 1\)&lt;/span&gt;, we can re-express our regression &lt;span class="math"&gt;\(\mu = \alpha + \beta_1 x_1 + \beta_2 x_2\)&lt;/span&gt; as &lt;span class="math"&gt;\(\mu = \alpha + (\beta_1 + \beta_2)x_1\)&lt;/span&gt;; in effect, we now have only 1 coefficient to solve for, and while the sum &lt;span class="math"&gt;\(\beta_1 + \beta_2\)&lt;/span&gt; is what we're trying to infer, the individual values &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_2\)&lt;/span&gt; are of no importance. (For example, if &lt;span class="math"&gt;\(\beta_1 + \beta_2 = 10\)&lt;/span&gt;, we could choose &lt;span class="math"&gt;\(\beta_1 = 3\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_2 = 7\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\beta_1 = 9\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_2 = 1\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\beta_1 = .01\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_2 = 9.99\)&lt;/span&gt; to no material difference.) In this case, while interpretation of the individual coefficients &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_2\)&lt;/span&gt; would be erroneous, we can still make perfectly sound predictions on &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; with the posterior for &lt;span class="math"&gt;\(\beta_1 + \beta_2\)&lt;/span&gt;. To close, this is but a tangential way of saying that while the posteriors of each individual &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt; will be of little informative value, the softmax itself will still work out just fine.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I've chosen the likelihood as the normal distribution with respective means &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; and a &lt;em&gt;shared&lt;/em&gt; standard deviation &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. First, I note that I hope this is the correct approach, i.e. "do it like you would with a typical linear model." Second, I chose a shared standard deviation (and, frankly, prayed it would be small) as my aim was to omit it from analysis/posterior prediction entirely: while simulating &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; seems perfectly sound, making a draw from the likelihood function, i.e. the normal distribution with mean &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; and standard deviation &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, would cause our simulated productivity-level proportions to no longer add up to 1! This seems like the worst of all evils. While the spread of the respective distributions &lt;em&gt;does&lt;/em&gt; seem to vary — thus suggesting we would be wise to infer a separate &lt;span class="math"&gt;\(\sigma_i\)&lt;/span&gt; for each — I chose to brush this fact aside because: one, the &lt;span class="math"&gt;\(p_i\)&lt;/span&gt;'s are not independent, i.e. as one goes up another must necessarily go down, which I hoped might be in some way "captured" by the single parameter, and two, I didn't intend to use the &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; posterior in the analysis for the reason mentioned above, checking only to see that it converged.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the end, I chose Option 3 for a few simple reasons. First, I have no reason to believe that the data were generated by a variety of distinct "week-type" distributions; a week is a rather large unit of time. In addition, the spread of the empirical distributions don't appear, by no particularly rigorous measure, that erratic. Conversely, if this were instead day-level data, this argument would be much more plausible and the data would likely corroborate this point. Second, Gelman suggests this approach in response to a similar question, adding "I’ve personally never had much success with Dirichlets."&lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;To build this model, I elect to use Stan in R, defining it as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;very_distracting&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;distracting&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;neutral&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;productive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;phi_a&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;phi_b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;phi_c&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;phi_d&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;phi_e&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;transformed&lt;/span&gt; &lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;mu_a&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;mu_b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;mu_c&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="n"&gt;mu_d&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
 &lt;span class="n"&gt;mu_a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;mu_b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;mu_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;mu_d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi_e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;phi_a&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;phi_e&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;phi_d&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;phi_c&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;phi_b&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;very_distracting&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;distracting&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;neutral&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
 &lt;span class="n"&gt;productive&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here (with &lt;em&gt;a, b, c, d, e&lt;/em&gt; corresponding respectively to "Very Distracting Time," "Distracting Time," "Neutral," "Productive Time," "Very Productive Time") I model the likelihoods of all but &lt;em&gt;e&lt;/em&gt;, as this can be computed deterministically from the posterior samples of &lt;em&gt;a, b, c&lt;/em&gt; and &lt;em&gt;d&lt;/em&gt; as &lt;span class="math"&gt;\(e = 1 - a - b - c - d\)&lt;/span&gt;. For priors, I place &lt;span class="math"&gt;\(\text{Normal}(0, 1)\)&lt;/span&gt; priors on &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt;, the magnitude of which should be practically irrelevant as stated previously. Finally, I give &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; a &lt;span class="math"&gt;\(\text{Uniform}(0, 1)\)&lt;/span&gt; prior, which seemed like a logical magnitude for mapping a vector of values that sum to 1 to another vector of values that sum (to something close to) to 1.&lt;/p&gt;
&lt;p&gt;Instinctually, this modeling framework seems like it might have a few leaks in the theoretical ceiling — especially with respect to my choices surrounding the shared &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; parameter. Should you have some feedback on this approach, please do drop a line in the comments below.&lt;/p&gt;
&lt;p&gt;To fit the model, I use the standard Stan NUTS engine to build 4 MCMC chains, following Richard McElreath's "four short chains to check, one long chain for inference!"&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;. The results — fortunately, quite smooth — are as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="traceplot" class="img-responsive" src="https://cavaunpeu.github.io/figures/poor_mans_dirichlet_traceplot.png"/&gt;&lt;/p&gt;
&lt;p&gt;The gray area of the plot pertains to the warmup period, while the white gives the valid samples. All four chains appear highly-stationary, well-mixing and roughly identical. Finally, let's examine the convergence diagnostics themselves:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Inference&lt;/span&gt; &lt;span class="n"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Stan&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;model.&lt;/span&gt;
&lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="n"&gt;chains&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;warmup&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;thin&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;warmup&lt;/span&gt; &lt;span class="n"&gt;draws&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;warmup&lt;/span&gt; &lt;span class="n"&gt;draws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4000&lt;/span&gt;&lt;span class="n"&gt;.&lt;/span&gt;

      &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="n"&gt;se_mean&lt;/span&gt;   &lt;span class="n"&gt;sd&lt;/span&gt; &lt;span class="m"&gt;1.5&lt;/span&gt;&lt;span class="o"&gt;% 98.5%&lt;/span&gt; &lt;span class="n"&gt;n_eff&lt;/span&gt; &lt;span class="n"&gt;Rhat&lt;/span&gt;
&lt;span class="n"&gt;mu_a&lt;/span&gt;  &lt;span class="m"&gt;0.07&lt;/span&gt;       &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt; &lt;span class="m"&gt;0.05&lt;/span&gt;  &lt;span class="m"&gt;0.09&lt;/span&gt;  &lt;span class="m"&gt;4000&lt;/span&gt;    &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;mu_b&lt;/span&gt;  &lt;span class="m"&gt;0.07&lt;/span&gt;       &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt; &lt;span class="m"&gt;0.05&lt;/span&gt;  &lt;span class="m"&gt;0.09&lt;/span&gt;  &lt;span class="m"&gt;4000&lt;/span&gt;    &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;mu_c&lt;/span&gt;  &lt;span class="m"&gt;0.20&lt;/span&gt;       &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt; &lt;span class="m"&gt;0.18&lt;/span&gt;  &lt;span class="m"&gt;0.22&lt;/span&gt;  &lt;span class="m"&gt;4000&lt;/span&gt;    &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;mu_d&lt;/span&gt;  &lt;span class="m"&gt;0.12&lt;/span&gt;       &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt; &lt;span class="m"&gt;0.10&lt;/span&gt;  &lt;span class="m"&gt;0.14&lt;/span&gt;  &lt;span class="m"&gt;4000&lt;/span&gt;    &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="m"&gt;0.07&lt;/span&gt;       &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0.00&lt;/span&gt; &lt;span class="m"&gt;0.06&lt;/span&gt;  &lt;span class="m"&gt;0.07&lt;/span&gt;  &lt;span class="m"&gt;1792&lt;/span&gt;    &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Both &lt;code&gt;Rhat&lt;/code&gt; — a value which we hope to equal 1, would be "suspicious at 1.01 and catastrophic at 1.10"&lt;sup id="fnref2:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; — and &lt;code&gt;n_eff&lt;/code&gt; — which expresses the "effective" number of samples, i.e. the samples not discarded due to high autocorrelation in the NUTS process — are right where we want them to be. Furthermore, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; ends up being rather small, and with a rather-tight 97% prediction interval to boot.&lt;/p&gt;
&lt;p&gt;Next, let's draw 2000 samples from the joint posterior and plot the respective distributions of &lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; against one another:&lt;/p&gt;
&lt;p&gt;&lt;img alt="posterior plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/poor_mans_dirichlet_posteriors.png"/&gt;&lt;/p&gt;
&lt;p&gt;Remember, the above posterior distributions are for the &lt;em&gt;expected values&lt;/em&gt; (mean) of each productivity-level proportion. In our model, we then insert this mean into a normal distribution (the likelihood function) with standard deviation &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; and draw our final value.&lt;/p&gt;
&lt;p&gt;Finally, let's compute the mean of each posterior for a final result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="donut plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/poor_mans_dirichlet_donut_plot.png"/&gt;&lt;/p&gt;
&lt;p&gt;In summary, I've got work to do. Time to cast off those "Neutral" clothes and toss it to the purple.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Additional Resources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://erikbern.com/2015/12/05/more-mcmc-analyzing-a-small-dataset-with-1-5-ratings/"&gt;More MCMC – Analyzing a small dataset with 1-5
    ratings&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;The code for this project can be found
&lt;a href="https://github.com/cavaunpeu/rescue-time-estimation"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior#cite_note-gelman_et_al-3"&gt;Conjugate Prior - Wikipedia&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="http://andrewgelman.com/2009/04/29/conjugate_prior/"&gt;Conjugate prior to
Dirichlets?&lt;/a&gt; &lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;McElreath, Richard. Statistical Rethinking. Chapman and
Hall/CRC, 20151222. VitalBook file. &lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Generating World Flags with Sparse Auto-Encoders</title><link href="https://cavaunpeu.github.io/2016/12/13/generating-world-flags-with-sparse-auto-encoders/" rel="alternate"></link><published>2016-12-13T20:55:00-05:00</published><updated>2016-12-13T20:55:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-12-13:/2016/12/13/generating-world-flags-with-sparse-auto-encoders/</id><summary type="html">&lt;p&gt;Hand-rolled sparse autoencoders to generate novel world flags.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've always been enchanted by the notion of encoding real-world entities into lists of numbers. In essence, I believe that hard truth is pristinely objective - like the lyrics of a song - and mathematics is the universe's all-powerful tool in expressing incontrovertibility.&lt;/p&gt;
&lt;p&gt;One of the expressed goals of machine learning is to learn structure in data. First, data, in line with the notion above, is a record of a thing that happened, or is. For example, data could be a piece of paper that lists all of the sales my company made yesterday. In addition, data could be a photograph which captures (via numerical pixel values) and an instant in time.&lt;/p&gt;
&lt;p&gt;So, what does structure in this data mean? Structure refers to patterns. Structure refers to high-level relationships and phenomena in this data. In the first case, finding structure could be discovering that Sunday is our most profitable day; in the second, structure could be discovering that in a large set of photographs of people, wherever we see a human nose, there are typically two eyes just above and a mouth just below.&lt;/p&gt;
&lt;p&gt;In machine learning, discovering structure in data in an unsupervised fashion - and especially when dealing with image, audio or video data - is typically performed via auto-encoders. The job of an auto-encoder is similar to that of a data compression model: take the original data and reduce it into something smaller that, &lt;em&gt;crucially&lt;/em&gt;, contains all of the information contained in the original. Said a different way, given the compressed representation of the data, we should be able to fully reconstruct the original input.&lt;/p&gt;
&lt;p&gt;In this post, I set out to discover structure in world flags. Specifically, I'd like to know: "what are the features that comprise these flags?" If successful, I should be able to &lt;em&gt;numerically encode&lt;/em&gt; a flag as not just its raw pixel values, but instead, "some red background, plus a green star in the middle" (in the case of Morocco). Of course, these features would only arise in a dataset full of flags: if viewed through the lens of pictures of cats, the Moroccan flag would instead be encoded as "a blood-red sunset, plus a cat in a green super-hero cape, minus the cat."&lt;/p&gt;
&lt;p&gt;In the family of auto-encoders, the sparse auto-encoder is one of the simplest. In effect, this is a neural network with a single hidden layer which takes an image as input and learns to predict that image as output. The hidden layer is typically of a size smaller than the input and output layers, and has non-linear activations. Finally, a sparsity constraint is enforced such that the model favors having only a few non-zero hidden-layer activation values. For a given image, these activations &lt;em&gt;are&lt;/em&gt; its compressed representation, i.e. it's "encoding."&lt;/p&gt;
&lt;p&gt;With a trained sparse auto-encoder, we can do a few things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We can visualize the "features" each hidden-layer node is "looking for." These are the high-level features that characterize our data, i.e. stars, stripes and crescents in a dataset of flags.&lt;/li&gt;
&lt;li&gt;Take a composition of existing encodings and generate a composite flag. For example, feed &lt;span class="math"&gt;\(\text{encoding}_{\text{colombia}} + \text{encoding}_{\text{laos}}\)&lt;/span&gt; into the hidden-layer of the network, pass it through to the final layer and see what results.&lt;/li&gt;
&lt;li&gt;Pass a vector of random values into our hidden-layer, pass it through to the final layer and generate a new flag entirely.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A more comprehensive technical primer on sparse auto-encoders is not the premise of this post, as I believe much better resources already exist. Here are a few links I like to get you started:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.deeplearningbook.org/contents/autoencoders.html"&gt;Deep Learning Book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/"&gt;UFLDL: Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ericlwilkinson.com/blog/2014/11/19/deep-learning-sparse-autoencoders"&gt;Deep Learning: Sparse Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following model was trained with a hand-rolled sparse auto-encoder found &lt;a href="https://github.com/cavaunpeu/vanilla-neural-nets/tree/master/vanilla_neural_nets/autoencoder/sparse_autoencoder"&gt;here&lt;/a&gt;. The technical specifications are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Downsize images to &lt;span class="math"&gt;\((68, 102, 3)\)&lt;/span&gt;, which is roughly proportional to the largest bounding box of the originals. Then, flatten to vectors of &lt;span class="math"&gt;\(68 * 102 * 3 = 20808\)&lt;/span&gt; values.&lt;/li&gt;
&lt;li&gt;Network dimensions are &lt;span class="math"&gt;\((20808, 64, 20808)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Learning rate &lt;span class="math"&gt;\(\alpha = .05\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Training for 1000 epochs.&lt;/li&gt;
&lt;li&gt;Sparsity parameter &lt;span class="math"&gt;\(\rho = .25\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Sparsity parameter in loss function &lt;span class="math"&gt;\(\beta = .25\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Initialize weights and biases with Gaussian of &lt;span class="math"&gt;\(\mu = 0\)&lt;/span&gt;, &lt;span class="math"&gt;\(\sigma = \frac{1}{\sqrt{20808}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The full dataset is of size &lt;span class="math"&gt;\((138, 20808)\)&lt;/span&gt;. Yes, it's tiny! Use the first 100 examples for training, the next 20 for validation, and the final 18 for testing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;First, let's see how well our network does. Again, its goal was to learn how to compress an image into a reduced representation containing enough information to recreate the original thereafter.&lt;/p&gt;
&lt;p&gt;Here's an image of the downsized flag of Afghanistan as passed into our network:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/afghanistan_reduced_bitmap.png"/&gt;
&lt;/p&gt;
&lt;p&gt;So, this is as good as we're ever going to do. When we pass this into our network, here's what it predicts:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/reconstructed_afghanistan_reduced_bitmap.png"/&gt;
&lt;/p&gt;
&lt;p&gt;Not terrible. Of course, this could be improved with, squarely, more training data.&lt;/p&gt;
&lt;p&gt;After training our auto-encoder, we solve for the 64 individual images that "maximally activate" each of the 64 "feature detectors," i.e. each of our hidden-layer nodes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedded features" class="img-responsive" src="https://cavaunpeu.github.io/figures/flag_embedding_features.png"/&gt;&lt;/p&gt;
&lt;p&gt;As anticipated, there does in fact appear to be some higher-level "structure" in our flags. In other words, we can now empirically see: a flag is a thing made up of some combination of horizontal stripes, vertical stripes, diagonal crosses, central emblems, the British crest, etc.&lt;/p&gt;
&lt;p&gt;Next, let's pass all images back through our network, obtain the 64-dimensional encoding for each, reduce these encodings into 2-dimensional space via the &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"&gt;TSNE&lt;/a&gt; algorithm, and plot.&lt;/p&gt;
&lt;p&gt;&lt;img alt="tsne plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/country_embeddings_tsne_plot.png"/&gt;&lt;/p&gt;
&lt;p&gt;Points that are close together indicate flags that are visually similar. So, what have we learned (or rather, what human intuition have we corroborated with empirical, numerical evidence)? Notable similarities include:&lt;/p&gt;
&lt;p&gt;Belgium, Chad and Mali:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/belgium_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/chad_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/mali_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p&gt;Malaysia, Liberia and Puerto Rico:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/malaysia_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/liberia_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/puerto_rico_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p&gt;Canada, Denmark and Peru:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/canada_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/denmark_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/peru_flag.jpg"/&gt;
&lt;/p&gt;
&lt;p&gt;Here, we see that similarity is defined not just across one type of feature, but necessarily, across all. Respectively, the above 3 groups seem heavy in: the "3 vertical bars" feature(s), the "stripes" and "thing in the top-left corner" feature(s), and the "cherry red" feature(s). (I include the optional "s" because the features are not particularly easy to identify nor apparently mutually exclusive in the feature map above.)&lt;/p&gt;
&lt;p&gt;Finally, let's generate some new flags. The following images are what happens when we pass the respective composite encodings into the hidden-layer of our auto-encoder, and feed-forward (i.e. pass it through the decoder). The result is then resized back to the original (where more resolution is inherently lost).&lt;/p&gt;
&lt;p&gt;Morocco:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/morocco_generated_flag.png"/&gt;
&lt;/p&gt;
&lt;p&gt;Morocco + Colombia:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/morocco_colombia_generated_flag.png"/&gt;
&lt;/p&gt;
&lt;p&gt;Morocco + Colombia + Malaysia:&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img class="img-responsive" src="https://cavaunpeu.github.io/images/morocco_colombia_malaysia_generated_flag.png"/&gt;
&lt;/p&gt;
&lt;p&gt;If only there were more countries in the world such that I could get more data. But hey, we need fewer borders, not more.&lt;/p&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/cavaunpeu/generate-world-flags"&gt;code&lt;/a&gt; and &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/generate-world-flags/blob/master/generate-world-flags.ipynb"&gt;notebook&lt;/a&gt; for this project can be found in the links.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Docker and Kaggle with Ernie and Bert</title><link href="https://cavaunpeu.github.io/2016/11/22/docker-and-kaggle-with-ernie-and-bert/" rel="alternate"></link><published>2016-11-22T13:39:00-05:00</published><updated>2016-11-22T13:39:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-11-22:/2016/11/22/docker-and-kaggle-with-ernie-and-bert/</id><summary type="html">&lt;p&gt;An introduction to what Docker is and why and how to use it for Kaggle.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is meant to serve as an introduction to what Docker is and why and how to use it for Kaggle. For simplicity, we will primarily speak about Sesame Street and cupcakes in lieu of computers and data.&lt;/p&gt;
&lt;p&gt;One Monday morning, Ernie from the 'Street climbs out from under his red-and-blue pinstriped covers, puts both feet on the ground and opens his bedroom window. He stares out into a bustling metropolis of cookies and fur, straightens his banana-yellow turtleneck, lets out a deep, vigorous, crescent-shaped morning yawn and exclaims aloud: "Today, I'm going to make cupcakes for my dear friend Bert."&lt;/p&gt;
&lt;p&gt;&lt;img alt="ernie and bert" class="img-responsive" src="https://cavaunpeu.github.io/images/ernie_and_bert.png"/&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, Ernie has never made cupcakes before. But no matter! He darts hastily to the kitchen, pulls out a cookbook, organizes the ingredients and turns on his small Easy-Bake oven. "I'll experiment here. I'll make the greatest cupcake known to all stuffed-animal-kind. And when I'm happy with the result, I'll make 50 more," he shouts.&lt;/p&gt;
&lt;p&gt;Hours later, Ernie's work is done: his cupcake - a 3-story stack of blueberry, strawberry and bacon-flavored sub-cakes - is the single best thing he's ever tasted. Far better than anything that fraud Cookie Monster had ever tried! Ernie is thrilled, and sits back in his now-filthy kitchen to admire the result. He thinks to Bert, and wonders how just quickly he can deliver his gift. "Now that I've baked the perfect cupcake, I'll just need to bake 50 more. This shouldn't be that hard. Right?"&lt;/p&gt;
&lt;p&gt;Ernie spins around to look at this Easy-Bake. "Well, that thing only bakes one cupcake at a time. At that rate, 50 would take me days!" Spirits still high, he runs to the local bakery and asks to use their oven - this one much larger. They happily oblige, and Ernie starts baking right away.&lt;/p&gt;
&lt;p&gt;Unfortunately, as he's mixing the ingredients he starts to have problems. The electric mixer breaks. The knife doesn't quite cut the strawberries in just the right way. The measuring cups have an ever-so-slightly different size. Ernie starts to stress. He thought he was at the finish line, but now realizes that he's really just at the start. While Ernie came equipped with the recipe to bake the cupcakes, he notes that he's now using all new tools in a completely new kitchen under completely different circumstances. "Can't a stuffed animal just bake a single cupcake in his small oven, then bring the recipe and ingredients to a bigger oven and bake a bunch more? Why does this need to be so complicated?"&lt;/p&gt;
&lt;h2&gt;Enter Docker&lt;/h2&gt;
&lt;p&gt;In sadness and despair, Ernie wanders to the seaport to clear his mind. There, he comes across hundreds of blue and white, SUV-sized shipping containers and gets a funny idea: "What if I did my baking in there? I'll move all of my tools inside - the cutting board, the knife, the mixer, the utensils - and write the recipe on the inner wall. The only thing missing will be the oven, but I can get that anywhere. That way, using the oven &lt;em&gt;chez moi,&lt;/em&gt; I can continue to bake one cupcake at a time; conversely, using the oven at the bakery I can bake a whole lot more. Perfect. Ernie grabs the first container he sees and races home to pack it full.&lt;/p&gt;
&lt;p&gt;After writing the ingredients on the container's inner wall, Ernie realizes that if he's going to bring this container to the bakery, it better be light. If not, he won't be able to carry it! Therefore, instead of actually including his tools - the knife, the mixer, etc. - he simply writes down the names and numbers of these products and instructions as to where they can be acquired. Similarly, instead of including the actual ingredients for the cupcakes, he expects them to be available at the bakery itself. Then, when the recipe says "take 3 tablespoons of sugar from the cupboard," that sugar will have already been placed in the cupboard itself.&lt;/p&gt;
&lt;h2&gt;Enter Kaggle&lt;/h2&gt;
&lt;p&gt;Baking cupcakes on Sesame Street is a metaphor for building models for Kaggle. Typically, we build small prototypes on our local machine, then temporarily rent a more powerful machine sitting on a farm somewhere in Virginia to do the heavy lifting. In Kaggle competitions, Ernie's initial problem is all too common: even after finding an electric mixer, measuring cups, etc. comparable to his own - i.e. even after installing all those libraries on our remote machine that we had on our local - the environments still weren't quite the same and problems therein arose. Docker containers solve this problem: if we can bake our cupcake once in our kitchen, we can deterministically re-bake it &lt;em&gt;n&lt;/em&gt; times in any kitchen - and preferably one with an oven much more powerful than our own.&lt;/p&gt;
&lt;h2&gt;Enter remote instances&lt;/h2&gt;
&lt;p&gt;A remote instance is the bakery: it is a computer, like ours, that can process data faster and in larger quantities. In other words, it is a kitchen with a much bigger oven.&lt;/p&gt;
&lt;h2&gt;Cooking utensils and ingredients&lt;/h2&gt;
&lt;p&gt;In lieu of including cooking utensils in our container we merely specify which utensils we need and how to acquire them. For a Kaggle competition, this is akin to installing the libraries - pandas, scikit-learn, etc. - necessary for the task at hand. Once more, we do not include these libraries in our container, but instead provide instructions as to where and how to install them. In practice, this often looks like a &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; in our &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In lieu of including ingredients in our container we merely assume they'll be available in our host bakery. This is a bit trickier than it sounds for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our host bakery is several blocks from our home. If we want ingredients to be available in that bakery, we're going to need to physically carry them there in some sense. 2. Even after physically bringing ingredients to the bakery, they still won't be immediately available inside the container. Remember, after bringing our container to the bakery, the cooking that transpires within the container is isolated from the rest of the bakery itself; it interfaces only with the bakery's oven.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a Kaggle competition, how do we make local data available &lt;em&gt;within the container&lt;/em&gt;, &lt;em&gt;on a remote machine?&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Docker Volumes&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://boxboat.com/2016/06/18/docker-data-containers-and-named-volumes/"&gt;Docker Volumes&lt;/a&gt; allow data to be shared between a directory inside of a container and a directory in the local file system of the machine hosting that container. This is akin to Ernie:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Carrying his ingredients to the bakery, along with (but not inside) his container.&lt;/li&gt;
&lt;li&gt;Upon arrival, placing a jar of sugar in a blue bucket in the corner of the room.&lt;/li&gt;
&lt;li&gt;Stipulating that, upon beginning to bake inside of the container at the bakery, ingredients should be shared between the blue bucket in the corner of the room and the cupboard. That way, when the recipe says "get a jar of sugar from the cupboard," Ernie can reach into the cupboard inside of the container and retrieve the jar of sugar &lt;em&gt;from the blue bucket sitting in the corner of the bakery.&lt;/em&gt; Remember: the container did not ship with any ingredients inside; the cupboard, therefore, would have itself been empty.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Carrying the container to the bakery is akin to a simple &lt;code&gt;docker run&lt;/code&gt; onto the remote machine. Carrying ingredients to the bakery, i.e. placing a data file on the local file system of the remote machine, is much less sexy. In the simplest sense, this is akin to using &lt;code&gt;scp&lt;/code&gt; or &lt;code&gt;rsync&lt;/code&gt; to transfer a file from the local machine to the remote, or even using &lt;code&gt;curl&lt;/code&gt; to download a file directly onto the remote machine itself.&lt;/p&gt;
&lt;p&gt;In practice, this often looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;docker&lt;/span&gt;
&lt;span class="err"&gt;    --tlsverify&lt;/span&gt;
&lt;span class="err"&gt;    --tlscacert="$HOME/.docker/machine/certs/ca.pem"&lt;/span&gt;
&lt;span class="err"&gt;    --tlscert="$HOME/.docker/machine/certs/cert.pem"&lt;/span&gt;
&lt;span class="err"&gt;    --tlskey="$HOME/.docker/machine/certs/key.pem" -H=tcp://12.34.56:78&lt;/span&gt;
&lt;span class="err"&gt;run&lt;/span&gt;
&lt;span class="err"&gt;    --rm&lt;/span&gt;
&lt;span class="err"&gt;    -i&lt;/span&gt;
&lt;span class="err"&gt;    -v&lt;/span&gt;
&lt;span class="err"&gt;    /data:/data kaggle-contest build_model.sh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Cooking tools that you can't buy at the store&lt;/h2&gt;
&lt;p&gt;To bake his cupcake, Ernie used a one-of-a-kind cutting board that Bert had hand-molded for him. How can he use this at the bakery? In Kaggle terms: how can I use a library in my project that is not available on a public package repository (i.e. one that I built myself)?&lt;/p&gt;
&lt;p&gt;To this end, there's really no secret sauce. With the cutting board/library, we can either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Include it in our container and deal with the extra weight.&lt;/li&gt;
&lt;li&gt;Treat it as an ingredient, carry it to the bakery, and access it via a Docker Volume.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Happy cooking&lt;/h2&gt;
&lt;p&gt;Moving your local development inside of a Docker container, and/or Dockerizing this local environment once you're ready to use a remote resource to do the heavier lifting, will ensure you only have to figure out how to bake the cupcake once. Prototype locally, then send stress-free to the bakery for mass production.&lt;/p&gt;
&lt;p&gt;Happy cooking.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Additional resources:&lt;/p&gt;
&lt;p&gt;Here's two resources I found very helpful when learning about Docker for Kaggle:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://speakerdeck.com/smly/workflow-serialization-and-docker-for-kaggle"&gt;Workflow, Serialization &amp;amp; Docker for Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2016/02/05/how-to-get-started-with-data-science-in-containers/"&gt;How to get started with data science in containers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="machine-learning"></category></entry><entry><title>While We Were Busy with Prosperity</title><link href="https://cavaunpeu.github.io/2016/11/10/while-we-were-busy-with-prosperity/" rel="alternate"></link><published>2016-11-10T17:10:00-05:00</published><updated>2016-11-10T17:10:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-11-10:/2016/11/10/while-we-were-busy-with-prosperity/</id><summary type="html">&lt;p&gt;Some whirling thoughts on the Trump election.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I address this post to my peers - to my liberal, driven, University-educated and multi-cultural counterparts.&lt;/p&gt;
&lt;p&gt;Like most of you, I spent the day of Donald Trump's election in a state of disbelief, paralysis and exasperation. Like many more, I had several long, critical conversations about what had just happened and where we go from here. In one conversation, a friend exclaimed:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I, admittedly, live in a bubble: I know no Trump supporters. You, Will, have a diverse group of friends: surely you know a few.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While my peers are very diverse in race, culture, gender and geography, I don't believe a single one voted for Trump.&lt;/p&gt;
&lt;p&gt;As it is now so grippingly clear, these people do exist, and in numbers! They are blue collar workers. They are white men and white women from Middle America. They are Latina females in Florida. They are tailgaters at Penn State football games. They are critical thinkers from Ohio. And I don't know a single one.&lt;/p&gt;
&lt;p&gt;&lt;img alt="trump rally" class="img-responsive" src="https://cavaunpeu.github.io/images/trump_rally.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;Republican presidential candidate, Donald Trump, waves to supporters after speaking at a campaign rally at the I-X Center Saturday, March 12, 2016, in Cleveland. (AP Photo/Tony Dejak)[/caption]&lt;/p&gt;
&lt;p&gt;We have spent our adult lives in swift, upward mobility. We went to strong Universities because we went to strong high schools. For many, our parents paid for our education, leaving us as debt-free graduates with little existential challenge beyond navigating the subway map. Thereafter, we moved to great cities and found great jobs. We traveled internationally. We learned to speak Russian, to play the viola, to build ceramic pots on the weekends and take pictures with a DSLR. Moreover, from birth, we were taught that education, self-diversification and intellectual curiosity were normal and cool. Simply, we were taught how to dream.&lt;/p&gt;
&lt;p&gt;While we are not oligarchs with gold watches, we are in no way average. Why? Because average is average. Average is having average ambition. Average is having average intellect. Average is receiving an average education. Average is living in the same town where you grew up, be it rural or urban. Average is average because it is average, and we - my liberal, creative, resilient, radically-capable peers - my entire Facebook feed - by trivial definition, are far from average.&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;So, while we were busy with prosperity, what was the average American doing?&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;
First and foremost, I hardly know. (This, I now resolutely realize, is a major problem; much more on this below.) However, at peripheral speculation, I'd posit the following:&lt;/p&gt;
&lt;p&gt;The average American is in the throes of a rapidly changing world and isn't sure how to react. This is an honest, decent taxi driver who's lost her job to Uber or a factory worker who's lost his job to a machine.&lt;/p&gt;
&lt;p&gt;The average American is a judicious high-school graduate without the resources to attend a strong University, and thereafter obtain a well-paying job. Vacation, mobility, and even food security are not promised.&lt;/p&gt;
&lt;p&gt;The average American is a nurse with no idea why they are working 20-hour shifts 4 times per week.&lt;/p&gt;
&lt;p&gt;Finally, the average American is an honest citizen who expects their government to facilitate a reasonable quality of life, whereby with average work ethic, average ambition and average creativity one can live a healthy, comfortable, average life.&lt;/p&gt;
&lt;p&gt;To me, this seems incredibly reasonable. As we know, this is less and less our reality. The last few Presidents have done very little for these people. They are rational, and they wanted something new. That is a simplified explanation as to why a reckless and woefully under-prepared Donald Trump now finds himself in the most powerful office in the world. So, what do we do from here?&lt;/p&gt;
&lt;h2&gt;Let's get to know each other&lt;/h2&gt;
&lt;p&gt;First and foremost, it is now brutally apparent that we must get to know the other side. Education is power, right? When we want to solve a problem, we first learn as much as we possibly can about the subject in question. Novelly, however, this process may be particularly and incredibly challenging when the object in question spits at your very existence: am I really meant to dialogue with someone flying a Nazi flag? Here, I'm saying yes. I'd like your thoughts as well.&lt;/p&gt;
&lt;h2&gt;Bigotry is but a vehicle for blame&lt;/h2&gt;
&lt;p&gt;We roil Trump and his base for bigotry, but bigotry is but a vehicle for blame. Blame for what, though? For the problems outlined above - for the increasingly elusive promise of a comfortable life for the typical citizen. Trump was elected because he represented a sidestep from the American politics that have underserved our masses for the last quarter-century. Throughout his campaign, he resonated with those people who had real problems that needed solving, all the while using immigrants and minorities as a scapegoat.&lt;/p&gt;
&lt;p&gt;Simply put, the American government is to largely blame for these problems - for perpetuating serving-to-few policies on wealth distribution, education and healthcare. Furthermore, if these problems didn't exist, I personally doubt that the people that screamed "time to go home, Apu" to a Google engineer in Silicon Valley yesterday would harbor blind hatred towards anyone at all, let alone complete strangers.  &lt;/p&gt;
&lt;h2&gt;Let's not go crazy&lt;/h2&gt;
&lt;p&gt;On average, lawnmowers kill &lt;a href="https://s22.postimg.org/xinxs3q01/Screen_Shot_2016_11_10_at_4_25_16_PM.png"&gt;35 times more people&lt;/a&gt; annually than "Islamic jihadist immigrants;" when I hear fears of the latter, I chuckle in basic rationality and instead fear my chances of crossing a Manhattan Avenue. On the same note, we must realize that the scary individual who spray-painted &lt;a href="http://www.philly.com/philly/blogs/clout/400560361.html"&gt;"Sieg Heil 2016"&lt;/a&gt; in downtown Philadelphia on the 78th anniversary of Kristallnacht is an &lt;em&gt;extreme minority&lt;/em&gt;, and while this case and any case of hate and bigotry should be treated extremely seriously, there is not (yet) reason to start running for the hills. Conversely, I would submit that the overwhelming majority of Trump's base is a rational and welcoming human like most of the world, motivated by little more than frustration and basic human selfishness, i.e. wanting the best future for yourself and those you love.&lt;/p&gt;
&lt;h2&gt;Crisis brings opportunity&lt;/h2&gt;
&lt;p&gt;This comes from my friend Adrian, and admittedly, I didn't initially understand what he meant.&lt;/p&gt;
&lt;p&gt;"If Hillary had won, we wouldn't be having this conversation: we would have virtually high-five'd each other and moved on with our days. We wouldn't be discussing political involvement. We wouldn't be discussing the problems in this country! We wouldn't be discussing what to do next."&lt;/p&gt;
&lt;p&gt;In times of "crisis," people are incredibly willing to taking meetings, take phone calls, hear ideas and share their own. We, liberals, now realize there are real problems in our country that we, capable and creative, can perhaps work to solve.&lt;/p&gt;
&lt;h2&gt;The majority of the country voted for Hillary&lt;/h2&gt;
&lt;p&gt;Let's not forget this. Furthermore, the millennial map was blue, blue and blue.&lt;/p&gt;
&lt;h2&gt;Talk is cheap&lt;/h2&gt;
&lt;p&gt;I frequently discuss geopolitics as a point of hobby. However, I've never involved myself in the political process. Presently, I live in Morocco. Let us remember - for as much as I and we offer our thoughts and ideas - talk is very cheap. None of this matters if we don't actually do something. If we don't build tools for education. If we don't facilitate communication between diverse groups of people. If we don't take time out of our days to actively learn about those that supported and support our to-be President, no matter how gut-wrenchingly challenging that may well be.&lt;/p&gt;
&lt;p&gt;As a point of closure, if you take any issue with this post, please let me know. Please tell me I'm ignorant of American realities. Please tell me I'm willfully blind. This is the point. Educate me and let's educate ourselves. No problem was ever really solved without first deeply understanding the problem itself. I, for one, have a lot of things to learn about half of the country in which I was born.&lt;/p&gt;</content><category term="life"></category></entry><entry><title>Recurrent Neural Network Gradients, and Lessons Learned Therein</title><link href="https://cavaunpeu.github.io/2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/" rel="alternate"></link><published>2016-10-18T14:00:00-04:00</published><updated>2016-10-18T14:00:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-10-18:/2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/</id><summary type="html">&lt;p&gt;Recurrent neural network gradients by hand.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've spent the last week hand-rolling recurrent neural networks. I'm currently taking Udacity's Deep Learning &lt;a href="https://www.udacity.com/course/deep-learning--ud730"&gt;course&lt;/a&gt;, and arriving at the section on RNN's and LSTM's, I decided to build a few for myself.&lt;/p&gt;
&lt;h3&gt;What are RNN's?&lt;/h3&gt;
&lt;p&gt;On the outside, recurrent neural networks differ from typical, feedforward neural networks in that they take a &lt;em&gt;sequence&lt;/em&gt; of input instead of an input of fixed length. Concretely, imagine we are training a sentiment classifier on a bunch of tweets. To embed these tweets in vector space, we create a bag-of-words model with vocabulary size 3. In a typical neural network, this implies an input layer of size 3; an input could be &lt;span class="math"&gt;\([4, 9, 3]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([1, 0, 5]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([0, 0, 6]\)&lt;/span&gt;, for example. In a recurrent neural network, our input layer has the same size 3, but instead of just a single size-3 input, we can feed it a sequence of size-3 inputs of any length. For example, an input could be &lt;span class="math"&gt;\([[1, 8, 5], [2, 2, 4]]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([[6, 7, 3], [6, 2, 4], [9, 17, 5]]\)&lt;/span&gt;, or &lt;span class="math"&gt;\([[2, 3, 0], [1, 1, 7], [5, 5, 3], [8, 18, 4]]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;On the inside, recurrent neural networks have a different feedforward mechanism than typical neural networks. In addition, each input in our sequence of inputs is processed individually and chronologically: the first input is fed forward, then the second, and so on. Finally, after all inputs have been fed forward, we compute some gradients and update our weights. Like in feedforward networks, we also use backpropagation. However, we must now backpropagate errors to our parameters at every step in time. In other words, we must compute gradients with respect to: the state of the world when we fed our first input forward, the state of the world when we fed our second input forward, and up until the state of the world when we fed our last input forward. This algorithm is called &lt;a href="https://en.wikipedia.org/wiki/Backpropagation_through_time"&gt;Backpropagation Through Time&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Other Resources, My Frustrations&lt;/h3&gt;
&lt;p&gt;There are many resources for understanding how to compute gradients using Backpropagation Through Time. In my view, &lt;a href="https://www.existor.com/en/ml-rnn.html"&gt;Recurrent Neural Networks Maths&lt;/a&gt; is the most mathematically comprehensive, while &lt;a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"&gt;Recurrent Neural Networks Tutorial Part 3&lt;/a&gt; is more concise yet equally clear. Finally, there exists Andrej Karpathy's &lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;Minimal character-level language model&lt;/a&gt;, accompanying his excellent &lt;a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;blog post&lt;/a&gt; on the general theory and use of RNN's, which I initially found convoluted and hard to understand.&lt;/p&gt;
&lt;p&gt;In all posts, I think the authors unfortunately blur the line between the derivation of the gradients and their (efficient) implementation in code, or at the very least jump too quickly from one to another. They define variables like &lt;code&gt;dbnext&lt;/code&gt;,  &lt;code&gt;delta_t&lt;/code&gt;, and &lt;span class="math"&gt;\(e_{hi}^{2f3}\)&lt;/span&gt; without thoroughly explaining their place in the analytical gradients themselves. As one example, the first post includes the snippet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}} =
e^{t=2f2}_{hi} \frac{\partial h^{t=2}_i}{\partial z^{t=2}_{hi}} \frac{\partial z^{t=2}_{hi}}{\partial w^{xh}_{mi}} +
e^{t=1f2}_{hi} \frac{\partial h^{t=1}_i} {\partial z^{t=1}_{hi}} \frac{\partial z^{t=1}_{hi}}{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So far, he's just talking about analytical gradients. Next, he gives hint to the implementation-in-code that follows.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So the thing to note is that we can delay adding in the backward propagated errors until we get further into the loop. In other words, we can initially compute the derivatives of &lt;em&gt;J&lt;/em&gt; with respect to the third unrolled network with only the first term:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=3}}{\partial w^{xh}_{mi}} =
e^{t=3f3}_{hi} \frac{\partial h^{t=3}_i}{\partial z^{t=3}_{hi}} \frac{\partial z^{t=3}_{hi}}{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;And then add in the other term only when we get to the second unrolled
network:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}} =
(e^{t=2f3}_{hi} + e^{t=2f2}_{hi}) \frac{\partial h^{t=2}_i}{\partial z^{t=2}_{hi}}
\frac{\partial z^{t=2}_{hi}}
{\partial w^{xh}_{mi}}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note the opposing definitions of the variable &lt;span class="math"&gt;\(\frac{\partial J^{t=2}}{\partial w^{xh}_{mi}}\)&lt;/span&gt;. As far as I know, the latter is, in a vacuum, categorically false. This said, I believe the author is simply providing an alternative definition of this quantity in line with a computational shortcut he later takes.&lt;/p&gt;
&lt;p&gt;Of course, these ambiguities become very emotional, very quickly. I myself was confused for two days. As such, the aim of this post is to derive recurrent neural network gradients from scratch, and emphatically clarify that all implementation "shortcuts" thereafter are nothing more than just that, with no real bearing on the analytical gradients themselves. In other words, if you can derive the gradients, you win. Write a unit test, code these gradients in the crudest way you can, watch your test pass, and then immediately realize that your code can be made more efficient. At this point, all "shortcuts" that the above authors (and myself, now, as well) take in their code will make perfect sense.&lt;/p&gt;
&lt;h3&gt;Backpropagation Through Time&lt;/h3&gt;
&lt;p&gt;In the simplest case, let's assume our network has 3 layers, and just 3 parameters to optimize: &lt;span class="math"&gt;\(\mathbf{W^{xh}}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt;. The foundational equations of this network are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{z_t} = \mathbf{W^{xh}}\mathbf{x} + \mathbf{W^{hh}}\mathbf{h_{t-1}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{h_t} = \tanh(\mathbf{z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{y_t} = \mathbf{W^{hy}}\mathbf{h_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{p_t} = \text{softmax}(\mathbf{y_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathbf{J_t} = \text{crossentropy}(\mathbf{p_t},
    \mathbf{\text{labels}_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I've written "softmax" and "cross-entropy" for clarity: before tackling the math below, it is important to understand what they do, and how to derive their gradients by hand.&lt;/p&gt;
&lt;p&gt;Before moving forward, let's restate the definition of a partial derivative itself.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A partial derivative, for example &lt;span class="math"&gt;\(\frac{\partial y}{\partial x}\)&lt;/span&gt;, measures how much &lt;span class="math"&gt;\(y\)&lt;/span&gt; increases with every 1-unit increase in &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our cost &lt;span class="math"&gt;\(\mathbf{J_t}\)&lt;/span&gt; is the &lt;em&gt;total&lt;/em&gt; &lt;em&gt;cost&lt;/em&gt; (i.e., not the average cost) of a given sequence of inputs. As such, a 1-unit increase in &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; will impact each of &lt;span class="math"&gt;\(\mathbf{J_1}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{J_2}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; individually. Therefore, our gradient is equal to the sum of the respective gradients at each time step &lt;span class="math"&gt;\(t\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{hy}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{hy}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{hy}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{hh}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{hh}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{hh}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{hh}}}\\
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}} =
\sum\limits_t \frac{\partial \mathbf{J_t}}{\partial
\mathbf{W^{xh}}} = \frac{\partial \mathbf{J_3}}{\partial
\mathbf{W^{xh}}} + \frac{\partial \mathbf{J_2}}{\partial
\mathbf{W^{xh}}} + \frac{\partial \mathbf{J_1}}{\partial
\mathbf{W^{xh}}}
$$&lt;/div&gt;
&lt;p&gt;Let's take this piece by piece.&lt;/p&gt;
&lt;h3&gt;Algebraic Derivatives&lt;/h3&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Starting with &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}}\)&lt;/span&gt;, we note that a change in &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; will only impact &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;: &lt;span class="math"&gt;\(\mathbf{W^{hy}}\)&lt;/span&gt; plays no role in computing the value of anything other than &lt;span class="math"&gt;\(\mathbf{y_3}\)&lt;/span&gt;. Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{W^{hy}}}\\
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{hy}}} =
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{W^{hy}}}\\
$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Starting with &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hh}}}\)&lt;/span&gt;, a change in &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; will impact our cost &lt;span class="math"&gt;\(\mathbf{J_3}\)&lt;/span&gt; in &lt;em&gt;3 separate ways:&lt;/em&gt; once, when computing the value of &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;; once, when computing the value of &lt;span class="math"&gt;\(\mathbf{h_2}\)&lt;/span&gt;, which depends on &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;; once, when computing the value of &lt;span class="math"&gt;\(\mathbf{h_3}\)&lt;/span&gt;, which depends on &lt;span class="math"&gt;\(\mathbf{h_2}\)&lt;/span&gt;, which depends on &lt;span class="math"&gt;\(\mathbf{h_1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;More generally, a change in &lt;span class="math"&gt;\(\mathbf{W^{hh}}\)&lt;/span&gt; will impact our cost &lt;span class="math"&gt;\(\mathbf{J_t}\)&lt;/span&gt; on &lt;span class="math"&gt;\(t\)&lt;/span&gt; separate occasions. Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hh}}} =
\sum\limits_{k=0}^{t} \frac{\partial \mathbf{J_t}}{\partial
\mathbf{h_t}} \frac{\partial \mathbf{h_t}}{\partial
\mathbf{h_k}} \frac{\partial \mathbf{h_k}}{\partial
\mathbf{z_k}} \frac{\partial \mathbf{z_k}}{\partial
\mathbf{W^{hh}}}
$$&lt;/div&gt;
&lt;p&gt;Then, with this definition, we compute our individual gradients as:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{hh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{hh}}} &amp;amp;=
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{hh}}}
\end{align*}
$$&lt;/div&gt;
&lt;h4&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;:&lt;/h4&gt;
&lt;p&gt;Similarly:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{xh}}} =
\sum\limits_{k=0}^{t} \frac{\partial \mathbf{J_t}}{\partial
\mathbf{h_t}} \frac{\partial \mathbf{h_t}}{\partial
\mathbf{h_k}} \frac{\partial \mathbf{h_k}}{\partial
\mathbf{z_k}} \frac{\partial \mathbf{z_k}}{\partial
\mathbf{W^{xh}}}
$$&lt;/div&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_3}}{\partial \mathbf{p_3}}
\frac{\partial \mathbf{p_3}}{\partial \mathbf{y_3}}
\frac{\partial \mathbf{y_3}}{\partial \mathbf{h_3}}
\frac{\partial \mathbf{h_3}}{\partial \mathbf{z_3}}
\frac{\partial \mathbf{z_3}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{W^{xh}}}\\ &amp;amp;+
\frac{\partial \mathbf{J_2}}{\partial \mathbf{p_2}}
\frac{\partial \mathbf{p_2}}{\partial \mathbf{y_2}}
\frac{\partial \mathbf{y_2}}{\partial \mathbf{h_2}}
\frac{\partial \mathbf{h_2}}{\partial \mathbf{z_2}}
\frac{\partial \mathbf{z_2}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{xh}}} &amp;amp;=
\frac{\partial \mathbf{J_1}}{\partial \mathbf{p_1}}
\frac{\partial \mathbf{p_1}}{\partial \mathbf{y_1}}
\frac{\partial \mathbf{y_1}}{\partial \mathbf{h_1}}
\frac{\partial \mathbf{h_1}}{\partial \mathbf{z_1}}
\frac{\partial \mathbf{z_1}}{\partial \mathbf{W^{xh}}}
\end{align*}
$$&lt;/div&gt;
&lt;h3&gt;Analytical Derivatives&lt;/h3&gt;
&lt;p&gt;Finally, we plug in the individual partial derivates to compute our final gradients, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{y_t}} = \mathbf{p_t} - \mathbf{\text{labels}_t}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\mathbf{\text{labels}_t}\)&lt;/span&gt; is a one-hot vector of the correct answer at a given time-step &lt;span class="math"&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{W^{hy}}} = (\mathbf{p_t} - \mathbf{\text{labels}_t})\mathbf{h_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{J_t}}{\partial \mathbf{h_t}} = (\mathbf{p_t} - \mathbf{\text{labels}_t})\mathbf{W^{hy}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{h_t}}{\partial \mathbf{z_t}} = 1 - \tanh^2(\mathbf{z_t}) = 1 - \mathbf{h_t}^2\)&lt;/span&gt;, as &lt;span class="math"&gt;\(\mathbf{h_t} = \tanh(\mathbf{z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{z_t}}{\mathbf{h_{t-1}}} = \mathbf{W^{hh}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{\partial \mathbf{z_t}}{\partial \mathbf{W^{xh}}} = \mathbf{x_t}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\frac{z_t}{\partial \mathbf{W^{hh}}} = \mathbf{h_{t-1}}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, you're done: you've computed your gradients, and you understand Backpropagation Through Time. From this point forward, all that's left is writing some for-loops.&lt;/p&gt;
&lt;h3&gt;Implementation Shortcuts&lt;/h3&gt;
&lt;p&gt;As you'll readily note, when computing the gradient for, for example, &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, we'll need access to our labels at time-steps &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, &lt;span class="math"&gt;\(t=2\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. For &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_2}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, we'll need our labels at time-steps &lt;span class="math"&gt;\(t=2\)&lt;/span&gt; and &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Finally, for &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_1}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, we'll need our labels at just &lt;span class="math"&gt;\(t=1\)&lt;/span&gt;. Naturally, we look to make this efficient: for, for example, &lt;span class="math"&gt;\(\frac{\partial \mathbf{J_3}}{\partial \mathbf{W^{xh}}}\)&lt;/span&gt;, how about just compute the &lt;span class="math"&gt;\(t=3\)&lt;/span&gt; parts at &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, and add in the rest at &lt;span class="math"&gt;\(t=2\)&lt;/span&gt;? Instead of explaining further, I leave this step to you: it is ultimately trivial, a good exercise, and when you're finished, you'll find that your code readily resembles much of that written in the above resources.&lt;/p&gt;
&lt;h3&gt;Lessons Learned&lt;/h3&gt;
&lt;p&gt;Throughout this process, I learned a few lessons.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When implementing neural networks from scratch, derive gradients by hand at the outset. &lt;em&gt;This makes thing so much easier.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Turn more readily to your pencil and paper before writing a single line of code. They are not scary and they absolutely have their place.&lt;/li&gt;
&lt;li&gt;The chain rule remains simple and clear. If a derivative seems to "supercede" the general difficulty of the chain rule, there's probably something else you're missing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Happy RNN's.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Key references for this article include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"&gt;Recurrent Neural Networks Tutorial Part 2 Implementing A Rnn With Python Numpy And Theano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"&gt;Recurrent Neural Networks Tutorial Part 3 Backpropagation Through Time And Vanishing Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.existor.com/en/ml-rnn.html"&gt;Machine Learning - Recurrent Neural Networks Maths&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Simulating the Colombian Peace Vote: Did the "No" Really Win?</title><link href="https://cavaunpeu.github.io/2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win/" rel="alternate"></link><published>2016-10-12T14:58:00-04:00</published><updated>2016-10-12T14:58:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-10-12:/2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win/</id><summary type="html">&lt;p&gt;A post-mortem statistical simulation of the &lt;a href="https://en.wikipedia.org/wiki/Colombian_peace_agreement_referendum,_2016"&gt;2016 Colombian plebiscite&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;On October 2nd, 2016, I watched in awe as Colombia's national plebiscite for its just-signed peace accord narrowly failed. For the following week, I brooded over the result: the disinformation campaign, Uribe's antics, and just how good the &lt;a href="https://www.youtube.com/playlist?list=PLa28R7QEiMblKeZ_OlZ_XfjjxjfeIhpuL"&gt;deal&lt;/a&gt; really seemed to be. Two days ago, I chanced upon this &lt;a href="https://theconversation.com/colombia-did-not-vote-no-in-its-peace-referendum-what-the-statistics-reveal-66471"&gt;post&lt;/a&gt;, which reminds us that the razor-thin margin - 6,431,376 "No" vs. 6,377,482 "Yes" - is not particularly convincing, nor, as it happens, immune to human error.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;And as with all manual voting systems, one cannot rule out at least some degree of misclassification of papers on some scale, no matter how small. We know of no evidence of cheating, and Colombia is to be lauded for the seriousness of its referendum process, but the distinction between intentional and unintentional misclassification by individual counters can occasionally become blurred in practice.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, it was humans - tired humans - counting ballots by hand.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The technology of tired humans sorting pieces of paper into four stacks is, at best, crude. As a large research literature has made clear, we can reasonably assume that even well-rested people would have made mistakes with between 0.5% and 1% of the ballots. On this estimate, about 65,000-130,000 votes would have been unintentionally misclassified. It means the number of innocent counting errors could easily be substantially larger than the 53,894 yes-no difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Is it possible that the majority wanted "Yes" and still happened to lose?&lt;/p&gt;
&lt;p&gt;&lt;img alt="plebiscite vote" class="img-responsive" src="https://cavaunpeu.github.io/images/colombian_plebiscite_vote.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;To answer this question, we can frame the vote as a simple statistical process and ask: "if we were to re-hold the vote many more times, how often would the 'Yes' vote actually win?"&lt;/p&gt;
&lt;p&gt;Should we choose, we could pursue this result analytically, i.e. solve the problem with a pencil and paper. This get messy quickly. Instead, we'll disregard closed-form theory and run a basic simulation; &lt;a href="https://speakerdeck.com/jakevdp/statistics-for-hackers"&gt;"if you can write a for-loop, you can do statistics."&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We'll frame our problem as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(V_t=13,066,047\)&lt;/span&gt; voters arrive to the polls.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{\text{yes}}\%\)&lt;/span&gt; of them intend to vote "Yes", &lt;span class="math"&gt;\((1-p_{\text{yes}})\%\)&lt;/span&gt; of them intend to vote "No."&lt;/li&gt;
&lt;li&gt;Each voter casts an invalid (unmarked or void) ballot with probability &lt;span class="math"&gt;\(p_{\text{invalid}}\%\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Of the valid ballots, the poll workers misclassify the vote with probability &lt;span class="math"&gt;\(p_{\text{misclassification}}\%\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Majority vote wins.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;YES_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6377482&lt;/span&gt;
&lt;span class="n"&gt;NO_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6431376&lt;/span&gt;
&lt;span class="n"&gt;UNMARKED_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;86243&lt;/span&gt;
&lt;span class="n"&gt;NULL_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;170946&lt;/span&gt;

&lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;YES_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;NO_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;UNMARKED_BALLOTS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;NULL_BALLOTS&lt;/span&gt;
&lt;span class="n"&gt;P_INVALID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;
&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;
&lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In each trial, we assume a true, underlying &lt;span class="math"&gt;\(p_{\text{yes}}\%\)&lt;/span&gt; for the voting populace. For example, if &lt;span class="math"&gt;\(p_{\text{yes}}\)&lt;/span&gt; is .48, we will have &lt;span class="math"&gt;\(V_t * p_{\text{yes}}\)&lt;/span&gt; individuals intending to vote "Yes," and &lt;span class="math"&gt;\(V_t * (1-p_{\text{yes}})\)&lt;/span&gt; voters intending to vote "No." We assume these values to be static: they are not generated by a random process.&lt;/p&gt;
&lt;p&gt;Next, each voter casts an invalid ballot with probability &lt;span class="math"&gt;\(p_{\text{invalid}}\)&lt;/span&gt;, which we model as a Binomial random variable. Each remaining, valid ballot is then misclassified with probability &lt;span class="math"&gt;\(p_{\text{misclassification}}\)&lt;/span&gt;. Finally, the tallies of "Yes" and "No" votes are counted, and the percentage of "Yes" votes is returned.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TOTAL_VOTES&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_votes&lt;/span&gt;

    &lt;span class="n"&gt;yes_votes_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;yes_votes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;no_votes_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N_TRIALS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;no_votes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;invalid_ballots_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;yes_votes_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_INVALID&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;invalid_ballots_no&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;no_votes_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;P_INVALID&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;valid_yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yes_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;invalid_ballots_yes&lt;/span&gt;
    &lt;span class="n"&gt;valid_no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;invalid_ballots_no&lt;/span&gt;

    &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_yes_votes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;no_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valid_yes_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt;

    &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_no_votes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;P_MISCLASSIFICATION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;yes_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;valid_no_votes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt;

    &lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_yes_voters&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;yes_votes_from_no_voters&lt;/span&gt;
    &lt;span class="n"&gt;tallied_no_votes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_no_voters&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;no_votes_from_yes_voters&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tallied_yes_votes&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tallied_no_votes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's try this out for varying values of &lt;span class="math"&gt;\(p_{\text{yes}}\)&lt;/span&gt;. To start, if the true, underlying percentage of "Yes" voters were 51%, how often would the "No" vote still win?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's comforting. Given our assumptions, if 51% of the Colombian people arrived at the polls intending to vote "Yes," the "No" vote would have nonetheless won in 0 of 100,000 trials. So, how close can we get before we start seeing backwards results?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;probability_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;
    &lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;simulate_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;proportion_of_trials_won_by_no&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;percentage_of_tallied_votes_that_were_yes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"p_yes: &lt;/span&gt;&lt;span class="si"&gt;{:1.6f}&lt;/span&gt;&lt;span class="s2"&gt;% | no_win_percentage: &lt;/span&gt;&lt;span class="si"&gt;{:1.3f}&lt;/span&gt;&lt;span class="s2"&gt;%"&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;probability_yes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;proportion_of_trials_won_by_no&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;60.000000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;51.000000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.100000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.010000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.191&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.001000&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;38.688&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.000100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;48.791&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;p_yes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.000010&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;no_win_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;50.063&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our first frustration comes at &lt;span class="math"&gt;\(p_{\text{yes}} = .5001\)&lt;/span&gt;: if &lt;span class="math"&gt;\(V_t * p_{\text{yes}} = 13,066,047 * .5001 \approx 6,534,330\)&lt;/span&gt; voters wanted "Yes" vs. &lt;span class="math"&gt;\(\approx 6,531,716\)&lt;/span&gt; who wanted "No," the "No" vote would have still won &lt;span class="math"&gt;\(0.191\%\)&lt;/span&gt; of the time. Again, this reversal derives from human error: both on the part of the voter in casting an invalid ballot, and on the part of the the poll-worker incorrectly classifying that ballot by hand.&lt;/p&gt;
&lt;p&gt;As we move further down, the results get tighter. At &lt;span class="math"&gt;\(p_{\text{yes}} = .50001\)&lt;/span&gt;, the "Yes" vote can only be expected to have won &lt;span class="math"&gt;\(1 - .38688 = 61.312\%\)&lt;/span&gt; of the time. Finally, at &lt;span class="math"&gt;\(p_{\text{yes}} = .5000001\)&lt;/span&gt; (which, keep in mind, implies an "I intend to vote 'Yes'" vs. "I intend to vote 'No'" differential of just &lt;span class="math"&gt;\(13,066,047 * (p_{\text{yes}} - (1 - p_{\text{yes}})) \approx 3\)&lt;/span&gt; voters), the "No" vote actually wins the &lt;em&gt;majority&lt;/em&gt; of the 100,000 hypothetical trials. At that point, we're really just flipping coins.&lt;/p&gt;
&lt;p&gt;In summary, as the authors of the above post suggest, it would be statistically irresponsible to claim a definitive win for the "No." Conversely, the true, underlying margin does prove to be extremely tight: maybe a majority vote just isn't the best way to handle these issues after all.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/colombia-vote-simulation/blob/master/colombia-vote-simulation.ipynb"&gt;notebook&lt;/a&gt; and &lt;a href="https://github.com/cavaunpeu/colombia-vote-simulation"&gt;repo&lt;/a&gt; for the analysis can be found here.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Key references include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://theconversation.com/colombia-did-not-vote-no-in-its-peace-referendum-what-the-statistics-reveal-66471"&gt;Colombia did not vote ‘no’ in its peace referendum – what the statistics reveal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://andrewgelman.com/2016/10/04/did-colombia-really-vote-no-in-that-peace-referendum/"&gt;Did Colombia really vote no in that peace referendum?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://plebiscito.registraduria.gov.co/99PL/DPLZZZZZZZZZZZZZZZZZ_L1.htm"&gt;Plebiscito Site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>My Open-Source Machine Learning Masters (in Casablanca, Morocco)</title><link href="https://cavaunpeu.github.io/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/" rel="alternate"></link><published>2016-07-29T15:23:00-04:00</published><updated>2016-07-29T15:23:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-07-29:/2016/07/29/my-open-source-machine-learning-masters-in-casablanca-morocco/</id><summary type="html">&lt;p&gt;A 9-month self-curated deep-dive into select topics in machine learning and distributed computing.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Open-Source Machine Learning Masters (OSMLM) is a self-curated deep-dive into select topics in machine learning and distributed computing. Educational resources are derived from online courses (&lt;a href="https://en.wikipedia.org/wiki/Massive_open_online_course"&gt;MOOCs&lt;/a&gt;), textbooks, predictive modeling competitions, academic research (&lt;a href="https://arxiv.org/"&gt;arXiv&lt;/a&gt;), and the open-source software community. In machine learning, both the quantity and quality of these resources - all available for free or at a trivial cost - is truly f*cking amazing.&lt;/p&gt;
&lt;h2&gt;Why Am I Doing This?&lt;/h2&gt;
&lt;p&gt;I want to become more of a technical expert in machine learning. I want to use this expertise to solve real-world problems that actually matter (moving the field forward on purely theoretical grounds is less interesting to me).&lt;/p&gt;
&lt;p&gt;To this end, I see two main roads: a traditional graduate program, and the OSMLM.&lt;/p&gt;
&lt;h3&gt;Why Not Graduate School&lt;/h3&gt;
&lt;p&gt;For me, graduate school is suboptimal for 3 key reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;It's expensive.&lt;/strong&gt; Upon a quick Google search, a 2-year graduate program would cost, conservatively, $80,000 in tuition fees alone. This is a wholly nontrivial sum of money that would impact how I structure the next 10 years of my life.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;There are &lt;em&gt;far&lt;/em&gt; more dependencies.&lt;/strong&gt; I have to apply. I have to get accepted. I have to find the right professor. I have to find a city suitable to my broader interests and lifestyle. This takes time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;By the time I finish, the field of machine learning will look fundamentally different than it did when I started.&lt;/strong&gt; This is the most important point of all. The only way to remain current with the latest tools and techniques is to do just that. Given the furious and only-accelerating-faster pace at which machine learning is moving, this requires much more than just a few hours on the weekend.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Why the OSMLM&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;I think the higher education paradigm is changing.&lt;/strong&gt; Access to critical, academic knowledge is increasingly democratic: &lt;a href="https://www.khanacademy.org/"&gt;Khan Academy&lt;/a&gt; can teach me about the Central Limit Theorem as well as any statistics professor. The ~$250,000 in tuition fees commanded by an undergraduate education at a private American university is, for some, several decades of debt and concession, and for others, prohibitive beyond comedy, reason and fantasy alike. If hard-skills are your end, online self-education is an immensely attractive, intuitive, and practical road to follow - especially in an industry as meritocratic as tech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I'm keenly aware of how productive I am in a self-teaching environment.&lt;/strong&gt; I'm largely self-taught in data science. Before that, it was online poker: a 5-year, $50 to $150,000 journey of instructional videos, online forums, critical discussion with other players and personal coaching - all from the comfort of my bedroom. I'm very effective at learning things online.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some of the most impactful projects I've completed professionally stemmed directly from those I'd completed personally.&lt;/strong&gt; I would not know how to ensemble models if not for &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;. I would not know how to perform hierarchical Bayesian inference if not for &lt;a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/"&gt;Bayesian Methods for Hackers&lt;/a&gt;. The open-source data science community continues to teach me creative ways to use data to solve challenging problems. To this end, I want to consume, consume, consume.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The road to further technical expertise is a function of little more than time and effort.&lt;/strong&gt; I have a few years' industry experience as a Data Scientist. I can write clean code and productionize machine learning things. For me, the OSMLM is nothing more than taking all of the extra-curricular time spent learning new tools and algorithms and making it a full-time job.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I'm extremely motivated.&lt;/strong&gt; The thought of studying machine learning all day has me smiling from ear to ear. Simply put, I f*cking love this stuff.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;How Long is the OSMLM?&lt;/h2&gt;
&lt;p&gt;9-12 months. Not forever.&lt;/p&gt;
&lt;h2&gt;Why Morocco?&lt;/h2&gt;
&lt;p&gt;I aim to speak indistinguishably fluent French and Spanish by the time I'm 30. I'm currently 27. The Spanish box is largely &lt;a href="https://www.youtube.com/watch?v=xqO0KW3O9uU"&gt;checked&lt;/a&gt;. With 6-9 months in Francophone Morocco, the French box will be largely checked as well.&lt;/p&gt;
&lt;p&gt;Furthermore, I've always wanted to live in a Muslim country: I grew up in a predominantly Jewish suburb of Philadelphia, and have had fantastic experiences traveling the Muslim world.&lt;/p&gt;
&lt;h2&gt;How Will I Spend My Time?&lt;/h2&gt;
&lt;p&gt;I'll be spending my best 8-10 hours of the day working from a co-working space. I'll be taking online courses, reading textbooks, participating in machine learning competitions and publishing open-source code. I intend to post frequently to this blog.&lt;/p&gt;
&lt;h2&gt;What Will I Learn?&lt;/h2&gt;
&lt;p&gt;I have 4 main areas of focus:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;"Deep Learning" with flavors of: auto-encoders, recommendation, and natural language processing.&lt;/strong&gt; I remain obsessed with encoding real-world entities as lists of numbers. I like applications that seek to understand people better than they understand themselves. Free-form text is everywhere (and relatively quick to process).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian Inference.&lt;/strong&gt; Because they taught me frequentist statistics in school.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game Theory and Reinforcement Learning.&lt;/strong&gt; I wrote an &lt;a href="https://honors.libraries.psu.edu/catalog/1947"&gt;undergraduate thesis&lt;/a&gt; in game theory and group dynamics and remain eager to tackle more. Reinforcement Learning seems like the hipster way to solve such problems these days.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Apache Spark and Distributed Computing.&lt;/strong&gt; I have a bit of professional experience with Spark. As data continues to grow in size, distributed computing will move from a thing Google does to a no-duh occupational necessity.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;What Does Success Look Like?&lt;/h2&gt;
&lt;p&gt;Success has a few faces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Technical.&lt;/strong&gt; Have the technical expertise to lead teams focused on each of the above 4 topics (weighted towards the former 3, realistically).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personal.&lt;/strong&gt; Learning how I best learn. How do I structure my ideal working day? Do I prefer working alone, or indeed as part of a team? What is my optimal balance of reading, thinking, and coding?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language.&lt;/strong&gt; I intend to speak French like it's my mother tongue.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;What Happens Afterwards?&lt;/h2&gt;
&lt;p&gt;I'm likely headed back to the Americas, where I intend to devote myself to an impossibly awesome technology project and team for a period of several years. I'd like a technical mentor as well.&lt;/p&gt;
&lt;h2&gt;How Can You Help?&lt;/h2&gt;
&lt;p&gt;In addition to self-study, I'd like to assist a few fascinating Moroccan technology organizations with their data problems. As such, if you know anyone in-country with even the most fleeting shared interest, please put me in touch.&lt;/p&gt;
&lt;h2&gt;In Two Sentences&lt;/h2&gt;
&lt;p&gt;The Open-Source Machine Learning Masters in Casablanca, Morocco allows me to pursue several significant personal goals at the same time. This is my Francophone machine learning adventure.&lt;/p&gt;</content><category term="life"></category></entry><entry><title>Vanilla Neural Nets</title><link href="https://cavaunpeu.github.io/2016/05/15/vanilla-neural-nets/" rel="alternate"></link><published>2016-05-15T21:27:00-04:00</published><updated>2016-05-15T21:27:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-05-15:/2016/05/15/vanilla-neural-nets/</id><summary type="html">&lt;p&gt;A Python library for some canonical neural networks, with a self-righteous emphasis on readability.&lt;/p&gt;</summary><content type="html">&lt;p&gt;To better understand neural networks, I've decided to implement &lt;a href="https://github.com/cavaunpeu/vanilla-neural-nets"&gt;several from scratch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Currently, this project contains a feedforward neural network with sigmoid activation functions, and both mean squared error and cross-entropy loss functions. Because everything is an object, adding future activation functions, loss functions, and optimization routines should be a breeze (in theory, of course; in practice, is this ever the case?).&lt;/p&gt;
&lt;p&gt;In addition to being highly compose-able, this project is highly readable. Too often, data science code is a veritable circus of variables named "wtxb", six-times-nested for loops, and 80-line functions. The code within is both explicit and straightforward; few readers should be left wondering: "what the f*ck does that variable mean?"&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;Code can be found &lt;a href="https://github.com/cavaunpeu/vanilla-neural-nets"&gt;here&lt;/a&gt;. An example &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/vanilla-neural-nets/blob/master/examples/mnist.ipynb"&gt;notebook&lt;/a&gt; is included. This is an ongoing project: I intend to add more loss functions, activation functions, convolutional and recurrent neural networks, and other optimization improvements in due time.&lt;/p&gt;</content><category term="machine-learning"></category></entry><entry><title>Single Neuron Gradient Descent</title><link href="https://cavaunpeu.github.io/2016/05/06/single-neuron-gradient-descent/" rel="alternate"></link><published>2016-05-06T00:42:00-04:00</published><updated>2016-05-06T00:42:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-05-06:/2016/05/06/single-neuron-gradient-descent/</id><summary type="html">&lt;p&gt;Performing gradient descent on a single neuron.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my experience, the gap between a conceptual understanding of how a machine learning model "learns" and a concrete, "I can do this with a pencil and paper" understanding is large. This gap is further exacerbated by the nature of popular machine learning libraries which allow you to use powerful models without knowing how they really work. This isn't such a bad thing. But knowledge is power. In this post, I aim to close the gap above for a vanilla neural network that learns by gradient descent: we will use gradient descent to learn a weight and a bias for a single neuron. From there, when learning an entire network of millions of neurons, we just do the same thing a bunch more times. The rest is details. The following assumes a cursory knowledge of &lt;a href="https://en.wikipedia.org/wiki/Linear_combination"&gt;linear combinations&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Activation_function"&gt;activation functions&lt;/a&gt;, &lt;a href="https://class.coursera.org/ml-005/lecture/6"&gt;cost functions&lt;/a&gt;, and how they all fit together in &lt;a href="https://www.youtube.com/watch?v=UJwK6jAStmg"&gt;forward propagation&lt;/a&gt;. It is math-heavy with some Python interspersed.&lt;/p&gt;
&lt;h2&gt;Problem setup&lt;/h2&gt;
&lt;p&gt;Our neuron looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="single_neuron_gradient_descent" class="img-responsive" src="https://cavaunpeu.github.io/images/single_neuron_gradient_descent.png"/&gt;&lt;/p&gt;
&lt;p&gt;Our parameters look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ACTIVATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;INITIAL_WEIGHT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;INITIAL_BIAS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;ACTUAL_OUTPUT_NEURON_ACTIVATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;N_EPOCHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Let's work backwards&lt;/h2&gt;
&lt;p&gt;We have an initial weight and bias of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After each iteration of gradient descent, we update these parameters via:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weight_gradient&lt;/span&gt;
&lt;span class="n"&gt;bias&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;bias_gradient&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is where the "learning" is concretized: changing a weight and a bias to a different weight and bias that makes our network better at prediction. So: how do we obtain the &lt;code&gt;weight_gradient&lt;/code&gt; and &lt;code&gt;bias_gradient&lt;/code&gt;? More importantly, &lt;em&gt;why do we want these things in the first place?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Why we want the gradient&lt;/h2&gt;
&lt;p&gt;I'll be keeping this simple because it is simple. Our initial weight (&lt;span class="math"&gt;\(w_0 = 3\)&lt;/span&gt;) and bias (&lt;span class="math"&gt;\(b_0 = 2\)&lt;/span&gt;) were chosen randomly. As such, our network will likely make terrible predictions. By definition, our cost will be high. We want to make our cost low. Let's pick a new weight and bias that change this cost by &lt;span class="math"&gt;\(\Delta C\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\Delta C\)&lt;/span&gt; is some strictly negative number. For our weight: Define &lt;span class="math"&gt;\(\Delta C\)&lt;/span&gt; as "how much our cost changes with respect to a 1 unit change in our weight" times "how much we changed our weight". In math, that looks like:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*} \Delta C &amp;amp;=\frac{\partial C}{\partial w} (w_{i+1} - w_i)\\
&amp;amp;=\nabla C \cdot \Delta w\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Our goal is to make &lt;span class="math"&gt;\(\Delta C\)&lt;/span&gt; strictly negative, such that every time we update our weight, we do so in a way that lowers our cost. Duh. Let's choose &lt;span class="math"&gt;\(\Delta w = -\eta\ \nabla C\)&lt;/span&gt;. Our previous expression becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*} \Delta C &amp;amp;=\nabla C \cdot \Delta w\\
&amp;amp;=\nabla C \cdot (-\eta\ \nabla C)\\
&amp;amp;=-\eta\|\nabla C\|\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\|\nabla C\|\)&lt;/span&gt; is strictly positive, and a positive number multiplied by a negative number (&lt;span class="math"&gt;\(-\eta\)&lt;/span&gt;) is strictly negative. So, by choosing &lt;span class="math"&gt;\(\Delta w = -\eta\ \nabla C\)&lt;/span&gt;, our &lt;span class="math"&gt;\(\Delta C\)&lt;/span&gt; is always negative; in other words, at each iteration of gradient descent - in which we perform &lt;code&gt;weight += delta_weight&lt;/code&gt;, a.k.a. &lt;code&gt;weight += -LEARNING_RATE * weight_gradient&lt;/code&gt; - our cost always goes down. Nice.&lt;/p&gt;
&lt;p&gt;For our bias, it's the very same thing.&lt;/p&gt;
&lt;h2&gt;Deriving &lt;span class="math"&gt;\(\frac{\partial C}{\partial w}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial C}{\partial b}\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Deriving both &lt;span class="math"&gt;\(\frac{\partial C}{\partial w}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial C}{\partial b}\)&lt;/span&gt; is pure 12th grade calculus. Plain and simple. If you forget your 12th grade calculus, spend ~2 minutes refreshing your memory with an article online. It's not difficult. Before we begin, we must first define a cost function and an activation function. Let's choose &lt;a href="https://en.wikipedia.org/wiki/Loss_function#Quadratic_loss_function"&gt;quadratic loss&lt;/a&gt; and a &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid&lt;/a&gt; respectively.&lt;/p&gt;
&lt;div class="math"&gt;$$
C(\hat{y}) = \frac{1}{2}(y - \hat{y})^2
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is the neuron's final output, &lt;span class="math"&gt;\(z\)&lt;/span&gt; is the linear combination (&lt;span class="math"&gt;\(wx+b\)&lt;/span&gt;) input, and &lt;span class="math"&gt;\(\hat{y} = \sigma(z)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Using the chain rule, our desired expression &lt;span class="math"&gt;\(\frac{\partial C}{\partial w}\)&lt;/span&gt; becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial C}{\partial w} &amp;amp;=
C'(\hat{y})\frac{d}{dw}\sigma(z)\\
&amp;amp;= C'(\hat{y})\sigma'(z)\frac{d}{dw}z\\
&amp;amp;= C'(\hat{y})\sigma'(z)\frac{d}{dw}(wx + b)\\
&amp;amp;= C'(\hat{y})\sigma'(z)x\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;For our bias, the expression &lt;span class="math"&gt;\(\frac{\partial C}{\partial b}\)&lt;/span&gt; is almost identical:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
\frac{\partial C}{\partial b} &amp;amp;=
C'(\hat{y})\frac{d}{db}\sigma(z)\\
&amp;amp;= C'(\hat{y})\sigma'(z)\frac{d}{db}z\\
&amp;amp;= C'(\hat{y})\sigma'(z)\frac{d}{db}(wx + b)\\
&amp;amp;= C'(\hat{y})\sigma'(z)\\
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;Now we need expressions for &lt;span class="math"&gt;\(C'\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma'\)&lt;/span&gt;. Let's derive them.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align*}
C'(\hat{y}) &amp;amp;= 2 \cdot \frac{1}{2}(y - \hat{y})\\
&amp;amp;= y - \hat{y}\\
\end{align*}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{align*} \sigma'(z) &amp;amp;= -1(1+e^{-z})^{-2}(e^{-z})(-1)\\
&amp;amp;=\frac{e^{-z}}{1+e^{-z}}\\
&amp;amp;=\frac{1}{1+e^{-z}}\frac{e^{-z}}{1+e^{-z}}\\
&amp;amp;=\frac{1}{1+e^{-z}}\frac{(1 + e^{-z}) - 1}{1+e^{-z}}\\
&amp;amp;=\frac{1}{1+e^{-z}}\bigg(1 - \frac{1}{1+e^{-z}}\bigg)\\
&amp;amp;=\sigma(z)(1-\sigma(z))
\end{align*}
$$&lt;/div&gt;
&lt;p&gt;As such, our final expressions for &lt;span class="math"&gt;\(\frac{\partial C}{\partial w}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\frac{\partial C}{\partial b}\)&lt;/span&gt; are:&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial C}{\partial w} = (y - \hat{y})(\sigma(wx + b)(1-\sigma(wx + b)))x
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\partial C}{\partial b} = (y - \hat{y})(\sigma(wx + b)(1-\sigma(wx + b)))
$$&lt;/div&gt;
&lt;p&gt;From there, we just plug in our values from the start (&lt;span class="math"&gt;\(x\)&lt;/span&gt; is our &lt;code&gt;ACTIVATION&lt;/code&gt;) to solve for &lt;code&gt;weight_gradient&lt;/code&gt; and &lt;code&gt;bias gradient&lt;/code&gt;. The result of each is a &lt;em&gt;real-valued number&lt;/em&gt;. It is no longer a function, nor expression, nor nebulous mathematical concept.&lt;/p&gt;
&lt;p&gt;Finally, as initially prescribed, we update our weight and bias via:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weight_gradient&lt;/span&gt;
&lt;span class="n"&gt;bias&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;bias_gradient&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Because &lt;span class="math"&gt;\(\Delta C = -\eta\|\nabla C\|\)&lt;/span&gt;, the resulting weight and bias will give a lower cost than before. Nice!&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;Here's a &lt;a href="http://nbviewer.jupyter.org/github/cavaunpeu/single-neuron-gradient-descent/blob/master/single-neuron-gradient-descent.ipynb"&gt;notebook&lt;/a&gt; showing this process in action. Happy gradient descent.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>dotify: Recommending Spotify Music Through Country Arithmetic</title><link href="https://cavaunpeu.github.io/2016/04/15/dotify-recommending-spotify-music-through-country-arithmetic/" rel="alternate"></link><published>2016-04-15T18:33:00-04:00</published><updated>2016-04-15T18:33:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2016-04-15:/2016/04/15/dotify-recommending-spotify-music-through-country-arithmetic/</id><summary type="html">&lt;p&gt;A web app for Spotify music recommendation using &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Implicit Matrix Factorization&lt;/a&gt; and "country arithmetic."&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever since the release of &lt;a href="https://en.wikipedia.org/wiki/Word2vec"&gt;word2vec&lt;/a&gt; I've been fascinated with embedding things - words, places, people - into vector space. Though not a mathematical historian, I don't believe this concept is at all new: matrix factorization methods like &lt;a href="https://en.wikipedia.org/wiki/Singular_value_decomposition"&gt;Singular Value Decomposition&lt;/a&gt; have given us this ability for years. This said, one of the most exciting revelations of word2vec is the remarkably intuitive results of taking arithmetic combinations of these vectors - adding, subtracting, multiplying, etc. - with&lt;/p&gt;
&lt;div class="math"&gt;$$vector_{king} - vector_{man} + vector_{girl} \approx vector_{queen}$$&lt;/div&gt;
&lt;p&gt;being the classic party trick. With &lt;a href="http://dotify.herokuapp.com/"&gt;dotify&lt;/a&gt;, I set out to embed countries into vector space myself, and use these embeddings to recommend music. What are some songs like "Colombia" .. times "Turkey" .. minus "Germany," you ask?&lt;/p&gt;
&lt;p&gt;&lt;img alt="dotify Background" class="img-responsive" src="https://cavaunpeu.github.io/images/spotify_not_available.png"/&gt;&lt;/p&gt;
&lt;p&gt;To embed, I employ the timeless Implicit Matrix Factorization of &lt;a href="http://yifanhu.net/PUB/cf.pdf"&gt;Hu, Koren, and Volinsky&lt;/a&gt; on daily Spotify Charts data. For a given song in a given country, implicit feedback is counted as the total number of times this song has been streamed throughout history. While the confidence matrix &lt;span class="math"&gt;\(C_{ui}\)&lt;/span&gt; is typically defined as &lt;span class="math"&gt;\(1 + \alpha(1 + R_{ui})\)&lt;/span&gt;, I tweaked it to be &lt;span class="math"&gt;\(1 + \alpha\log{(1 + R_{ui})}\)&lt;/span&gt;; this performed better empirically - likely due to the fact that many songs have stream counts orders of magnitude higher than others. I've been trying to implement more algorithms by hand as of late, which you'll find in the code. To use dotify: begin typing the name of a country, or simply press the down arrow, in order to select a country with your mouse or the enter key. Next, a dropdown of arithmetic operators will appear; select one in identical fashion. Feel free to keep entering countries and operators: your expression can be as long as you like. Finally, when you're ready to terminate your expression and receive song recommendations, simply enter the equals sign ("=") in the operator dropdown. For example:&lt;/p&gt;
&lt;p&gt;&lt;img alt="dotify Screenshot" class="img-responsive" src="https://cavaunpeu.github.io/images/dotify_screenshot.png"/&gt;&lt;/p&gt;
&lt;p&gt;The app is built with the following core technologies: Flask as both API endpoints and a web server; React and LESS on the front-end; Webpack for asset compilaton; Postgres database; Heroku for deployment.&lt;/p&gt;
&lt;p&gt;Feedback is appreciated as always. Thanks for reading.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;Code can be found on &lt;a href="https://github.com/cavaunpeu/dotify"&gt;GitHub&lt;/a&gt;, and dotify can be found &lt;a href="http://dotify.herokuapp.com/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>So You Want to Implement a Custom Loss Function?</title><link href="https://cavaunpeu.github.io/2015/11/18/so-you-want-to-implement-a-custom-loss-function/" rel="alternate"></link><published>2015-11-18T02:12:00-05:00</published><updated>2015-11-18T02:12:00-05:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2015-11-18:/2015/11/18/so-you-want-to-implement-a-custom-loss-function/</id><summary type="html">&lt;p&gt;Implementing custom loss functions in Python using &lt;a href="https://github.com/HIPS/autograd"&gt;autograd&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's often not so hard.&lt;/p&gt;
&lt;p&gt;My venerable boss recently took a trip to Amsterdam. As we live in New York City, he needed to board a plane. Days before, we discussed the asymmetric risk of airport punctuality: "if I get to the gate early, it's really not so bad; if I arrive too late and miss my flight, it really, really sucks." When deciding just when to leave the house - well - machine learning can help.&lt;/p&gt;
&lt;p&gt;Let's assume we have a 4-feature dataset, &lt;em&gt;X&lt;/em&gt;. In order, each columns denotes: a binary "holiday/no-holiday", a binary "weekend/no-weekend", a normalized continuous value dictating "how does the traffic on my route compare to the average day of traffic on this route", and the number of seats on the plane. Each observation is paired with a response, y, where &lt;span class="math"&gt;\(y = 0\)&lt;/span&gt; indicates the flight will leave on time, and &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt; indicates the flight will leave later (meaning we can arrive later ourselves).&lt;/p&gt;
&lt;p&gt;As mentioned, missing a plane is bad. You incur stress. You have to book another ticket, which is often inconveniently expensive. You have to eat your next 2 meals in the airport, which is a whole other bag of misery. If we want to train a learning algorithm to predict flight delay - and therefore inform us as to when to leave the house - we'll have to teach it just how bad the Terminal 3 beef and broccoli really is.&lt;/p&gt;
&lt;p&gt;To train a supervised learning algorithm, we need a few things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Some training data.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A model.&lt;/strong&gt; Let's choose logistic regression. It's simple, deterministic, and interpretable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A loss function&lt;/strong&gt; - also known as a cost function - which quantitatively answers the following: &lt;em&gt;"The real label was 1, but I predicted 0: is that bad?"&lt;/em&gt; Answer: &lt;em&gt;"Yeah. That's bad. That's .. 500 bad."&lt;/em&gt; Many supervised algorithms come with standard loss functions in tow. SVM likes the &lt;a href="https://en.wikipedia.org/wiki/Hinge_loss"&gt;hinge loss&lt;/a&gt;. Logistic regression likes &lt;a href="https://www.kaggle.com/wiki/MultiClassLogLoss"&gt;log loss&lt;/a&gt;, or &lt;a href="https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function"&gt;0-1 loss&lt;/a&gt;. Because our loss is asymmetric - an incorrect answer is more bad than a correct answer is good - we're going to create our own.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A way to optimize our loss function.&lt;/strong&gt; This is just a fancy way of saying: "Look. We have these 4 factors that advise us as to whether or not that airplane is going to take off on time. And we saw what happened in the past. So what we really want to know is: if it's a holiday, is our plane more or less likely to stay on schedule? How about a weekend? Does the traffic have any impact? What about the size of the plane? Finally, once I know this - once I know how each of my variables relates to the thing I'm trying to predict - I want to keep the real world in mind: 'the plane left on time, but I showed up late: is that bad?'"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To do this, we often employ an algorithm called gradient descent (and its variants), which is quick, effective, and easy to understand. Gradient descent requires we take the gradient - or derivative - of our loss function. With most typical loss functions (hinge loss, least squares loss, etc.), we can easily differentiate with a pencil and paper. With more complex loss functions, we often can't. Why not get a computer to do it for us, so we can move onto the fun part of actually fitting our model?&lt;/p&gt;
&lt;p&gt;Introducing &lt;a href="https://github.com/HIPS/autograd"&gt;autograd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Autograd is a pure Python library that "efficiently computes derivatives of numpy code" via automatic differentiation. Automatic differentiation exploits the calculus chain rule to differentiate our functions. In fact, for the purposes of this post, the implementation details aren't that important. What is important, though, is how we can use it: with autograd, obtaining the gradient of your custom loss function is as easy as &lt;code&gt;custom_gradient = grad(custom_loss_function)&lt;/code&gt;. It's really that simple.&lt;/p&gt;
&lt;p&gt;Let's return to our airplane. We have some data - with each column encoding the 4 features described above - and we have our corresponding target. (In addition, we initialize our weights to 0, and define an epsilon with which to clip our predictions in (0, 1)).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.3213&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4856&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2995&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5044&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.3005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4757&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2974&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.4691&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.5638&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.8005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3381&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.3102&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.5281&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6542&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3129&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.1298&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.3221&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5126&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3085&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.6147&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.3055&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4885&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.289&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.4957&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.3276&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5185&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3218&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.6013&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.5313&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.7028&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3266&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.1543&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.4728&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6399&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3062&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0597&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.3221&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5126&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3085&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.6147&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-15&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We have our model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wTx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;logistic_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wTx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And we define our loss function. Here, I've decided on a variant of log loss, with the latter logarithmic term exponentiated and the sign reversed. In English, it says: "If the flight actually leaves on time, and we predict that it leaves on time (and leave our house early enough), then all is right with the world. However, if we instead predict that it will be delayed (and leave our house 30 minutes later), then we'll be stressed, -$1000, and dining on rubber for the following few hours.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;custom_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_predicted&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y_predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;custom_loss_given_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;y_predicted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logistic_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;custom_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's see what this function looks like, for each of &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(y = 0\)&lt;/span&gt;, and varying values of y_hat.&lt;/p&gt;
&lt;p&gt;&lt;img alt="y_equals_1" class="img-responsive" src="https://cavaunpeu.github.io/figures/y_equals_1.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="y_equals_0" class="img-responsive" src="https://cavaunpeu.github.io/figures/y_equals_0.png"/&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;, our cost is that of the typical logarithmic loss. At worst, our cost is &lt;span class="math"&gt;\(\approx 5\)&lt;/span&gt;. However, when &lt;span class="math"&gt;\(y = 0\)&lt;/span&gt;, the dynamic is different: if we guess &lt;span class="math"&gt;\(0.2\)&lt;/span&gt;, it's not so bad; if we guess &lt;span class="math"&gt;\(0.6\)&lt;/span&gt; it's not so bad; if we guess &lt;span class="math"&gt;\(&amp;gt; 0.8\)&lt;/span&gt;, it's really, really bad. &lt;span class="math"&gt;\(\approx 25\)&lt;/span&gt; bad. Finally, we compute our gradient. As easy as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;custom_loss_given_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And, lastly, apply basic gradient descent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it. Implementing a custom loss function into your machine learning model with autograd is as easy as "call the &lt;code&gt;grad&lt;/code&gt; function."&lt;/p&gt;
&lt;p&gt;Why don't people use this more often?&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;All code used in this post can be found in a Jupyter &lt;a href="http://nbviewer.ipython.org/github/cavaunpeu/automatic-differentiation/blob/master/automatic_differentiation.ipynb"&gt;notebook&lt;/a&gt; on my &lt;a href="https://github.com/cavaunpeu"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Travel Recommendations with Jaccard Similarities</title><link href="https://cavaunpeu.github.io/2015/10/03/travel-recommendations-with-jaccard-similarities/" rel="alternate"></link><published>2015-10-03T02:40:00-04:00</published><updated>2015-10-03T02:40:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2015-10-03:/2015/10/03/travel-recommendations-with-jaccard-similarities/</id><summary type="html">&lt;p&gt;A Scala-based web app that gives travel recommendations via Twitter data and the Jaccard similarity.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently finished building a &lt;a href="http://countryrecommender.herokuapp.com/"&gt;web app&lt;/a&gt; that recommends travel destinations. You input a country, and it provides you with 5 other countries which you might also enjoy. The recommendations are generated via a basic application of &lt;a href="https://en.wikipedia.org/wiki/Collaborative_filtering"&gt;collaborative filtering&lt;/a&gt;. In effect, you query for a country, and the engine suggests additional countries enjoyed by other users of similar taste. The methodology is pretty simple:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Capture travel-related tweets from the &lt;a href="http://twitter4j.org/en/"&gt;Twitter API&lt;/a&gt;, defined as any containing the strings "#ttot", "#travel", "#lp", "vacation", "holiday", "wanderlust", "viajar", "voyager", "destination", "tbex", or "tourism."&lt;/li&gt;
&lt;li&gt;If a tweet contains the name of one of 248 countries or territories, or their respective capitals, label this tweet with this country. For example, the tweet "backpacking Iran is awesome! #travel" would be labled with "Iran."&lt;/li&gt;
&lt;li&gt;Represent each country as a set of the user_id's who have tweeted about it.&lt;/li&gt;
&lt;li&gt;Compute a &lt;a href="https://en.wikipedia.org/wiki/Jaccard_index"&gt;Jaccard similarity&lt;/a&gt; - defined as the size of the intersection of 2 sets divided by the size of their union - between all combinations of countries.&lt;/li&gt;
&lt;li&gt;When a country is queried, return the 5 countries Jaccard-most similar. The length of the bars on the plot are the respective similarity scores. So - let's try a few out!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="colombia recommendations" class="img-responsive" src="https://cavaunpeu.github.io/figures/colombia_recommendations.png"/&gt;&lt;/p&gt;
&lt;p&gt;Not bad. Venezuela - neighbor to the East - is recommended most highly. Again, this implies that those tweeting about Colombia were also tweeting about Venezuela.&lt;/p&gt;
&lt;p&gt;&lt;img alt="malaysia recommendations" class="img-responsive" src="https://cavaunpeu.github.io/figures/malaysia_recommendations.png"/&gt;&lt;/p&gt;
&lt;p&gt;Seems logical.&lt;/p&gt;
&lt;p&gt;&lt;img alt="india recommendations" class="img-responsive" src="https://cavaunpeu.github.io/figures/india_recommendations.png"/&gt;&lt;/p&gt;
&lt;p&gt;Strange one, maybe? Then again, all countries - especially Greece, Italy, and France - are universally popular travel destinations, just like our query country India. As such, it's certainly conceivable that they were being promoted by similar users. We'd want to read the tweets to be sure.&lt;/p&gt;
&lt;p&gt;Moving forward, I plan to implement a more robust similarity metric/methodology. While a Jaccard similarity is effective, it's a little bit "bulky": most notably, being a &lt;em&gt;set&lt;/em&gt; similarity metric, it doesn't consider repeat tweets about a given country. For example, if User A tweeted 12 times about Hungary and 1 time about Turkey, a Jaccard similarity would consider these behaviors equal (by simply including User A in each country's respective set). As such, a cosine-based metric might be more appropriate. See &lt;a href="http://www.benfrederickson.com/distance-metrics/"&gt;here&lt;/a&gt; for an excellent overview of possible approaches to a "people who like this also like" analysis.&lt;/p&gt;
&lt;p&gt;Once more, the app is linked &lt;a href="http://countryrecommender.herokuapp.com/"&gt;here&lt;/a&gt;. In addition, the code powering the app can be found on my &lt;a href="https://github.com/cavaunpeu/countryrecommender"&gt;GitHub&lt;/a&gt;. Backend in Scala, and front-end in HTML, CSS, and native JavaScript. Do take it for a spin, and &lt;a href="https://twitter.com/WillTravelLife"&gt;let me know&lt;/a&gt; what you think!&lt;/p&gt;</content><category term="machine-learning"></category></entry><entry><title>Markovian Muse: Conjuring the Backpacker Within</title><link href="https://cavaunpeu.github.io/2015/06/29/markovian-muse-conjuring-the-backpacker-within/" rel="alternate"></link><published>2015-06-29T00:17:00-04:00</published><updated>2015-06-29T00:17:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2015-06-29:/2015/06/29/markovian-muse-conjuring-the-backpacker-within/</id><summary type="html">&lt;p&gt;Auto-generating travel blog posts via Markovian processes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I haven't travel-blogged in a while - mainly because I haven't been traveling. When I was, the writing flowed easily: I'd wander, hike, try and fail, and humbling lessons about the world and its creatures seemed around every corner. There was plenty about which to write. Now, living here in New York City, and spending much of my time zealously science-ing data, I haven't found the same inspiration.&lt;/p&gt;
&lt;p&gt;Piqued by Andrej Karpathy's &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;post&lt;/a&gt; on auto-generating text with recurrent neural networks, and indeed wanting to write some more travel posts, I've decided to conjure the backpacker within. My tool of choice is a Markov chain - simple, and less black-box-y than an &lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;RNN&lt;/a&gt; - which will do the writing for us.&lt;/p&gt;
&lt;p&gt;First thing's first: what is a Markov chain, and how does it work?&lt;/p&gt;
&lt;p&gt;Markov chains are stochastic processes with many "states," or "nodes." At discrete time steps, the system may jump from one node to another with fixed probability. For example, assume our system is at State A at time &lt;span class="math"&gt;\(t = 0\)&lt;/span&gt;. At &lt;span class="math"&gt;\(t = 1\)&lt;/span&gt;, we jump to State B with probability &lt;span class="math"&gt;\(p = 0.4\)&lt;/span&gt;, State C with probability &lt;span class="math"&gt;\(p = 0.3\)&lt;/span&gt;, and remain at State A with probability &lt;span class="math"&gt;\(p = 0.3\)&lt;/span&gt;. In addition, the system is "memoryless": the probability distribution (&lt;span class="math"&gt;\(A=0.3, B=.4, C=.3\)&lt;/span&gt; in the previous case) governing our move to the next state depends &lt;em&gt;only&lt;/em&gt; on the current state, and none previous. &lt;/p&gt;
&lt;p&gt;I first learned about Markov chain's in undergrad as a way to model industrial processes. To auto-generate text with a Markov chain, the process is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize an empty string, &lt;code&gt;s&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Choose a word at random from a given body of text, and call it &lt;code&gt;word_1&lt;/code&gt;. Call the subsequent word &lt;code&gt;word_2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;word_1&lt;/code&gt; plus a whitespace to our string, such that &lt;code&gt;s += word_1 + ' '&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Find all occurrences of &lt;code&gt;word_1 word_2&lt;/code&gt; in the text. Then, create an array of each word that follows. For example, if &lt;code&gt;word_1 = "Will"&lt;/code&gt; and &lt;code&gt;word_2 = "went"&lt;/code&gt;, and our text contains the snippets &lt;code&gt;"Will went to"&lt;/code&gt;, &lt;code&gt;"Will went further"&lt;/code&gt;, and &lt;code&gt;"Will went north"&lt;/code&gt;, then this array would be defined as &lt;code&gt;["to", "further", "north"]&lt;/code&gt;. Duplicate elements should remain in the array.&lt;/li&gt;
&lt;li&gt;Select a word at random from the array we just created and call it &lt;code&gt;word_3&lt;/code&gt;. Then, set &lt;code&gt;word_1 = word_2&lt;/code&gt; and &lt;code&gt;word_2 = word_3&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Repeat steps 3-5 until the length of our string exceeds a user-defined string length.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The dataset I'm using is a full corpus of my &lt;a href="http://willtravellife.com"&gt;travel blog&lt;/a&gt;. Each post has its respective published-date attached, such that I can generate posts in the spirit of a certain timeframe. Let's give it a whirl!&lt;/p&gt;
&lt;p&gt;Between January 1st, 2012 (the ~start of my journey), and March 1st, 2012, I was in Kenya, Tanzania, Uganda, Rwanda, United Arab Emirates, and Brazil. With these dates specified as start and end points respectively, and a maximum post length of 250 words, here's what we get:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So--Ill Just Tell You What Ive Been Trying To Stitch&lt;/p&gt;
&lt;p&gt;Paths, and a chicken pie and a terribly anti-climactic pile of roughly 11 baked beans. Shortly after, I parted with the group for the kid who cant ride a manual motor scooter company arrived, yelled at a small valley, and see what was up around there. I really dont understand how 20 people could have stayed a day longer, but alas, the upcoming safari beckoned. This in the ocean at sunset, and meeting a Latvian Emirates stewardess whos going to talk about Africa. Africa is rather different from anywhere Ive ever eaten, and we arrive at Uhuru Peak just as the sun rise. 7:30AM: Eggs, toast, bananas, mangoes on the east coast. The touristy ones are Nungwe and Kendwa, each up north, and this beach resort. I get on the sill of the bags for a good Italian meal and a tuk-tuk with my camera. Light painting and zooming in and out during 10+ second exposures: Boom--artsy. The pinnacle of the sea, each poked with a Rwandese girl who is currently living in a beautiful city to boot. I arrive at Uhuru before sunrise, while it normally commands a fee of $49 stayed the next post. Kilimanjaro. Mount Kilimanjaro. F*cking beauty. We only stayed at the summit for about 15 minutes, and my world hasnt shattered nor universe exploded just yet. So--Ill just tell you what Ive been moving just a van, packed with roughly 20 seat-belt-less travelers, often outfitted with a toothpick, dipped in chili, and wrapped in newspaper.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Fast-forward a year. Between January 1st, 2013, and March 1st, 2013, I was in Senegal and Guinea. Using these dates and the same post length, we get:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Stockpiled With Noodles, Cookies, And Overused Kitchenware-Wandered By 4 Or&lt;/p&gt;
&lt;p&gt;Wait a bit. You said this would work at noon. OK. Im going to inadvertently insult someone sooner or later. Its just going to be back in an hour. Maybe tomorrow. Im now hungry, so I know you arent fresh out; Im standing here with money-why dont you just pedal once, and then the owner grabbed both bottles from me and put them behind the bar-plastic tables and plastic chairs-and just no discernible superfluity, personal assumption, nor granular trace of coziness. Most are powered by generators, and once in a minute. 11:15am rolls around. Sir, whats the deal. It will be looked at, pointed at, and called names. I was 15 years old. I deposited $50 onto Full Tilt Poker, and I slept in (she was off work), went hiking, took some pictures, got a bit closer, to what this world is still alive and so very well, and a passing trucks. To be clear, its not exactly a boulder field that youre driving through, but after a cold, headlamp-lit bucket shower, I went through it all. Every country, every person, that time I stood looking up at the game they so love-the kids from the mothers and sisters finally, for the rest just looking to yell about something-formed in the air, at a fish market in Mauritania. 1592 National Pride in the car-only 3 seats still to fill. I read the first passing truck. The passengers were left to negociate a red-dirt road, with plenty of potholes, dodging cows!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So eloquent! Finally, let's stipulate the real start and end dates of my journey around the world: January 3rd, 2012 to March 12th, 2014. In all its glory:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To Be A Travel Day Without Knowing Where Ill Ride&lt;/p&gt;
&lt;p&gt;Just wait a bit. Ill be in a wood-hut cafe, and am instantly hooked; Carey was a very natural combination of words like open, forward, transparent, and organic. Individual, on the front. I cant wait. Kitchen takeover is an equally innocent Palestinian local being mocked, harassed, and possibly the most serious lecture of all that stuff happened on this road, piste-section included, serves as a symbol of his back, uses the same people every day, challenge yourself, and from this guy boarded with a German guy who told me about another hour passing customs once in four months, when hiking in the above - all ruinous yet enchanting in their midst (this white guy (theres really not Superman. There is a language spoken by both the older and younger generations. If you dont, youre doing it yourself. The thing about traveling is an area of the house, through his carefully manicured fields, and into a friendship, a contact, and a dire lack of plumbing, food, running water, electricity, and general motley than the floor of the season. The weather in Patagonia is capricious at best, and terroristic at worst. Days like this place was an absolute nightmare of sog and wet. Follow the trail begins at the rink. Furthermore, the NHL guys were the only word I understood. As I said, its probably much more real. Emotionally, and certainly in big cities. 4. Study Blogs if Traversing Kazakhstan, Uzbekistan, and Tajikistan. My tips are based on my laptop at the.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thanks for reading.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;p&gt;Code for this project can be found on my &lt;a href="https://github.com/cavaunpeu/markovian-muse"&gt;GitHub&lt;/a&gt;. It takes heavy inspiration from Greg Reda's &lt;a href="http://www.gregreda.com/2015/03/30/beer-review-markov-chains/"&gt;Nonsensical beer reviews via Markov chains&lt;/a&gt;, who does a terrific job with a similar topic.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Statistically Cycling Southeast Asia</title><link href="https://cavaunpeu.github.io/2014/05/16/statistically-cycling-southeast-asia/" rel="alternate"></link><published>2014-05-16T00:36:00-04:00</published><updated>2014-05-16T00:36:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2014-05-16:/2014/05/16/statistically-cycling-southeast-asia/</id><summary type="html">&lt;p&gt;Predicting the difficulty of the road ahead: a bike ride through Southeast Asia.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I spent 6 months of my trip around the world traveling on a bicycle. First, I rode from Istanbul, Turkey to Bishkek, Kyrgyzstan, and then through small parts of China and Laos. While you may think I'm crazy, the other cyclists I met along the way would offer a very different perspective. Why? Because the majority of them were Western Europeans, cycling from their front door (in France, for example) all the way to Indonesia.&lt;/p&gt;
&lt;p&gt;When I met other cyclists, I'd ask for advice. "What's the next country like? How are the roads? Is it easy to get food and water? How tough is the cycling?" While the information I would receive was valuable, it was always too relative to take at face value. &lt;em&gt;You&lt;/em&gt; may find a country difficult or easy - &lt;em&gt;you&lt;/em&gt; may have cycled fast or slow. I once met a Polish guy outside a bar in &lt;a href="willtravellife.com/blog/2013/10/25/hail-linkin-park-crazy-guy-bike/"&gt;Yerevan, Armenia&lt;/a&gt; who had cycled there from Warsaw - roughly 4,000 kilometers away if you divert through Eastern Europe and Turkey as he had done - in a month. There was just no point in comparing myself to this Polish Hercules.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cycling the weird and wide in Central (f*cking) Asia" class="img-responsive" src="https://cavaunpeu.github.io/images/pamir_bike.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;For a better approximation of what's to come - one relative to your own experiences and diagnostic approach - perhaps there's a better way.&lt;/p&gt;
&lt;p&gt;After arriving in Kyrgyzstan, I had a strong picture of how easy or difficult I personally felt it was to cycle through the preceding Central Asian countries. If I were to have continued to Indonesia like the rest - through China, Vietnam, Laos, Thailand, Malaysia and Singapore - could I have then predicted how easy or difficult these upcoming countries would be? In this post, I build a linear regression model to do just that.&lt;/p&gt;
&lt;p&gt;To each of the countries I had already traversed - Turkey, Georgia, Armenia, Azerbaijan, Kazakhstan, Uzbekistan, Tajikistan and Kyrgyzstan - I assign a difficulty index from 0 to 100, where 0 is very easy and 100 is very hard, based on my own personal experience. Then, I choose some explanatory variables that I feel contribute heavily to this index, as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Percentage of total land that is arable in each country:&lt;/strong&gt; A rough indicator of how much food, specifically produce, will be available in a village market. Food is important when you cycle all day.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Percentage of total roads that are paved:&lt;/strong&gt; Smooth roads are exponentially easier to cycle than bumpy ones, and even more so when your bike weighs 50kg.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Population density (persons per square kilometer):&lt;/strong&gt; In sparsely populated areas/countries - the parts of Kazakhstan and Uzbekistan through which I cycled, for example - it is rather difficult to obtain food, shelter and help if you need it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Percentage of population with access to potable water:&lt;/strong&gt; The smaller this statistic, the harder it should be to obtain clean water.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Topography index:&lt;/strong&gt; A discrete integer value from 1 to 4 based upon how hilly/mountainous the country is. I assign this value to each country myself based on my own personal experience.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data for the first 4 statistics, using values from the most recent years available, is then pulled from &lt;a href="http://data.un.org/Explorer.aspx?d=WDI&amp;amp;f=Indicator_Code%3aNV.IND.TOTL.ZS"&gt;UNdata&lt;/a&gt;, scaled by mean and standard deviation, and fit with a linear regression model in R. The output quickly shows that only coefficients for "percent paved," "population density" and "water access" are significant to our model. We remove the rest and refit. Below are the coefficients and significances of this new fit.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="m"&gt;60.625&lt;/span&gt;      &lt;span class="m"&gt;1.301&lt;/span&gt;  &lt;span class="m"&gt;46.594&lt;/span&gt; &lt;span class="m"&gt;1.27e-06&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;perc.paved&lt;/span&gt;     &lt;span class="m"&gt;12.496&lt;/span&gt;      &lt;span class="m"&gt;1.909&lt;/span&gt;   &lt;span class="m"&gt;6.544&lt;/span&gt;  &lt;span class="m"&gt;0.00282&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;
&lt;span class="n"&gt;pop.dens&lt;/span&gt;      &lt;span class="m"&gt;-13.295&lt;/span&gt;      &lt;span class="m"&gt;1.647&lt;/span&gt;  &lt;span class="m"&gt;-8.074&lt;/span&gt;  &lt;span class="m"&gt;0.00128&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;
&lt;span class="n"&gt;water.access&lt;/span&gt;  &lt;span class="m"&gt;-13.980&lt;/span&gt;      &lt;span class="m"&gt;1.742&lt;/span&gt;  &lt;span class="m"&gt;-8.025&lt;/span&gt;  &lt;span class="m"&gt;0.00131&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;First, we note that all explanatory variables are significant at the p = .05 level, as all p-values are less than .05. Next, we make commentary on the coefficients themselves. It makes sense that an increase in water access should decrease our difficulty index; in other words, more water means the country is easier to cycle. The same is true for population density - more people means an easier ride (although this would likely plateau at a certain point - cycling in dense urban areas is never fun). Lastly, we see that an increase in paved roads means an &lt;em&gt;increased&lt;/em&gt; difficulty index. At first glance, this makes little sense: shouldn't more paved roads mean the country is easier to cycle? This is certainly possible. However, consider the alternative. A country with a small percentage of total roads paved is likely to be of lower economic standing. The few roads that &lt;em&gt;are&lt;/em&gt; paved are more likely to be highly connective and heavily trafficked, and given that building a road on flat land is far easier and cheaper than building roads that burrow through mountains (ahem, China), these roads are more likely to be on easier terrain than they are to be on harder. As a cyclist, I want to be fairly linear and efficient with my route choice, and if we infer that there are only a few highly connective cross-country arteries in a given country, these are more or less the roads to which I am forced to stick. Therefore, in a country without many paved roads, one could infer that on average, the roads I end up taking may in fact be flatter and easier than in a country with a higher percentage of its roads paved. This is one explanation for the positive coefficient.&lt;/p&gt;
&lt;p&gt;After determining that our model is a good fit (assessing coefficient p-values, and examining the distribution of residuals), we can then pull new statistics for our model's 3 explanatory variables - "percent paved," "population density" and "water access" - for each of China, Vietnam, Laos, Thailand, Malaysia and Singapore - also from UNdata. The table below shows these values as well as the predicted results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;row.names&lt;/span&gt;   &lt;span class="n"&gt;perc.paved&lt;/span&gt;   &lt;span class="n"&gt;pop.dens&lt;/span&gt;   &lt;span class="n"&gt;water.access&lt;/span&gt;    &lt;span class="n"&gt;diffIDX.pred&lt;/span&gt;
&lt;span class="n"&gt;China&lt;/span&gt;       &lt;span class="m"&gt;53.5&lt;/span&gt;     &lt;span class="m"&gt;141.69292&lt;/span&gt;  &lt;span class="m"&gt;91.7&lt;/span&gt;            &lt;span class="m"&gt;60.21643&lt;/span&gt;
&lt;span class="n"&gt;Indonesia&lt;/span&gt;   &lt;span class="m"&gt;56.9&lt;/span&gt;     &lt;span class="m"&gt;126.36795&lt;/span&gt;  &lt;span class="m"&gt;84.3&lt;/span&gt;            &lt;span class="m"&gt;71.22042&lt;/span&gt;
&lt;span class="n"&gt;Lao&lt;/span&gt; &lt;span class="n"&gt;PDR&lt;/span&gt;     &lt;span class="m"&gt;13.7&lt;/span&gt;     &lt;span class="m"&gt;27.00892&lt;/span&gt;   &lt;span class="m"&gt;69.6&lt;/span&gt;            &lt;span class="m"&gt;73.15184&lt;/span&gt;
&lt;span class="n"&gt;Malaysia&lt;/span&gt;    &lt;span class="m"&gt;80.4&lt;/span&gt;     &lt;span class="m"&gt;85.74957&lt;/span&gt;   &lt;span class="m"&gt;99.6&lt;/span&gt;            &lt;span class="m"&gt;61.14540&lt;/span&gt;
&lt;span class="n"&gt;Singapore&lt;/span&gt;   &lt;span class="m"&gt;100.0&lt;/span&gt;    &lt;span class="m"&gt;8218.39644&lt;/span&gt; &lt;span class="m"&gt;100.0&lt;/span&gt;           &lt;span class="m"&gt;33.20916&lt;/span&gt;
&lt;span class="n"&gt;Thailand&lt;/span&gt;    &lt;span class="m"&gt;98.5&lt;/span&gt;     &lt;span class="m"&gt;129.40705&lt;/span&gt;  &lt;span class="m"&gt;95.8&lt;/span&gt;            &lt;span class="m"&gt;73.19419&lt;/span&gt;
&lt;span class="n"&gt;Vietnam&lt;/span&gt;     &lt;span class="m"&gt;47.6&lt;/span&gt;     &lt;span class="m"&gt;268.46655&lt;/span&gt;  &lt;span class="m"&gt;95.6&lt;/span&gt;            &lt;span class="m"&gt;52.23755&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, how do our predictions look? I buy 'em. Singapore would definitely be the easiest in which to cycle. Indonesia and Lao would likely be the hardest. China in the middle, though, with an index of 60? Again - I'd buy it. Assuming you can speak some Chinese. Finally, here's a map of our findings. If you have a friend cycling in the area - do pass it along.&lt;/p&gt;
&lt;p&gt;&lt;img alt="diffIDX Map" class="img-responsive" src="https://cavaunpeu.github.io/figures/cycling_difficulty_index.jpg"/&gt;&lt;/p&gt;</content><category term="machine-learning"></category></entry><entry><title>Text-Mining South American Constitutions</title><link href="https://cavaunpeu.github.io/2014/05/06/text-mining-south-american-constitutions/" rel="alternate"></link><published>2014-05-06T20:11:00-04:00</published><updated>2014-05-06T20:11:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2014-05-06:/2014/05/06/text-mining-south-american-constitutions/</id><summary type="html">&lt;p&gt;Rudimentary text-mining of Latin American constitutions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;With the advent of social media, and the Babylonian deluge of tweets, YouTube comments, and wall posts traversing the internet daily, text-mining has become an increasingly important tool of the data scientist. Basically, all of these tweets, Facebook posts, etc. form a valuable and robust set of data, and text-mining helps us understand what it all really means. Universal Studios might mine text to process mass reviews of its latest movie on social media; Ryanair might mine text to gauge just how shitty the public thinks its service really is. In this post, I set out to understand what South America stands for, or purportedly stands for, as a whole.&lt;/p&gt;
&lt;p&gt;To do this, I first choose a set of data, and what better source than the documents that govern the countries themselves? Then, using R's "tm" (short for text-mining) package, I can easily process these documents and see what they have to offer.&lt;/p&gt;
&lt;p&gt;The countries considered in this analysis are only those that are Spanish-speaking: Argentina, Bolivia, Chile, Colombia, Ecuador, Paraguay, Peru, Uruguay, and Venezuela. If we were to include the remaining four - Guyana, French Guiana, Suriname and Brazil - we'd have to obtain a translation of their constitutions, so in order keep our analysis unaffected by pure lingual differences as well as translational inaccuracies, I've decided to omit them altogether.&lt;/p&gt;
&lt;p&gt;To begin, we first copy and paste each constitution from &lt;a href="http://pdba.georgetown.edu/constitutions/constitutions.html"&gt;Georgetown's Center for Latin American Studies&lt;/a&gt; (and that of Venezuela from the &lt;a href="http://www.tsj.gov.ve/legislacion/constitucion1999.htm"&gt;Venezuela Supreme Court's website&lt;/a&gt;) into separate TextEdit documents. We then save each as a UTF-8 encoded .txt file (note to self and others: this step is very important). From there, we move to R.&lt;/p&gt;
&lt;p&gt;The documents are first loaded and then cleaned. To clean, spanish &lt;a href="http://en.wikipedia.org/wiki/Stop_words"&gt;stop-words&lt;/a&gt; are removed - words like "el, en, porque" or "the, in, because" in English. In addition, we remove a few other words that are particularly universal/inconsequential, namely "ley, artículo, caso, estado, nacional, nación, constitución," or "law, article, case, state, national, nation, constitution," respectively.&lt;/p&gt;
&lt;p&gt;Next, we make everything lowercase, and remove numbers and punctuation. Lastly, we convert the text into a term-document-matrix, where the number of occurrences of each word in each document is calculated and put into a matrix. A "TDM" is generally the all-things-possible starting point of a text-mining analysis.&lt;/p&gt;
&lt;p&gt;From there, we visualize! For this visualization, I choose to create a word-cloud with R's "wordcloud" package, and only include words that occur 20 times or more across the 9 constitutions. The bigger words occur more frequently, and vice versa. The biggest word, "derecho," therefore occurs most frequently across the documents.&lt;/p&gt;
&lt;p&gt;&lt;img alt="constitution word cloud" class="img-responsive" src="https://cavaunpeu.github.io/figures/constitution_wordcloud.png"/&gt;&lt;/p&gt;
&lt;p&gt;So, what do we have? Again, "derecho," or "right" (as in your "right" as a citizen) is used most frequently. In addition, it appears that the majority of countries have presidents, as the word "presidente" is rather large, as well as consider themselves republics, as the word "república" is large as well. Interestingly, we see in yellow that the word "podrá" is used frequently, a future-tense verb meaning "will be able to"; why is this word used instead of "puede," a present-tense conjugation of the same verb meaning "is &lt;em&gt;now&lt;/em&gt; able to?" Given that most of these countries gained independence from Spain in the last 200 years, perhaps this small caveat is a reflection of founders' general apprehension for their country's future and true independence?&lt;/p&gt;
&lt;p&gt;As one can see, a word-cloud is a really nice visualization of a given text, or commonalities between multiple texts. While it's really only a high-level view of the data at hand, it does paint a rather pretty picture.&lt;/p&gt;</content><category term="machine-learning"></category></entry><entry><title>Clustering Continued: A Gaucho on Vacation</title><link href="https://cavaunpeu.github.io/2014/05/02/clustering-continued-a-gaucho-on-vacation/" rel="alternate"></link><published>2014-05-02T19:11:00-04:00</published><updated>2014-05-02T19:11:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2014-05-02:/2014/05/02/clustering-continued-a-gaucho-on-vacation/</id><summary type="html">&lt;p&gt;In our &lt;a href="https://cavaunpeu.github.io/machine-learning/clustering-the-airports.md"&gt;previous post&lt;/a&gt; we chose to cluster South American airports into &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; distinct groups; moving forward, we'll take a closer look at what this really means.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;In our &lt;a href="https://cavaunpeu.github.io/machine-learning/clustering-the-airports.md"&gt;previous post&lt;/a&gt; we chose to cluster South American airports into &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; distinct groups. Moving forward, we'll take a closer look into what this really means.&lt;/p&gt;
&lt;p&gt;As mentioned previously, the k-means algorithm incorporates some element of mathematical randomness. On one k-means trial, the algorithm may assign 30% of our airports to Cluster 1, 65% to Cluster 2, and 5% to Cluster 3. Other times, this distribution could look more like 35%, 55%, and 10% respectively. The more clusters we input, or the larger we make k, the less the distributions vary by trial.&lt;/p&gt;
&lt;p&gt;Irrespective, we previously deemed &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; to be the way to go, informed by statistical methods and not by counting the number of McDonald's each airport houses, so with &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; we will proceed. Qualitatively, these clusters can be said to define airports as "major" airports, "semi-major" airports, and "non-major" airports.&lt;/p&gt;
&lt;p&gt;In addition to cluster sizes varying by trial, the actual number assigned to each cluster - 1, 2, or 3 - will vary by trial as well. For example, in Argentina, Buenos Aires' international airport is assumed to be consistently placed in the cluster pertaining to "major" airports; however, the cluster number assigned to this group on a given trial could be 1, 2, or 3. Below is a histogram of cluster sizes across 100 k-means trials for the 293 South American airports being examined.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cluster Assignment Frequency Histogram" class="img-responsive" src="https://cavaunpeu.github.io/figures/airport_cluster_frequency_histograms.png"/&gt;&lt;/p&gt;
&lt;p&gt;Since numbers assigned to each cluster (1, 2, or 3) change by trial, this graph isn't particularly useful. However, it does given significant evidence that cluster sizes will indeed vary by trial, which we'll use later on. As such, it follows that clusters should not be judged by their respective cluster numbers, but rather, by the mean "centers" values associated with the airports grouped within. This is what really defines the clusters themselves, or in other words, what makes each cluster pertain to "major," "semi-major," and "non-major" air hubs (I'll continue to keep these words in quotations, since k-means clustering is ultimately an attempt to give a quantitative definition to an ultimately qualitative distinction, which is always, at best, an approximation).&lt;/p&gt;
&lt;p&gt;Upon first examining which airports were actually clustered together - and again, we're using &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt;, and considering all routes between all airports in South America - it is immediately clear that airports from the same country are consistently put into the same cluster groups. Even though Buenos Aires' airport, with 50 distinct routes continent-wide, and Santa Rosa, Argentina's airport, with only 1 distinct route continent-wide (that being to Buenos Aires), are clearly categorically different in "major-ness," they are consistently put into the same cluster. This is probably for one or both of the following reasons: "non-major" airports are "piggy-backing" onto the more "major" domestic airports in their respective countries (as they are generally just 1 flight away, as is the case with Santa Rosa); or, the extensiveness of the domestic air network itself outweighs the international, continent-wide connectivity that a single airport can offer, therefore grouping "major" and "non-major" airports from the same country together more frequently than "major" and "semi-major" airports from different countries. Clearly, our goal is to consistently have, at a minimum, the continent's most "major" airports grouped together - those of Buenos Aires, São Paulo, Bogotá, Lima, and Santiago, for example - but unfortunately this is not the case. Back to the drawing board.&lt;/p&gt;
&lt;p&gt;Instead, what I choose to do in this post is compare and contrast the clusterings of individual domestic networks. For this, I choose only the countries with at least 3 airports running domestic routes (as we of course need as many airports as we do clusters), being Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Peru, and Venezuela. Our aim here to figure out the proportion of "major" airports, "semi-major" airports, and "non-major" airports in each country.&lt;/p&gt;
&lt;p&gt;To do this, we first cluster and then examine the means of each cluster's centers. From there, we simply take the means - average shortest path lengths for airports in each cluster - and order from smallest to biggest. This will ensure that the smallest means correspond to our "major" airports, second-smallest means correspond to "semi-major" airports, and largest correspond to "non-major" airports. One problem still remains: cluster sizes will still vary by trial, as shown clearly in the graph above. Therefore, I run 100 k-means trials for each country, compute population proportions across these trials, and compare with a stacked-bar ggplot. The red bars are for "major" airports, green for "semi-major," and blue for "non-major."&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# create stacked bar chart in ggplot&lt;/span&gt;
&lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;km_by_country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FractOfWhole&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Cluster&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;geom_bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"stack"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"identity"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"Percentage of Total Domestic Airports"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"Cluster Proportionality of Domestic Airports"&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="Stack Bar Cluster Props" class="img-responsive" src="https://cavaunpeu.github.io/figures/cluster_proportions_stacked_barchart.png"/&gt;&lt;/p&gt;
&lt;p&gt;Now for the fun part.&lt;/p&gt;
&lt;p&gt;First, we see that Argentina, Colombia, and Peru have comparatively few "major" airports; most routes in these countries will be sourced by a select few hubs. In Argentina, this is primarily Buenos Aires; in Colombia, primarily Bogotá and Cali (and to a surprising extent Rio Negro and San Andrés Island); and in Peru, primarily Lima. At the opposite end of the spectrum, Brazil and Bolivia house a relatively even distribution of airport types. In Brazil, this is likely due to the sheer volume and variety of domestic routes (~120 working airports), meaning that no matter where you are, you're never that far from anywhere else. In Bolivia, with only ~15 working airports, it seems that the load is simply shared rather evenly across the board, with no one airport as the single, outright major hub, and smaller airports servicing a nice handful of routes themselves.&lt;/p&gt;
&lt;p&gt;So - what does this all mean? Countries with more evenly distributed "major," "semi-major," and "non-major" airports make travel much easier. If you're in Central Argentina and want to go somewhere by air, you're rather likely to require a layover in the nation's capital (which is not near the center of the country either) before moving to your destination. In Colombia, while there are many active airports, if you want to travel somewhere a bit "off-path" you're likely to require just a few more layovers than you had hoped. Lastly, if you're in Brazil, unless you're stuck on a canoe in the Amazon Rainforest, you're never really in the middle of logistical nowhere.&lt;/p&gt;
&lt;p&gt;In a future post, it will be interesting to look more closely at the economic causes and effects for these air distributions. For now, let's just be thankful we're not gauchos in &lt;a href="http://willtravellife.com/blog/2013/04/22/photo-essay-the-conical-cathedral-of-patagonias-fitzroy/"&gt;Patagonia&lt;/a&gt; planning a vacation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gaucho Photo" class="img-responsive" src="https://cavaunpeu.github.io/images/guachos_on_vacation.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Photo Credit: &lt;a href="http://www.beforethey.com/tribe/gauchos"&gt;Jimmy Nelson&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry><entry><title>Clustering South American Airports</title><link href="https://cavaunpeu.github.io/2014/04/25/clustering-the-airports/" rel="alternate"></link><published>2014-04-25T03:51:00-04:00</published><updated>2014-04-25T03:51:00-04:00</updated><author><name>Will Wolf</name></author><id>tag:cavaunpeu.github.io,2014-04-25:/2014/04/25/clustering-the-airports/</id><summary type="html">&lt;p&gt;A data-enthused look at air travel on my favorite continent: South America and the Falkland Islands.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Greetings, all, and welcome to my new website! For those that know me, I am the increasingly lazy creator and curator of &lt;a href="http://willtravellife.com"&gt;Will Travel Life&lt;/a&gt; - where I post stories, photos, and philosophical muse from a 2+ year backpacking and cycling trip around the world. For those that don't, it's a pleasure to have you at my journalistic side.&lt;/p&gt;
&lt;p&gt;In this blog, I'll be posting on a variety of mathematics and machine learning topics. Many will have a travel flavor. In this post, we'll take a look at air travel and airports on my favorite continent: South America and the Falkland Islands.&lt;/p&gt;
&lt;p&gt;Traveling by plane in South America is not particularly pragmatic nor affordable: the air network is relatively limited, and flights are therefore costly. Most travelers opt to take the bus instead. When flying to South America, you're likely to enter via one of its major international air hubs: Bogotá, Lima, Santiago, Buenos Aires or São Paulo, for example. Needless to say, these airports are major connecting points for intra-continental travel as well.&lt;/p&gt;
&lt;p&gt;In addition to these big airports we'll have a series of, qualitatively speaking, "semi-big" airports, "small" airports, "super tiny single-runway Cessna Jet only" airports, etc. Perhaps we can be more explicit with our grouping? It's one thing to attempt to classify these airports into distinct groups by how many McDonald's each houses. It's another to use the data. Here, I'll attempt this classification using &lt;a href="http://en.wikipedia.org/wiki/Shortest_path_problem"&gt;shortest path analysis&lt;/a&gt; and &lt;a href="http://en.wikipedia.org/wiki/K-means_clustering"&gt;k-means clustering&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, I grab a list of all worldwide flight routes from &lt;a href="http://openflights.org/data.html"&gt;OpenFlights&lt;/a&gt;, current as of January 2012 and containing "59036 routes between 3209 airports on 531 airlines spanning the globe." We filter for South America and employ R's "igraph" package - a library dedicated to network analysis and visualization. I then produce a basic social network graph for all flights between Chile, Argentina and Paraguay merely for example - a visual of some of the data at hand and a testament to the graphical power of R.&lt;/p&gt;
&lt;p&gt;&lt;img alt="SNA Plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/sa_airports_sna_plot.png"/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;simplify&lt;/code&gt; function is used to eliminate reverse routes: we don't need to map flights between Buenos Aires and Santiago as well as those from Santiago to Buenos Aires. As assumed, the visual confirms that most flights between these countries are done through the capitals: Buenos Aires, Santiago, and Asunción. Also, within Argentina, there appears to be a few "middle-range" airports as well, namely but not limited to Mendoza, Bariloche, and El Calafate. Clearly, there are countless "bottom-range" airports too serviced only by one city (within these three countries); Ciudad Del Este, for example. How many distinct groups of varying size/volume/connectivity can we form?&lt;/p&gt;
&lt;p&gt;To answer this question, we compute the shortest path - or number of distinct airports through which one must fly - from every city in South America to all other cities. These values are then be used to inform centroid locations for our k-means analysis.&lt;/p&gt;
&lt;p&gt;Unfortunately, we still must tell R how many clusters we seek. Should there be 4 distinct airport groups? 8? Why? The more clusters we have, the closer values within each cluster move to their respective centroid mean, a metric given by &lt;code&gt;withinss&lt;/code&gt; or "within groups sum of squares." Therefore, we first run the analysis for varying numbers of clusters in search of the k value at which the "marginal return of adding one more cluster is less than was the marginal return for adding the clusters prior to that." We run trials for &lt;span class="math"&gt;\(k = 2\)&lt;/span&gt; to &lt;span class="math"&gt;\(k = 9\)&lt;/span&gt;. The article &lt;a href="http://blog.revolutionanalytics.com/2013/12/k-means-clustering-86-single-malt-scotch-whiskies.html"&gt;"K-means Clustering 86 Single Malt Scotch Whiskies"&lt;/a&gt; on the Revolution Analytics blog was referenced heavily for this step.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Within SS Plot" class="img-responsive" src="https://cavaunpeu.github.io/figures/sa_airports_withinss_plot.png"/&gt;&lt;/p&gt;
&lt;p&gt;From the graph, it appears that &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; is the value we want: "the marginal return of adding one more cluster is less than was the marginal return for adding the clusters prior to that." However, the k-means clustering algorithm does incorporate a random number generator, so it is important to examine the impact of randomness on our results. In addition, after producing this graph a few times, it was immediately clear that our trend varies slightly across trials. In solution, we'll run 100 trials in ggplot, plot them in points, and examine the smoothing line instead.&lt;/p&gt;
&lt;p&gt;&lt;img alt="K-Means Smooth" class="img-responsive" src="https://cavaunpeu.github.io/figures/sa_airports_kmeans_smooth.png"/&gt;&lt;/p&gt;
&lt;p&gt;The plot seems to corroborate what we previously thought: &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; clusters is the best choice for our data set. To be absolutely certain, we can explicitly compute the predicted "loess" values that fall on our smooth line for integers in &lt;span class="math"&gt;\([2,9]\)&lt;/span&gt; and see where the "marginal returns cutoff point" really is. Simple subtraction.&lt;/p&gt;
&lt;p&gt;&lt;img alt="For k-means clustering analysis on SA airports." class="img-responsive" src="https://cavaunpeu.github.io/figures/sa_airports_predicted_withinss_plot.png"/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;PredSumVal&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
&lt;span class="m"&gt;1&lt;/span&gt;   &lt;span class="m"&gt;43854.32&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="m"&gt;2&lt;/span&gt;   &lt;span class="m"&gt;33887.51&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;
&lt;span class="m"&gt;3&lt;/span&gt;   &lt;span class="m"&gt;27447.83&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The graph shows the marginal returns clearly. And since &lt;span class="math"&gt;\((43854.32 - 33887.51) \gt (33887.51 - 27447.83)\)&lt;/span&gt;, our assumption is confirmed definitively. We want to choose &lt;span class="math"&gt;\(k = 3\)&lt;/span&gt; clusters: there are three levels of size/volume/connectivity for airports in South America. In the next post, we'll delve deeper into what this analysis can help us learn about each country and the continent as a whole.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine-learning"></category></entry></feed>